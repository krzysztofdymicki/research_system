{
  "totalHits": 535056,
  "limit": 5,
  "offset": 0,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Chambers, L."
        },
        {
          "name": "Gaber, M."
        },
        {
          "name": "Pechenizkiy, M."
        },
        {
          "name": "Tromp, E."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/29582286",
        "https://api.core.ac.uk/v3/outputs/52396296"
      ],
      "createdDate": "2015-09-29T10:12:02",
      "dataProviders": [
        {
          "id": 695,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/695",
          "logo": "https://api.core.ac.uk/data-providers/695/logo"
        },
        {
          "id": 1899,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1899",
          "logo": "https://api.core.ac.uk/data-providers/1899/logo"
        }
      ],
      "depositedDate": "",
      "abstract": null,
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/29582286.pdf",
      "fieldOfStudy": null,
      "fullText": "Mobile Sentiment Analysis \nLorraine Chambers1, Erik Tromp2, Mykola Pechenizkiy2, Mohamed Medhat Gaber1 \n1School of Computing  \nUniversity of Portsmouth \nPortsmouth, Hampshire, PO1 3HE, England, UK \n{lorraine.chambers@myport.ac.uk, mohamed.gaber@port.ac.uk} \n2Department of Computer Science  \nEindhoven University of Technology \nP.O. Box 513, 5600 MB Eindhoven, the Netherlands \n{e.t.tromp@gmail.com,m.pechenizkiy@tue.nl} \nAbstract. Mobile devices play a significant part in a user’s communication me-\nthods and much data that they read and write is received and sent via mobile \nphones, for instance SMS messages, e-mails, Twitter tweets and social media \nnetworking feeds.  One of the main goals is to make people aware of how much \nnegative and positive content they read and write via their mobile phones.  Ex-\nisting sentiment analysis applications perform sentiment analysis on down-\nloaded data from mobile phones or use an application installed on another com-\nputer to perform the analysis.  The sentiment analysis described in this paper is \nto be performed locally on the mobile phone enabling immediate and private \nanalysis of personal messages and social media contents, allowing the users to \nbe able to reason about their mood and stress level that may be affected by what \nthey had been receiving. Experimental results showed the effectiveness of the \nproposed system on Android smartphones with varying computational capabili-\nties. \nKeywords. Sentiment analysis, data mining, and mobile computing \n1 Introduction \nSentiment analysis is a branch of natural language processing and one stage in the \nprocess of opinion mining.  To achieve opinion mining there are five tasks that need \nto be carried out, these are: Entity extraction and grouping, aspect orientation and \ngrouping, opinion holder and time extraction, aspect sentiment classification and opi-\nnion quintuple generation for summarization of the opinions.  The stage of aspect \nsentiment classification attempts to classify the sentiment of a particular aspect of a \nsentence; in the context of this paper, the general sentiment of the text will be consi-\ndered and not the sentiment of a particular aspect of the sentence, as the aim is to \nprovide the user with a general view of the sentiment of the texts within the mobile \ndevice and not the sentiments of particular products or services. \nSMS messages are a means of sending short text messages not longer than 160 \ncharacters for the Latin alphabet.  Although nowadays most mobile phones have the \ncapability to split longer texts into multiple messages and the recipient’s phone to \nreceive this as one message, an SMS message is typically one or two sentences long.  \nThis is similar to tweets on Twitter which allows text-based posts of up to 140 charac-\nters and has been described as “the SMS of the internet” [14]; because of these simi-\nlarities, SMS messages will be considered to be the same as tweets.  \nNowadays most personal communication is recorded digitally in mediums such as \nSMS text messages, e-mails, tweets, Facebook updates and other social media net-\nworking sites.  All of these are accessible from mobile phones and as these become \nfaster and more powerful, the possibility of performing sentiment analysis on mobile \ndevices has become more attractive, enabling the user to have sentiment analysis per-\nformed locally, rather than having personal information sent to a server or down-\nloaded to another application on a computer to be analyzed.  The overall goal is to \nperform sentiment analysis locally on the user’s mobile phones to enable the user to \nreflect upon how much general positive or negative content they are reading or writ-\ning. \nWe have developed a mobile application for the Android operating system that per-\nforms sentiment analysis locally on the mobile phone for SMS messages.  Due to the \nsimilarity of Twitter messages to SMS messages, the work reported in this paper gen-\nerally target both Twitter tweets and SMS messages. It is based on the SentiCorr [2] \nsystem which performs multi-lingual sentiment analysis of personal correspondence \non e-mails, Twitter tweets, Facebook and other social networking media.  The multi-\nlingual aspect is beyond the scope of this paper, and the only language that shall be \nconsidered for SMS messages in this paper is English. It is worth noting that the sys-\ntem can be easily extended to handle all text received on the mobile phone. Our sys-\ntem reported in this paper has extended the SentiCorr system in the following aspects: \n(1) applying a number of POS taggers and experimentally assessing their computa-\ntional performance and accuracy; (2) experimental study of the performance with \nrespect to the system configuration of the mobile device; and (3) a temporal aspect of \nthe sentiment has also been developed allowing the users to reason about the effect on \nthe sentiment on their level of stress. \nIn the following section work related to the different methods of sentiment analysis \nand the applications of sentiment analysis to mobile applications are considered.  \nSection 3 describes our approach to mobile sentiment analysis, while section 4 sum-\nmarizes our experimental study. Finally, section 5 concludes the findings and suggests \nfurther work. \n2 Related Work \nThere has been much work done on sentiment analysis, especially in the last five \nyears due to the abundance of data being available due to the explosion of social me-\ndia sites and blogging sites like Twitter.  This is reflected in the differences in Liu’s \nopinion mining and sentiment analysis chapters in subsequent versions [6] published \nin 2006 and [7] published in 2011. There has also been a comprehensive survey of \nsentiment analysis techniques as summarized by Pang [1].  In this paper sentiment \nanalysis shall be considered at the sentence level where there are two tasks to be per-\nformed; subjectivity classification to determine whether it is a subjective sentence or \nan objective sentence and sentiment classification – if the sentence is subjective, de-\ntermine if it contains a positive or negative opinion.  The assumption at this level is \nthat the sentence expresses a single opinion from a single opinion holder.  Both of \nthese tasks are classification problems, so supervised learning methods are applicable.  \nEarly methods used the Naïve Bayes classifier [9], subsequent methods have applied \nregression models [11] and support vector regression and boosting [12].  Semi-\nsupervised [10] and unsupervised techniques have also been applied [13]. \nFor mobile devices, sentiment analysis has been utilized for reflection of personal \ninformatics; analyzing SMS messages and emails to provide a general overview of a \nperson’s life at a particular point, but this is not performed on the mobile device but \nby another application to which the data is downloaded [3], and sentiment analysis \nhas been employed with SMS messages to gather feedback about teaching, where \nSMS messages are sent by students at the end of a lecture where they are automatical-\nly analyzed at the server-end to provide information about the teaching [8].  There are \nmobile phone applications available that involve sentiment analysis; for instance, for \nthe iPhone there is an application available that tells you what people think about your \narea by analyzing the sentiment of tweets around your location by sending them to an \nanalytics engine for sentiment analysis [4]. There is also an Android mobile applica-\ntion that analyses tweets and tries to determine if the attitude of the writer is positive \nor negative; it uses the internet and returns the results that the twitter sentiment analy-\nsis engine has generated [5].  To our knowledge there are no mobile applications that \nlocally execute sentiment analysis on the mobile phone. \n3 Mobile Sentiment Analysis System Description \nThe target platform selected for the mobile sentiment analysis application is An-\ndroid due to its “open” nature, availability of development resources and its wide \nusage. The sentiment analysis is to be performed using the principles of the SentiCorr \nsentiment analysis engine; firstly it will be described how SentiCorr achieves senti-\nment analysis and then the different aspects of enabling it to work on the Android \nplatform will be further discussed. \nSentiCorr achieves sentiment classification at the sentence level by using POS (Po-\nsition Of Speech) tagging to identify the types of the words in the sentence; the sub-\njectivity detection stage then uses the POS tags to identify opinion lexicon and hence \nif the sentence is subjective or objective; the polarity detection stage also utilizes the \nPOS tags to search for patterns in the sentence that indicate positive or negative ex-\npressions.   \nFor the POS tagging stage, Stuttgart University’s Tree Tagger was employed; \nAdaBoost for subjectivity detection and an in-house method called Rule-Based Emis-\nsion Model (RBEM) was developed for the polarity detection stage.  It is not the aim \nof this paper to propose new solutions for the sentiment analysis as extensive experi-\nments have been conducted in comparing these classification techniques with others \nsuch as Majority class, Prior Polarity, Naïve Bayes and Support Vector Machines, in \nwhich AdaBoost and RBEM outperform these other methods [15].  The following \nparagraphs summarize the algorithms used for subjectivity detection and polarity \ndetection, a comprehensive description can be found in [15]. \nThe principle employed for subjectivity detection is boosting, by use of the Ada-\nBoost (adaptive boosting algorithm) [19] and is a general method for generating a \nstrong classifier from a set of weak classifiers.  Each time an instance is incorrectly \nclassified, it is given a greater weight for use in the next round of classification.  This \nprocess continues until the maximum number of rounds is reached or the weighted \nerror is more than 50%.  The weak learners used are decision stumps of the form if f \npresent then label = a else label = b where f is a feature and a and b are labels.  Fea-\ntures utilized are POS tags, pre-defined lexicons that contain positive, negative and \nnegation words, the presence of exactly one positive word, the presence of multiple \npositive words, the presence of exactly one negative word and the presence of mul-\ntiple negative words, and whenever a positive or negative word is directly preceded \nby a word from the negation list, its polarity is flipped. \nThe principle employed for polarity detection is RBEM which uses rules to define \nan emissive model.  The rules emerge from eight different pattern groups which are \npositive patterns, negative patterns, amplifier patterns, attenuator patterns, right flip \npatterns, left flip patterns, continuator patterns and stop patterns.  These patterns are \ncombined with rules to define an emissive model.  A model is constructed by \nrepresenting patterns as lists of words and corresponding POS tags. In the patterns, \nword wildcards are allowed which means that wildcards for a word can appear at any \nposition of a pattern. Single-position wildcards are allowed so that a single entity in \nthe pattern can be any word and any POS tag.  Multi-position wildcards are also al-\nlowed so that any number of word tag pairs can occur in-between two elements that \nare not multi-position or single position wildcards.  The model consists of a set of \npatterns per pattern group, each pattern except for the positive and negative patterns \nadhere to an action radius, which is set to 4 in this case. \nWhen classifying previously unseen data, all of the patterns that match the sen-\ntence are collected from the model, and a rule associated with each pattern group is \napplied to each pattern in the message.  All patterns of all groups are evaluated for a \nmatch within the sentence; if there is a match, the start position and the end position \nof the pattern in the sentence is recorded.  Some patterns may occur within other pat-\nterns in the sentence, if so these subsumed patterns are removed from the final pattern \ncollection.  Once the patterns that occur in the sentence have been collected, the rules \nfor each pattern group are applied.  The rules must be applied in the correct order as \noutlined in the following paragraph. \nThe first rule to be applied is Setting Stops; this sets a stop at the starting position \nof all the left flip and stop patterns.  The second rule to be applied is Removing Stops; \nif there is a stop to the left of a continuator pattern within the pre-set action radius it is \nremoved.  The third rule to be applied is Positive Sentiment Emission; for each posi-\ntive pattern an emission value is calculated based on the distance of the elements in \nthe sentence from the centre of the positive pattern, which decays the further the ele-\nment is from the centre of the pattern, e-x is used as the decaying function, this is cal-\nculated for each element until stops are reached.   \nThe fourth rule to be applied is Negative Sentiment Emission; this is handled the \nsame way as Positive Sentiment Emission except that the decay function is –e-x.  The \nfifth rule to be applied is Amplifying Sentiment; amplifier patterns amplify sentiment \nemitted by positive or negative patterns and similarly to the positive and negative \npatterns, amplification reduces over distance.  The function used is 1+ e-x where x is \nthe distance within the action radius.  The sixth rule to be applied is Attenuating Sen-\ntiment; this performs the reverse of Amplifying Sentiment and the decay function \napplied is 1 – e-x.   \nThe seventh rule to be applied is Right Flipping Sentiment; if there is a right flip \npattern the emission of sentiment is flipped to the right and if there is a stop at the \nexact centre of the right flip pattern, it is ignored. The eighth rule to be applied is Left \nFlipping Sentiment; this mirrors the effect of the right flip pattern.  \nOnce the rules have been applied, every element of the sentence has an emission \nvalue and the final polarity of the message is calculated by summing the emission \nvalues for each element. If the final polarity of the sentence is greater than zero, the \nsentence is positive; if it is less than zero the sentence is negative, if it is zero the po-\nlarity of the sentence is unknown due to insufficient patterns in the sentence model. \nThe main aim of mobile sentiment analysis is to perform sentiment analysis on \nSMS messages which were earlier likened to tweets and that language identification \nwas beyond the scope of this paper, this reduces the original SentiCorr framework to \nthat of the POS tagger, Subjectivity Detection and Polarity Detection. Although we \nhave focused in this paper on short text sentiment analysis as applied to SMS messag-\nes and tweets received onboard the mobile phone, the work could be easily genera-\nlized to other social media items like Facebook and LinkedIn. \nAndroid mobile phones use ARM processors (Advanced RISC Machine) and the \noriginal POS tagger (TreeTagger) could not be used on the Android operating system \nas the source code was not available for re-compilation suitable for an ARM proces-\nsor.  This reduced the POS taggers available as they are constrained to POS taggers \nwritten in Java for which either a library where all the components and dependencies \nare capable of executing on an ARM compiler or the source code is available so it can \nbe compiled to execute on an ARM processor. The shortlist of POS taggers tested \nwere Stanford POS tagger and OpenNLP POS tagger. \nThe subjectivity detection stage and the polarity detection stages required that the \nPOS tags were in the format of the Penn Tree Bank set 1 for the software to operate \ncorrectly with the pre-trained models used within the subjectivity and polarity detec-\ntion stages.  \nPOS taggers are usually supervised and as such require a model.  Loading these \nmodels contributed to how the software was architected.  On a PC, the time to load \nthese models is small compared to the time to load them on a mobile device.  In Sen-\ntiCorr, the language of the text is assessed at the sentence level and if the sentence is \nin a different language to the previous one, a different model needs to be loaded for \nthe POS tagging, subjectivity and polarity classification stages.  As we are not yet \nconsidering the multi-lingual aspect, the model is loaded once at startup of the appli-\ncation and the same language is assumed throughout each usage of the application and \nthe model is loaded only once per application usage. These constraints shaped the \nworkflow of the mobile sentiment analysis system as shown in Figure 1. \n \n \nFig. 1. Mobile Sentiment Analysis Workflow \nIn the mobile sentiment analysis solution the POS taggers are dynamically inter-\nchangeable for analysis purposes and can be loaded via settings menus and the output \nform each POS tagger are transformed into a standard output so that the interface to \nthe rest of the application remains constant regardless of the POS tagger.  Tagging of \nthe sentence splits the text into sentences and tags them using the selected POS tag-\nger.  The tagged sentences are then passed to the subjectivity detection stage where \nthe algorithm operates in the same way as within SentiCorr as described in the pre-\nvious paragraphs.   \nOnce the subjectivity has been determined, if it is subjective, the sentence is then \npassed to the polarity detection stage where the algorithm operates in the same way as \nthe SentiCorr algorithm also as summarized in the previous paragraphs; if the sen-\ntence is objective, no further processing is applied to it. \nThe sentiment analysis code is implemented as a standard Java library that the An-\ndroid application uses, ensuring its use is not limited to an Android mobile applica-\ntion. The models for the subjectivity and polarity detection stages are stored in XML \nformat and serializable directly into Java objects using Simple XML [20] so that it has \nthe possibility to be extended to allow creation of new models that can be stored in \nthe correct format for later use. \nHaving discussed the technical and implementation details of our mobile sentiment \nanalysis system, the following section provides an experimental study of the system \nproving empirically its feasibility and efficiency. \n4 Experimental Results \nThe mobile sentiment analysis application was installed on three mobile phones, \nthe specifications of which are shown in Table 1. The aim of varying the mobile \nphones is to conduct stress testing, so that to reveal the minimum configuration of \ncomputational power that is able to run our system.  \nThe load time of the POS tagger models was the major factor in the duration of the \nexecution time of the application.  The POS taggers evaluated were OpenNLP and \nStanford POS Tagger. Each of these taggers has a number of models which are sum-\nmarized in the following paragraphs; these models are also listed in Table 2 along \nwith the time taken to load these models for each phone. Each POS tagger model was \nloaded ten times and an average taken to give the model load times in Table 2. \nThe Open NLP POS tagger comes with two different types of models: maximum \nentropy model and a perceptron model, each with the option of using a dictionary.  A \ntoken can have many tag possibilities depending on the token and the context. Open \nNLP uses a probability model to guess the correct POS tag out of the tag set, to limit \nthe possible tags, a dictionary can be used.  The maximum entropy model with a dic-\ntionary produced the best results at 87%.  The perceptron model is a linear classifier \nand relies on Viterbi decoding of training examples [18]. \nThe Stanford POS tagger comes with trained English tagger models and taggers \ntrained on the Wall Street Journal corpus from the Penn Treebank Project., as de-\npicted by ‘wsj’ in the model name; the ‘left3’ in the model name means that the mod-\nel uses the left3 words architecture and includes word shape features; the ‘distsim’ \npart of the tagger name means that the model includes distributional similarity fea-\ntures.  All of the Stanford POS tagger models utilize a maximum entropy method of \ntagging where a probability is assigned for every tag in a set of possible tags for a \nword where the possible tags are determined from the sequence of words preceding \nthe word that is to be tagged [16].  The Stanford POS tagger model that performed the \nbest on the tested data was the english-left3words-distsim model and the wsj-0-18-\nleft3words model, showing that in general, the left3 words architecture was a success-\nful model for the data.  The bidirectional models use a cyclic dependency networks to \nachieve bi-directional traversing of the words in a given sentence [17]. \nAs the sentiment analysis code is implemented as a standard java library, we were \nable to calculate the accuracy of the sentiment analysis for each model on a PC where \nthe accuracy is taken as the number of correctly tagged tokens divided by the total \nnumber of tokens and represented as a percentage.  The sentiment was analyzed on \npart of the original data used in the evaluation of SentiCorr and is based on 60 texts \nand utilizes the POS tags from the original data and uses this as the gauge for accura-\ncy. These 60 texts were obtained from Twitter by scraping all public data and manual-\nly labeling 20 negative, 20 objective and 20 positive tweets, according to the tweet's \ntext. The results are shown in table 2.  The peak memory usage of the application \nduring the loading of the POS tagger models was also recorded and is shown in Table \n2. \nTable 1. POS tagger model load times on mobile phone \nMobile Phone \nModel A\nn\ndr\no\nid\n \n \nV\ner\nsio\nn\n \nSD\n \nC\na\nrd\n \nA\nv\na\nila\nbl\ne \nSp\na\nce\n \n(G\nB)\n \nIn\nte\nrn\na\nl \nPh\no\nn\ne \nSt\no\nra\nge\n \n(G\nB)\n \nTo\nta\nl A\nv\na\nila\n-\nbl\ne \nM\nem\no\nry\n \n(G\nB)\n \nGT540 2.1 1.77 0.099 1.869 \nHTC Desire HD 2.3.5 3.0 0.815 3.815 \nGalaxy Nexus 4.0.2 N/A N/A 13.33 \nTable 2. POS tagger model size and accuracy  \nPOS Tagger Model M\no\nde\nl S\niz\ne \n(M\nB)\n \nA\ncc\nu\nra\ncy\n \n(%\n) \nPe\na\nk \nM\nem\no\nry\n \n \nU\nsa\nge\n \n(M\nB)\n \nModel Load Times (s) MinSpec \nG\nT5\n40\n \nH\nTC\n \nD\nes\nir\ne \nH\nD\n \nG\na\nla\nx\ny \nN\nex\nu\ns \nM\nin\n \n \nA\nn\ndr\no\nid\n \nV\ner\n \nSt\no\nra\nge\n \nSp\na\nce\n \n(M\nB)\n \nH\nea\np \nSp\na\nce\n \n(M\nB)\n \nOpen \nNLP \nMaxent no dictio-\nnary \n5.46 85 105 OSM 17.78 23.03 2.1 180 61 \nMaxent with \ndictionary \n5.56 87 - Invalid format for dictionary \nfile \n- - - \nPerceptron no \ndictionary \n3.78 83 105 OSM 17.78 22.21 2.1 178 62 \nPerceptron with \ndictionary \n3.88 85 - Invalid format for dictionary \nfile \n- - - \nStan-\nford \nenglish-\nleft3words-distsim \n20.15 90 230 OSM OHM 250.79 3.0 182 233 \nenglish-caseless-\nleft3words-distsim \n19.77 88 225 OSM OHM 180.26 3.0 182 229 \nwsj-0-18-\nleft3words-distsim \n17.38 87 207 OSM OHM 205.44 3.0 181 207 \nwsj-0-18-\nleft3words \n7.98 90 110 OSM OHM 50.71 3.0 177 110 \nwsj-0-18-caseless-\nleft3words-distsim \n17.12 88 201 OSM OHM 179.71 3.0 180 201 \nwsj-0-18-\nbidirectional-\ndistsim \n31.65 87 285 OSM OHM OHM 3.0 - - \nOSM = Out of Storage Memory, OHM = Out of Heap Memory \n \nThe accuracy of the models ranges from 83% to 90% across the Stanford and the \nOpen NLP POS tagger models, with the most accurate being the Stanford English left \n3 words and the Stanford wsj-0-18left3words which also had a low load time but only \nran on the Galaxy Nexus phone.  In fact, no Stanford model could be loaded on a \nphone that had an Android version of lower than 3.0 because the Android application \nsetting largeHeap was required to be set to true in the Android manifest file to \nallow more memory to be dynamically allocated to the application, hence no results \nfor the Stanford POS tagger for the LG GT540 or the HTC Desire HD mobile phones \ncould be recorded for the Stanford POS taggers.  No results could be recorded for the  \nGT540 phone as there was not enough storage memory.  From this information mini-\nmum phone specifications for each tagger have been calculated and included in Table \n2.  Overall, the Stanford POS tagger model was more accurate but requires more mo-\nbile phone resources such as memory and load time, which limits the targets it can be \ninstalled on. In most cases, it took nearly ten times as long to load the Stanford mod-\nels as it did the Open NLP model, for a 2% increase in accuracy. \nThe duration of the loading of the POS models has been focused on but the Ada-\nBoost and Emission Miner models also have an overhead when they are loaded.  The \nAdaBoost and Emission Miner models use the POS tags that are produced by the POS \ntaggers.  The original POS tagger – TreeTagger output Penn tree bank I tag set whe-\nreas the Stanford POS tagger and Open NLP POS tagger both output Penn tree bank \nII tag set.  The difference in these tag sets are that in tree bank II the individual tags \nfor “VH” tags are now included in “VB” tags as re “VV” tags, meaning that the verbs \n“to have” and “to take” are now included under the general verb tags.  This meant that \nthe existing AdaBoost and Emission Miner models had to be adjusted to take this into \naccount.  The result is that if the POS tagger model is changed to one which uses a \ndifferent tag set, then different AdaBoost and Emission Miner models are also re-\nquired to be loaded.  The time it takes to load these models will be critical if a dynam-\nic multi-language approach is required as in the original SentiCorr system. \n5 Conclusion and Future Work \nSentiment analysis can be performed locally on a mobile phone, as we have proved \nempirically in this paper.  To the best of our knowledge, the proposed and imple-\nmented system reported in this paper is the first mobile sentiment analyzer. The \namount of time and resources this takes largely depends upon the POS tagger model \nthat is utilized.  The Stanford POS tagger is more accurate, but takes longer to load \nand requires more memory. In a non-language dynamic environment, the model is \nonly loaded once per start-up of the application but limits the user to the same lan-\nguage unless this is specifically changed by the user.  The change of a POS tagger \ncould affect the subsequent subjectivity and polarity detection stages, meaning that \nnew models for these stages may also need to be loaded.  If the application was to \nincorporate dynamic language selection then the load time of the models would be-\ncome critical.   \nFuture work is to include experimentation on a wider set of Android mobile phones \nwith differing memory and Android operating systems, and inclusion of analysis on \nother POS taggers implemented in Java and those specifically aimed at Twitter data \nsuch as Tweet NLP.  Experimentation could also be extended to the training of small-\ner models to widen the target mobile phones of the application such that the applica-\ntion could be extended to dynamically interrogate and change the language during \nsentiment classification. \nReferences \n1. Pang, B., Lillian, L.: Opinion Mining and Sentiment Analysis. In: Foundations and Trends \nin Information Retrieval 2, pp. 1-135 (2008) \n2. Tromp, E., Pechenizkiy, M.: SentiCorr: Multilingual Sentiment Analysis of Personal Cor-\nrespondence. In: IEEE 11th International Conference on Data Mining Workshops, pp. \n1247-1250 (2011) \n3. Hangal, S., Lam, M.: Sentiment Analysis on Personal Email Archives. Proceedings of the \n24th annual ACM symposium on User interface software and technology (2011) \n4. Chatterbox analytics, http://cbanalytics.co.uk/apps/sentimental \n5. SmmMobile – Sentiment Analyser \nhttps://market.android.com/details?id=com.cyhex.smmMobile \n6. Liu, B.: Chapter 11 Opinion Mining. In: Web Data Mining. pp. 411-448. Springer , Hei-\ndelberg (2006) \n7. Liu, B.: Chapter 11 Opinion Mining. In: Web Data Mining. pp. 459-526. Springer , Hei-\ndelberg (2011) \n8. Leong, C.K., Lee, Y.H.: Mining sentiments in SMS texts for teaching evaluation. In Expert \nSystems with Applications 39, pp. 2584-2589 (2012) \n9. Yu, H.: Towards Answering Opinion Questions: Separating facts from Opinions and iden-\ntifying the Polarity of Opinion Sentences.  In: EMNLP ’03 Proceedings of the 2003 confe-\nrence on Empirical methods in natural language processing pp. 129-136 (2003) \n10. Dasgupta, S.: Mine the easy, classify the hard: A semi-supervised approach to Automatic \nSentiment Classification. In: ACL ’09 Proceedings of the joint Conference of the 47th An-\nnual Meeting of the ACL and the 4th International Joint Conference on Natural Language \nProcessing of the AFNLP 2 pp. 701-709 (2009) \n11. Hatzivassiloglou, V.: Predicting the semantic orientation of Adjectives. In: ACL ’98 Pro-\nceedings of the 35th Annual Meeting of the Association for Computational Linguistics and \nEighth Conference of the European Chapter of the Association for Computational Linguis-\ntics. Pp. 174-181 (1997) \n12. Wilson, T., Wiebe, J., Hwa, R.: Just How Mad Are you? Finding Strong and Weak Opi-\nnion Clauses. In: Proceedings of the National Conference on Artificial Intelligence. 10 \npp.761-769 (2004) \n13. Turner, D.: Thunbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised \nClassification of Reviews. In: Proceedings of the 40th Annual Meeting of the Association \nfor Computational Linguistics (ACL). pp. 417-424 (2002) \n14. Wikipedia – Twitter. http://en.wikipedia.org/wiki/Twitter \n15. Tromp, E.: Multilingual Sentiment Analysis on Social Media. \nhttp://alexandria.tue.nl/extra1/afstversl/wsk-i/tromp2011.pdf \n16. Toutanova, K., Manning, C.: Enriching the Knowledge Sources Used in a Maximum En-\ntropy Part-of-Speech Tagger. In: Proceedings of the Joint SIGDAT Conference on Empiri-\ncal Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-\n2000), pp. 63-70 (2000) \n17. Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. In: Feature-Rich \nPart-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-\nNAACL 2003, pp. 252-259 (2003) \n18. Collins, M.: Discriminative Training Methods for Hidden Markov Models: Theory and \nExperiments with Perceptron Algorithms. In: Proceedings of the Conference on Empirical \nMethods in Natural Language Processing (EMNLP). pp. 1-8 (2002) \n19. Freund, Y., Shapire, R.: A decision-theoretic generalization of on-line learning and an ap-\nplication to boosting. In: Proceedings of the Second European Conference on Computa-\ntional Learning Theory, pp. 23-37 (1995) \n20. Gallagher, N.: Simple Framework for XML. \nhttp://simple.sourceforge.net/home.php \n",
      "id": 18098239,
      "identifiers": [
        {
          "identifier": "oai:researchportal.port.ac.uk:publications/782ac36b-80c7-4842-aebe-69c9406ba3b4",
          "type": "OAI_ID"
        },
        {
          "identifier": "29582286",
          "type": "CORE_ID"
        },
        {
          "identifier": "52396296",
          "type": "CORE_ID"
        }
      ],
      "title": "Mobile sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:researchportal.port.ac.uk:publications/782ac36b-80c7-4842-aebe-69c9406ba3b4"
      ],
      "publishedDate": "2012-09-10T01:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 38740203,
          "title": "A decision-theoretic generalization of on-line learning and an application to boosting. In:",
          "authors": [],
          "date": "1995",
          "doi": "10.1006/jcss.1997.1504",
          "raw": "Freund, Y., Shapire, R.: A decision-theoretic generalization of on-line learning and an application to boosting. In: Proceedings of the Second European Conference on Computational Learning Theory, pp. 23-37 (1995)",
          "cites": null
        },
        {
          "id": 38740188,
          "title": "Chapter 11 Opinion Mining. In: Web Data Mining.",
          "authors": [],
          "date": "2011",
          "doi": "10.1007/978-3-642-19460-3_11",
          "raw": "Liu, B.: Chapter 11 Opinion Mining. In: Web Data Mining. pp. 459-526. Springer , Heidelberg (2011)",
          "cites": null
        },
        {
          "id": 38740202,
          "title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In:",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1118693.1118694",
          "raw": "Collins, M.: Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1-8 (2002)",
          "cites": null
        },
        {
          "id": 38740198,
          "title": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In:",
          "authors": [],
          "date": "2000",
          "doi": "10.3115/1117794.1117802",
          "raw": "Toutanova, K., Manning, C.: Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In: Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC2000), pp. 63-70 (2000)",
          "cites": null
        },
        {
          "id": 38740195,
          "title": "How Mad Are you? Finding Strong and Weak Opinion Clauses. In:",
          "authors": [],
          "date": "2004",
          "doi": null,
          "raw": "Wilson, T., Wiebe, J., Hwa, R.: Just How Mad Are you? Finding Strong and Weak Opinion Clauses. In: Proceedings of the National Conference on Artificial Intelligence. 10 pp.761-769 (2004)",
          "cites": null
        },
        {
          "id": 38740199,
          "title": "In: Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.",
          "authors": [],
          "date": "2003",
          "doi": "10.3115/1073445.1073478",
          "raw": "Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. In: Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLTNAACL 2003, pp. 252-259 (2003)",
          "cites": null
        },
        {
          "id": 38740193,
          "title": "Mine the easy, classify the hard: A semi-supervised approach to Automatic Sentiment Classification. In:",
          "authors": [],
          "date": "2009",
          "doi": "10.3115/1690219.1690244",
          "raw": "Dasgupta, S.: Mine the easy, classify the hard: A semi-supervised approach to Automatic Sentiment Classification. In: ACL ’09 Proceedings of the joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP 2 pp. 701-709 (2009)",
          "cites": null
        },
        {
          "id": 38740197,
          "title": "Multilingual Sentiment Analysis on Social Media.",
          "authors": [],
          "date": null,
          "doi": "10.3115/v1/w14-2611",
          "raw": "Tromp, E.: Multilingual Sentiment Analysis on Social Media. http://alexandria.tue.nl/extra1/afstversl/wsk-i/tromp2011.pdf",
          "cites": null
        },
        {
          "id": 38740184,
          "title": "Opinion Mining and Sentiment Analysis. In:",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "Pang, B., Lillian, L.: Opinion Mining and Sentiment Analysis. In: Foundations and Trends in Information Retrieval 2, pp. 1-135 (2008)",
          "cites": null
        },
        {
          "id": 38740194,
          "title": "Predicting the semantic orientation of Adjectives. In:",
          "authors": [],
          "date": "1997",
          "doi": "10.3115/976909.979640",
          "raw": "Hatzivassiloglou, V.: Predicting the semantic orientation of Adjectives. In: ACL ’98 Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics. Pp. 174-181 (1997)",
          "cites": null
        },
        {
          "id": 38740186,
          "title": "SentiCorr: Multilingual Sentiment Analysis of Personal Correspondence. In:",
          "authors": [],
          "date": "2011",
          "doi": "10.1109/icdmw.2011.152",
          "raw": "Tromp, E., Pechenizkiy, M.: SentiCorr: Multilingual Sentiment Analysis of Personal Correspondence. In: IEEE 11th International Conference on Data Mining Workshops, pp. 1247-1250 (2011)",
          "cites": null
        },
        {
          "id": 38740187,
          "title": "Sentiment Analysis on Personal Email Archives.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Hangal, S., Lam, M.: Sentiment Analysis on Personal Email Archives. Proceedings of the 24th annual ACM symposium on User interface software and technology (2011)",
          "cites": null
        },
        {
          "id": 38740204,
          "title": "Simple Framework for XML.",
          "authors": [],
          "date": null,
          "doi": "10.4324/9780080522210",
          "raw": "Gallagher, N.: Simple Framework for XML. http://simple.sourceforge.net/home.php",
          "cites": null
        },
        {
          "id": 38740196,
          "title": "Thunbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In:",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1073083.1073153",
          "raw": "Turner, D.: Thunbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). pp. 417-424 (2002)",
          "cites": null
        },
        {
          "id": 38740192,
          "title": "Towards Answering Opinion Questions: Separating facts from Opinions and identifying the Polarity of Opinion Sentences. In:",
          "authors": [],
          "date": "2003",
          "doi": "10.3115/1119355.1119372",
          "raw": "Yu, H.: Towards Answering Opinion Questions: Separating facts from Opinions and identifying the Polarity of Opinion Sentences.  In: EMNLP ’03 Proceedings of the 2003 conference on Empirical methods in natural language processing pp. 129-136 (2003)",
          "cites": null
        },
        {
          "id": 38740191,
          "title": "Y.H.: Mining sentiments in SMS texts for teaching evaluation.",
          "authors": [],
          "date": "2012",
          "doi": "10.1016/j.eswa.2011.08.113",
          "raw": "Leong, C.K., Lee, Y.H.: Mining sentiments in SMS texts for teaching evaluation. In Expert Systems with Applications 39, pp. 2584-2589 (2012)",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "https://researchportal.port.ac.uk/portal/services/downloadRegister/152710/k12gen-092.pdf"
      ],
      "updatedDate": "2021-07-22T16:23:48",
      "yearPublished": 2012,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/29582286.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/29582286"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/29582286/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/29582286/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/18098239"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1509.06041",
      "authors": [
        {
          "name": "Jin, Hailin"
        },
        {
          "name": "Luo, Jiebo"
        },
        {
          "name": "Yang, Jianchao"
        },
        {
          "name": "You, Quanzeng"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/386117133",
        "https://api.core.ac.uk/v3/outputs/103268817"
      ],
      "createdDate": "2016-08-03T02:20:56",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms.Comment: 9 pages, 5 figures, AAAI 201",
      "documentType": "research",
      "doi": "10.1609/aaai.v29i1.9179",
      "downloadUrl": "http://arxiv.org/abs/1509.06041",
      "fieldOfStudy": null,
      "fullText": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\nTransferred Deep Networks\nQuanzeng You and Jiebo Luo\nDepartment of Computer Science\nUniversity of Rochester\nRochester, NY 14623\n{qyou, jluo}@cs.rochester.edu\nHailin Jin and Jianchao Yang\nAdobe Research\n345 Park Avenue\nSan Jose, CA 95110\n{hljin, jiayang}@adobe.com\nAbstract\nSentiment analysis of online user generated content is\nimportant for many social media analytics tasks. Re-\nsearchers have largely relied on textual sentiment anal-\nysis to develop systems to predict political elections,\nmeasure economic indicators, and so on. Recently, so-\ncial media users are increasingly using images and\nvideos to express their opinions and share their expe-\nriences. Sentiment analysis of such large scale visual\ncontent can help better extract user sentiments toward\nevents or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is comple-\nmentary to textual sentiment analysis. Motivated by the\nneeds in leveraging large scale yet noisy training data to\nsolve the extremely challenging problem of image sen-\ntiment analysis, we employ Convolutional Neural Net-\nworks (CNN). We first design a suitable CNN archi-\ntecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment\nalgorithm to label Flickr images. To make use of such\nnoisy machine labeled data, we employ a progressive\nstrategy to fine-tune the deep network. Furthermore, we\nimprove the performance on Twitter images by induc-\ning domain transfer with a small number of manually\nlabeled Twitter images. We have conducted extensive\nexperiments on manually labeled Twitter images. The\nresults show that the proposed CNN can achieve better\nperformance in image sentiment analysis than compet-\ning algorithms.\nIntroduction\nOnline social networks are providing more and more con-\nvenient services to their users. Today, social networks have\ngrown to be one of the most important sources for people to\nacquire information on all aspects of their lives. Meanwhile,\nevery online social network user is a contributor to such\nlarge amounts of information. Online users love to share\ntheir experiences and to express their opinions on virtually\nall events and subjects.\nAmong the large amount of online user generated data, we\nare particularly interested in people’s opinions or sentiments\ntowards specific topics and events. There have been many\nCopyright c© 2015, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Examples of Flickr images related to the 2012\nUnited States presidential election.\nworks on using online users’ sentiments to predict box-\noffice revenues for movies (Asur and Huberman 2010), po-\nlitical elections (O’Connor et al. 2010; Tumasjan et al. 2010)\nand economic indicators (Bollen, Mao, and Zeng 2011;\nZhang, Fuehres, and Gloor 2011). These works have sug-\ngested that online users’ opinions or sentiments are closely\ncorrelated with our real-world activities. All of these results\nhinge on accurate estimation of people’s sentiments accord-\ning to their online generated content. Currently all of these\nworks only rely on sentiment analysis from textual content.\nHowever, multimedia content, including images and videos,\nhas become prevalent over all online social networks. In-\ndeed, online social network providers are competing with\neach other by providing easier access to their increasingly\npowerful and diverse services. Figure 1 shows example im-\nages related to the 2012 United States presidential election.\nClearly, images in the top and bottom rows convey opposite\nsentiments towards the two candidates.\nA picture is worth a thousand words. People with differ-\nent backgrounds can easily understand the main content of\nan image or video. Apart from the large amount of easily\navailable visual content, today’s computational infrastruc-\nture is also much cheaper and more powerful to make the\nanalysis of computationally intensive visual content analy-\nsis feasible. In this era of big data, it has been shown that\nthe integration of visual content can provide us more reli-\nable or complementary online social signals (Jin et al. 2010;\nYuan et al. 2013).\nTo the best of our knowledge, little attention has been paid\nto the sentiment analysis of visual content. Only a few recent\nworks attempted to predict visual sentiment using features\nar\nX\niv\n:1\n50\n9.\n06\n04\n1v\n1 \n [c\ns.C\nV]\n  2\n0 S\nep\n 20\n15\nfrom images (Siersdorfer et al. 2010; Borth et al. 2013b;\nBorth et al. 2013a; Yuan et al. 2013) and videos (Morency,\nMihalcea, and Doshi 2011). Visual sentiment analysis is ex-\ntremely challenging. First, image sentiment analysis is in-\nherently more challenging than object recognition as the lat-\nter is usually well defined. Image sentiment involves a much\nhigher level of abstraction and subjectivity in the human\nrecognition process (Joshi et al. 2011), on top of a wide vari-\nety of visual recognition tasks including object, scene, action\nand event recognition. In order to use supervised learning, it\nis imperative to collect a large and diverse labeled training\nset perhaps on the order of millions of images. This is an\nalmost insurmountable hurdle due to the tremendous labor\nrequired for image labeling. Second, the learning schemes\nneed to have high generalizability to cover more different\ndomains. However, the existing works use either pixel-level\nfeatures or a limited number of predefined attribute features,\nwhich is difficult to adapt the trained models to images from\na different domain.\nThe deep learning framework enables robust and accurate\nfeature learning, which in turn produces the state-of-the-art\nperformance on digit recognition (LeCun et al. 1989; Hin-\nton, Osindero, and Teh 2006), image classification (Cires¸an\net al. 2011; Krizhevsky, Sutskever, and Hinton 2012), mu-\nsical signal processing (Hamel and Eck 2010) and natural\nlanguage processing (Maas et al. 2011). Both the academia\nand industry have invested a huge amount of effort in build-\ning powerful neural networks. These works suggested that\ndeep learning is very effective in learning robust features in a\nsupervised or unsupervised fashion. Even though deep neu-\nral networks may be trapped in local optima (Hinton 2010;\nBengio 2012), using different optimization techniques, one\ncan achieve the state-of-the-art performance on many chal-\nlenging tasks mentioned above.\nInspired by the recent successes of deep learning, we are\ninterested in solving the challenging visual sentiment anal-\nysis task using deep learning algorithms. For images related\ntasks, Convolutional Neural Network (CNN) are widely\nused due to the usage of convolutional layers. It takes into\nconsideration the locations and neighbors of image pixels,\nwhich are important to capture useful features for visual\ntasks. Convolutional Neural Networks (LeCun et al. 1998;\nCires¸an et al. 2011; Krizhevsky, Sutskever, and Hinton\n2012) have been proved very powerful in solving computer\nvision related tasks. We intend to find out whether applying\nCNN to visual sentiment analysis provides advantages over\nusing a predefined collection of low-level visual features or\nvisual attributes, which have been done in prior works.\nTo that end, we address in this work two major challenges:\n1) how to learn with large scale weakly labeled training\ndata, and 2) how to generalize and extend the learned model\nacross domains. In particular, we make the following contri-\nbutions.\n• We develop an effective deep convolutional network ar-\nchitecture for visual sentiment analysis. Our architecture\nemploys two convolutional layers and several fully con-\nnected layers for the prediction of visual sentiment labels.\n• Our model attempts to address the weakly labeled nature\nof the training image data, where such labels are machine\ngenerated, by leveraging a progressive training strategy\nand a domain transfer strategy to fine-tune the neural net-\nwork. Our evaluation results suggest that this strategy is\neffective for improving the performance of neural net-\nwork in terms of generalizability.\n• In order to evaluate our model as well as competing algo-\nrithms, we build a large manually labeled visual sentiment\ndataset using Amazon Mechanical Turk. This dataset will\nbe released to the research community to promote further\ninvestigations on visual sentiment.\nRelated Work\nIn this section, we review literature closely related to our\nstudy on visual sentiment analysis, particularly in sentiment\nanalysis and Convolutional Neural Networks.\nSentiment Analysis\nSentiment analysis is a very challenging task (Liu et al.\n2003; Li et al. 2010). Researchers from natural language\nprocessing and information retrieval have developed differ-\nent approaches to solve this problem, achieving promising\nor satisfying results (Pang and Lee 2008). In the context of\nsocial media, there are several additional unique challenges.\nFirst, there are huge amounts of data available. Second, mes-\nsages on social networks are by nature informal and short.\nThird, people use not only textual messages, but also images\nand videos to express themselves.\nTumasjan et al. (2010) and Bollen et al. (2011) employed\npre-defined dictionaries for measuring the sentiment level\nof Tweets. The volume or percentage of sentiment-bearing\nwords can produce an estimate of the sentiment of one par-\nticular tweet. Davidov et al. (2010) used the weak labels\nfrom a large amount of Tweets. In contrast, they manually\nselected hashtags with strong positive and negative senti-\nments and ASCII smileys are also utilized to label the sen-\ntiments of tweets. Furthermore, Hu et al. (2013) incorpo-\nrated social signals into their unsupervised sentiment anal-\nysis framework. They defined and integrated both emotion\nindication and correlation into a framework to learn param-\neters for their sentiment classifier.\nThere are also several recent works on visual sentiment\nanalysis. Siersdorfer et al. (2010) proposes a machine learn-\ning algorithm to predict the sentiment of images using pixel-\nlevel features. Motivated by the fact that sentiment involves\nhigh-level abstraction, which may be easier to explain by\nobjects or attributes in images, both (Borth et al. 2013a)\nand (Yuan et al. 2013) propose to employ visual entities or\nattributes as features for visual sentiment analysis. In (Borth\net al. 2013a), 1200 adjective noun pairs (ANP), which may\ncorrespond to different levels of different emotions, are ex-\ntracted. These ANPs are used as queries to crawl images\nfrom Flickr. Next, pixel-level features of images in each\nANP are employed to train 1200 ANP detectors. The re-\nsponses of these 1200 classifiers can then be considered as\nmid-level features for visual sentiment analysis. The work\nin (Yuan et al. 2013) employed a similar mechanism. The\nmain difference is that 102 scene attributes are used instead.\n256\n256\n227\n227\n3\n3\n227\n227\n11\n11\n5\n5\n96\n55\n55\n256\n27\n27\n512 512\n24\n2\nFigure 2: Convolutional Neural Network for Visual Sentiment Analysis.\nConvolutional Neural Networks\nConvolutional Neural Networks (CNN) have been very suc-\ncessful in document recognition (LeCun et al. 1998). CNN\ntypically consists of several convolutional layers and sev-\neral fully connected layers. Between the convolutional lay-\ners, there may also be pooling layers and normalization lay-\ners. CNN is a supervised learning algorithm, where parame-\nters of different layers are learned through back-propagation.\nDue to the computational complexity of CNN, it has only be\napplied to relatively small images in the literature. Recently,\nthanks to the increasing computational power of GPU, it is\nnow possible to train a deep convolutional neural network on\na large scale image dataset (Krizhevsky, Sutskever, and Hin-\nton 2012). Indeed, in the past several years, CNN has been\nsuccessfully applied to scene parsing (Grangier, Bottou, and\nCollobert 2009), feature learning (LeCun, Kavukcuoglu, and\nFarabet 2010), visual recognition (Kavukcuoglu et al. 2010)\nand image classification (Krizhevsky, Sutskever, and Hinton\n2012). In our work, we intend to use CNN to learn features\nwhich are useful for visual sentiment analysis.\nVisual Sentiment Analysis\nWe propose to develop a suitable convolutional neural net-\nwork architecture for visual sentiment analysis. Moreover,\nwe employ a progressive training strategy that leverages the\ntraining results of convolutional neural network to further\nfilter out (noisy) training data. The details of the proposed\nframework will be described in the following sections.\nVisual Sentiment Analysis with regular CNN\nCNN has been proven to be effective in image classifica-\ntion tasks, e.g., achieving the state-of-the-art performance\nin ImageNet Challenge (Krizhevsky, Sutskever, and Hin-\nton 2012). Visual sentiment analysis can also be treated\nas an image classification problem. It may seem to be a\nmuch easier problem than image classification from Ima-\ngeNet (2 classes vs. 1000 classes in ImageNet). However,\nvisual sentiment analysis is quite challenging because senti-\nments or opinions correspond to high level abstractions from\na given image. This type of high level abstraction may re-\nquire viewer’s knowledge beyond the image content itself.\nMeanwhile, images in the same class of ImageNet mainly\ncontain the same type of object. In sentiment analysis, each\nclass contains much more diverse images. It is therefore ex-\ntremely challenging to discover features which can distin-\nguish much more diverse classes from each other. In addi-\ntion, people may have totally different sentiments over the\nsame image. This adds difficulties to not only our classifi-\ncation task, but also the acquisition of labeled images. In\nother words, it is nontrivial to obtain highly reliable labeled\ninstances, let alone a large number of them. Therefore, we\nneed a supervised learning engine that is able to tolerate a\nsignificant level of noise in the training dataset.\nThe architecture of the CNN we employ for sentiment\nanalysis is shown in Figure 2. Each image is resized to\n256 × 256 (if needed, we employ center crop, which first\nresizes the shorter dimension to 256 and then crops the mid-\ndle section of the resized image). The resized images are\nprocessed by two convolutional layers. Each convolutional\nlayer is also followed by max-pooling layers and normaliza-\ntion layers. The first convolutional layer has 96 kernels of\nsize 11 × 11 × 3 with a stride of 4 pixels. The second con-\nvolutional layer has 256 kernels of size 5 × 5 with a stride\nof 2 pixels. Furthermore, we have four fully connected lay-\ners. Inspired by (C¸aglar Gu¨lc¸ehre et al. 2013), we constrain\nthe second to last fully connected layer to have 24 neurons.\nAccording to the Plutchik’s wheel of emotions (Plutchik\n1984), there are a total of 24 emotions belonging to two cate-\ngories: positive emotions and negative emotions. Intuitively.\nwe hope these 24 nodes may help the network to learn the 24\nemotions from a given image and then classify each image\ninto positive or negative class according to the responses of\nthese 24 emotions.\nThe last layer is designed to learn the parameter w by\nmaximizing the following conditional log likelihood func-\ntion (xi and yi are the feature vector and label for the i-th\ninstance respectively):\nl(w) =\nn∑\ni=1\nln p(yi = 1|xi, w) + (1− yi) ln p(yi = 0|xi, w)\n(1)\nwhere\np(yi|xi, w) =\nexp(w0 +\n∑k\nj=1 wjxij)\nyi\n1 + exp(w0 +\n∑k\nj=1 wjxij)\nyi\n(2)\n... ...\n... f(·)PredictCNN\nPCNN\n1) Input\nTrain convolutional Neural Network\n2) CNN model\n3) 4) Sampling\n5) Fine-tune\n6) PCNN model\nFigure 3: Progressive CNN (PCNN) for visual sentiment analysis.\nVisual Sentiment Analysis with Progressive CNN\nSince the images are weakly labeled, it is possible that the\nneural network can get stuck in a bad local optimum. This\nmay lead to poor generalizability of the trained neural net-\nwork. On the other hand, we found that the neural network\nis still able to correctly classify a large proportion of the\ntraining instances. In other words, the neural network has\nlearned knowledge to distinguish the training instances with\nrelatively distinct sentiment labels. Therefore, we propose to\nprogressively select a subset of the training instances to re-\nduce the impact of noisy training instances. Figure 3 shows\nthe overall flow of the proposed progressive CNN (PCNN).\nWe first train a CNN on Flickr images. Next, we select train-\ning samples according to the prediction score of the trained\nmodel on the training data itself. Instead of training from the\nbeginning, we further fine-tune the trained model using these\nnewly selected, and potentially cleaner training instances.\nThis fine-tuned model will be our final model for visual sen-\ntiment analysis.\nAlgorithm 1 Progressive CNN training for Visual Sentiment\nAnalysis\nInput: X = {x1, x2, . . . , xn} a set of images of size 256×\n256\nY = {y1, y2, . . . , yn} sentiment labels of X\n1: Train convolutional neural network CNN with input X\nand Y\n2: Let S ∈ Rn×2 be the sentiment scores of X predicted\nusing CNN\n3: for si ∈ S do\n4: Delete xi from X with probability pi (Eqn.(3))\n5: end for\n6: Let X ′ ⊂ X be the remaining training images, Y ′ be\ntheir sentiment labels\n7: Fine-tune CNN with input X ′ and Y ′ to get PCNN\n8: return PCNN\nIn particular, we employ a probabilistic sampling algo-\nrithm to select the new training subset. The intuition is that\nwe want to keep instances with distinct sentiment scores\nbetween the two classes with a high probability, and con-\nversely remove instances with similar sentiment scores for\nboth classes with a high probability. Let si = (si1, si2) be\nthe prediction sentiment scores for the two classes of in-\nstance i. We choose to remove the training instance i with\nprobability pi given by Eqn.(3). Algorithm 1 summarizes the\nsteps of the proposed framework.\npi = max (0, 2− exp(|si1 − si2|)) (3)\nWhen the difference between the predicted sentiment scores\nof one training instance are large enough, this training in-\nstance will be kept in the training set. Otherwise, the smaller\nthe difference between the predicted sentiment scores be-\ncome, the larger the probability of this instance being re-\nmoved from the training set.\nExperiments\nWe choose to use the same half million Flickr images\nfrom SentiBank1 to train our Convolutional Neural Network.\nThese images are only weakly labeled since each image be-\nlongs to one adjective noun pair (ANP). There are a total\nof 1200 ANPs. According to the Plutchik’s Wheel of Emo-\ntions (Plutchik 1984), each ANP is generated by the combi-\nnation of adjectives with strong sentiment values and nouns\nfrom tags of images and videos (Borth et al. 2013b). These\nANPs are then used as queries to collect related images\nfor each ANP. The released SentiBank contains 1200 ANPs\nwith about half million Flickr images. We train our convolu-\ntional neural network mainly on this image dataset. We im-\nplement the proposed architecture of CNN on the publicly\navailable implementation Caffe (Jia 2013). All of our exper-\niments are evaluated on a Linux X86 64 machine with 32G\nRAM and two NVIDIA GTX Titan GPUs.\nComparisons of different CNN architectures\nThe architecture of our model is shown in Figure 2. How-\never, we also evaluate other architectures for the visual sen-\ntiment analysis task. Table 1 summarizes the performance\nof different architectures on a randomly chosen Flickr test-\ning dataset. In Table 1, iCONV-jFC indicates that there are\n1http://visual-sentiment-ontology.appspot.com/\ni convolutional layers and j fully connected layers in the ar-\nchitecture. The model in Figure 2 shows slightly better per-\nformance than other models in terms of F1 and accuracy. In\nthe following experiments, we mainly focus on the evalua-\ntion of CNN using the architecture in Figure 2.\nTable 1: Summary of performance of different architectures\non randomly chosen testing data.\nArchitecture Precision Recall F1 Accuracy\n3CONV-4FC 0.679 0.845 0.753 0.644\n3CONV-2FC 0.69 0.847 0.76 0.657\n2CONV-3FC 0.679 0.874 0.765 0.654\n2CONV-4FC 0.688 0.875 0.77 0.665\nBaselines\nWe compare the performance of PCNN with three other\nbaselines or competing algorithms for image sentiment clas-\nsification.\nLow-level Feature-based Siersdorfer et al. (2010) defined\nboth global and local visual features. Specifically, the global\ncolor histograms (GCH) features consist of 64-bin RGB his-\ntogram. The local color histogram features (LCH) first di-\nvided the image into 16 blocks and used the 64-bin RGB\nhistogram for each block. They also employed SIFT features\nto learn a visual word dictionary. Next, they defined bag of\nvisual word features (BoW) for each image.\nMid-level Feature-based Damian et al. (2013a; 2013b)\nproposed a framework to build visual sentiment ontology\nand SentiBank according to the previously discussed 1200\nANPs. With the trained 1200 ANP detectors, they are able\nto generate 1200 responses for any given test image using\nthese pre-trained 1200 ANP detectors. A sentiment classifier\nis built on top of these mid-level features according to the\nsentiment label of training images. Sentribute (Yuan et al.\n2013) also employed mid-level features for sentiment pre-\ndiction. However, instead of using adjective noun pairs, they\nemployed scene-based attributes (Patterson and Hays 2012)\nto define the mid-level features.\nDeep Learning on Flickr Dataset\nWe randomly choose 90% images from the half million\nFlickr images as our training dataset. The remaining 10%\nimages are our testing dataset. We train the convolutional\nneural network with 300,000 iterations of mini-batches\n(each mini-batch contains 256 images). We employ the sam-\npling probability in Eqn.(3) to filter the training images ac-\ncording to the prediction score of CNN on its training data.\nIn the fine-tuning stage of PCNN, we run another 100,000\niterations of mini-batches using the filtered training dataset.\nTable 2 gives a summary of the number of data instances in\nour experiments. Figure 4 shows the filters learned in the\nfirst convolutional layer of CNN and PCNN, respectively.\nThere are some differences between 4(a) and 4(b). While\nit is somewhat inconclusive that the neural networks have\nreached a better local optimum, at least we can conclude that\nthe fine-tuning stage using a progressively cleaner training\nTable 2: Statistics of the number of Flickr image dataset.\nModels training testing # of iterations\nCNN 401,739 44,637 300,000\nPCNN 369,828 44,637 100,000\nTable 3: Performance on the Testing Dataset by CNN and\nPCNN.\nAlgorithm Precision Recall F1 Accuracy\nCNN 0.714 0.729 0.722 0.718\nPCNN 0.759 0.826 0.791 0.781\ndataset has prompted the neural networks to learn different\nknowledge. Indeed, the evaluation results suggest that this\nfine-tuning leads to the improvement of performance.\nTable 3 shows the performance of both CNN and PCNN\non the 10% randomly chosen testing data. PCNN outper-\nformed CNN in terms of Precision, Recall, F1 and Accu-\nracy. The results in Table 3 and the filters from Figure 4\nshows that the fine-tuning stage of PCNN can help the neu-\nral network to search for a better local optimum.\n(a) Filters learned from CNN\n(b) Filters learned from PCNN\nFigure 4: Filters of the first convolutional layer.\nTwitter Testing Dataset\nWe also built a new image dataset from image tweets. Im-\nage tweets refer to those tweets that contain images. We\nbuilt a total of 1269 images as our candidate testing im-\nages. We employed crowd intelligence, Amazon Mechani-\ncal Turk (AMT), to generate sentiment labels for these test-\ning images, in a similar fashion to (Borth et al. 2013b). We\nrecruited 5 AMT workers for each of the candidate image.\nTable 4 shows the statistics of the labeling results from the\nAmazon Mechanical Turk. In the table, “five agree” indi-\ncates that all the 5 AMT workers gave the same sentiment\nlabel for a given image. Only a small portion of the images,\n153 out of 1269, had significant disagreements between the\nTable 5: Performance of different algorithms on the Twitter image dataset (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc\nCNN 0.749 0.869 0.805 0.722 0.707 0.839 0.768 0.686 0.691 0.814 0.747 0.667\nPCNN 0.77 0.878 0.821 0.747 0.733 0.845 0.785 0.714 0.714 0.806 0.757 0.687\nTable 4: Summary of AMT labeled results for the Twitter\ntesting dataset.\nSentiment Five Agree At Least FourAgree\nAt Least\nThree Agree\nPositive 581 689 769\nNegative 301 427 500\nSum 882 1116 1269\n5 workers (3 vs. 2). We evaluate the performance of Con-\nvolutional Neural Networks on this manually labeled image\ndataset according to the model trained on Flickr images. Ta-\nble 5 shows the performance of the two frameworks. Not\nsurprisingly, both models perform better on the less ambigu-\nous image set (“five agree” by AMT). Meanwhile, PCNN\nshows better performance than CNN on all the three label-\ning sets in terms of both F1 and accuracy. This suggests that\nthe fine-tuning stage of CNN effectively improves the gen-\neralizability extensibility of the neural networks.\nTransfer Learning\nHalf million Flickr images are used in our CNN training.\nThe features learned are generic features on these half mil-\nlion images. Table 5 shows that these generic features also\nhave the ability to predict visual sentiment of images from\nother domains. The question we ask is whether we can fur-\nther improve the performance of visual sentiment analysis\non Twitter images by inducing transfer learning. In this sec-\ntion, we conduct experiments to answer this question.\nThe users of Flickr are more likely to spend more time\non taking high quality pictures. Twitter users are likely to\nshare the moment with the world. Thus, most of the Twitter\nimages are casually taken snapshots. Meanwhile, most of the\nimages are related to current trending topics and personal\nexperiences, making the images on Twitter much diverse in\ncontent as well as quality.\nIn this experiment, we fine-tune the pre-trained neural net-\nwork model in the following way to achieve transfer learn-\ning. We randomly divide the Twitter images into 5 equal par-\ntitions. Every time, we use 4 of the 5 partitions to fine-tune\nour pre-trained model from the half million Flickr images\nand evaluate the new model on the remaining partition. The\naveraged evaluation results are reported. The algorithm is\ndetailed in Algorithm 2.\nSimilar to (Borth et al. 2013b), we also employ 5-fold\ncross-validation to evaluate the performance of all the base-\nline algorithms. Table 6 summarizes the averaged perfor-\nmance results of different baseline algorithms and our two\nCNN models. Overall, both CNN models outperform the\nbaseline algorithms. In the baseline algorithms, Sentribute\ngives slightly better results than the other two baseline al-\nFigure 5: Positive (top block) and Negative (bottom block)\nexamples. Each column shows the negative example im-\nages for each algorithm (PCNN, CNN, Sentribute, Sen-\ntibank, GCH, LCH, GCH+BoW, LCH+BoW). The images\nare ranked by the prediction score from top to bottom in a\ndecreasing order.\nAlgorithm 2 Transfer Learning to fine-tune CNN\nInput: X = {x1, x2, . . . , xn} a set of images of size 256×\n256\nY = {y1, y2, . . . , yn} sentiment labels of X\nPre-trained CNN model M\n1: Randomly partition X and Y into 5 equal groups\n{(X1, Y1), . . . , (X5, Y5)}.\n2: for i from 1 to 5 do\n3: Let (X ′, Y ′) = (X,Y )− (Xi, Yi)\n4: Fine-tune M with input (X ′, Y ′) to obtain model Mi\n5: Evaluate the performance of Mi on (Xi, Yi)\n6: end for\n7: return The averaged performance of Mi on (Xi, Yi) (i\nfrom 1 to 5)\ngorithms. Interestingly, even the combination of using low-\nTable 6: 5-Fold Cross-Validation Performance of different algorithms on the Twitter image dataset. Note that compared with\nTable 5, both fine-tuned CNN models have been improved due to domain transfer learning (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc\nGCH 0.708 0.888 0.787 0.684 0.687 0.84 0.756 0.665 0.678 0.836 0.749 0.66\nLCH 0.764 0.809 0.786 0.71 0.725 0.753 0.739 0.671 0.716 0.737 0.726 0.664\nGCH + BoW 0.724 0.904 0.804 0.71 0.703 0.849 0.769 0.685 0.683 0.835 0.751 0.665\nLCH + BoW 0.771 0.811 0.79 0.717 0.751 0.762 0.756 0.697 0.722 0.726 0.723 0.664\nSentiBank 0.785 0.768 0.776 0.709 0.742 0.727 0.734 0.675 0.720 0.723 0.721 0.662\nSentribute 0.789 0.823 0.805 0.738 0.75 0.792 0.771 0.709 0.733 0.783 0.757 0.696\nCNN 0.795 0.905 0.846 0.783 0.773 0.855 0.811 0.755 0.734 0.832 0.779 0.715\nPCNN 0.797 0.881 0.836 0.773 0.786 0.842 0.811 0.759 0.755 0.805 0.778 0.723\nlevel features local color histogram (LCH) and bag of visual\nwords (BoW) shows better results than SentiBank on our\nTwitter dataset. Both fine-tuned CNN models have been im-\nproved. This improvement is significant given that we only\nuse four fifth of the 1269 images for domain adaptation.\nBoth neural network models have similar performance on\nall the three sets of the Twitter testing data. This suggests\nthat the fine-tuning stage helps both models to find a better\nlocal minimum. In particular, the knowledge from the Twit-\nter images starts to determine the performance of both neural\nnetworks. The previously trained model only determines the\nstart position of the fine-tuned model.\nMeanwhile, for each model, we respectively select the top\n5 positive and top 5 negative examples from the 1269 Twit-\nter images according to the evaluation scores. Figure show\nthose examples for each model. In both figures, each column\ncontains the images for one model. A green solid box means\nthe prediction label of the image agrees with the human la-\nbel. Otherwise, we use a red dashed box. The labels of top\nranked images in both neural network models are all cor-\nrectly predicted. However, the images are not all the same.\nThis on the other hand suggests that even though the two\nmodels achieve similar results after fine-tuning, they may\nhave arrived at somewhat different local optima due to the\ndifferent starting positions, as well as the transfer learning\nprocess. For all the baseline models, it is difficult to say\nwhich kind of images are more likely to be correctly clas-\nsified according to these images. However, we observe that\nthere are several mistakenly classified images in common\namong the models using low-level features (the four right-\nmost columns in Figure ). Similarly, for Sentibank and Sen-\ntribute, several of the same images are also in the top ranked\nsamples. This indicates that there are some common learned\nknowledge in the low-level feature models and mid-level\nfeature models.\nConclusions\nVisual sentiment analysis is a challenging and interesting\nproblem. In this paper, we adopt the recent developed con-\nvolutional neural networks to solve this problem. We have\ndesigned a new architecture, as well as new training strate-\ngies to overcome the noisy nature of the large-scale train-\ning samples. Both progressive training and transfer learning\ninducted by a small number of confidently labeled images\nfrom the target domain have yielded notable improvements.\nThe experimental results suggest that convolutional neural\nnetworks that are properly trained can outperform both clas-\nsifiers that use predefined low-level features or mid-level vi-\nsual attributes for the highly challenging problem of visual\nsentiment analysis. Meanwhile, the main advantage of us-\ning convolutional neural networks is that we can transfer\nthe knowledge to other domains using a much simpler fine-\ntuning technique than those in the literature e.g., (Duan et al.\n2012).\nIt is important to reiterate the significance of this work\nover the state-of-the-art (Siersdorfer et al. 2010; Borth et al.\n2013b; Yuan et al. 2013). We are able to directly leverage\na much larger weakly labeled data set for training, as well\nas a larger manually labeled dataset for testing. The larger\ndata sets, along with the proposed deep CNN and its training\nstrategies, give rise to better generalizability of the trained\nmodel and higher confidence of such generalizability. In the\nfuture, we plan to develop robust multimodality models that\nemploy both the textual and visual content for social me-\ndia sentiment analysis. We also hope our sentiment analysis\nresults can encourage further research on online user gener-\nated content.\nWe believe that sentiment analysis on large scale online\nuser generated content is quite useful since it can provide\nmore robust signals and information for many data analytics\ntasks, such as using social media for prediction and forecast-\ning. In the future, we plan to develop robust multimodal-\nity models that employ both the textual and visual content\nfor social media sentiment analysis. We also hope our senti-\nment analysis results can encourage further research on on-\nline user generated content.\nAcknowledgments\nThis work was generously supported in part by Adobe Re-\nsearch. We would like to thank Digital Video and Multime-\ndia (DVMM) Lab at Columbia University for providing the\nhalf million Flickr images and their machine-generated la-\nbels.\nReferences\n[Asur and Huberman 2010] Asur, S., and Huberman, B. A.\n2010. Predicting the future with social media. In WI-IAT,\nvolume 1, 492–499. IEEE.\n[Bengio 2012] Bengio, Y. 2012. Practical recommendations\nfor gradient-based training of deep architectures. In Neural\nNetworks: Tricks of the Trade. Springer. 437–478.\n[Bollen, Mao, and Pepe 2011] Bollen, J.; Mao, H.; and Pepe,\nA. 2011. Modeling public mood and emotion: Twitter sen-\ntiment and socio-economic phenomena. In ICWSM.\n[Bollen, Mao, and Zeng 2011] Bollen, J.; Mao, H.; and\nZeng, X. 2011. Twitter mood predicts the stock market.\nJournal of Computational Science 2(1):1–8.\n[Borth et al. 2013a] Borth, D.; Chen, T.; Ji, R.; and Chang,\nS.-F. 2013a. Sentibank: large-scale ontology and classifiers\nfor detecting sentiment and emotions in visual content. In\nACM MM, 459–460. ACM.\n[Borth et al. 2013b] Borth, D.; Ji, R.; Chen, T.; Breuel, T.;\nand Chang, S.-F. 2013b. Large-scale visual sentiment ontol-\nogy and detectors using adjective noun pairs. In ACM MM,\n223–232. ACM.\n[C¸aglar Gu¨lc¸ehre et al. 2013] C¸aglar Gu¨lc¸ehre; Cho, K.; Pas-\ncanu, R.; and Bengio, Y. 2013. Learned-norm pooling for\ndeep neural networks. CoRR abs/1311.1780.\n[Cires¸an et al. 2011] Cires¸an, D. C.; Meier, U.; Masci, J.;\nGambardella, L. M.; and Schmidhuber, J. 2011. Flexible,\nhigh performance convolutional neural networks for image\nclassification. In IJCAI, 1237–1242. AAAI Press.\n[Davidov, Tsur, and Rappoport 2010] Davidov, D.; Tsur, O.;\nand Rappoport, A. 2010. Enhanced sentiment learning using\ntwitter hashtags and smileys. In ICL, 241–249. Association\nfor Computational Linguistics.\n[Duan et al. 2012] Duan, L.; Xu, D.; Tsang, I.-H.; and Luo,\nJ. 2012. Visual event recognition in videos by learning from\nweb data. IEEE PAMI 34(9):1667–1680.\n[Grangier, Bottou, and Collobert 2009] Grangier, D.; Bot-\ntou, L.; and Collobert, R. 2009. Deep convolutional net-\nworks for scene parsing. In ICML 2009 Deep Learning\nWorkshop, volume 3. Citeseer.\n[Hamel and Eck 2010] Hamel, P., and Eck, D. 2010. Learn-\ning features from music audio with deep belief networks. In\nISMIR, 339–344.\n[Hinton, Osindero, and Teh 2006] Hinton, G. E.; Osindero,\nS.; and Teh, Y.-W. 2006. A fast learning algorithm for deep\nbelief nets. Neural computation 18(7):1527–1554.\n[Hinton 2010] Hinton, G. 2010. A practical guide to training\nrestricted boltzmann machines. Momentum 9(1):926.\n[Hu et al. 2013] Hu, X.; Tang, J.; Gao, H.; and Liu, H. 2013.\nUnsupervised sentiment analysis with emotional signals. In\nWWW, 607–618. International World Wide Web Confer-\nences Steering Committee.\n[Jia 2013] Jia, Y. 2013. Caffe: An open source convolutional\narchitecture for fast feature embedding. http://caffe.\nberkeleyvision.org/.\n[Jin et al. 2010] Jin, X.; Gallagher, A.; Cao, L.; Luo, J.; and\nHan, J. 2010. The wisdom of social multimedia: using flickr\nfor prediction and forecast. In ACMMM, 1235–1244. ACM.\n[Joshi et al. 2011] Joshi, D.; Datta, R.; Fedorovskaya, E.; Lu-\nong, Q.-T.; Wang, J. Z.; Li, J.; and Luo, J. 2011. Aesthetics\nand emotions in images. IEEE Signal Processing Magazine\n28(5):94–115.\n[Kavukcuoglu et al. 2010] Kavukcuoglu, K.; Sermanet, P.;\nBoureau, Y.-L.; Gregor, K.; Mathieu, M.; and LeCun, Y.\n2010. Learning convolutional feature hierarchies for visual\nrecognition. In NIPS, 5.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classifi-\ncation with deep convolutional neural networks. In NIPS,\n4.\n[LeCun et al. 1989] LeCun, Y.; Boser, B.; Denker, J. S.; Hen-\nderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. D.\n1989. Backpropagation applied to handwritten zip code\nrecognition. Neural computation 1(4):541–551.\n[LeCun et al. 1998] LeCun, Y.; Bottou, L.; Bengio, Y.; and\nHaffner, P. 1998. Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE 86(11):2278–\n2324.\n[LeCun, Kavukcuoglu, and Farabet 2010] LeCun, Y.;\nKavukcuoglu, K.; and Farabet, C. 2010. Convolutional\nnetworks and applications in vision. In ISCAS, 253–256.\nIEEE.\n[Li et al. 2010] Li, G.; Hoi, S. C.; Chang, K.; and Jain, R.\n2010. Micro-blogging sentiment detection by collaborative\nonline learning. In ICDM, 893–898. IEEE.\n[Liu et al. 2003] Liu, B.; Dai, Y.; Li, X.; Lee, W. S.; and Yu,\nP. S. 2003. Building text classifiers using positive and unla-\nbeled examples. In ICDM, 179–186. IEEE.\n[Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.;\nHuang, D.; Ng, A. Y.; and Potts, C. 2011. Learning word\nvectors for sentiment analysis. In ACL, 142–150.\n[Morency, Mihalcea, and Doshi 2011] Morency, L.-P.; Mi-\nhalcea, R.; and Doshi, P. 2011. Towards multimodal senti-\nment analysis: Harvesting opinions from the web. In ICMI,\n169–176. New York, NY, USA: ACM.\n[O’Connor et al. 2010] O’Connor, B.; Balasubramanyan, R.;\nRoutledge, B. R.; and Smith, N. A. 2010. From tweets to\npolls: Linking text sentiment to public opinion time series.\nICWSM 11:122–129.\n[Pang and Lee 2008] Pang, B., and Lee, L. 2008. Opinion\nmining and sentiment analysis. Foundations and trends in\ninformation retrieval 2(1-2):1–135.\n[Patterson and Hays 2012] Patterson, G., and Hays, J. 2012.\nSun attribute database: Discovering, annotating, and recog-\nnizing scene attributes. In CVPR.\n[Plutchik 1984] Plutchik, R. 1984. Emotions: A general psy-\nchoevolutionary theory. Approaches to emotion 1984:197–\n219.\n[Siersdorfer et al. 2010] Siersdorfer, S.; Minack, E.; Deng,\nF.; and Hare, J. 2010. Analyzing and predicting sentiment\nof images on the social web. In ACM MM, 715–718. ACM.\n[Tumasjan et al. 2010] Tumasjan, A.; Sprenger, T. O.; Sand-\nner, P. G.; and Welpe, I. M. 2010. Predicting elections\nwith twitter: What 140 characters reveal about political sen-\ntiment. ICWSM 178–185.\n[Yuan et al. 2013] Yuan, J.; Mcdonough, S.; You, Q.; and\nLuo, J. 2013. Sentribute: image sentiment analysis from\na mid-level perspective. In Proceedings of the Second In-\nternational Workshop on Issues of Sentiment Discovery and\nOpinion Mining, 10. ACM.\n[Zhang, Fuehres, and Gloor 2011] Zhang, X.; Fuehres, H.;\nand Gloor, P. A. 2011. Predicting stock market indicators\nthrough twitter i hope it is not as bad as i fear. Procedia-\nSocial and Behavioral Sciences 26:55–62.\n",
      "id": 24729442,
      "identifiers": [
        {
          "identifier": "10.1609/aaai.v29i1.9179",
          "type": "DOI"
        },
        {
          "identifier": "386117133",
          "type": "CORE_ID"
        },
        {
          "identifier": "103268817",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/9179",
          "type": "OAI_ID"
        },
        {
          "identifier": "42637726",
          "type": "CORE_ID"
        },
        {
          "identifier": "1509.06041",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.687.4407",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1509.06041",
          "type": "OAI_ID"
        }
      ],
      "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1509.06041",
        "oai:citeseerx.psu:10.1.1.687.4407",
        "oai:ojs.aaai.org:article/9179"
      ],
      "publishedDate": "2015-02-09T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://www.cs.rochester.edu/u/qyou/papers/sentiment_analysis_final.pdf",
        "http://arxiv.org/abs/1509.06041"
      ],
      "updatedDate": "2024-02-09T07:01:46",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1509.06041"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24729442"
        }
      ]
    }
  ],
  "searchId": "b8c1c50754c8ed0119f23daf4410256c"
}