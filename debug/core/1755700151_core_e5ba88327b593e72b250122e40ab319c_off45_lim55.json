{
  "totalHits": 535056,
  "limit": 55,
  "offset": 45,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": "1904.02839",
      "authors": [
        {
          "name": "Augenstein, Isabelle"
        },
        {
          "name": "Cotterell, Ryan"
        },
        {
          "name": "Hoyle, Alexander"
        },
        {
          "name": "Wallach, Hanna"
        },
        {
          "name": "Wolf-Sonkin, Lawrence"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/322819664"
      ],
      "createdDate": "2019-06-02T02:00:56",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 1238,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1238",
          "logo": "https://api.core.ac.uk/data-providers/1238/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "When assigning quantitative labels to a dataset, different methodologies may\nrely on different scales. In particular, when assigning polarities to words in\na sentiment lexicon, annotators may use binary, categorical, or continuous\nlabels. Naturally, it is of interest to unify these labels from disparate\nscales to both achieve maximal coverage over words and to create a single, more\nrobust sentiment lexicon while retaining scale coherence. We introduce a\ngenerative model of sentiment lexica to combine disparate scales into a common\nlatent representation. We realize this model with a novel multi-view\nvariational autoencoder (VAE), called SentiVAE. We evaluate our approach via a\ndownstream text classification task involving nine English-Language sentiment\nanalysis datasets; our representation outperforms six individual sentiment\nlexica, as well as a straightforward combination thereof.Comment: To appear in NAACL-HLT 201",
      "documentType": "research",
      "doi": "10.18653/v1/n19-1065",
      "downloadUrl": "https://core.ac.uk/download/322819664.pdf",
      "fieldOfStudy": null,
      "fullText": "Combining Sentiment Lexica with a\nMulti-View Variational Autoencoder\nAlexander Hoyle@ Lawrence Wolf-SonkinS Hanna WallachZ\nRyan CotterellH Isabelle AugensteinP\n@University College London, London, UK\nSDepartment of Computer Science, Johns Hopkins University, Baltimore, USA\nZMicrosoft Research, New York City, USA\nHDepartment of Computer Science and Technology, University of Cambridge, Cambridge, UK\nPDepartment of Computer Science, University of Copenhagen, Copenhagen, Denmark\nalexander.hoyle.17@ucl.ac.uk, lawrencews@jhu.edu\nhanna@dirichlet.net, rdc42@cam.ac.uk, genstein@di.ku.dk\nAbstract\nWhen assigning quantitative labels to a\ndataset, different methodologies may rely on\ndifferent scales. In particular, when assigning\npolarities to words in a sentiment lexicon,\nannotators may use binary, categorical, or\ncontinuous labels. Naturally, it is of interest to\nunify these labels from disparate scales to both\nachieve maximal coverage over words and to\ncreate a single, more robust sentiment lexicon\nwhile retaining scale coherence. We introduce\na generative model of sentiment lexica to\ncombine disparate scales into a common latent\nrepresentation. We realize this model with\na novel multi-view variational autoencoder\n(VAE), called SentiVAE. We evaluate our\napproach via a downstream text classification\ntask involving nine English-Language sen-\ntiment analysis datasets; our representation\noutperforms six individual sentiment lexica, as\nwell as a straightforward combination thereof.\n1 Introduction\nSentiment lexica provide an easy way to automat-\nically label texts with polarity values, and are also\nfrequently transformed into features for supervised\nmodels, including neural networks (Palogiannidi\net al., 2016; Ma et al., 2018). Indeed, given their\nutility, a veritable cottage industry has emerged\nfocusing on the design of sentiment lexica. In prac-\ntice, using any single lexicon, unless specifically\nand carefully designed for the particular domain\nof interest, has several downsides. For example,\nany lexicon will typically have low coverage\ncompared to the language’s entire vocabulary,\nand may have misspecified labels for the domain.\nIn many cases, it may therefore be desirable to\ncombine multiple sentiment lexica into a single\nrepresentation. Indeed, some research on unifying\nFigure 1: A depiction of the “encoder” portion of Sen-\ntiVAE. The word peppy has polarity values of 0.65 and\npos in the SenticNet and Hu-Liu lexica, respectively.\nThese values are “encoded” into two three-dimensional\nvectors, which are then summed and added to (1, 1, 1)\n(not shown) to form the parameters of a Dirichlet over\nthe latent representation of the word’s polarity value.\nsuch lexica has emerged (Emerson and Declerck,\n2014; Altrabsheh et al., 2017), borrowing ideas\nfrom crowdsourcing (Raykar et al., 2010; Hovy\net al., 2013). However, this is a non-trivial task,\nbecause lexica can use binary, categorical, or\ncontinuous scales to quantify polarity—in addition\nto different interpretations for each—and thus\ncannot easily be combined. In Fig. 1, we show an\nexample of the same word labeled using different\nlexica to illustrate the nature of the challenge.\nTo combine sentiment lexica with disparate\nscales, we introduce SentiVAE, a novel multi-\nview variant of the variational autoencoder (VAE)\n(Kingma and Welling, 2014). SentiVAE, visualized\nas a graphical model in Fig. 2, differs from the orig-\ninal VAE in two ways: (i) it uses a Dirichlet latent\nvariable (rather than a Gaussian) for each word in\nthe combined vocabulary, and (ii) it has multiple\nemission distributions—one for each lexicon. Be-\ncause the latent variables are shared across the lex-\nar\nX\niv\n:1\n90\n4.\n02\n83\n9v\n1 \n [c\ns.C\nL]\n  5\n A\npr\n 20\n19\nLexicon Source N Dom\nSentiWordNet WordNet 14107 [−1, 1]2\nMPQA Newswire 4397 {0, 1}\nSenticNet — 100000 [−1, 1]\nHu-Liu Product reviews 6790 {0, 1}\nGI — 4206 {0, 1}\nVADER Social media 7489 {0, . . . , 8}10\nTable 1: Descriptive statistics for the sentiment lexica.\nN : vocabulary size. Dom: Domain of polarity values.\nica, we are able to derive a common latent represen-\ntation of the words’ polarities. The resulting model\nis spiritually related to a multi-view learning ap-\nproach (Sun, 2013), where each view corresponds\nto a different lexicon. Experimentally, we use\nSentiVAE to combine six commonly used English-\nlanguage sentiment lexica with disparate scales.\nWe evaluate the resulting representation via\na text classification task involving nine English-\nlanguage sentiment analysis datasets. For each\ndataset, we transform each text into an average\npolarity value using either our representation, one\nof the six commonly used sentiment lexica, or a\nstraightforward combination thereof. We then train\na classifier to predict the overall sentiment of each\ntext from its average polarity value. We find that\nour representation outperforms the individual lex-\nica, as well as the straightforward combination for\nsome datasets. Our representation is particularly\nefficacious for datasets from domains that are not\nwell-supported by standard sentiment lexica.1\nThe existing research that is most closely re-\nlated to our work is SentiMerge (Emerson and De-\nclerck, 2014), a Bayesian approach for aligning\nsentiment lexica with different continuous scales.\nSentiMerge consists of two steps: (i) aligning the\nlexica via rescaling, and (ii) combining the rescaled\nlexica using a Gaussian distribution. The authors\nperform token-level evaluation using a single senti-\nment analysis dataset where each token is labeled\nwith its contextually dependent sentiment. Because\nSentiMerge can only combine lexica with continu-\nous scales, we do not include it in our evaluation.\n2 Sentiment Lexica and Scales\nWe use the following commonly used English-\nlanguage sentiment lexica: SentiWordNet (Bac-\ncianella et al., 2010), MPQA (Wilson et al., 2005),\nSenticNet 5 (Cambria et al., 2014), Hu-Liu (Hu and\n1Our representation and code are available at https://\ngithub.com/ahoho/SentiVAE.\nLiu, 2004), GI (Stone et al., 1962), and VADER\n(Hutto and Gilbert, 2014). Descriptive statistics for\neach lexicon are shown in Tab. 1. Each word in\nSentiWordNet is labeled with two real values, each\nin the interval [0, 1], corresponding to the strength\nof positive and negative sentiment (e.g., the label\n(0 0) is neutral, while the label (1 0) is maximally\npositive). Each word in VADER is labeled by ten\ndifferent human evaluators, with each evaluator pro-\nviding a polarity value on a nine-point scale (where\nthe midpoint is neutral), yielding a 10-dimensional\nlabel. MPQA, Hu-Liu, and GI all use binary scales.\nLastly, each word in SenticNet is labeled with a\nreal value in the interval [−1, 1], where 0 is neutral.\n3 SentiVAE\nWe first describe a figurative generative process for\na single sentiment lexicon d ∈ D, where D is a set\nof sentiment lexica. Imagine there is a true (latent)\npolarity value zw associated with each word w\nin the lexicon’s vocabulary. When the lexicon’s\ncreator labels that word according to their chosen\nscale (e.g., thumbs-up or thumbs-down, a real\nvalue in the interval [0, 1]), they deterministically\ntransform this true value to their chosen scale\nvia a function f( · ; θd).2 Sometimes, noise is\nintroduced during this labeling process, corrupting\nthe label as it leaves the ethereal realm and\nproducing the (observed) polarity label xwd . They\nthen add this potentially noisy label to the lexicon.\nGiven a lexicon of observed polarity labels, the\nlatent polarity values can be inferred using a VAE.\nThe original VAE posits a generative model of ob-\nserved data X and latent variables Z: P (X ,Z) =\nP (X | Z)P (Z). Inference of Z then proceeds by\napproximating the (intractable) posterior P (Z | X )\nwith a Gaussian distribution, factorized over the in-\ndividual latent variables. A parameterized encoder\nfunction compresses X into Z , while a parameter-\nized decoder function reconstructs X from Z .\nSentiVAE extends the original VAE model to\ncombine multiple lexica with disparate scales, pro-\nducing a common latent representation of the polar-\nity value for each word in the combined vocabulary.\nGenerative process. Given a set of sentiment\nlexica D with a combined vocabulary W , Senti-\nVAE posits a common latent representation zw of\nthe polarity value for each word w ∈ W , where zw\nis a three-dimensional categorical distribution over\n2Parameterized by lexicon-specific weights θd.\nαw\nzw\nρwd θd\nxwd\nw ∈ W d ∈ D\nFigure 2: Generative model for SentiVAE.\nthe sentiments positive, negative, and neutral.\nThe generative process starts by drawing each\nlatent polarity value zw from a three-dimensional\nDirichlet prior, parameterized by αw = (1, 1, 1):\nzw ∼ Dir(αw). (1)\nIf the word is uncontroversial,3 we spur this\nprior somewhat using the number of lexica in\nwhich the word appears c(w). Specifically, we\nadd c(w) to the parameter for the sentiment\nassociated with that word in the lexica, e.g.,\nαSUPERB = (1 + c(SUPERB), 1, 1). This has the ef-\nfect of regularizing the inferred latent polarity value\ntoward the desired distribution over sentiments.\nHaving generated zw, the process proceeds by\n“decoding” zw into each lexicon’s chosen scale.\nFirst, for each lexicon d ∈ D, zw is determinis-\ntically transformed via neural network f( · ; θd)\nwith a single 32-dimensional hidden layer,\nparameterized by lexicon-specific weights θd:\nρwd = f(z\nw;θd). (2)\nThe transformed value ρwd is then used to generate\nthe (observed) polarity label xwd for that lexicon:\nxwd ∼ Pd(xwd | ρwd ). (3)\nThe dimensionality of ρwd and the emission distribu-\ntion Pd are lexicon-specific. For SentiWordNet, Pd\n3We say that a word is uncontroversial if there is strong\nagreement across the sentiment lexica in which it appears.\nEven without this spurring, the inferred latent representation\ntypically separates into the three sentiment classes, but perfor-\nmance on our text classification task is somewhat diminished.\nDataset Source N Classes\nIMDB Movies 25000 2\nYelp Product reviews 100000 5 / 3\nSemEval Twitter 7668 3\nMultiDom Product reviews 6500 2\nACL Scientific reviews 248 5 / 3\nICLR Scientific reviews 2166 10 / 3\nTable 2: Descriptive statistics for the training portions\nof the sentiment analysis datasets. N : number of texts.\nis a two-dimensional Gaussian with mean ρwd and\na diagonal covariance matrix equal to 0.01I; for\nVADER, Pd consists of ten nine-dimensional cate-\ngorical distributions, collectively parameterized by\nρwd ; for MPQA, Hu-Liu, and GI, Pd is a Bernoulli\ndistribution, parameterized by ρwd ; and for Sen-\nticNet, Pd is a univariate Gaussian with mean and\nvariance each an element in a two-dimensional ρwd .\nInference. Inference involves forming the pos-\nterior distribution over the latent polarity values\nZ given the observed polarity labels X . Because\ncomputing the normalizing constant P (X ) is in-\ntractable, we instead approximate the posterior\nwith a family of distributions Qλ(Z), indexed by\nvariational parameters λ. Specifically, we use\nQλ(Z) =\n∏\nw∈W\nQβw(z\nw) =\n∏\nw∈W\nDir(βw). (4)\nTo construct βw, we first define a neural net-\nwork g(·; φd), with a single 32-dimensional hid-\nden layer, which “encodes” xwd into a three-\ndimensional vector. The output of this neural net-\nwork is then transformed via a softmax as follows:\nωwd = softmax\n(\ng(xwd ; φd)\n)\n(5)\nβw = 1 +\n∑\nd∈D\nωwd . (6)\nThe intuition behind βw can be understood by\nappealing to the “pseudocount” interpretation of\nDirichlet parameters. Each lexicon contributes ex-\nactly one pseudocount, divided among positive,\nnegative, and neutral, to what would otherwise be\na symmetric, uniform Dirichlet distribution. As a\nconsequence of this construction, words that ap-\npear in more lexica will have more concentrated\nDirichlets. Intuitively, this property is appealing.\nWe optimize the resulting ELBO objective (Blei\net al., 2017) with respect to the variational parame-\nters via stochastic variational inference (Hoffman\nIMDB 2C Yelp 5C Yelp 3C SemEval 3C MultiDom 2C ACL 5C ACL 3C ICLR 10C ICLR 3C\nSentiVAE EQ[zw] 72.7 49.8 57.5 46.0 70.8 66.7 73.3 92.6 87.0\nSentiVAE βw 73.4 49.7 59.4 52.2 74.7 73.3 80.0 92.6 86.5\nSentiWordNet 63.4 36.0 47.6 32.2 62.0 60.0 53.3 89.1 83.5\nMPQA 65.4 44.0 53.0 29.9 67.4 60.0 53.3 89.1 83.5\nSenticNet 60.5 38.4 43.4 37.2 62.3 60.0 53.3 89.1 83.9\nHu-Liu 67.2 46.6 56.4 31.5 69.4 60.0 53.3 89.1 83.5\nGI 58.4 40.7 47.9 31.3 61.6 60.0 53.3 89.1 83.5\nVADER 71.7 46.8 59.3 38.5 73.5 66.7 66.7 94.3 86.1\nCombined 75.6 51.0 64.1 50.6 75.4 66.7 66.7 93.9 86.1\nTable 3: Classification accuracies for our representation, six lexica, and a straightforward combination thereof.\net al., 2013) using Adam (Kingma and Ba, 2015)\nin the Pyro framework (Bingham et al., 2018). The\nstandard reparameterization trick used in the origi-\nnal VAE does not apply to models with Dirichlet-\ndistributed latent variables, so we use the general-\nized reparameterization trick of Ruiz et al. (2016).\n4 Experiments and Results\nTo evaluate our approach, we first use SentiVAE\nto combine the six lexica described in §2. For\neach word w in the combined vocabulary, we ob-\ntain an estimate of zw by taking the mean of\nQβw(z\nw) = Dir(βw)—i.e., by normalizing βw.\nWe compare this representation to using βw di-\nrectly, because βw contains information about Sen-\ntiVAE’s certainty about the word’s latent polar-\nity value. We evaluate our common latent rep-\nresentation via a text classification task involving\nnine English-language sentiment analysis datasets:\nIMDB (Maas et al., 2011), Yelp (Zhang et al.,\n2015), SemEval 2017 Task 4 (SemEval, Rosen-\nthal et al. (2017)), multi-domain sentiment analysis\n(MultiDom, Blitzer et al. (2007)), and PeerRead\n(Kang et al., 2018) with splits ACL 2017 and ICLR\n2017 (Kang et al., 2018). Each dataset consists of\nmultiple texts (e.g., tweets, articles), each labeled\nwith an overall sentiment (e.g., positive). Descrip-\ntive statistics for each dataset are shown in Tab. 2.\nFor the datasets with more than three sentiment la-\nbels, we consider two versions—the original and a\nversion with only three (bucketed) sentiment labels.\nFor each dataset, we transform each text into an\naverage polarity value using either our represen-\ntation, one of the six lexica,4 or a straightforward\ncombination thereof, where the polarity value for\n4We bucket the upper four and lower four points of\nVADER’s nine-point scale, to yield a three-point scale. With-\nout this bucketing, our representation outperforms VADER\non four of the nine datasets. We do not bucket VADER when\nusing it in SentiVAE or in the straightforward combination.\neach word in the (combined) vocabulary is a 16-\ndimensional vector that consists of a concatenation\nof polarity values. (Unlike SentiVAE, this concate-\nnation does not yield a single sentiment lexicon\nthat retains scale coherence, while achieving maxi-\nmal coverage over words.) Specifically, we replace\neach token with its corresponding polarity value,\nand then average the these values (Go et al., 2009;\nO¨zdemir and Bergler, 2015; Kiritchenko et al.,\n2014). We then use the training portion of the\ndataset to learn a logistic regression classifier to\npredict the overall sentiment of each text from its\naverage polarity value. Finally, we use the testing\nportion to compute the accuracy of the classifier.\nResults. The results in Tab. 3 show that our rep-\nresentation using βw outperforms the individual\nlexica for all but one dataset, and that our repre-\nsentation using the mean of Qβw(zw) outperforms\nthem for six datasets. This is likely because Senti-\nVAE has a richer representation of sentiment than\nany individual lexicon, and it has greater coverage\nover words (see Tab. 4). The results in Tab. 5 sup-\nport the former reason: even when we limit the\nwords in our representation to match those in an\nindividual lexicon, our representation still outper-\nforms the individual lexicon. Unsurprisingly, our\nrepresentation especially outperforms lexica with\nunidimensional scales. We also find that our rep-\nresentation outperforms the straightforward com-\nbination for datasets from domains that are not\nwell supported by the individual lexica (see Tabs. 1\nand 2 for lexicon and dataset sources, respectively).\nBy combining lexica from different domains, our\nrepresentation captures a general notion of senti-\nment that is not tailored to any specific domain.\n5 Conclusion\nWe introduced a generative model of sentiment\nlexica to combine disparate scales into a common\nIMDB SemEval Multi ICLR\nSentiVAE 70 64 81 71\nSentiWordNet 15 14 24 16\nMPQA 10 7 18 9\nSenticNet 40 39 53 45\nHu-Liu 7 5 13 5\nGI 8 7 15 6\nVADER 7 6 13 5\nTable 4: Coverage over words (percentage) by lexicon\nfor the training portions of four of the nine datasets.\nIMDB 2C SemEval 3C\nSV Lex SV Lex\nSentiVAE 74.7 – 72.4 –\nSentiWordNet 70.6 63.4 67.4 55.1\nMPQA 73.5 66.6 62.6 51.8\nSenticNet 74.4 60.9 72.1 59.5\nHu-Liu 73.6 68.4 59.1 51.1\nGI 71.4 59.3 63.8 54.0\nVADER 73.6 73.1 60.9 58.7\nTable 5: Classification accuracies for a 10% validation\nportion of two of the datasets. The first row, labeled\nSentiVAE, contains the classification accuracy for our\nrepresentation using βw. Subsequent (lexicon-specific)\nrows compare our representation (SV), restricted to the\nvocabulary of that lexicon, to the lexicon itself (Lex).\nlatent representation, and realized this model with\na novel multi-view variational autoencoder, called\nSentiVAE. We then used SentiVAE to combine six\ncommonly used English-language sentiment lex-\nica with binary, categorical, and continuous scales.\nVia a downstream text classification task involving\nnine English-language sentiment analysis datasets,\nwe found that our representation outperforms the\nindividual lexica, as well as a straightforward com-\nbination thereof. We also found that our represen-\ntation is particularly efficacious for datasets from\ndomains that are not well-supported by standard\nsentiment lexica. Finally, we note that our approach\nis more general than SentiMerge (Emerson and De-\nclerck, 2014). While SentiMerge can only combine\nsentiment lexica with continuous scales, SentiVAE\nis designed to combine lexica with disparate scales.\n6 Acknowledgements\nWe would like to thank to Adam Forbes for the de-\nsign of Fig. 1. We further acknowledge the support\nof the NVIDIA Corporation with the donation of\nthe Titan Xp GPU used to conduct this research.\nReferences\nNabeela Altrabsheh, Mazen El-Masri, and Hanady\nMansour. 2017. Combining Sentiment Lexicons of\nArabic Terms. In AMCIS. Association for Informa-\ntion Systems.\nStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-\ntiani. 2010. SentiWordNet 3.0: An Enhanced Lex-\nical Resource for Sentiment Analysis and Opinion\nMining. 10(2010):2200–2204.\nEli Bingham, Jonathan P. Chen, Martin Jankowiak,\nFritz Obermeyer, Neeraj Pradhan, Theofanis Kar-\naletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall,\nand Noah D. Goodman. 2018. Pyro: Deep Universal\nProbabilistic Programming. CoRR, abs/1810.09538.\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.\n2017. Variational inference: A review for statisti-\ncians. Journal of the American Statistical Associa-\ntion, 112:859–877.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, Boom-boxes and Blenders:\nDomain Adaptation for Sentiment Classification. In\nProceedings of the 45th Annual Meeting of the As-\nsociation of Computational Linguistics, pages 440–\n447, Prague, Czech Republic. Association for Com-\nputational Linguistics.\nErik Cambria, Daniel Olsher, and Dheeraj Rajagopal.\n2014. SenticNet 3: A Common and Common-\nsense Knowledge Base for Cognition-driven Sen-\ntiment Analysis. In Proceedings of the Twenty-\nEighth AAAI Conference on Artificial Intelligence,\nAAAI’14, pages 1515–1521. AAAI Press.\nGuy Emerson and Thierry Declerck. 2014. Sen-\ntiMerge: Combining Sentiment Lexicons in a\nBayesian Framework. In Proceedings of Work-\nshop on Lexical and Grammatical Resources for\nLanguage Processing, pages 30–38. Association for\nComputational Linguistics and Dublin City Univer-\nsity.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twitter\nSentiment Classification using Distant Supervision.\nProcessing, pages 1–6.\nMatthew D. Hoffman, David M. Blei, Chong Wang,\nand John William Paisley. 2013. Stochastic varia-\ntional inference. Journal of Machine Learning Re-\nsearch, 14(1):1303–1347.\nDirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,\nand Eduard H. Hovy. 2013. Learning Whom to\nTrust with MACE. In Proceedings of the 2013 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1120–1130. Association\nfor Computational Linguistics.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the Tenth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pages 168–177.\nACM.\nC. J. Hutto and Eric Gilbert. 2014. VADER: A Parsi-\nmonious Rule-Based Model for Sentiment Analysis\nof Social Media Text. In Eighth International Con-\nference on Weblogs and Social Media (ICWSM-14).\nDongyeop Kang, Waleed Ammar, Bhavana Dalvi,\nMadeleine van Zuylen, Sebastian Kohlmeier, Ed-\nuard H. Hovy, and Roy Schwartz. 2018. A Dataset\nof Peer Reviews (PeerRead): Collection, Insights\nand NLP Applications. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 1647–1661. Association for Computational\nLinguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nDiederik P. Kingma and Max Welling. 2014. Auto-\nEncoding Variational Bayes. In Proceedings of the\nSecond International Conference on Learning Rep-\nresentations (ICLR).\nSvetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-\nhammad. 2014. Sentiment Analysis of Short Infor-\nmal Texts. Journal of Machine Learning Research,\n50:723–762.\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-\ngeted Aspect-Based Sentiment Analysis via Embed-\nding Commonsense Knowledge into an Attentive\nLSTM. In AAAI, pages 5876–5883. AAAI Press.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning Word Vectors for Sentiment Analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nCanberk O¨zdemir and Sabine Bergler. 2015. A Com-\nparative Study of Different Sentiment Lexica for\nSentiment Analysis of Tweets. In Proceedings of the\nInternational Conference Recent Advances in Natu-\nral Language Processing, pages 488–496. INCOMA\nLtd. Shoumen, BULGARIA.\nElisavet Palogiannidi, Athanasia Kolovou, Fenia\nChristopoulou, Filippos Kokkinos, Elias Iosif, Niko-\nlaos Malandrakis, Haris Papageorgiou, Shrikanth\nNarayanan, and Alexandros Potamianos. 2016.\nTweester at SemEval-2016 Task 4: Sentiment Anal-\nysis in Twitter Using Semantic-Affective Model\nAdaptation. In Proceedings of the 10th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2016), pages 155–163. The Association for Com-\nputer Linguistics.\nVikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-\nardo Hermosillo Valadez, Charles Florin, Luca Bo-\ngoni, and Linda Moy. 2010. Learning From Crowds.\nJournal of Machine Learning Research, 11:1297–\n1322.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 Task 4: Sentiment Analysis in Twit-\nter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 502–518, Vancouver, Canada. Association for\nComputational Linguistics.\nFrancisco R. Ruiz, Michalis K. Titsias, and David M.\nBlei. 2016. The Generalized Reparameterization\nGradient. In Advances in Neural Information Pro-\ncessing Systems, pages 460–468.\nPhilip J. Stone, Robert F. Bales, J. Zvi Namenwirth,\nand Daniel M. Ogilvie. 1962. The general inquirer:\nA computer system for content analysis and retrieval\nbased on the sentence as a unit of information. Be-\nhavioral Science, 7(4):484–498.\nShiliang Sun. 2013. A Survey on Multi-view Learning.\nNeural Computing and Applications, 23(7-8):2031–\n2038.\nTheresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005. Recognizing Contextual Polarity in Phrase-\nLevel Sentiment Analysis. In Proceedings of Hu-\nman Language Technology Conference and Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 347–354, Vancouver, British\nColumbia, Canada. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassification. In Advances in Neural Information\nProcessing Systems, pages 649–657.\n",
      "id": 58966289,
      "identifiers": [
        {
          "identifier": "oai:pure.atira.dk:publications/908a0dbc-3c85-4d18-a8a7-a08b6d256453",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1904.02839",
          "type": "OAI_ID"
        },
        {
          "identifier": "1904.02839",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "10.18653/v1/n19-1065",
          "type": "DOI"
        },
        {
          "identifier": "200821605",
          "type": "CORE_ID"
        },
        {
          "identifier": "322819664",
          "type": "CORE_ID"
        }
      ],
      "title": "Combining Sentiment Lexica with a Multi-View Variational Autoencoder",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:pure.atira.dk:publications/908a0dbc-3c85-4d18-a8a7-a08b6d256453",
        "oai:arxiv.org:1904.02839"
      ],
      "publishedDate": "2019-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://curis.ku.dk/ws/files/240629212/OA_Combining_Sentiment_Lexica.pdf",
        "http://arxiv.org/abs/1904.02839"
      ],
      "updatedDate": "2022-08-25T03:53:34",
      "yearPublished": 2019,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/322819664.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/322819664"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/322819664/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/322819664/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/58966289"
        }
      ]
    },
    {
      "acceptedDate": "2018-02-06T00:00:00",
      "arxivId": "1712.00732",
      "authors": [
        {
          "name": "Guo, Minyi"
        },
        {
          "name": "Hou, Min"
        },
        {
          "name": "Liu, Qi"
        },
        {
          "name": "Wang, Hongwei"
        },
        {
          "name": "Xie, Xing"
        },
        {
          "name": "Zhang, Fuzheng"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/466610688"
      ],
      "createdDate": "2017-12-15T09:10:02",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2018-01-01T00:00:00",
      "abstract": "In online social networks people often express attitudes towards others,\nwhich forms massive sentiment links among users. Predicting the sign of\nsentiment links is a fundamental task in many areas such as personal\nadvertising and public opinion analysis. Previous works mainly focus on textual\nsentiment classification, however, text information can only disclose the \"tip\nof the iceberg\" about users' true opinions, of which the most are unobserved\nbut implied by other sources of information such as social relation and users'\nprofile. To address this problem, in this paper we investigate how to predict\npossibly existing sentiment links in the presence of heterogeneous information.\nFirst, due to the lack of explicit sentiment links in mainstream social\nnetworks, we establish a labeled heterogeneous sentiment dataset which consists\nof users' sentiment relation, social relation and profile knowledge by\nentity-level sentiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework\nto extract users' latent representations from heterogeneous networks and\npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep\nautoencoders to map each user into a low-dimension feature space while\npreserving the network structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommendation in two\nreal-world datasets. The experimental results also prove the efficacy of SHINE\nin cold start scenario.Comment: The 11th ACM International Conference on Web Search and Data Mining\n  (WSDM 2018",
      "documentType": "research",
      "doi": "10.1145/3159652.3159666",
      "downloadUrl": "http://arxiv.org/abs/1712.00732",
      "fieldOfStudy": null,
      "fullText": "SHINE: Signed Heterogeneous Information Network Embedding\nfor Sentiment Link Prediction\nHongwei Wang∗\nShanghai Jiao Tong University\nShanghai, China\nwanghongwei55@gmail.com\nFuzheng Zhang\nMicrosoft Research Asia\nBeijing, China\nfuzzhang@microsoft.com\nMin Hou\nUniversity of Science and Technology\nof China, Hefei, Anhui, China\nhmhoumin@gmail.com\nXing Xie\nMicrosoft Research Asia\nBeijing, China\nxingx@microsoft.com\nMinyi Guo†\nShanghai Jiao Tong University\nShanghai, China\nguo-my@cs.sjtu.edu.cn\nQi Liu\nUniversity of Science and Technology\nof China, Hefei, Anhui, China\nqiliuql@ustc.edu.cn\nABSTRACT\nIn online social networks people often express attitudes towards\nothers, which forms massive sentiment links among users. Predict-\ning the sign of sentiment links is a fundamental task in many areas\nsuch as personal advertising and public opinion analysis. Previous\nworks mainly focus on textual sentiment classification, however,\ntext information can only disclose the “tip of the iceberg” about\nusers’ true opinions, of which the most are unobserved but implied\nby other sources of information such as social relation and users’\nprofile. To address this problem, in this paper we investigate how\nto predict possibly existing sentiment links in the presence of het-\nerogeneous information. First, due to the lack of explicit sentiment\nlinks in mainstream social networks, we establish a labeled het-\nerogeneous sentiment dataset which consists of users’ sentiment\nrelation, social relation and profile knowledge by entity-level sen-\ntiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding\n(SHINE) framework to extract users’ latent representations from\nheterogeneous networks and predict the sign of unobserved sen-\ntiment links. SHINE utilizes multiple deep autoencoders to map\neach user into a low-dimension feature space while preserving the\nnetwork structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommen-\ndation in two real-world datasets. The experimental results also\nprove the efficacy of SHINE in cold start scenario.\nACM Reference Format:\nHongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, and Qi\nLiu. 2018. SHINE: Signed Heterogeneous Information Network Embedding\nfor Sentiment Link Prediction. In Proceedings of the 11th ACM International\nConference on Web Search and Data Mining (WSDM’18). ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3159652.3159666\n1 INTRODUCTION\nThe past decade has witnessed the proliferation of online social\nnetworks such as Facebook, Twitter and Weibo. In these social\nnetwork sites, people often share feelings and express attitudes\ntowards others, e.g., friends, movie stars or politicians, which forms\n∗This work is done while H. Wang and M. Hou are visiting Microsoft Research Asia.\n†M. Guo is the corresponding author.\nWSDM’18, February 5–9, 2018, Marina Del Rey, CA, USA\n2018. ACM ISBN 978-1-4503-5581-0/18/02. . . $15.00\nhttps://doi.org/10.1145/3159652.3159666\nsentiment links among these users. Different from explicit social\nlinks indicating friend or follow relationship, sentiment links are\nimplied by the semantic content posted by users, and involve dif-\nferent types: positive sentiment links express like, trust or support\nattitudes, while negative sentiment links signify dislike or disap-\nproval of others. For example, a tweet saying “Vote Trump!” shows\na positive sentiment link from the poster to Donald Trump, and\n“Trump is mad...” indicates the opposite case.\nFor a given sentiment link, we define its sign to be positive\nor negative depending on whether its related content expresses\na positive or negative attitude from the generator of the link to\nthe recipient [14], and all such sentiment links form a new net-\nwork topology called sentiment network. Previous work [6, 11, 15]\nmainly focuses on sentiment classification based on the concrete\ncontent posted by users. However, they cannot detect the existence\nof sentiment links without any prior content information, which\ngreatly limits the number of possible sentiment links that could be\nfound. For example, if a user does not post any word concerning\nTrump, it is impossible for traditional sentiment classifiers to ex-\ntract the user’s attitude towards him because “one cannot make\nbricks without straw”. Therefore, a fundamental question is, can\nwe predict the sign of a given sentiment link without observing its\nrelated content? The solution to this problem will benefit a great\nmany online services such as personalized advertising, new friends\nrecommendation, public opinion analysis, opinion polls, etc.\nDespite the great importance, there is little prior work concern-\ning predicting the sign of sentiment links among users in social\nnetworks. The challenges are two-fold. On the one hand, lack of\nexplicit sentiment labels makes it difficult to determine the polarity\nof existing and potential sentiment links. On the other hand, the\ncomplexity of sentiment generation and the sparsity of sentiment\nlinks make it hard for algorithms to achieve desirable performance.\nRecently, several studies [12, 14, 31, 35] propose methods to solve\nthe problem of predicting signed links. However, they rely heavily\non manually designed features and cannot work well in real-world\nscenarios. Another promising approach called network embedding\n[8, 17, 23, 26], which automatically learns features of users in net-\nwork, seems plausible to solve the task. However, they can only\napply to networks with positive-weighted (i.e., unsigned) and single-\ntype (i.e., homogeneous) edges, which limits their power in the task\nof practical sentiment link prediction.\nar\nX\niv\n:1\n71\n2.\n00\n73\n2v\n1 \n [s\ntat\n.M\nL]\n  3\n D\nec\n 20\n17\nAlice\nFemale\nCalifornia\n…\nBob\nMale\nTexas\n…\nDonald Trump\nMale\nPolitician\nAmerican\n1946\n…\nEmma Watson\nFemale\nActress\nBritish\n1990\n…\nAlbert Einstein\nMale\nScientist\nGerman\n1879\n…\npositive sentiment link\nnegative sentiment link\nsocial link\nFig. 1: Illustration of a snippet of heterogeneous networks\nwith sentiment, social relationship and user profile.\nBased on the above facts, in this paper we investigate the prob-\nlem of predicting sentiment links in absence of sentiment related\ncontent in online social networks. Our work is two-step. First, con-\nsidering the lack of labeled data, we establish a labeled sentiment\ndataset fromWeibo, one of the most popular social network sites in\nChina. We leverage state-of-the-art entity-level sentiment extrac-\ntion method to calculate the sentiment of the poster towards the\ncelebrity in each tweet. Besides, to handle the sparsity problem, we\ncollect two additional types of side information: social relationship\namong users and profile knowledge of users and celebrities. Our\nchoices are enlightened by [27] and [34], respectively, in which\n[27] demonstrates that the structural information of social net-\nworks can greatly affect users’ preference towards online items,\nand [34] proves that information from knowledge base could boost\nthe performance of recommendation. The heterogeneous informa-\ntion networks are illustrated in Fig. 1.\nTo explore more possible sentiment links from the network, in\nthe second step, we propose a novel end-to-end framework termed\nas Signed Heterogeneous Information Network Embedding (SHINE).\nGreatly different from existing network embedding approaches,\nSHINE is able to learn user representation and predict sentiment\nfrom signed heterogeneous networks. Specifically, SHINE adopts\nmultiple deep autoencoders [20], a type of deep-learning-based\nembedding technique, to extract users’ highly nonlinear represen-\ntations from the sentiment network, social network and profile\nnetwork, respectively. The learned three types of user represen-\ntations are subsequently fused together by specific aggregation\nfunction for further sentiment prediction. In addition to the adapt-\nability to signed heterogeneous networks, the superiority of SHINE\nalso lies in its end-to-end prediction technology and high flexibil-\nity of adding or removing modules of side information (i.e., social\nrelationship and profile knowledge), which is discuss in Section 5.\nWe conduct extensive experiments on two real-world datasets.\nThe results show that SHINE achieves substantial gains compared\nwith baselines. Specifically, SHINE outperforms other strong base-\nlines by 8.8% to 16.8% in the task of link prediction on Accuracy,\nand by 17.2% to 219.4% in the task of node recommendation on\nRecall@100 for positive nodes. The results also prove that SHINE\nis able to utilize the side information efficiently, and maintains a\ndecent performance in cold start scenario.\n2 RELATEDWORK\n2.1 Signed Link Prediction\nOur problem of predicting positive and negative sentiment links\nconnects to a large body of work on signed social networks, in-\ncluding trust propagation [9], spectral analysis [13], and social me-\ndia mining [22]. For the link prediction problem in signed graphs,\nLeskovec. et al. [14] adopt signed triads as features for prediction\nbased on structural balance theory. Ye et al. [31] utilize transfer\nlearning to leverage edge sign information from source network\nand improve prediction accuracy in target network. Tang et al. de-\nsign NeLP framework [21] which exploits positive links in social\nmedia to predict negative links. The difference between the above\nwork and ours is that we construct a labeled dataset by entity-level\nsentiment extraction method, as there is no explicit signed links in\nmainstream online social networks. Besides, we use state-of-the-art\ndeep learning approach to learn the representation of links.\n2.2 Network Embedding\nThere is a long history of work on network embedding. Earlier\nworks such as IsoMap [24] and Laplacian Eigenmap [1] first con-\nstruct the affinity graph of data using the feature vectors and then\nembed the affinity graph into a low-dimension space. Recently,\nDeepWalk [17] deploys random walk to learning representations\nof social network. LINE [23] proposes objective functions that pre-\nserve both local and global network structures for network embed-\nding. Node2vec [8] designs a biased randomwalk procedure to learn\na mapping of nodes that maximizes the likelihood of preserving\nnetwork neighborhoods of nodes. SDNE [26] uses autoencoder to\ncapture first-order and second-order network structures and learn\nuser representation. However, these methods can only address un-\nsigned and homogeneous networks. Additionally, several studies\nfocus on representation learning in the scenario of heterogeneous\nnetwork [3, 32], attributed network [10], or signed network [29, 33].\nHowever, these methods are specialized in only one particular type\nof networks, which is not applicable to the problem of sentiment\nprediction in real-world signed and heterogeneous networks.\n3 DATASET ESTABLISHMENT\nIn this section we introduce the process of collecting data from\nonline social networks, and discuss the details of how to extract\nsentiment towards celebrities from tweets.\n3.1 Data Collection\n3.1.1 Weibo Tweets. We select Weibo1 as the online social net-\nworks studied in this work. Weibo is one of the most popular social\nnetwork sites in China which is akin to a hybrid of Facebook and\nTwitter. We collected 2.99 billion tweets on Weibo from August 14,\n2009 to May 23, 2014 as raw dataset. To filter out useful data which\ncontains sentiment towards celebrities, we first apply Jieba2, the\nmost popular Chinese text segmentation tool, to tag the part of\nspeech (POS) of each word for each tweet. Then we select those\ntweets containing words with POS tagging as “person name” which\nexist in our established celebrity database (detailed in Section 3.1.4).\n1http://weibo.com\n2https://github.com/fxsjy/jieba\nTable 1: Statistics of Weibo sentiment datasets. “celebrities\nv.” means the celebrities owning verified accounts onWeibo.\n# users 12,814 # social links 71,268\n# celebrities 1,723 # tweets 126,380\n# celebrities v. 706 # pos. tweets 108,906\n# ordinary users 11,091 # neg. tweets 17,474\nAfter getting the set of candidate tweets, for each tweet we calculate\nits sentiment value (-1 to +1) towards the mentioned celebrities,\nand select those tweets with high absolute sentiment values. The\nfinal dataset consists of a set of triples (a,b, s), where a is the user\nwho posts the tweet, b is the certain celebrity mentioned in the\ntweet, and s ∈ {+1,−1} is the sentiment polarity of user a towards\nuser b. The method of calculating sentiment values is detailed in\nSection 3.2.\n3.1.2 Social Relation. In addition to the sentiment dataset, we\nalso collect the social relation among users fromWeibo. The dataset\nof social relation consists of tuples (a,b), where a is the follower\nand b is the followee.\n3.1.3 Profile of Ordinary Users. The profile of ordinary users\nare collected from Weibo. For each ordinary user, we extract two of\nhis attributes, gender and location, as his profile information. The\nattribute values are represented as one-hot vectors.\n3.1.4 Profile of Celebrities. We use Microsoft Satori3 knowledge\nbase to extract profile of celebrities. First, we traverse the knowledge\nbase and select terms with object type as “person”. Then we filter\nout popular celebrities with high edit frequency in knowledge base\nand high appearance frequency in Weibo tweets. For each of these\n“hot” celebrities, we extract 9 attributes as his profile information:\nplace of birth, date of birth, ethnicity, nationality, specialization, gen-\nder, height, weight, and astrological sign. Values of these attributes\nare discretized so that every celebrity’s attribute values can be ex-\npressed as one-hot vectors. Furthermore, we remove celebrities\nwith ambiguous names as well as other noises.\n3.2 Sentiment Extraction\nTo extract users’ sentiment towards celebrities in tweets, we first\ngenerate a sentiment lexicon consisting of words and their sentiment\norientation (SO) scores. To achieve this, we manually construct a\nemoticon-sentiment mapping file and map each tweet to positive\nor negative class according to the label of emoticon appeared in\nthe tweet. For example, “I love Kobe! [kiss]” is mapped to positive\nclass if the key-value pair ([kiss], positive) exists in the emoticon-\nsentiment mapping file. Note that the class of emoticon cannot\nbe directly regarded as the sentiment towards celebrities since we\nfound a large number of mismatch cases, e.g., “Miss you Taylor Swift\n[cry][cry]”. Afterwards, for each word (segmented by Jieba) with\noccurrence frequency from 2,000 to 10,000,000 in the raw tweets\ndatasets, similar to [2], we calculate its SO score as\nSO(word) = PMI (word,pos) − PMI (word,neд), (1)\nwhere PMI is the point-wise mutual information [25] defined as\nPMI (x ,y) = log p(x,y)p(x )p(y) , pos and neд are the tweets of positive and\n3http://searchengineland.com/library/bing/bing-satori\n+1\n+1\n+1\n？\n-1\n-1\n(a) Sentiment network (b) Social network\ngender\nnationality\nspecialization\n(c) Profile network\nFig. 2: Illustration of the three studied networks.\nnegative class, respectively. SO scores are subsequently normalized\nto [−1, 1].\nAfter getting the lexicon, we use SentiCircle [19] to calculate\nsentiment towards celebrities in each tweet. Given a piece of tweet\nas well as the mentioned celebrity, we represent the contextual\nsemantics of the celebrity as a polar coordinate space, where the\ncelebrity is situated in the origin and other terms in the tweet are\nscattered around. Specifically, for celebrity term c , the coordinate\nof term ti is (ri ,θi ), where ri is the inverse of distance between\nc and ti in syntax dependence graph generated by LTP [4], and\nθi = SO(ti ) · π . The overall sentiment towards the celebrity c is,\ntherefore, approximated as the geometric center of all terms ci .\nWe take the projection of the geometric center on y-axis as final\nsentiment value towards the celebrity.\nTo validate the effectiveness of sentiment extraction, we ran-\ndomly select 1,000 tweets (500 positive and 500 negative tagged\nby our method) in Weibo sentiment dataset, and manually label\neach one of them. The result shows that the precision is 95.2% for\npositive class and 91.0% for negative class, which we believe is\naccurate enough for subsequent experiments. The basic statistics\nof Weibo sentiment datasets is presented in Table 1.\n4 PROBLEM FORMULATION\nIn this section we formulate the problem of predicting sentiment\nlinks in heterogeneous information networks. For better illustration,\nwe split the original heterogeneous network into the following three\nsingle-type networks:\nSentiment network. The directed sentiment network is denoted\nas Gs = (V , S), where V = {1, ..., |V |} represents the set of users\n(either ordinary users or celebrities) and S = {si j | i ∈ V , j ∈ V }\nrepresents sentiment links among users. Each si j can take the value\nof +1, −1 or 0, representing that user i holds a positive, negative,\nor unobserved sentiment towards user j, respectively.\nSocial network. The directed social network is denoted asGr =\n(V ,R), where R = {ri j | i ∈ V , j ∈ V } represents social links among\nusers. Each ri j can take the value of 1 or 0, representing that user i\nfollows user j or not in the social network.\nProfile network. We denote A = {A1, ...,A |A |} the set of\nuser’s attributes, and akl ∈ Ak the l-th possible value of attribute\nAk . We take the union of all possible values of attributes and renum-\nber them as U =\n⋃\nAk = {aj | j = 1, ...,\n∑\nk |Ak |}. Then the undi-\nrected bipartite profile network can be denoted as Gp = (V ,U , P),\nwhere P = {pi j | i ∈ V ,aj ∈ U } represents profile links between\nusers and attribute values. Each pi j can take the value of 1 or 0,\nrepresenting that user i possesses attribute value j or not.\nBob\nTrump\n“Vote Trump!”\nBob\n… … ……\nsentiment embedding\nsentiment autoencoder\nsentiment embedding\n… … ……\nsocial embedding\nsocial autoencoder\n… … ……\nprofile embedding\nprofile autoencoder\nheterogeneous \nembedding\nheterogeneous \nembedding\naggregation\naggregation\n+1\nsocial embedding\nprofile embedding\n𝑓(∙,∙)\nҧ𝑠\npredicted \nsentiment\n+1\ntarget\ntraining\nsentiment \nextraction\ntweet\nsentiment link\nsentiment network\nprofile network\nsocial network\nFig. 3: Framework of the end-to-end SHINE model. To clearly demonstrate the model, we only show the encoder part of all\nthe three autoencoders and leave out the decoder part in this figure.\nThe three networks are illustrated in Fig. 2.\nSentiment links prediction. We define the problem of pre-\ndicting sentiment links in heterogeneous information networks as\nfollows: Given the sentiment network Gs , social network Gr and\nprofile networkGp , we aim to predict the sentiment of unobserved\nlinks between users in Gs .\n5 SIGNED HETEROGENEOUS INFORMATION\nNETWORK EMBEDDING\nIn this section we introduce the proposed SHINE model. We first\nshow the whole framework of SHINE. Then we present the details\nof the SHINE model, including how to extract user representation\njointly from the three networks as well as the learning algorithm.\nAt last we give some discussions on the model.\n5.1 Framework\nIn this paper we propose an end-to-end SHINE model to predict\nsentiment links. The framework of SHINE is shown in Fig. 3. In\ngeneral, the whole framework consists of three major components:\nsentiment extraction and heterogeneous networks construction\n(the left part), user representation extraction (the middle part), as\nwell as representation aggregation and sentiment prediction (the\nright part). For each tweet mentioning a specific celebrity, we first\ncalculate the associated sentiment (discussed in Section 3.1), and\nrepresent the user and the celebrity in this sentiment link by us-\ning their neighborhood information from the three constructed\nnetworks (introduced in Section 4). We then design three distinct\nautoencoders to extract short and dense embeddings from origi-\nnal sparse neighborhood-based representation respectively, and\naggregate these three kinds of embeddings into final heterogeneous\nembedding. The predicted sentiment can thus be calculated by ap-\nplying specific similarity measurement function (e.g., inner product\nor logistic regression) to the two heterogeneous embeddings, and\nthe whole model can be trained based on the predicted sentiment\nand the target (i.e., the ground truth obtained in sentiment extrac-\ntion step). In the following subsections we will introduce SHINE\nmodel in detail.\n5.2 Sentiment Network Embedding\nGiven the sentiment graph Gs = (V , S), for each user i ∈ V , we\ndefine its sentiment adjacency vector xi = {si j | j ∈ V } ∪ {sji | j ∈\nV }. Note that xi fully contains the global incoming and outgoing\nsentiment information of user i . However, it is impractical to take xi\ndirectly as the sentiment representation of user i , as the adjacency\nvector is too long and sparse for further processing. Recently, a lot of\nnetwork embedding models [8, 17, 23, 26] are proposed, which aim\nto learn low-dimension representations of vertices while preserving\nthe network structure. Among those models, deep autoencoder is\nproved to be one of state-of-the-art solutions, as it is able to capture\nhighly nonlinear network structure by using deep models [26].\nIn general, autoencoder [20] is an unsupervised neural network\nmodel of codings aiming to learn a representation of a set of data.\nAutoencoder consists of two parts, the encoder and the decoder,\nwhich contains multiple nonlinear functions (layers) for mapping\nthe input data to representation space and reconstructing original\ninput from representation, respectively. In our SHINE model, we\npropose to use autoencoders for efficiently user representation\nlearning.\nFig. 4 illustrates the autoencoder for sentiment network embed-\nding. As shown in Fig. 4, the sentiment autoencoder maps each\nuser to a low-dimension latent representation space and recover\noriginal information from latent representation by using multiple\nfully-connected layers. Given the input xi , the hidden representa-\ntions for each layer are\nxki = σ\n(\nWks x\nk−1\ni + b\nk\ns\n)\n, k = 1, 2, ...,Ks , (2)\nwhere Wks and bks are weight and bias parameters of layer k in the\nsentiment autoencoder, respectively, σ (·) is the nonlinear activation\nfunction, Ks is the number of layers of sentiment autoencoder, and\nx0i = xi . For simplicity, we denote x\n′\ni = x\nKs\ni the reconstruction of\nxi .\nThe basic goal of the autoencoder is to minimize the reconstruc-\ntion loss between input and output representations. Similar to [26],\nin SHINE model the reconstruction loss term of sentiment autoen-\ncoder is defined as\nLs =\n∑\ni ∈V\n\r\r(xi − x′i ) ⊙ li \r\r22 , (3)\nsentiment \nnetwork\n…\n…\n… …… …\nsentiment \nadjacency \nvector\n… …\nhidden layers hidden layers\nreconstructed \nsentiment \nadjacency vector\nsentiment \nembedding\n𝑊𝑠\n1 𝑊𝑠\n2 𝑊𝑠\n3 𝑊𝑠\n6𝑊𝑠\n5𝑊𝑠\n4\n𝐱𝑖\n1\n𝐱𝑖\n𝐱𝑖\n2\n𝐱𝑖\n3\n𝐱𝑖\n4 𝐱𝑖\n5\n𝐱𝑖\n6\nuser 𝑖\nFig. 4: Illustration of a 6-layer autoencoder for sentiment\nnetwork embedding.\nwhere ⊙ denotes theHadamard product, and li = (li,1, li,2, ..., li,2 |V |)\nis the sentiment reconstruction weight vector in which\nli, j =\n{\nα > 1, i f si j = ±1;\n1, i f si j = 0.\n(4)\nThe meaning of the above loss term lies in that we impose more\npenalty to the reconstruction error of the non-zero elements than\nthat of zero elements in input xi, as a non-zero si j carries more\nexplicit sentiment information than an implicit zero si j . Note that\nthe sentiment embedding of user i can be obtained from the layer\nKs/2 in the sentiment autoencoder, and we denote x̂i = xKs /2i the\nsentiment embedding of user i for simplicity.\n5.3 Social Network Embedding\nSimilar to previous sentiment network embedding, we apply au-\ntoencoder to extract user representation from the social network.\nGiven the social network Gr = (V ,R), for each user i ∈ V , we\ndefine its social adjacency vector yi = {ri j | j ∈ V } ∪ {r ji | j ∈ V },\nwhich fully contains the structural information of user i in the\nsocial network. The hidden representations of each layer in the\nsocial autoencoder are\nyki = σ\n(\nWkr y\nk−1\ni + b\nk\nr\n)\n, k = 1, 2, ...,Kr , (5)\nwhere the meaning of notations are similar to those in Eq. (2).\nWe also denote y′i = y\nKr\ni the reconstruction of yi . Similarly, the\nreconstruction loss term of social autoencoder is\nLr =\n∑\ni ∈V\n\r\r(yi − y′i ) ⊙ mi \r\r22 , (6)\nwhere mi = (mi,1,mi,2, ...,mi,2 |V |) is the social reconstruction\nweight vector in which if ri j = 1,mi, j = α > 1, elsemi, j = 1. The\nsocial embedding of user i is denoted as ŷi = yKr /2i .\n5.4 Profile Network Embedding\nThe profile networkGp = (V ,U , P) is an undirected bipartite graph\nwhich consists of two disjoint sets of users and attribute values.\nFor each user i ∈ V , its profile adjacency vector is defined as\nzi = {pi j | j ∈ U }. User i’s hidden representations of each layer in\nthe profile autoencoder are\nzki = σ\n(\nWkp z\nk−1\ni + b\nk\np\n)\n, k = 1, 2, ...,Kp , (7)\nwhere the meaning of notations are similar to those in Eq. (2). We\nalso use the notation z′i to denote the reconstruction of zi . Therefore,\nthe reconstruction loss term of profile autoencoder is\nLp =\n∑\ni ∈V\n\r\r(zi − z′i ) ⊙ ni \r\r22 , (8)\nwhere ni is the profile reconstruction weight vector defined sim-\nilarly to mi in the previous subsection. The profile embedding of\nuser i is denoted as ẑi = z\nKp/2\ni .\n5.5 Representation Aggregation and Sentiment\nPrediction\nOnce we obtain the sentiment embedding x̂i , social embedding ŷi ,\nand profile embedding ẑi of user i , we can aggregate these embed-\ndings into final heterogeneous embedding ei by specific aggregation\nfunctionд(·, ·, ·). We list some of the available aggregation functions\nas follows:\n• Summation [34], i.e., ei = x̂i + ŷi + ẑi ;\n• Max pooling [28], i.e., ei = element-wise-max (̂xi , ŷi , ẑi );\n• Concatenation [23], i.e., ei = ⟨̂xi , ŷi , ẑi ⟩.\nFinally, given two users i and j as well as their heterogeneous\nembedding ei and ej , the predicted sentiment s¯i j can be calculated\nas s¯i j = f (i, j), where f (·, ·) is specific similarity measurement\nfunction. For example:\n• Inner product [3, 5], i.e., s¯i j = eTi ej + b, where b is a trainable\nbias parameter;\n• Euclidean distance [26], i.e., s¯i j = −∥ei − ej ∥2 +b, where b is a\ntrainable bias parameter;\n• Logistic regression [17], i.e., s¯i j = WT⟨ei , ej ⟩+b, whereW and\nb are trainable weights and bias parameters.\nWe will study the choices of f and д in the experimental part.\n5.6 Optimization\nThe complete objective function of SHINE model is as follows:\nL =\n∑\ni ∈V\n\r\r(xi − x′i ) ⊙ li \r\r22 + λ1∑i ∈V \r\r(yi − y′i ) ⊙ mi \r\r22\n+ λ2\n∑\ni ∈V\n\r\r(zi − z′i ) ⊙ ni \r\r22 + λ3∑si j=±1 ( f (ei , ej ) − si j )2\n+ λ4Lr eд ,\n(9)\nwhere λ1, λ2, λ3 and λ4 are balancing parameters. The first three\nterms in Eq. (9) are the reconstruction loss terms of sentiment au-\ntoencoder, social autoencoder, and profile autoencoder, respectively.\nThe fourth term in Eq. (9) is the supervised loss term for penaliz-\ning the divergence between predicted sentiment and ground truth.\nThe last term in Eq. (9) is the regularization term that prevents\nover-fitting, i.e.,\nLr eд =\nKs∑\nk=1\n\r\rWks \r\r22 + Kr∑\nk=1\n\r\rWkr \r\r22 + Kp∑\nk=1\n\r\rWkp \r\r22 + \r\rf \r\r22, (10)\nwhere Wks , Wkr , Wkp are the weight parameters of layer k in the\nsentiment autoencoder, social autoencoder, and profile autoencoder,\nrespectively, and ∥ f ∥22 is the regularization penalty for similarity\nmeasurement function f (·, ·) (if appropriate).\nWe employ the AdaGrad [7] algorithm to minimize the objective\nfunctions in Eq. (9). In each iteration, we randomly select a batch\nof sentiment links from training dataset and compute the gradient\nof the objective function with respect to each trainable parameter\nrespectively. Then we update each trainable parameter according\nto the AdaGrad algorithm till convergence.\n5.7 Discussions\n5.7.1 Asymmetry. Many real-world networks are directed, which\nimplies that for two nodes i and j in the network, edges (i, j) and\n(j, i) may coexist and their values are not necessarily identical. A\nfew recent studies have focused on this asymmetry issue [16, 36]. In\nthis work, whether the basic SHINE model can characterize asym-\nmetry depends on the choice of similarity measurement function f .\nSpecifically, SHINE is capable of dealing with the direction of a link\nif and only if f (i, j) , f (j, i) (e.g., logistic regression). However (and\nfortunately), even if we choose a symmetric function (e.g., inner\nproduct or Euclidean distance) as f , we can still easily extend the\nbasic SHINE model to asymmetry-aware version by setting two\ndistinct sets of autoencoders to extract representation of source\nnode and target node respectively. From this point of view, in basic\nSHINE model the parameters of autoencoders are actually shared\nfor source node and target node to alleviate over-fitting, and we\ncan choose to explicitly distinguish the two sets of autoencoders\nfor asymmetry reasons.\n5.7.2 Cold start problem. A practical issue for network embed-\nding is how to learn representations for newly arrived node, which\nis the cold start problem. Almost all existing models cannot work\nwell in cold start scenario because they only use the information\nfrom the target network (e.g., sentiment network in this paper),\nwhich is not applicable for the newly arrived node who has little in-\nteraction with the existing target network. However, SHINE is free\nof the cold start problem, as it makes full use of side information\nand incorporate it naturally into the target network when learning\nuser representations. We will further study the performance of\nSHINE in cold start scenario in the experiment part.\n5.7.3 Flexibility. It is worth noticing that SHINE is also a frame-\nwork with high flexibility. For any other new available side infor-\nmation of users (e.g., users’ browsing history), we can easily design\na new parallel processing component and “plug” it in the original\nSHINE framework to assist learning representation. Contrarily, we\ncan also “pull out” social autoencoder or profile autoencoder from\nSHINE framework if such side information is unavailable. Besides,\nthe flexibility of SHINE also lies in that one can choose different\naggregation functions д and similarity measurement functions f ,\nas discussed in Section 5.5.\n6 EXPERIMENTS\nIn this section, we evaluate the performance of our proposed SHINE\non real-world datasets. We first introduce the datasets, baselines,\nand parameter settings for experiments, then present the experi-\nmental results of SHINE and baselines.\n6.1 Datasets\nTo comprehensively demonstrate the effectiveness of SHINE frame-\nwork, we use the following two datasets for experiments:\n• Weibo-STC: Our proposed Weibo Sentiment Towards Celebri-\nties dataset consists of three heterogeneous networks with\n12,814 users, 126,380 tweets, 71,268 social links and 37,689\nprofile values, of which the detail is presented in Section 3.\n• Wiki-RfA: Wikipedia Requests for Adminship [30] is a signed\nnetwork with 10,835 nodes and 159,388 edges, corresponding\nto votes cast by Wikipedia uses in election for promoting\nindividuals to the role of administrator. A signed link indicates\na positive or negative vote by one user on the promotion\nof another. Note that Wiki-RfA does not contain any side\ninformation of nodes, therefore, this dataset is used to validate\nthe efficacy of the basic sentiment autoencoder in SHINE.\n6.2 Baselines\nWe use the following five methods as baselines, in which the first\nthree are network embedding methods, FxG is a signed link predic-\ntion approach, and LIBFM is a generic classification model. Note\nthat the first three methods are not directly applicable to signed\nheterogeneous networks, so we use them to learn user representa-\ntions from positive and negative part of each network respectively,\nand concatenate them to form the final embeddings. For FxG on\nWeibo-STC dataset, we only use the sentiment network as input\nbecause the FxG model cannot utilize the side information of nodes.\n• LINE: Large-scale Information Network Embedding [23] de-\nfines loss functions to preserve the first-order and second-\norder proximity and learns representations of vertices.\n• Node2vec: Node2vec [8] designs a biased random walk proce-\ndure to learn a mapping of nodes that maximizes the likelihood\nof preserving network neighborhoods of nodes.\n• SDNE: Structural Deep Network Embedding [26] is a semi-\nsupervised network embedding model using autoencoder to\ncapture local and global structure of target networks.\n• FxG: Fairness and Goodness [12] predicts the weights of edges\nin weighted signed networks by introducing two measures of\nnode behavior: goodness (i.e., how much the node is liked by\nother nodes) and fairness (i.e., how fair the node is in rating\nother nodes’ likeability).\n• LIBFM: LIBFM [18] is a state-of-the-art feature based factor-\nization model. In this paper, we use the concatenated one-hot\nvectors of users in three networks as input to feed LIBFM.\n6.3 Parameter Setttings\nWe design a 4-layer autoencoder in SHINE for each network, in\nwhich the hidden layer is with 1,000 units and the embedding layer\nis with 100 units. Deeper architectures cannot further improve the\nperformance but incur heavier computational overhead according\nto our experimental results. We choose concatenation as the aggre-\ngation function д and inner product as the similarity measurement\nfunction f . Besides, we set the reconstruction weight of non-zero\nelements α = 10, the balancing parameters λ1 = 1, λ2 = 1, λ3 = 20,\nand λ4 = 0.01 for SHINE. We will study the sensitivity of these\nparameters in Section 6.6. For LINE, we concatenate the first-order\nand second-order representations to form the final 100-dimension\nembeddings for each node, and the total number of samples is 100\nmillion. For node2vec, the number of embedding dimension is set\nas 100. For SDNE, the reconstruction weight of non-zero elements\nis 10 and the weight of first-order term is 0.05. For LIBFM, the\ndimensionality of the factorization machine is set as {1, 1, 0} and\n0 5 10\n \n SHINE LINE node2vec SDNE FxG LIBFM\n0.2 0.4 0.6 0.8 1.0\n0.6\n0.7\n0.8\n0.9\nPercentage of training set\nAc\ncu\nra\ncy\n \n \n(a) Accuracy on Weibo-STC\n0.2 0.4 0.6 0.8 1.00.6\n0.7\n0.8\n0.9\nPercentage of training set\nM\nic\nro\n−F\n1\n \n \n(b) Micro-F1 on Weibo-STC\n0.2 0.4 0.6 0.8 1.00.65\n0.70\n0.75\n0.80\n0.85\nPercentage of training set\nAc\ncu\nra\ncy\n \n \n(c) Accuracy on Wiki-RfA\n0.2 0.4 0.6 0.8 1.00.65\n0.70\n0.75\n0.80\n0.85\n0.90\nPercentage of training set\nM\nic\nro\n−F\n1\n \n \n(d) Micro-F1 on Wiki-RfA\nFig. 5: Accuracy and micro-F1 on Weibo-STC and Wiki-RfA for link prediction.\nwe use SGD method for training with learning rate of 0.5 and 200\niterations. Other parameters in these baselines are set as default.\nIn the following subsections, we conduct experiments on two\ntasks: link prediction and node recommendation.\n6.4 Link Prediction\nIn link prediction setting, our task is to predict the sign of an unob-\nserved link between two given nodes. As the existing links in the\noriginal network are known and can serve as the ground truth, we\nrandomly hide 20% of links in the sentiment network and select a\nbalanced test set (i.e., the number of positive links is the same as\nnegative links) out of them, while use the remaining network to\ntrain SHINE as well as all baselines. We use Accuracy and Micro-F1\nas the evaluation metrics in link prediction task. For a more fine-\ngrained analysis, we compare the performance while varying the\npercentage of training set from 10% to 100%. The result is presented\nin Fig. 5, from which we have the following observations:\n• Fig. 5 shows that our methods SHINE achieves significant im-\nprovements in Accuracy and Micro-F1 over the baselines in\nboth datasets. Specifically, in Weibo-STC, SHINE outperforms\nLINE, node2vec, and SDNE by 13.8%, 16.2%, and 8.78% respec-\ntively on Accuracy, and achieves 15.5%, 17.6%, 9.71% gains\nrespectively on Micro-F1.\n• Among the three state-of-the-art network embeddingmethods,\nSDNE performs best while LINE and node2vec show relatively\npoor performance. Note that SDNE also uses autoencoder to\nlearning the embedding of nodes, which proves the superiority\nof autoencoder in extracting highly nonlinear representations\nof networks from a side.\n• FxG performs much better in Wiki-RfA than in Weibo-STC.\nThis is probably due to the following two reasons: 1) Unlike\nother methods, FxG cannot utilize the side information in\nWeibo-STC dataset. 2) Weibo-STC is sparser than Wiki-RfA,\nwhich is unfavorable to the computing of goodness and fair-\nness of nodes in FxG model.\n• Although LIBFM is not specially designed for network-structured\ndata, it still achieves fine performance compared with other\nnetwork embedding methods. However, during experiments\nwe find that LIBFM is unstable and prone to parameters tuning.\nThis can also be validated by the fluctuating curves of LIBFM\nin Fig. 5c and Fig. 5d.\nTo compare the performance of SHINE and baselines in cold\nstart scenario, we construct a test set of newly arrived users for\nTable 2: Comparison of models in terms of Accuracy and\nMicro-F1 on Weibo-STC in cold start scenario.\nModel Accuracy Micro-F1all users new users all users new users\nSHINE 0.855 0.834 0.881 0.858\nLINE 0.751 0.664 0.763 0.739\nnode2vec 0.736 0.653 0.749 0.667\nSDNE 0.786 0.667 0.803 0.751\nFxG 0.732 0.601 0.765 0.652\nLIBFM 0.748 0.639 0.802 0.746\nWeibo-STC, in which the associated ordinary user of each senti-\nment link dose not appear in the training set. We report Accuracy\nand Micro-F1 for all users and new users in Table 2. From the results\nin Table 2 it is evident that SHINE can still maintain a decent perfor-\nmance in the cold start scenario, as it fully exploits the information\nfrom social network and profile network to compensate for the\nlack of sentiment links. By comparison, the performance of other\nbaselines degrades significantly in cold start scenario. Specifically,\nthe Accuracy decreases by 2.46% for SHINE and by 11.58%, 11.28%,\n15.14%, 17.90%, 14.57% respectively for LINE, node2vec, SDNE, FxG\nand LIBFM, which proves that SHINE are more capable of effec-\ntively transferring knowledge among heterogeneous information\nnetworks, especially in cold start scenario.\n6.5 Node Recommendation\nIn addition to link prediction, we also conduct experiments on node\nrecommendation, in which for each user we aim to recommend a set\nof users who have not been explicitly expressed attitude to but may\nbe liked by the user. The performance of node recommendation can\nreveal the quality of learned representations as well. Specifically, for\neach user, we calculate his sentiment score toward all other users,\nand selectK users with largest sentiment score for recommendation.\nFor completeness, we recommend not only the nodes that a user\nmay like but also the nodes that he may dislike. Therefore, we use\npositive and negative Precision@K and Recall@K respectively for\nevaluation in corresponding experimental scenarios. The results\nare shown in Fig. 6, which provides us the following observations:\n• The curve of SHINE is almost consistently above the curves\nof baselines, which proves that SHINE can better learn the\nrepresentations of heterogeneous networks and perform rec-\nommendation than baselines.\n0 5 10\n \n SHINE LINE node2vec SDNE FxG LIBFM\n20 40 60 80 1000\n0.05\n0.1\nK\npo\ns.\n P\nre\ncis\nio\nn@\nK\n(a) pos. Precision@K on Weibo-STC\n20 40 60 80 1000\n0.02\n0.04\n0.06\n0.08\nK\nn\ne\ng.\n P\nre\ncis\nio\nn@\nK\n(b) neg. Precision@K on Weibo-STC\n20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\nK\npo\ns.\n R\nec\nal\nl@\nK\n(c) pos. Recall@K on Weibo-STC\n20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\nK\nn\ne\ng.\n R\nec\nal\nl@\nK\n(d) neg. Recall@K on Weibo-STC\n20 40 60 80 1000\n0.02\n0.04\n0.06\n0.08\nK\npo\ns.\n P\nre\ncis\nio\nn@\nK\n(e) pos. Precision@K on Wiki-RfA\n20 40 60 80 1000\n0.02\n0.04\nK\nn\ne\ng.\n P\nre\ncis\nio\nn@\nK\n(f) neg. Precision@K on Wiki-RfA\n20 40 60 80 1000\n0.05\n0.10\n0.15\n0.20\n0.25\nK\npo\ns.\n R\nec\nal\nl@\nK\n(g) pos. Recall@K on Wiki-RfA\n20 40 60 80 1000\n0.1\n0.2\n0.3\nK\nn\ne\ng.\n R\nec\nal\nl@\nK\n(h) neg. Recall@K on Wiki-RfA\nFig. 6: Positive and negative Precision@K and Recall@K on Weibo-STC and Wiki-RfA for node recommendation.\nTable 3: Accuracy on Weibo-STC w.r.t. the combinations of\nsimilarity measurement function and aggregate function.\nf\nд\nSummation Max pooling Concatenation\nInner product 0.802 0.761 0.855\nEuclidean distance 0.788 0.779 0.837\nLogistic regression 0.816 0.782 0.842\n• Negative precision is low than positive precision while nega-\ntive recall is higher than positive recall for most methods. This\nis because negative links are far fewer than positive links in\nboth datasets, which makes it easier to cover more negative\nlinks in the recommendation set.\n• In general, the results of precision and recall on Weibo-STC is\nbetter than Wiki-RfA, which is in accordance with the results\nin link prediction. The reason lies in that Weibo-STC provides\nmore side information which can greatly improve the quality\nof learned user representations.\n6.6 Parameters Sensitivity\nSHINE involves a number of hyper-parameters. In this subsection\nwe examine how the different choices of parameters affect the\nAccuracy of SHINE onWeibo-STC dataset. Except for the parameter\nbeing tested, all other parameters are set as default.\nSimilaritymeasurement function f and aggregation func-\ntion д.We first investigate how the similarity measurement func-\ntion f and aggregation function д affect the performance by testing\non all combinations of f and д, and present the results in Table 3.\nIt is clear that the combination of inner product and concatenation\nachieves the best Accuracy, while max pooling performs worst,\nwhich is probably due to the reason that concatenation preserves\nmore information out of the three types of embeddings than sum-\nmation and max pooling during embedding aggregation. It should\nalso be noted that there is no absolute advantage of all the three f\nfunctions according to the results in Table 3.\nDimension of embedding layer and reconstructionweight\nof non-zero elements α . We also show how the dimension of\nembedding layer in the three autoencoders of SHINE and the hyper-\nparameterα affect the performance in Fig. 7a.We have the following\ntwo observations: 1) The performance is initially improved with\nthe increase of dimension, because more bits in embedding layer\ncan encode more useful information. However, the performance\ndrops when the dimension further increases, as too large number\nof dimensions may introduce noises which mislead the subsequent\nprediction. 2) α controls the reconstruction weight of non-zero\nelements in autoencoders. When α is too small (e.g., α = 1), SHINE\nwill reconstruct the zero and non-zero elements without much\ndiscrimination, which deteriorates the performance because non-\nzero elements are more informative than zero ones. However, the\nperformance will decrease if α gets too large (e.g., α = 30), because\nlarge α will lead SHINE to totally ignore the dissimilarity (i.e., zero\nelements) among users.\nBalancing parameters λ1, λ2, and λ3. λ1, λ2, and λ3 balance\nthe loss terms of the objective function in Eq. (9). We treat λ1 and\nλ2 as binary parameters and vary the value of λ3 to study the per-\nformance of SHINE. Note that whether λ1 or λ2 equals 1 indicates\nthat whether we use the additional social information or profile\ninformation in link prediction. Therefore, the study of λ1 and λ2\ncan also be seen as to validate the effectiveness of social network\nembedding module and profile network embedding module. The\nresult is presented in Fig. 7b, from which we can conclude that:\n1) The curve of λ1 = 1, λ2 = 0 and λ1 = 0, λ2 = 1 are both above\nthe curve of λ1 = 0, λ2 = 0, which demonstrates the significant\ngain by incorporating the social information and profile informa-\ntion (especially the latter) into the sentiment network. Moreover,\ncombining both additional information can further improve the\nperformance. 2) Increasing the value of λ3 can greatly boost the\naccuracy, as SHINE will concentrate more on the prediction er-\nror rather than the reconstruction error. However, similar to other\nhyper-parameters, too large λ3 is not satisfactory since it breaks\nthe trade-off among loss terms in objective function.\n10 50 100 200 500\n0.65\n0.70\n0.75\n0.80\n0.85\nDimension of embedding layer\nAc\ncu\nra\ncy\n \n \nα = 1\nα = 10\nα = 20\nα = 30\n(a) dim. of embedding layer and α\n0.1 1 5 10 20 300.70\n0.75\n0.80\n0.85\nλ3\nAc\ncu\nra\ncy\n \n \nλ1=0, λ2=0\nλ1=1, λ2=0\nλ1=0, λ2=1\nλ1=1, λ2=1\n(b) λ1, λ2, and λ3\nFig. 7: Parameter sensitivity w.r.t. the dimension of embed-\nding layers, α , λ1, λ2, and λ3.\n7 CONCLUSIONS\nIn this paper we study the problem of predicting sentiment links in\nabsence of sentiment related content in online social networks. We\nfirst establish a labeled, heterogeneous, and entity-level sentiment\ndataset from Weibo due to the lack of explicit sentiment links. To\nefficiently learn from these heterogeneous networks, we propose\nSigned Heterogeneous Information Network Embedding (SHINE),\na deep-learning-based network embedding framework to extract\nusers’ highly nonlinear representations while preserving the struc-\nture of original networks. We conduct extensive experiments to\nevaluate the performance of SHINE. Experimental results prove\nthe competitiveness of SHINE against several strong baselines and\ndemonstrate the effectiveness of usage of social relation and profile\ninformation, especially in cold start scenario.\nACKNOWLEDGMENTS\nWe thank our anonymous reviewers for their feedback and sug-\ngestions. This work was partially sponsored by the National Basic\nResearch 973 Program of China under Grant 2015CB352403.\nREFERENCES\n[1] Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral\ntechniques for embedding and clustering. In NIPS, Vol. 14. 585–591.\n[2] Felipe Bravo-Marquez, Eibe Frank, and Bernhard Pfahringer. 2015. Positive,\nnegative, or neutral: Learning an expanded opinion lexicon from emoticon-\nannotated tweets. In IJCAI 2015, Vol. 2015. AAAI Press, 1229–1235.\n[3] Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and\nThomas S Huang. 2015. Heterogeneous network embedding via deep archi-\ntectures. In Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM, 119–128.\n[4] Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technol-\nogy platform. In Proceedings of the 23rd International Conference on Computational\nLinguistics: Demonstrations. Association for Computational Linguistics, 13–16.\n[5] Xin Dong, Lei Yu, Zhonghuo Wu, Yuxia Sun, Lingfeng Yuan, and Fangxi Zhang.\n2017. A Hybrid Collaborative Filtering Model with Deep Structure for Recom-\nmender Systems. In Thirty-First AAAI Conference on Artificial Intelligence.\n[6] Cícero Nogueira Dos Santos and Maira Gatti. 2014. Deep Convolutional Neural\nNetworks for Sentiment Analysis of Short Texts.. In COLING. 69–78.\n[7] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods\nfor online learning and stochastic optimization. Journal of Machine Learning\nResearch 12, Jul (2011), 2121–2159.\n[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM, 855–864.\n[9] Ramanthan Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins.\n2004. Propagation of trust and distrust. In Proceedings of the 13th international\nconference on World Wide Web. ACM, 403–412.\n[10] Xiao Huang, Jundong Li, and Xia Hu. 2017. Label informed attributed network\nembedding. In Proceedings of the Tenth ACM International Conference on Web\nSearch and Data Mining. ACM, 731–739.\n[11] Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014.\nNRC-Canada-2014: Detecting aspects and sentiment in customer reviews. In\nProceedings of the 8th International Workshop on Semantic Evaluation (SemEval\n2014). 437–442.\n[12] Srijan Kumar, Francesca Spezzano, VS Subrahmanian, and Christos Faloutsos.\n2016. Edge weight prediction in weighted signed networks. In Data Mining\n(ICDM), 2016 IEEE 16th International Conference on. IEEE, 221–230.\n[13] Jérôme Kunegis, Stephan Schmidt, Andreas Lommatzsch, Jürgen Lerner,\nErnesto W De Luca, and Sahin Albayrak. 2010. Spectral analysis of signed\ngraphs for clustering, prediction and visualization. In Proceedings of the 2010\nSIAM International Conference on Data Mining. SIAM, 559–570.\n[14] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010. Predicting pos-\nitive and negative links in online social networks. In Proceedings of the 19th\ninternational conference on World wide web. ACM, 641–650.\n[15] Thien Hai Nguyen and Kiyoaki Shirai. 2015. PhraseRNN: Phrase Recursive Neural\nNetwork for Aspect-based Sentiment Analysis.. In EMNLP. 2509–2514.\n[16] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, andWenwu Zhu. 2016. Asymmet-\nric transitivity preserving graph embedding. In Proc. of ACM SIGKDD. 1105–1114.\n[17] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining. ACM, 701–710.\n[18] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on\nIntelligent Systems and Technology (TIST) 3, 3 (2012), 57.\n[19] Hassan Saif. 2015. Semantic Sentiment Analysis of Microblogs. Ph.D. Dissertation.\nThe Open University.\n[20] Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. International\nJournal of Approximate Reasoning 50, 7 (2009), 969–978.\n[21] Jiliang Tang, Shiyu Chang, Charu Aggarwal, and Huan Liu. 2015. Negative\nlink prediction in social media. In Proceedings of the Eighth ACM International\nConference on Web Search and Data Mining. ACM, 87–96.\n[22] Jiliang Tang, Yi Chang, Charu Aggarwal, and Huan Liu. 2016. A survey of signed\nnetwork mining in social media. ACM Computing Surveys (CSUR) 49, 3 (2016),\n42.\n[23] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.\n2015. Line: Large-scale information network embedding. In Proceedings of the\n24th International Conference on World Wide Web. ACM, 1067–1077.\n[24] Joshua B Tenenbaum, Vin De Silva, and John C Langford. 2000. A global geometric\nframework for nonlinear dimensionality reduction. science 290, 5500 (2000), 2319–\n2323.\n[25] Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied\nto unsupervised classification of reviews. In Proceedings of the 40th annual meet-\ning on association for computational linguistics. Association for Computational\nLinguistics, 417–424.\n[26] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network em-\nbedding. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM, 1225–1234.\n[27] Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, and Minyi Guo. 2017.\nJoint-Topic-Semantic-aware Social Recommendation for Online Voting. In Pro-\nceedings of the 26th ACM International Conference on Conference on Information\nand Knowledge Management. ACM, 347–356.\n[28] Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi\nCheng. 2015. Learning hierarchical representation model for nextbasket rec-\nommendation. In Proceedings of the 38th International ACM SIGIR conference on\nResearch and Development in Information Retrieval. ACM, 403–412.\n[29] Suhang Wang, Jiliang Tang, Charu Aggarwal, Yi Chang, and Huan Liu. 2017.\nSigned network embedding in social media. In Proceedings of the 2017 SIAM\nInternational Conference on Data Mining. SIAM, 327–335.\n[30] Robert West, Hristo S Paskov, Jure Leskovec, and Christopher Potts. 2014. Ex-\nploiting social network structure for person-to-person sentiment analysis. arXiv\npreprint arXiv:1409.2450 (2014).\n[31] Jihang Ye, Hong Cheng, Zhe Zhu, and Minghua Chen. 2013. Predicting positive\nand negative links in signed social networks by transfer learning. In Proceedings\nof the 22nd international conference on World Wide Web. ACM, 1477–1488.\n[32] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandel-\nwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation:\nA heterogeneous information network approach. In Proceedings of the 7th ACM\ninternational conference on Web search and data mining. ACM, 283–292.\n[33] Shuhan Yuan, Xintao Wu, and Yang Xiang. 2017. SNE: Signed Network Em-\nbedding. In Pacific-Asia Conference on Knowledge Discovery and Data Mining.\nSpringer, 183–195.\n[34] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.\n2016. Collaborative knowledge base embedding for recommender systems. In\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. ACM, 353–362.\n[35] Quan Zheng and David B Skillicorn. 2015. Spectral embedding of signed networks.\nIn Proceedings of the 2015 SIAM International Conference on Data Mining. SIAM,\n55–63.\n[36] Chang Zhou, Yuqiong Liu, Xiaofei Liu, Zhongyi Liu, and Jun Gao. 2017. Scalable\nGraph Embedding for Asymmetric Proximity.. In AAAI. 2942–2948.\n",
      "id": 45956798,
      "identifiers": [
        {
          "identifier": "2772021946",
          "type": "MAG_ID"
        },
        {
          "identifier": "10.1145/3159652.3159666",
          "type": "DOI"
        },
        {
          "identifier": "141534803",
          "type": "CORE_ID"
        },
        {
          "identifier": "466610688",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1712.00732",
          "type": "OAI_ID"
        },
        {
          "identifier": "1712.00732",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "info:doi/10.1145%2f3159652.3159666",
          "type": "OAI_ID"
        }
      ],
      "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment\n  Link Prediction",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2772021946",
      "oaiIds": [
        "info:doi/10.1145%2f3159652.3159666",
        "oai:arxiv.org:1712.00732"
      ],
      "publishedDate": "2017-12-03T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1712.00732"
      ],
      "updatedDate": "2024-02-23T20:47:25",
      "yearPublished": 2017,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1712.00732"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/45956798"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1403.6067",
      "authors": [
        {
          "name": "Gao, Huiji"
        },
        {
          "name": "Mahmud, Jalal"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/386121020",
        "https://api.core.ac.uk/v3/outputs/100481049"
      ],
      "createdDate": "2014-10-24T19:25:15",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Twitter has been increasingly used for spreading messages about campaigns.\nSuch campaigns try to gain followers through their Twitter accounts, influence\nthe followers and spread messages through them. In this paper, we explore the\nrelationship between followers sentiment towards the campaign topic and their\nrate of retweeting of messages generated by the campaign. Our analysis with\nfollowers of multiple social-media campaigns found statistical significant\ncorrelations between such sentiment and retweeting rate. Based on our analysis,\nwe have conducted an online intervention study among the followers of different\nsocial-media campaigns. Our study shows that targeting followers based on their\nsentiment towards the campaign can give higher retweet rate than a number of\nother baseline approaches",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/1403.6067",
      "fieldOfStudy": null,
      "fullText": "  \nWhy Do You Spread This Message?  \nUnderstanding Users Sentiment in Social Media Campaigns \nJalal Mahmud\n1\n and Huiji Gao\n2\n \n1IBM Research - Almaden       2Arizona State University \njumahmud@us.ibm.com                        Huiji.Gao@asu.edu \n \n \n \nAbstract \nTwitter has been increasingly used for spreading messages \nabout campaigns. Such campaigns try to gain followers \nthrough their Twitter accounts, influence the followers and \nspread messages through them. In this paper, we explore the \nrelationship between followers’ sentiment towards the cam-\npaign topic and their rate of retweeting of messages gener-\nated by the campaign.  Our analysis with followers of mul-\ntiple social-media campaigns found statistical significant \ncorrelations between such sentiment and retweeting rate.  \nBased on our analysis, we have conducted an online inter-\nvention study among the followers of different social-media \ncampaigns. Our study shows that targeting followers based \non their sentiment towards the campaign can give higher re-\ntweet rate than a number of other baseline approaches.  \n \nIntroduction   \nRecent years have seen a rapid growth in micro-blogging \nand the rise of popular micro-blogging services such as \nTwitter. With the growing usage of such micro-blogging \nservices, a wide number of social-media campaigns rang-\ning from politics and government to social issues also exist \nin Twitter. Such campaigns maintain Twitter accounts, \nwant to gain a large number of followers through their \nTwitter accounts, influence the followers and spread mes-\nsages through them. However, in reality, all followers may \nnot be equally engaged with the campaign (Chen et al. \n2012). Chen et al. found that users’ activity level, prior \ntopic interest, prior interpersonal relation and geographical \nlocation is correlated with their level of engagement \n(measured by retweets and hashtag usage) with Occupy \nWall Street\n1\n campaign (Chen et al. 2012).  \n    Motivated by their findings, we explore whether follow-\ners’ sentiment towards the campaign topic is correlated \nwith their engagement level with the campaign, where en-\ngagement is measured as the rate of retweeting of the mes-\n                                                 \n \n1\n https://twitter.com/OccupyWallSt \nsages generated by the campaign. Furthermore, we investi-\ngate whether such retweet rate can be predicted from their \nsentiment towards the campaign topic.  \n    Our research is also inspired by an established theoreti-\ncal framework in psychological and marketing research on \nattitudes and attitude models, where attitude is defined as a \nunified concept containing three aspects: “feelings”, “be-\nliefs”, and “actions” (Schiffman  et al. 2010). According to \nthe framework, beliefs are acquired on attitude object (e.g., \na topic or product), which in turns influences the feelings \non the object and the actions with regard to the attitude ob-\nject. Since user’s  belief is hard to observe from social me-\ndia data, we focused on understanding relation between \nfeelings (sentiment) towards a campaign topic and actions \nwhich result from such feelings (sentiment).  We hypothe-\nsize that user’s sentiment towards social media campaign \nhas positive effects on their actions (e.g., retweet) related \nto the campaign, and validate this hypothesis by this re-\nsearch. We have also conducted an online intervention \nstudy to understand the effect of sentiment on increasing \nretweet rates of campaign relevant messages.  \n Our study shows that targeting followers of a campaign \nbased on their higher positive sentiment towards the cam-\npaign gives higher retweeting rate than a number of base-\nlines, such as random targeting and topic based targeting. \nThus, a campaign can be more effective by sending target-\ned messages to followers with stronger sentiment towards \nthe campaign topic.  \nDataset \nFor the sake of concreteness, we limit our exploration on \nthe campaign topic “fracking”2, and analyze data from \nTwitter accounts of several campaigns either supporting or  \n                                                 \n2 fracking or hydrolic fracturing is the process of extracting natural gas \nfrom shale rock layers. This is very controversial due to its potential im-\npact on energy and environment \n \nSentiment Tweet \nPositive The science & economics of #fracking says \nyes to fracking http://ow.ly/ll1J4 \nNegative #Fracking wastewater threatens to drown \nOhio: http://t.co/ft768gkW \nTable 2.  Example tweets related to fracking \nopposing “fracking”. We manually selected 4 Twitter cam-\npaigns related to “fracking” on July 31, 2013. Table 1 \nshows the data-statistics of the campaigns. We crawled \ntheir tweets from last one month and identified tweets \nwhich contained keywords or hashtags related to fracking, \nsuch as “fracking”, “shale”, “#fracking”, “#shale”, “oil”. \nWe denoted these tweets as topic relevant tweets. We also \nmanually inspected those tweets and found that they were \nindeed topically relevant. Table 2 shows few such tweets \nwith their sentiment. The rest of the tweets of each cam-\npaign in last one month are denoted as general tweets. \nThen, we crawled followers of each campaign and their \ntweets from last one month. These tweets are denoted as \nrecent tweets. However, we excluded followers who had \nprotected accounts or who had posted fewer than 100 \ntweets. We also crawled historical tweets (200 max.) of \nthose followers before last one month (tweets until July 1, \n2013), and denoted such tweets as historical tweets.  \nSentiment and Retweet Behavior \nOur goal is to investigate whether followers’ sentiment to-\nwards the campaign topic has any correlation with how \nthey engage with the campaign. However, we do not have \nground truth sentiment for such followers. Hence, we in-\nferred their sentiment from historical tweets. \n \nSentiment Prediction Model \nMotivated by prior works on sentiment analysis (Barbosa \net al. 2010, Davidov et al. 2010), we developed a simple \ncontent-based sentiment prediction model which predicts \nthe sentiment of a Twitter user towards “fracking”. We \nfirst created a labeled dataset for sentiment prediction from \nTwitter. We used Twitter’s streaming API from January, \n2013 to March, 2013 for tweets containing keywords or \nhashtags related to fracking (e.g., “fracking”, “shale”, \n“#fracking”, “#shale”, “oil”). In total, we collected about \n1.68 million tweets. We identified retweets from our data, \nand computed how many times each tweet was retweeted.  \n We selected the tweets which were retweeted at least \n100 times, and manually analyzed them for ground-truth \ncreation. In particular, we found 163 such tweets which we \nlabeled as either positive or negative towards “fracking”. \n22 tweets were positive (pro-fracking tweets), and 141 \ntweets were negative (anti-fracking tweets). From our data, \nwe identified all users who had retweeted those tweets. \nThere were 5384 such users in total:  1562 users retweeted \npro-fracking tweets, and 3822 users retweeted anti-\nfracking tweets. We considered the users who retweeted \npro-fracking tweets as positive towards “fracking”, and \nthose who retweeted anti-fracking tweets as negative to-\nwards “fracking”. This is based on the traditional assump-\ntion that retweet is an act of endorsement of the original \ntweet (Boyd et al. 2010). From the 5384 users (1562 posi-\ntive, 3822 negative), we randomly sampled 1000 positive \n(pro-fracking) users and another 1000 negative (anti-\nfracking) users. Then, we used Twitter’s REST API to \ncrawl the historical tweets (200 max.) of those users before \nthe retweet. Furthermore, we used Twitter's streaming API \nto  randomly sample another 1000 users who were as-\nsumed to have neutral sentiment towards “fracking”. We \nalso crawled their historical (200 max.) tweets. Thus, our \nfinal dataset for sentiment prediction model contained \n3000 users: 1000 positive, 1000 negative and 1000 neutral \ntowards “fracking”.   \n From the above data, we constructed classification-\nbased model with three categories (positive, negative, neu-\ntral). These are sentiment polarity for users' sentiment pre-\ndiction. Our model used unigrams computed from histori-\ncal tweets as features.  We tried a number of different sta-\ntistical models from WEKA, a widely used machine learn-\ning toolkit. SVM-based models outperformed others, and \nachieved ~92% accuracy (0.92 recall, 0.93 precision, 0.925 \nF-measure) under 10-fold cross-validation.  \n \nAccuracy of Sentiment Prediction for Campaign \nFollowers \nWe conducted a study where participants labeled the sen-\ntiment (positive, negative, neutral) of the followers by \nlooking at their historical tweets (tweets until July 1, \n2013). Then, we compared this manually labeled sentiment \nwith inferred sentiment by our algorithm and computed ac-\ncuracy. We randomly selected 50 followers from each \ncampaign (200 followers in total) and collected their his-\ntorical tweets. We recruited 100 participants from Crowd-\nFlower (http://crowdflower.com/), a popular crowd sourc-\ning platform. Each of them labeled 4 followers, and each of \nthe follower was labeled by two participants. Participants \nwere in agreement for 126 followers, and we used those as \nground-truth to compute the accuracy of our sentiment \nprediction model. We applied our sentiment prediction \nmodel to infer the sentiment of those 126 followers. Our \nmodel's inferred sentiment matched manually labeled sen-\ntiment for 110 users which corresponds to 87.3% accuracy. \nTwitter account \nname of the cam-\npaign \nType \n(pro/anti \nfracking) \n# of fol-\nlowers as \nof July \n31, 2013 \n# of  Tweets \nposted as of \nJuly 31, \n2013  \nshalebiz pro 2776 6940 \nEnergy From Shale pro 3965 4826 \nFracking News anti 1219 3062 \nFrackfree America anti 1042 6133 \nTable 1. Social Media Campaigns in our dataset \nTable 4. Regression results over 10 fold cross-validation \nTwitter account \nname of the \ncampaign \nMean Absolute Error (MAE) \nPredicting  \ntopical msg  \nPredicting  \ngeneral msg  \nPred. \nfrom \nSent. \npolarity  \nPred. \nfrom \nSent. \nstrength  \nPred. \nfrom \nSent.  \npolarity  \nPred. \nfrom \nSent. \nstrength  \nshalebiz 0.32 0.25 0.35 0.26 \nEnergy From \nShale \n0.30 0.24 0.32 0.28 \nFracking News 0.28 0.25 0.33 0.27 \nFrackfree  \nAmerica \n0.27 0.23 0.3 0.25 \n \nCorrelation of Sentiment with Retweets \nWe applied the sentiment prediction model to infer the sen-\ntiment of followers of each campaign from their historical \ntweets (tweets until July 1, 2013). Then, we investigated \nwhether followers' predicted sentiment towards fracking \nhas any correlation with their actions (e.g., retweets). To-\nwards that, we analyzed the recent tweets of each follower, \nand computed the number of times they retweeted any top-\nic relevant tweets of the campaign. We divided that num-\nber by the total number of topic relevant tweets of the \ncampaign. The resultant ratio is named as retweet-rate-of-\ntopic-related-campaign-msg. \n We do a similar analysis for other tweets generated from \nthe campaign and computed the ratio: retweet-rate-of-\ngeneral-campaign-msg. Since some followers can be more \nactive in retweeing than others, we divided each of these \nratios by the total number of retweets of each follower over \nlast one month. This resulted  normalized retweet rates \nwhich we used for our analysis. \n   Next, we conducted a pearson correlation analysis be-\ntween sentiment and retweet rates.  For such analysis, we \nconverted predicted sentiment to numeric scores. For fol-\nlowers of pro-fracking campaigns, the numeric scores are \nas follows: 1 for positive, 0 for neutral and -1 for negative. \nFor followers of anti-fracking campaigns, we do the oppo-\nsite (1 for negative, 0 for neutral and -1 for positive). Our \nsentiment prediction model also returned a probability es-\ntimate associated with sentiment prediction. We multiplied \nthis probability with the numeric score for predicted senti-\nment to obtain sentiment strength of each follower. Then, \nwe also investigated the correlation between sentiment \nstrength and retweet rates.   \n As shown in Table 3, we found positive correlations be-\ntween sentiment polarity, and retweet rates for all cam-\npaigns. This indicates that followers who express similar \nsentiment of the campaign are also more likely to retweet \nmessages generated by the campaign. For retweeting topic \nrelevant campaign tweets, such correlations are statistically \nsignificant for all campaigns. However, for retweeting \ngeneral campaign tweets, correlations are not statistically \nsignificant. Table 3 also shows the correlations between \nsentiment strength and retweeting rates. Statistical signifi-\ncant correlations are observed in each case. We also see a \nhigher correlation value for sentiment strength, which indi-\ncates its effectiveness to impact followers’ retweets.   \n \nPrediction Model \nWe also built predictive models of retweeting a campaign’s \nmessage based on the sentiment of the followers. We per-\nformed both regression analysis and a classification study \nusing WEKA, a widely used machine learning toolkit.  For \nboth cases, we tried two settings - predicting from senti-\nment polarity, and predicting from sentiment strength.  \n For regression analysis, sentiment polarity (first setting) \nand sentiment strength (second setting) are used as the val-\nue of the independent variable in the regression model. Re-\ntweet rates are used as dependent variable in the model. \nWe tried a number of regression approaches (e.g., logistic \nregression, SVM regression) and performed 10-fold cross \nvalidation. Logistic regression performed the best, and the \nresults in terms of mean absolute error (MAE) are shown \nin Table 4. We find lower prediction errors when predict-\ning from sentiment strength. In particular, retweeting topi-\nTwitter account \nname of the \ncampaign \nArea Under ROC Curve (AUC) \nPredicting  \ntopical msg  \nPredicting  \ngeneral msg  \nPred. \nfrom \nSent.  \npolarity  \nPred. \nfrom \nSent. \nstrength  \nPred. \nfrom \nSent. \npolarity  \nPred. \nfrom \nSent. \nstrength  \nshalebiz 0.67 0.74 0.63 0.70 \nEnergy From \nShale \n0.69 0.76 0.67 0.72 \nFracking News 0.70 0.77 0.66 0.73 \nFrackfree Amer-\nica \n0.73 0.80 0.70 0.74 \nTable 5. Classification results over 10 fold cross-validation \nTwitter account \nname of the \ncampaign \nCorrelation Analysis \nnormalized-\nretweet-rate-of-\ntopic-related-\ncampaign-msg \nnormalized-\nretweet-rate-of-\ngeneral-campaign-\nmsg  \nCorr. \nwith \nSent.  \npolarity  \nCorr. \nwith \nSent. \nstrength  \nCorr. \nwith \nSent.  \npolarity  \nCorr. \nwith \nSent. \nstrength  \nShalebiz 0.65* 0.7* 0.6 0.62* \nEnergy From \nShale \n0.71* 0.73* 0.64 0.65* \nFracking News 0.8* 0.82* 0.77 0.8* \nFrackfree  \nAmerica \n0.6* 0.63* 0.5 0.55* \nTable 3. Pearson correlation between sentiment and norma-\nlized retweet rates (* means significant, p < 0.05) \ncal messages can be predicted within 23%-25% MAE and \ngeneral messages can be predicted within 25%-28% MAE.  \n For classification study, we used supervised binary ma-\nchine learning algorithms to classify followers with above \nmedian levels of retweet rates. We experimented with a \nnumber of classifiers from WEKA, including naive Bayes,  \nSVM, J48, Random Forest. SVM slightly outperformed the \nrest. Table 5 shows the classification result in terms of \nAUC under 10-fold cross validation. We see that classify-\ning high (above median) or low (below median) retweeters \nfrom sentiment can be done quite accurately.  \nTable 6.  Example intervention messages     \nSentiment Example Intervention Message \nPositive \"@zas Plz RT “Fracking saves us money; frack-\ning creates jobs\"”   (pro-fracking) \n \nNegative \"To anyone speaking of the economic ”benefits” \nof fracking: what use is that money if your food \nand water are full of poison. Plz RT\"(anti-\nfracking) \nOnline Engagement Study \nWe conducted an engagement study among followers with \ndifferent sentiment. In our study, we sent campaign rele-\nvant messages to such followers. We first created 6 ac-\ncounts on Twitter, 3 for sending pro-fracking and another 3 \nfor sending anti-fracking messages.  We maintained a pool \nof 10 pro-fracking messages and 10 anti-fracking messag-\nes. Each such message asked the recipient to retweet the \nmessage.  \n We ranked the positive sentiment followers of pro-\nfracking campaigns according to their sentiment strength \ntowards the “fracking” topic and selected top-500 (senti-\nment-ranked-top-followers) from the ranked list. From the \nremaining pro-fracking followers, we randomly selected \n500 followers and denoted them as  followers. We obtained \nanother set of 500 users from random sampling from Twit-\nter stream. Finally, we looked for Twitter stream for users \nwho mentioned the term “fracking” and obtained another \nset of 500 users from such matched users (topic-relevant).  \nFrom our pro-fracking accounts, we sent pro-fracking \nmessages to each group of users. Thus, 500 messages were \nsent for each target group. In each such message sending, \nwe randomly selected a message from the pool of 10 pro-\nfracking messages. We waited about a week, and recorded \nhow many messages were retweeted in each case. Then, \nwe computed the retweet rates (defined as the ratio of the \nnumber of messages retweeted and the total number of \nmessages sent) obtained in each case.  \n We do a similar study for anti-fracking followers. Thus, \nwe sent anti-fracking messages to sentiment-ranked-top-\nfollowers of anti-fracking campaigns. We also sent those \nmessages to randomly selected 500 followers of those \ncampaigns, randomly selected 500 users from Twitter, and \nrandomly selected 500 topic-relevant users from Twitter. \nFigure 1 shows the retweet rates obtained in each case.  We \nobserve that sentiment-ranked approach clearly outper-\nformed the other approaches. Our study shows that a cam-\npaign can be more effective by sending targeted messages \nto followers with stronger sentiment towards the campaign.  \nConclusion \nIn this paper, we investigated the relationship between sen-\ntiment of social media users who are following a campaign \nand their likelihood of spreading campaign messages. We \nfound that sentiment has a statistically significant effect on \nsuch activity. Furthermore, spreading topically relevant \ncampaign messages has stronger correlation with sentiment \nof the campaign followers. Our engagement study provides \nfurther insight for designing better intervention to spread \ncampaign messages. There are various directions for future \nresearch. First, we like to apply similar analysis for other \ntypes of actions such as hashtag usage, mentions or tweet \ncreation. Second, we will explore how  other factors (e.g., \ngeneral activity, prior interaction as suggested by Chen et \nal. 2012) together with sentiment predict such actions. \nThird, we will conduct a survey among campaign follow-\ners to understand their demographics, personality, network \nsize, etc. and analyze whether these factors affect their en-\ngagement with the campaign. Finally, we would like to in-\nvestigate the generality of our findings by applying similar \nanalysis for social media campaigns on different topics.  \nReferences  \nBarbosa, L. and Feng, J. Robust sentiment detection on twitter \nfrom biased and noisy data. In Proc. COLING, 2010. \nBoyd, D. Golder, S., Lotan, G. 2010, Tweet, Tweet, Retweet: \nConversational Aspects of Retweeting on Twitter. In Proc. \nHICSS 2010.  \nChen, J. Pirolli, P. 2012. Why You Are More Engaged: Factors \nInfluencing Twitter Engagement.  In Proc. ICWSM 2012.   \nDavidov, D., Tsur, O. and Rappoport, A. Enhanced sentiment \nlearning using twitter hashtags and smileys. In Proc. COLING \n2010.  \nSchiffman, L.G. Kanuk, L.L. Wisenblit, J. Consumer Behavior. \n10th edition. Prentice Hall - 2010.  \n \nFigure 1.  Retweet rates for different target groups (500 mes-\nsages were sent in each case) \n",
      "id": 17187601,
      "identifiers": [
        {
          "identifier": "386121020",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1403.6067",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.485.8858",
          "type": "OAI_ID"
        },
        {
          "identifier": "1403.6067",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "25013051",
          "type": "CORE_ID"
        },
        {
          "identifier": "100481049",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/14561",
          "type": "OAI_ID"
        }
      ],
      "title": "Why Do You Spread This Message? Understanding Users Sentiment in Social\n  Media Campaigns",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:ojs.aaai.org:article/14561",
        "oai:arxiv.org:1403.6067",
        "oai:citeseerx.psu:10.1.1.485.8858"
      ],
      "publishedDate": "2014-03-24T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1403.6067",
        "http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewFile/8030/8090/"
      ],
      "updatedDate": "2022-07-14T04:40:33",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1403.6067"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17187601"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Altrabsheh, Nabeela"
        },
        {
          "name": "Cocea, Mihaela"
        },
        {
          "name": "Gaber, M."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/52398881",
        "https://api.core.ac.uk/v3/outputs/29584880"
      ],
      "createdDate": "2015-09-29T10:13:11",
      "dataProviders": [
        {
          "id": 695,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/695",
          "logo": "https://api.core.ac.uk/data-providers/695/logo"
        },
        {
          "id": 1899,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1899",
          "logo": "https://api.core.ac.uk/data-providers/1899/logo"
        }
      ],
      "depositedDate": "",
      "abstract": null,
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/29584880.pdf",
      "fieldOfStudy": null,
      "fullText": "SA-E: Sentiment Analysis for Education \n \nNabeela Altrabsheh, Mohamed Medhat Gaber, Mihaela Cocea  \nSchool of Computing, Buckingham Building, \nLionTerrace,Portsmouth,Hampshire,PO13HE,UK \nE-mail: nabeela.altrabsheh@port.ac.uk, mohamed.gaber@port.ac.uk, \nmihaela.cocea@port.ac.uk  \nAbstract. Educational data mining (EDM) is an important research area that is used to \nimprove education by monitoring students performance and trying to understand the \nstudents’ learning. Taking feedback from students at the end of the semester, however, \nhas the disadvantage of not benefitting the students that have already taken the course. \nTo benefit the cur-rent students, feedback should be given in real time and addressed in \nreal time. This would enable students and lecturers to address teaching and learning \nissues in the most beneficial way for the students. Analysing students’ feedback using \nsentiment analysis techniques can identify the students’ positive or negative feelings, \nor even more refined emotions, that students have towards the current teaching. \nFeedback can be collected in a variety of ways, with previous research using student \nresponse systems such as clickers, SMS and mobile phones. This paper will discuss \nhow feedback can be collected via social media such as Twitter and how using \nsentiment analysis on educational data can help improve teaching. The paper also \nintroduces our proposed system Sentiment Analysis for Education (SA-E).  \nKeywords. Education Data Mining, Sentiment Analysis, Naive Bayes, SVM, Student \nResponse Systems  \n1. Introduction  \nEducational Data Mining (EDM) is application area of data mining that is developed to \naddress problems in education. Addressing such problems can lead to helping students who \nneed advice, removing and adding material to the unit according to students comprehension \nand finding students opinions about the course. Feedback in education can be categorised \nin to: 1. Feedback from the lecturer to the students, this is for the self improvement of the \nstudents; 2. Feedback from the students to the lecturer, this allows them to guide the \nlecturer into teaching the course in ways they understand best.  \nStudent Response Systems (SRS) is used for feedback in the classroom, given by \nstudents to the lecturer, via devices such as clickers and mobiles. However SRSs fall short \nto provide detailed feedback about what might have gone wrong. Social media can be a \ngood tool for students to provide such detailed feedback. Among those media comes \nTwitter. Twitter can provide students with a convenient way to express their feedback in \nfree text. This paper will show that using Sentiment Analysis on students feedback \nprovided by Twitter has many advantages. This is not the first time twitter has been used to \ntake students feedback. In [1] Twitter was used and the lecturer had to analyse the results, \nwhich caused work overload on the lecturer.  \nSentiment Analysis is a field that works on making sense out of textual feedback and \nopinions. Opinions can be negative or positive, different emotions can be associated with \nthose opinions. Emotions can be negative such as confused, bored, and irritated an example \nfor this is I do not know [2]. Positive emotions such as confident and enthusiastic can be \nexpressed such as when students writes in a louder style such as bold writing [2]. Neutral \nfeedback is when a student does not express negative or positive emotion [2]. Different \ntechniques have been used in sentiment analysis and a few have proved to give superior \nperformance such as Naive Bayes (NB), Max Entropy (MaxEnt) and Support Vector \nMachines (SVM).  \nThe paper is organised as follows. Section 2 will focus on student feedback through-\nout the lecture and discuss the disadvantages of clickers and Short Message Service (SMS). \nSection 3 will highlight the advantages and disadvantages of the sentiment analysis \ntechniques mentioned above and discuss more about sentiment analysis. Section 4 will \nshow how both areas can be combined and what can be implemented. Finally the paper is \nconcluded with a summary and future directions to work in Section 5.  \n2. Students Feedback  \nStudents feedback is important because it can help the lecturers understand the students \nlearning behaviour. Sometimes students do not understand what the lecturer is trying to \nexplain, thus by providing feedback students can indicate this to the lecturer. Students \nfeedback can also help in understanding different issues that students have including the \nstudents not understanding the lecture.  \nFeedback needs to be taken in order to make improvements in teaching [5]. If the \nstudents do not participate in giving feedback then there is no way in finding out if the \nteaching needs improvement. Students often act as observers in the classroom and expect \nthe lecturer to feed them with information, this is a problem especially for international \nstudents that have come from different backgrounds and experienced different teaching \nmethods. Student engagement is important in education and one way of measuring it is \nthrough participation [6].The traditional way of students asking questions is raising their \nhand to ask, although this way does not suit everyone such as shy people. A study about \nstudent engagement [6] showed that participation was lower by means of raising hands \ncompared with the use of clickers, meaning that clickers are more popular with students \nthan raising hands as means of engaging in learning. Another disadvantage to hand raising \nis that students can look to see other responses before making a response and copy from \nother students [7]  \nStudents lack of participation is a common concern for educators [6]. In large classes it \nmay be time consuming if every person needed one question then not much material will \nbe covered. In [6] the author attempted to find out how student response systems impact \nstudent learning in large lectures [6], where students have less chance to ask questions \nbecause of the class size [8]. Lack of participation could be from students not paying \nattention. Students can be daydreaming due to having difficulty in maintaining attention \nthroughout classes [7].Some interesting results showed that students that participate in \nclass achieve better results than students who do not [9]  \nFeedback can be collected through a variety of SRSs, including clickers and mo-bile \nphones. However, in addition to specifically asking for feedback lecturers can ask students \nquestions and students can ask lecturer questions. Therefore, SRS devices can help in \ncommunication between the lecturer and student. One common disadvantage of SRS is the \ncost of the clickers and mobile phones, but nowadays it is very rare to find a student \nwithout a mobile so there is not an extra cost for the student or the university. The \nfollowing subsections present in more detail three types of SRS: Clickers, Mobile Phones \nand Social Media.  \n2.1. Clickers  \nClickers are handheld devices which usually contain one or more buttons. In [10] study, the \nclickers that were used had one button which was labelled yes and this button was used to \nrespond to the lecturers enquiries. The lecturer explained how the clickers are used and \nthen throughout the lecture the lecturer asked the students questions for instance if they \nwere ready for the lecture to be continued and if they understood a certain point.  \nOne the many advantages of clickers is allowing students to focus longer on the ma-\nterial and learning it by participation instead of focusing on taking notes throughout the \nentire lecture [7]. On the other hand, in addition to cost, clickers have the disadvantage of \nstudents losing them, breaking them or forgetting to bring them to class. Moreover, clickers \ncan distract students in class [6].To solve the problems of clicker cost and the limited \ninformation derived from the data mobile phones came as an alternative solution.  \n2.2. Mobile phones  \nMobile phones are popular with students, with 98% of students owning a mobile phone \n[11]. Students usually leave their mobiles on throughout class, therefore being able to use \nthem for feedback. However, students can also use mobile in other activities such as \nsending messages to their friends [11] ,[12] . One example of students wrongly using their \nmobiles is flooding the system with inappropriate messages [12] .The use of mobile phones \nin the classroom also has some negative outcomes or distractions such as students phones \nringing during class. Feedback can be taken from mobile phones using applications and \ntext messages which are presented in more detail below.  \n \n2.2.1. Clicker Application  \nAn example of Clicker Application is found in [13]. A study that analysed the feedback of \nthe students via mobile phones this project name is Crowd Feedback. The students could \npost feedback about the lesson and the lecturers phone would buzz if something interesting \nwas found from feedback analysis. The system was not only giving feedback for the \nlecturer but for students as well in order for them to interact and know what each of the \nchildren comprehended. The system also showed that the students had given feedback all \nthroughout the lecture and that there were two high ”dislike” peaks. Further study can be \ndone to find out what had gone wrong at those two times. The paper acknowledges that \nfeedback would be more beneficial if it were richer than just providing yes and no answers. \nAs clicker application solved the solution to the cost of devices it still did not solve the fact \nthat the data received from the students was limited. There is need for students to write \nsentences to give a cleared feedback and SMS solved this problem.  \n2.2.2. Short Message Service (SMS)  \nThe idea of collecting feedback through SMS in the education system was proposed in \n[14]. In their research they created a model that collected learning feedback from the \nstudents via SMS messages. One aim to their system is to improve the delivery of the \nlesson by finding out students opinions. Although the system had many benefits they found \nthat taking feedback via SMS had many flaws such as the limitation of the message space \nto a certain amount of characters and incomplete SMS due to this limitation. Other \ndownsides to this project found were the spelling mistakes that the audience texted, this \nwas solved by a model called the corrected model. The corrected model corrected text to \nsimilar words such as slp to sleep. Another model that they created was the sentiment \nmodel. This model implemented sentiment mining on the correction model to find \ninterestingness and divide the concepts into true and false. Although words can have \ndifferent meanings and can be positive and negative according to the student involved.  \nOne aspect that the authors [14] did not take into consideration is the cost of the SMS \ntexting service, [15] suggested lowering the cost barrier may help in increasing feedback in \nlectures. Also the authors in [14] tested the students feedback at the end of each lesson, this \nis great for improving the lectures over time as the lecturer can modify the teaching after \nreceiving this feedback in the next lesson, however this would be more effective if the \nfeedback was throughout the lesson in timescales to insure that students get the greatest \npossible out of each lesson. It was suggested in [14] that including timestamp and the date \nwith the feedback can help monitor the improvement of teaching over time. Such a trend \nanalysis would be a useful tool for lecturers that would help them improve their teaching.  \n \n2.3. Social Media  \nIt would be easier to compare between tweet or Facebook posts especially with the social \nnetworks becoming the most popular communication nowadays. Social media is popular, \nstatistics show that 93.5% of 18 year olds and 95.4% of 19 year olds in the USA were \nfound to use social networking on a regular basis [1]. 52.1% of academics in 2010 say that \nthey have used Twitter [1]. In addition to that over 470 universities worldwide are using \nsocial networks such as Facebook and Twitter to communicate with students [1].  \nFrom Twitter advantages in education is that students are familiar with the tool and \ntraining will not be needed [1]. Another great benefit to using twitter as feedback is that it \nis free as twitter can be opened from the their own mobiles on the university wireless \nnetwork. Twitter has some disadvantages such as it being a distraction for the students or \nthe lecturer to have to multitask [8]. Also the tweets appear sequentially so the lecturer has \nto read from the beginning to understand what is going on therefore time loss happens \n[8].Twitter solved the issue of the SMS cost issue as it is a free tool that anyone can use \nand students can access it via the universities network. The characters are limited which \ncan be seen as an advantage to students using as less words as possible and this makes \nthem focus on the important words to create a sentence as meaningful as possible. Some of \nthe disadvantages where that the lecturer has to scroll through the tweets in order to \nunderstand the students opinions and analyse the text manually. This is where Sentiment \nAnalysis for Education tool will help as the lecturer will only have to stop a minute every \n20 minutes for the student to post their tweets and the tool will analyse the results and \nreturn them immediately.  \n3. Sentiment Analysis  \nSentiment Analysis also known as subjectivity analysis, opinion mining, and appraisal \nextraction [16] is an application of natural language processing, computational linguistics \nand text analytics to identify and retrieve certain information from the text, this is done by \nstudying the subjectivity or the opinion. When looking for the success of a product it is \nimportant to know what features the user liked or disliked, the term for this is feature \nextraction. Sentiment polarity is usually either positive or negative but polarity can also be \nexpressed as a range such as how much the user liked the product this range can be into an \nn-point scale, e.g., very good, good, satisfactory, bad, very bad [17]. Finding positive and \nnegative words without the n point scale is easier than trying to determine the weight of \nthat word. Sentiment analysis can also be used to extract different users emotions from the \ntext such as Love, Joy, Anger, Frustration, and Neutral [18]. Subjectivity and emotion are \ntwo close concepts subjectivity represents facts and also emotion, feelings, views and \nbeliefs. [19] found that sometimes positive words can be put into negative reviews and vice \nversa. They also found that different users have different opinions about positive rooms \nsome like it to be clean some like it to be big so there is a variety in what people put as \nnegative and positive but in the end a pattern can be made to find that most people like a \ncertain characteristic and then it can be positive. The word can have a positive meaning in \nsome ways and negative in other ways such as the word small it can be negative if \ndescribing a hotel and positive if describing a mobile. It was found through tests that \nsentences do matter in the sentiment and a word by itself can be positive or negative \naccording to the sentence it is put it.  \nReviews are affected by students emotions [20] and this is why it is important to \nunderstand if the students are struggling in the course and what they dislike about the \ncourse. [18] looked into emotions of the e-learners through their texts this allows us to \nbreak the distant of emotions between the e-learner and the lecturer. [18] found that most \nof the research that has been done toward the e-learners cognition and not their emotion or \nsentiment, the distant between the lecturer and the student is a main cause for this. The \nfacial expression plays an important role in understanding the persons emotions. E-learning \nhas rarely any face to face communication unless the lecturer and student have both \nequipped web cam and this is why there is need to understand the students text and to \nanalyse it to find what the students feelings are toward the lecture and this will hopefully \nimprove from the quality of teaching.  \nAnalysing text can help the lecturer understand the student more carefully , the \nemotion types in [18] were categorised into Love, Joy, Anger, Frustration, and Neutral. \nThe student can express his feelings in short expressions or words. [18] created patterns to \nfind what words are associated more with emotions, they also give solutions to relieve the \ne-learner such when an e-learner typed that he is depressed because his supervisor does not \nlike his report. Table 1 shows the adjustment strategy to help the user:  \nTable [1]: Example to E-learners Emotion \nEvent set  emotion  Sentiment adjustment strategy  \ncriticized by teachers  frustration  Suggest him to communicate with \nthe teacher  \nBlamed by teacher  frustration and anxiety  Say something like that you had \ndone your best, so dont blame \nyourself  \nYelled by teacher  frustration  Tell him that Experience is the \nmother of wisdom, do a better job \nnext time  \n \n3.1. Sentiment Analysis Techniques  \nThe most common technique in sentiment analysis are Naive Bayes , Maximum entropy \nand Support vector machine. These features have been proved to work well with sentiment \ndata and have been used and praised in the following: 1: Joachims, 1998 cited in Prabowo \nand Thelwall, 2009; 2: Pang, Lee and Vaithyanathan, 2002; 3: Vachaspati and Wu 2012; 4: \nGo Bhayani and Huang 2009 and 5: Lu, Peng, Li and Ahmed, 2006.  \nIn [21] Naive Bayes gave good accurate results when implemented on reviews, and \nblog posts. As for Lexical method it varied, and gave good and bad results. SVM method \nwas used for reviews, and alone it showed small accuracy but when accompanied with \nNaive Bayes or Lexical methods the results become higher. Therefore, to summarise it is \nbetter to combine the methods to get better accuracy results. The research showed that[21] \nhad different results with[22] which claimed that SVM is the best classifier after comparing \nthe Naive Bayes and the SVM classifier. And [23] concluded that the three classifier he \nused, Naive Bayes, maximum entropy and SVM had similar performance. in [31] the best \nresults was found with the SVM classifier, comparing the same three classifier and  \n[22] also said that although SVM is the best classifier there are some differences in the \noverall performance.  \n3.2. Features  \nAfter deciding which technique to use features that will be considered in the experiment \nshould be decided. Features allow a more accurate analysis of the sentiments, and for a \nmore detailed summarization of the results [21]. These are some features that can be used : \n1. Term Presence and Frequency : the frequency of the word and the presence of some \nwords. 2. N-grams: position of the word and taking one two or three words.  \n3. Part-of-Speech: Adjective are good indicators that there is some kind of opinion 4. \nSyntax 5. Negation: Negation is flipping the positive into negative or negative to positive, \nNegators can affect the word and make it the opposite Examples to some negators are not, \nnone, nobody, never, and nothing [24], problems with negation could be: a. Include the \nnegation to a term close to negation such as like and not , the location sometimes affects \nthis like including not at the end [21] ; b. Negating the sentiment in the sentence such as No \nwonder everyone loves it. [21] Some examples of negators from [24] are shown in Table 2:  \n \nTable [2]: Negator Examples \nExample  Negation  \nNobody gives a good performance \nin this movie.  \nNobody negates good  \nOut of every one of the fourteen \ntracks, none of them approach be-\ning weak and are all stellar.  \nNone negates weak  \nJust a V-5 engine, nothing \nspectacular  \nNothing negates spectacular  \n \n3.3. Data Source  \nThe internet has become a main source for the users to express their opinion . More users \nare willing to express their opinions online [25]. This has a good advantage as more \nopinions can be extracted from a wider source. Social networks has been used in data \nmining for many years.[26] gave some good advantages in using twitter such as twitter is \nup-to-date and reflects the current news and events happening around the world. For this \nproject the data has to be in real-time and twitter will be used  \n3.4. Pre-processing data  \nAfter obtaining the data from the source, the data has to be pre-processed before the \nsentiment analysis stage this is to increase accuracy and to reduce error in the data. Pre-\nprocessing can be discovering emoticons, words in upper case, removing stop words, \nremoving unnecessary punctuation, finding exclamation marks or question marks, and \nremoving inconsistent casing of letters [27].  \n4. Sentiment Analysis in Educational Data  \nSentiment analysis has not been implemented on the educational sector yet. When \ncombining these areas together education can be improved by saving time in analysing \nreal-time feedback. Students can use twitter to express their opinions about the lecturer for \nexample the lecture can change the pace of the lecture [28]. When finding out students \nopinions over time intervals the lecturer can alter teaching style according to the results, \nrepeat a part that the majority of the students did not understand and answer any questions \nwithout the need for student to interrupt the lecturer. The following areas will be discussed \nin the framework: 1. The method used to send feedback; 2. The time slots in the lecturer \nand how to post feedback 3. The Techniques used to analyse the feedback 4. The results  \n \n4.1. SA-E System Architecture  \n \nThe research steps will be implemented in the classroom as followed: Students will pro-\nvide feedback to the lecturer via social networks such as twitter. The reason that social \nnetworks will be the source of data is because it is free and popular among students \nnowadays. In 2010 Twitter had 106 million users, and 180 million visitors every month \n[29]. Twitter company revealed that 300,000 new users were signing up every day and that \nit received 600 million queries daily through its search engine. 37 % of Twitters users used \ntheir phone to send messages [29].This will overcome the problems of SMS being \nexpensive or clickers giving limited feedback.  \nThe students feedback can be taken at anytime in the lecture but there will also be time \nslots chosen by the lecturer according to stop point which he thinks that the previous topic \nwill follow on to the next topic. The time slots can also be randomly chosen at a 20 minute \ntime slot to insure that students understand every part of the lecturer and that they will not \nget lost in understanding any part of the lecture. The lecturer can also change the slots \nwhile giving the lecture according to the results, so for example if the students had not \nunderstood a part the lecturer may decide to put more slots at the next part of the session to \nmake the students understand more. Within a amount of time if the lecturer found that \nstudents are often finding difficulty in one part then he may decide to explain it in another \nway. Time series is an important part in this project, students may show a high response \nwithin a certain time, this can be traced back to find what went wrong at the time, and why \nstudents are not understanding a particular module and many more. After a certain time \npatterns can be derived from this data and future students will have an even better \neducation.  \nStudents feedback will be pre-processed and then analysed via Naive Bayes and \nSupport Vector Machine individually or combined. These techniques were chosen be-cause \nthey have been proved to work well with reviews and educational data. This will analyse \nthat the document phrase and this is the post or tweet as a whole is positive or negative. \nThere could be positive words in negative reviews and vice versa, this will be looked into \nas well. The raw data will be pre-processed to insure the quality of the data. Then different \nfeatures will be applied to insure that the focus is going to the correct part of data. The \nfeatures will be decided throughout experiments. The results will be passed through an \napplication where the lecturer can decide to act according to the results.  \nAfter obtaining the results verification methods can be applied to assure that the \nsentiment analysis has been a success. From these methods can be exploring students facial \nexpressions to find if students emotions match with the results obtained. The lecturer can \nask questions to insure that the students responses are correct. The results will be kept for \nfurther study and other use will be made out of the data by analysing it over long periods of \ntime.  \n \nFigure 1: SA-E System Architecture  \nSentiment Analysis in the education sector can be a wide area and hold many \npossibilities. Although this paper has only shown a few point on how this can be \nimplemented, there can be further points that can be researched. Figure 1 shows SA-E \nArchitecture.  \nThis paper provides the reader with a new perspective that can open areas of research \nin educational data mining. This can be achieved with the adoption of the fast growing area \nof Sentiment Analysis.  \n5. Conclusion  \nIn this paper, a brief background of educational data mining and sentiment analysis was \npresented including different methods for collecting feedback from students. We noted that \nthe adoption of the emerging area of data mining sentiment analysis in educational systems \nhas a great potential. Sentiment analysis techniques were explained briefly with a \ncomparison, and it was decided that Naive Bayes and SVM techniques were superior for \neducation data [21] . We have also discussed how these two are could be combined for the \nanalysis of students feedback in real-time. We have introduced our system architecture \ntermed Systems Analysis for Education (SA-E). The realisation for SA-E opens the door \nfor a potentially fruitful application of sentiment analysis.  \nReferences  \n[1] Novak, Jeremy, and Michael Cowling. ”The implementation of social networking as a tool for improving \nstudent participation in the classroom.” (2011).  \n[2] Litman, Diane J., and Kate Forbes-Riley. ”Predicting student emotions in computer-human tutoring \ndialogues.” Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. \nAssociation for Computational Linguistics, 2004.  \n[3] Cummins, Stephen, Liz Burd, and Andrew Hatch. ”Using Feedback Tags and Sentiment Analysis to \nGenerate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags in a \nProgramming Course.” Advanced Learning Technologies (ICALT), 2010 IEEE 10th International \nConference on. IEEE, 2010.  \n[4] Agrawal, Rakesh, et al. ”Data mining for improving textbooks.” ACM SIGKDD Explorations Newsletter  \n13.2 (2012): 7-19.  \n[5] Poulos, Ann, and Mary Jane Mahony. ”Effectiveness of feedback: the students perspective.” Assessment \nand Evaluation in Higher Education 33.2 (2008): 143-154.  \n[6] Denker, Katherine J. ”Student Response Systems and Facilitating the Large Lecture Basic Communica-tion \nCourse: Assessing Engagement and Learning.” Communication Teacher 27.1 (2013).  \n[7] Padhy, Neelamadhab, Pragnyaban Mishra, and Rasmita Panigrahi. ”The Survey of Data Mining Appli-\ncations And Feature Scope.” International Journal of Computer Science (2012).  \n[8] Gehringer, Edward F. ”Ac 2012-4769: Applications For Supporting Collaboration In The Classroom.” \n(2012).  \n[9] Gauci, Sally A., et al. ”Promoting student-centered active learning in lectures with a personal response \nsystem.” Advances in Physiology Education 33.1 (2009): 60-71.  \n[10] Poulis, J., et al. ”Physics lecturing with audience paced feedback.” American Journal of Physics 66 (1998): \n439.  \n[11] Scornavacca, Eusebio, Sid Huff, and Stephen Marshall. ”Mobile phones in the classroom: If you can´t beat \nthem, join them.” Communications of the ACM 52.4 (2009): 142-146.  \n[12] Br, Henning, Erik Tews, and Guido Rling. ”Improving feedback and classroom interaction using mobile \nphones.” Proceedings of Mobile Learning (2005): 55-62.  \n[13] Teevan, Jaime, et al. ”Displaying Mobile Feedback During a Presentation.” (2012).  \n[14] Leong, Chee Kian, Yew Haur Lee, and Wai Keong Mak. ”Mining sentiments in SMS texts for teaching \nevaluation.” Expert Systems with Applications 39.3 (2012): 2584-2589.  \n[15] Kinsella, Stephen. ”Many to one: Using the mobile phone to interact with large classes.” British Journal of \nEducational Technology 40.5 (2009): 956.  \n[16] Pang, Bo, and Lillian Lee. Opinion mining and sentiment analysis. Now Pub, 2008.  \n[17] Prabowo, Rudy, and Mike Thelwall. ”Sentiment analysis: A combined approach.” Journal of Informet-rics \n3.2 (2009): 143-157.  \n[18] Tian, Feng, et al. ”Can e-Learner’s emotion be recognized from interactive Chinese texts?.” Computer \nSupported Cooperative Work in Design, 2009. CSCWD 2009. 13th International Conference on. IEEE, \n2009.  \n[19] Rahayu, D. A., et al. ”RnR: Extracting Rationale from Online Reviews and Ratings.” Data Mining \nWorkshops (ICDMW), 2010 IEEE International Conference on. IEEE, 2010.  \n[20] Binali, Haji H., Chen Wu, and Vidyasagar Potdar. ”A new significant area: Emotion detection in E-learning \nusing opinion mining techniques.” Digital Ecosystems and Technologies, 2009. DEST’09. 3rd IEEE \nInternational Conference on. IEEE, 2009.  \n[21] Mejova, Yelena. ”Sentiment Analysis: An Overview. Comprehensive Exam Paper.” Computer Science \nDepartment (2009).  \n[22] de Groot, R. ”Data Mining for Tweet Sentiment Classification.” (2012).  \n[23] Go, Alec, Richa Bhayani, and Lei Huang. ”Twitter sentiment classification using distant supervision.” \nCS224N Project Report, Stanford (2009): 1-12.  \n[24] Taboada, Maite, et al. ”Lexicon-based methods for sentiment analysis.” Computational Linguistics 37.2 \n(2011): 267-307.  \n[25] Zuo, Mingzhang, et al. ”Data mining strategies and techniques of internet education public sentiment \nmonitoring and analysis system.” Future Computer and Communication (ICFCC), 2010 2nd International \nConference on. Vol. 2. IEEE, 2010.  \n[26] Sriram, Bharath, et al. ”Short text classification in twitter to improve information filtering.” Proceeding of \nthe 33rd international ACM SIGIR conference on research and development in information retrieval. ACM, \n2010.  \n[27] Prasad, Suhaas. Micro-blogging Sentiment Analysis Using Bayesian Classification Methods. Technical \nReport, 2010.  \n[28] Cummings, Richard G., and Maxwell Hsu. ”The effects of student response systems on performance and \nsatisfaction: An investigation in a tax accounting class.” Journal of College Teaching and Learning (TLC) \n4.12 (2011).  \n[29] Bifet, Albert, and Eibe Frank. ”Sentiment knowledge discovery in twitter streaming data.” Discovery \nScience. Springer Berlin/Heidelberg, 2010.  \n[30] Bhargavi, P., and S. Jyothi. ”Applying Naive Bayes data mining technique for classification of agricul-tural \nland soils.” International Journal of Computer Science and Network Security 9.8 (2009): 117-122.  \n[31] Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. ”Thumbs up?: sentiment classification using \nmachine learning techniques.” Proceedings of the ACL-02 conference on Empirical methods in natural \nlanguage processing-Volume 10. Association for Computational Linguistics, 2002.  \n",
      "id": 18098469,
      "identifiers": [
        {
          "identifier": "29584880",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:researchportal.port.ac.uk:publications/c99ed217-4c60-494f-8c2c-4f049cf43585",
          "type": "OAI_ID"
        },
        {
          "identifier": "52398881",
          "type": "CORE_ID"
        }
      ],
      "title": "SA-E: Sentiment Analysis for Education",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:researchportal.port.ac.uk:publications/c99ed217-4c60-494f-8c2c-4f049cf43585"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 38691011,
          "title": "A new significant area: Emotion detection in E-learning using opinion mining techniques.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1109/dest.2009.5276726",
          "raw": "Binali, Haji H., Chen Wu, and Vidyasagar Potdar. ”A new significant area: Emotion detection in E-learning using opinion mining techniques.” Digital Ecosystems and Technologies, 2009. DEST’09. 3rd IEEE International Conference on. IEEE, 2009.",
          "cites": null
        },
        {
          "id": 38690986,
          "title": "Ac 2012-4769: Applications For Supporting Collaboration In The Classroom.”",
          "authors": [],
          "date": "2012",
          "doi": null,
          "raw": "Gehringer, Edward F. ”Ac 2012-4769: Applications For Supporting Collaboration In The Classroom.” (2012).",
          "cites": null
        },
        {
          "id": 38691030,
          "title": "Applying Naive Bayes data mining technique for classification of agricul-tural land soils.”",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "Bhargavi, P., and S. Jyothi. ”Applying Naive Bayes data mining technique for classification of agricul-tural land soils.” International Journal of Computer Science and Network Security 9.8 (2009): 117-122.",
          "cites": null
        },
        {
          "id": 38691007,
          "title": "Can e-Learner’s emotion be recognized from interactive Chinese texts?.” Computer Supported Cooperative Work in Design,",
          "authors": [],
          "date": "2009",
          "doi": "10.1109/cscwd.2009.4968116",
          "raw": "Tian, Feng, et al. ”Can e-Learner’s emotion be recognized from interactive Chinese texts?.” Computer Supported Cooperative Work in Design, 2009. CSCWD 2009. 13th International Conference on. IEEE, 2009.",
          "cites": null
        },
        {
          "id": 38690977,
          "title": "Data mining for improving textbooks.”",
          "authors": [],
          "date": "2012",
          "doi": "10.1145/2207243.2207246",
          "raw": "Agrawal, Rakesh, et al. ”Data mining for improving textbooks.” ACM SIGKDD Explorations Newsletter 13.2 (2012): 7-19.",
          "cites": null
        },
        {
          "id": 38691014,
          "title": "Data Mining for Tweet Sentiment Classification.”",
          "authors": [],
          "date": "2012",
          "doi": null,
          "raw": "de Groot, R. ”Data Mining for Tweet Sentiment Classification.” (2012).",
          "cites": null
        },
        {
          "id": 38691019,
          "title": "Data mining strategies and techniques of internet education public sentiment monitoring and analysis system.”",
          "authors": [],
          "date": "2010",
          "doi": "10.1109/icfcc.2010.5497355",
          "raw": "Zuo, Mingzhang, et al. ”Data mining strategies and techniques of internet education public sentiment monitoring and analysis system.” Future Computer and Communication (ICFCC), 2010 2nd International Conference on. Vol. 2. IEEE, 2010.",
          "cites": null
        },
        {
          "id": 38690994,
          "title": "Displaying Mobile Feedback During a Presentation.”",
          "authors": [],
          "date": "2012",
          "doi": "10.1145/2371574.2371633",
          "raw": "Teevan, Jaime, et al. ”Displaying Mobile Feedback During a Presentation.” (2012).",
          "cites": null
        },
        {
          "id": 38690981,
          "title": "Effectiveness of feedback: the students perspective.” Assessment and Evaluation",
          "authors": [],
          "date": "2008",
          "doi": "10.1080/02602930601127869",
          "raw": "Poulos, Ann, and Mary Jane Mahony. ”Effectiveness of feedback: the students perspective.” Assessment and Evaluation in Higher Education 33.2 (2008): 143-154.",
          "cites": null
        },
        {
          "id": 38690993,
          "title": "Improving feedback and classroom interaction using mobile phones.”",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": "Br, Henning, Erik Tews, and Guido Rling. ”Improving feedback and classroom interaction using mobile phones.” Proceedings of Mobile Learning (2005): 55-62.",
          "cites": null
        },
        {
          "id": 38691016,
          "title": "Lexicon-based methods for sentiment analysis.”",
          "authors": [],
          "date": "2011",
          "doi": "10.1162/coli_a_00049",
          "raw": "Taboada, Maite, et al. ”Lexicon-based methods for sentiment analysis.” Computational Linguistics 37.2 (2011): 267-307.",
          "cites": null
        },
        {
          "id": 38691004,
          "title": "Many to one: Using the mobile phone to interact with large classes.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1111/j.1467-8535.2008.00888.x",
          "raw": "Kinsella, Stephen. ”Many to one: Using the mobile phone to interact with large classes.” British Journal of Educational Technology 40.5 (2009): 956.",
          "cites": null
        },
        {
          "id": 38691023,
          "title": "Micro-blogging Sentiment Analysis Using Bayesian Classification Methods.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "Prasad, Suhaas. Micro-blogging Sentiment Analysis Using Bayesian Classification Methods. Technical Report, 2010.",
          "cites": null
        },
        {
          "id": 38690992,
          "title": "Mobile phones in the classroom: If you can´t beat them, join them.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1145/1498765.1498803",
          "raw": "Scornavacca, Eusebio, Sid Huff, and Stephen Marshall. ”Mobile phones in the classroom: If you can´t beat them, join them.” Communications of the ACM 52.4 (2009): 142-146.",
          "cites": null
        },
        {
          "id": 38691005,
          "title": "Opinion mining and sentiment analysis. Now Pub,",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "Pang, Bo, and Lillian Lee. Opinion mining and sentiment analysis. Now Pub, 2008.",
          "cites": null
        },
        {
          "id": 38690991,
          "title": "Physics lecturing with audience paced feedback.”",
          "authors": [],
          "date": "1998",
          "doi": "10.1119/1.18883",
          "raw": "Poulis, J., et al. ”Physics lecturing with audience paced feedback.” American Journal of Physics 66 (1998): 439.",
          "cites": null
        },
        {
          "id": 38690961,
          "title": "Predicting student emotions in computer-human tutoring dialogues.”",
          "authors": [],
          "date": "2004",
          "doi": "10.3115/1218955.1219000",
          "raw": "Litman, Diane J., and Kate Forbes-Riley. ”Predicting student emotions in computer-human tutoring dialogues.” Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2004.",
          "cites": null
        },
        {
          "id": 38690989,
          "title": "Promoting student-centered active learning in lectures with a personal response system.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1152/advan.00109.2007",
          "raw": "Gauci, Sally A., et al. ”Promoting student-centered active learning in lectures with a personal response system.” Advances in Physiology Education 33.1 (2009): 60-71.",
          "cites": null
        },
        {
          "id": 38691009,
          "title": "RnR: Extracting Rationale from Online Reviews and Ratings.” Data Mining Workshops (ICDMW),",
          "authors": [],
          "date": "2010",
          "doi": "10.1109/icdmw.2010.167",
          "raw": "Rahayu, D. A., et al. ”RnR: Extracting Rationale from Online Reviews and Ratings.” Data Mining Workshops (ICDMW), 2010 IEEE International Conference on. IEEE, 2010.",
          "cites": null
        },
        {
          "id": 38691006,
          "title": "Sentiment analysis: A combined approach.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1016/j.joi.2009.01.003",
          "raw": "Prabowo, Rudy, and Mike Thelwall. ”Sentiment analysis: A combined approach.” Journal of Informet-rics 3.2 (2009): 143-157.",
          "cites": null
        },
        {
          "id": 38691013,
          "title": "Sentiment Analysis: An Overview.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "Mejova, Yelena. ”Sentiment Analysis: An Overview. Comprehensive Exam Paper.” Computer Science Department (2009).",
          "cites": null
        },
        {
          "id": 38691029,
          "title": "Sentiment knowledge discovery in twitter streaming data.” Discovery Science.",
          "authors": [],
          "date": "2010",
          "doi": "10.1007/978-3-642-16184-1_1",
          "raw": "Bifet, Albert, and Eibe Frank. ”Sentiment knowledge discovery in twitter streaming data.” Discovery Science. Springer Berlin/Heidelberg, 2010.",
          "cites": null
        },
        {
          "id": 38691022,
          "title": "Short text classification in twitter to improve information filtering.”",
          "authors": [],
          "date": "2010",
          "doi": "10.1145/1835449.1835643",
          "raw": "Sriram, Bharath, et al. ”Short text classification in twitter to improve information filtering.” Proceeding of the 33rd international ACM SIGIR conference on research and development in information retrieval. ACM, 2010.",
          "cites": null
        },
        {
          "id": 38690985,
          "title": "Student Response Systems and Facilitating the Large Lecture Basic Communica-tion Course: Assessing Engagement and",
          "authors": [],
          "date": "2013",
          "doi": "10.1080/17404622.2012.730622",
          "raw": "Denker, Katherine J. ”Student Response Systems and Facilitating the Large Lecture Basic Communica-tion Course: Assessing Engagement and Learning.” Communication Teacher 27.1 (2013).",
          "cites": null
        },
        {
          "id": 38691025,
          "title": "The effects of student response systems on performance and satisfaction: An investigation in a tax accounting class.”",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Cummings, Richard G., and Maxwell Hsu. ”The effects of student response systems on performance and satisfaction: An investigation in a tax accounting class.” Journal of College Teaching and Learning (TLC) 4.12 (2011).",
          "cites": null
        },
        {
          "id": 38690959,
          "title": "The implementation of social networking as a tool for improving student participation in the classroom.”",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Novak, Jeremy, and Michael Cowling. ”The implementation of social networking as a tool for improving student participation in the classroom.” (2011).",
          "cites": null
        },
        {
          "id": 38691034,
          "title": "Thumbs up?: sentiment classification using machine learning techniques.”",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1118693.1118704",
          "raw": "Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. ”Thumbs up?: sentiment classification using machine learning techniques.” Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics, 2002.",
          "cites": null
        },
        {
          "id": 38691015,
          "title": "Twitter sentiment classification using distant supervision.” CS224N Project Report,",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "Go, Alec, Richa Bhayani, and Lei Huang. ”Twitter sentiment classification using distant supervision.” CS224N Project Report, Stanford (2009): 1-12.",
          "cites": null
        },
        {
          "id": 38690963,
          "title": "Using Feedback Tags and Sentiment Analysis to Generate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags",
          "authors": [],
          "date": "2010",
          "doi": "10.1109/icalt.2010.186",
          "raw": "Cummins, Stephen, Liz Burd, and Andrew Hatch. ”Using Feedback Tags and Sentiment Analysis to Generate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags in a Programming Course.” Advanced Learning Technologies (ICALT), 2010 IEEE 10th International Conference on. IEEE, 2010.",
          "cites": null
        },
        {
          "id": 38691000,
          "title": "Yew Haur Lee, and Wai Keong Mak. ”Mining sentiments in SMS texts for teaching evaluation.” Expert Systems with",
          "authors": [],
          "date": "2012",
          "doi": "10.1016/j.eswa.2011.08.113",
          "raw": "Leong, Chee Kian, Yew Haur Lee, and Wai Keong Mak. ”Mining sentiments in SMS texts for teaching evaluation.” Expert Systems with Applications 39.3 (2012): 2584-2589.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "https://researchportal.port.ac.uk/portal/services/downloadRegister/209654/idt13-014-published.pdf"
      ],
      "updatedDate": "2021-08-17T07:17:57",
      "yearPublished": 2013,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/29584880.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/29584880"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/29584880/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/29584880/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/18098469"
        }
      ]
    },
    {
      "acceptedDate": "2009-07-28T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Adam",
        "James"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/143907117",
        "https://api.core.ac.uk/v3/outputs/192641348",
        "https://api.core.ac.uk/v3/outputs/11309106",
        "https://api.core.ac.uk/v3/outputs/147598315"
      ],
      "createdDate": "2013-07-10T11:53:32",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        }
      ],
      "depositedDate": "2009-01-01T00:00:00",
      "abstract": "Evaluation of sentiment analysis, like large-scale IR evalu-\n\nation, relies on the accuracy of human assessors to create\n\njudgments. Subjectivity in judgments is a problem for rel-\n\nevance assessment and even more so in the case of senti-\n\nment annotations. In this study we examine the degree to\n\nwhich assessors agree upon sentence-level sentiment anno-\n\ntation. We show that inter-assessor agreement is not con-\n\ntingent on document length or frequency of sentiment but\n\ncorrelates positively with automated opinion retrieval per-\n\nformance. We also examine the individual annotation cate-\n\ngories to determine which categories pose most di±culty for\n\nannotators",
      "documentType": "research",
      "doi": "10.1145/1571941.1572127",
      "downloadUrl": "https://core.ac.uk/download/11309106.pdf",
      "fieldOfStudy": "computer science",
      "fullText": "A Study of Inter-Annotator Agreement for Opinion Retrieval\nAdam Bermingham and Alan F. Smeaton\nCLARITY: Centre for Sensor Web Technologies\nDublin City University\nDublin, Ireland.\n{abermingham,asmeaton}@computing.dcu.ie\nABSTRACT\nEvaluation of sentiment analysis, like large-scale IR evalu-\nation, relies on the accuracy of human assessors to create\njudgments. Subjectivity in judgments is a problem for rel-\nevance assessment and even more so in the case of senti-\nment annotations. In this study we examine the degree to\nwhich assessors agree upon sentence-level sentiment anno-\ntation. We show that inter-assessor agreement is not con-\ntingent on document length or frequency of sentiment but\ncorrelates positively with automated opinion retrieval per-\nformance. We also examine the individual annotation cate-\ngories to determine which categories pose most difficulty for\nannotators.\nCategories and Subject Descriptors\nH.3.4 [Information Retrieval]: Systems and Software Per-\nformance evaluation (efficiency and effectiveness)\nGeneral Terms\nExperimentation, Measurement, Human Factors\n1. INTRODUCTION\nWith the abundance of user-generated content on the Inter-\nnet in recent years, there has been much effort to model the\nsentiment in online texts. Annotated documents are nec-\nessary to evaluate systems designed to automatically clas-\nsify, rank or score documents with respect to opinion. In\nsome domains, such as film reviews, a sentiment polarity\nscore is often readily available as users annotate their doc-\numents with a quantified summary e.g. 4 out of 5 stars.\nIn other domains an author annotation is not available and\nwe rely on human assessors to create annotations or judge-\nments. There are a number of subjective variables associ-\nated specifically with opinion annotation which affect agree-\nment including domain expertise, personal opinion, ambigu-\nity of language, and context of interpretation. One other\nissue is granularity of sentiment and previous annotation ef-\nforts have varied from the document level [3] to sentence-\nand sub-sentence-levels [5]. There have also been efforts at\nmulti-lingual sentence-level opinion annotation which have\nyielded moderately high rates of agreement for Japanese and\nChinese but low agreement for English texts [4].\nCopyright is held by the author/owner(s).\nSIGIR’09, July 19–23, 2009, Boston, Massachusetts, USA.\nACM 978-1-60558-483-6/09/07.\nUsing documents from the Blogs06 corpus used at the\nTREC Blog Track [3], we asked participants to identify\nopinion at sentence-level. We then measure sentence-level\ninter-annotator reliability for all sentences and repeat this\nfor each document and topic. Extrapolating annotations\nto the document-level, we then draw comparisons between\nsentence-level and document-level agreement. Finally, we\nconvert the annotations to binary judgements for each an-\nnotation class to allow per-class analysis.\n2. EXPERIMENTAL SETUP\nOur 15 participants were postgraduate students and post-\ndoctoral researchers, 5 of whom have worked in sentiment\nanalysis and 13 of whom were native English speakers. 15\ntopics were selected out of the 150 topics used in the Blog\nTrack at TREC 2008 based on median TREC participant\nperformance per topic, evenly distributed from low-performing\ntopics to high-performing topics. A pool of documents was\nselected from our own baseline TREC run [1], up to a max-\nimum of 8 documents per topic. All of the documents se-\nlected were judged by the TREC relevance assessments to\ncontain opinion on the topic and consisted of plain text blog\nentries extracted from HTML and passed through the noise\nremoval portion of our TREC system.\nIn the annotation process, participants were presented\nwith a series of 30 topic/document pairings and asked to\nannotate the sentences in each document as one of five cate-\ngories: “non-relevant”, “relevant” (relevant and no opinion),\n“positive”, “negative”, “mixed”. When a document is initially\npresented to a participant, all of the sentences are annotated\nas non-relevant. After completing the sentence-level anno-\ntation, participants were then asked to rate the document\nfor negative opinion from 1 (“no negative topic-directed opin-\nion”) to 5 (“very obvious and intense negative topic-directed\nopinion”) and similarly for positive opinion.\nIn total, 115 documents were judged by an average of 3.6\nannotators yielding 26,375 sentence annotations.\n3. RESULTS AND EVALUATION\nWe use Krippendorff’s alpha[2] for measuring inter-annotator\nagreement. This is a robust statistic which takes into ac-\ncount the probability that observed variability is due to\nchance and does not require that each annotator annotate\neach document. α for sentence-level annotation with respect\nto the 5 classes in Section 2 is 0.4219. This indicates a signif-\nicant agreement between annotators but is less than the level\nrecommended by Krippendorff for reliable data (α = 0.8) or\nfor tentative reliability (α = 0.667).\nFigure 1: α for Binary Judgements\nFigure 2: α and mean TREC MAP (ρ = 0.53,τ = 0.41)\nIf we examine α for each of the 115 documents, we see\nlittle correlation between α and the number of sentences per\ndocument (Pearson’s ρ = −0.123, Kendall’s τ = −0.13) or\nbetween α and the proportion of sentences annotated as con-\ntaining opinion (ρ = −0.045, τ = −0.015). This indicates\nthat the consistency between annotators is not dependent\nupon the proportion of sentiment-bearing sentences or the\noverall length of the document.\nCalculating α for each of the 15 topics, we see a significant\npositive correlation between the retrieval performance of the\ntopics at TREC and α for each topic (ρ = 0.53, τ = 0.41).\nThis reflects the increased ambiguity and obscurity among\nthe low-performing topics which hampers both automated\nopinion retrieval and manual annotation efforts similarly. A\nranking of the 15 topics by α demonstrates no discernable\npattern in terms of topic nature.\nIn order to simulate document-level annotations, we ex-\ntrapolate document-level annotations from sentence-level an-\nnotations. For example, a document containing positive sen-\ntences but no mixed or negative sentences would be consid-\nered a positive document. Agreement for these document-\nlevel annotations is slightly higher than for sentence-level\n(α = 0.4461) suggesting that although annotators may dif-\nfer in their reasons for document annotation, they converge\na small amount at document-level. It should be noted that\nthe simulated document-level annotations are not necessar-\nily the same as would be obtained had the annotators been\nexplicitly asked to annotate at the document-level.\nTo look at the individual classes more closely we map\nthe 5-way annotations to binary judgements for each of the\nclasses (Figure 1). The most striking difference in agree-\nment between document and sentence-level annotation is for\nthe mixed class. Agreement for this class is highest at the\ndocument-level and lowest at the sentence-level. It is also\nworth noting that there is much less agreement for negativity\nthan positivity at document-level and that agreement is very\nlow for the relevant class, particularly at the sentence-level.\nIf we look at an additional aggregate class, All Relevant,\nthere is a surprisingly low α for extrapolated document-level\nrelevance. This possibly reflects the fact that 11% of docu-\nments were annotated as non-relevant, despite none of them\nbeing judged that way at TREC.\nFinally, we determine a binary opinion judgement from\nthe two document-level 5 point scales. A document is de-\nfined as opinionated if either of the scales record a value\ngreater than 1 for that document. If each of these opinion\njudgements is compared with its corresponding opinion an-\nnotation extrapolated from sentence annotations, we see a\nvery high level of agreement (α = 0.8263). This shows con-\nsistency between each annotator’s sentence and document-\nlevel annotations.\n4. CONCLUSIONS AND DISCUSSION\nWe have found that sentence-level sentiment annotation yields\na moderate level of inter-annotator agreement and that this\nis independent of the nature of the sentiment, specifically the\nfrequency of the sentiment and document length. We sug-\ngest that the 5 classifications used here (and in TREC) are\nnot ideal categories for sentiment annotation. In particular,\nthe mixed category shows very low agreement at sentence-\nlevel. At document-level there is high agreement but only\ndue to the broad definition of mixed as a document con-\ntaining both positive and negative opinions. This does not\nnecessarily reflect the overriding sentiment in a document as\nboth sides of a discussion are frequently cited in distinctly\npolarised documents, yielding an artificially high proportion\nof mixed documents.\nAnnotators reported frequently feeling uneasy about their\njudgements, particularly where domain or background knowl-\nedge was required. For this reason we suggest an indetermi-\nnate class which they are encouraged to use when they are\nnot confident about their annotation. We would also like to\nexamine the task description and annotator training more\nclosely to see what effect it may have on agreement.\nFinally, we show an increase in agreement can be achieved\nby simulating document-level judgements, suggesting that\nsentence-level annotation is too granular. For future work\nwe would like to compare annotation at the document, para-\ngraph, sentence and passage levels in an effort to identify the\nmost appropriate sentiment granularity, both for annotation\nand automated opinion retrieval.\nAcknowledgments\nThis work is supported by Science Foundation Ireland under\ngrant 07/CE/I1147.\n5. REFERENCES\n[1] A. Bermingham, A. Smeaton, J. Foster, and D. Hogan.\nDCU at the TREC 2008 Blog Track. In The\nSeventeenth Text REtrieval Conference (TREC 2008)\nProc., 2008.\n[2] A. F. Hayes and K. Krippendorff. Answering the call\nfor a standard reliability measure for coding data. In\nCommunication Methods and Measures, 2007.\n[3] I. Ounis, C. MacDonald, and I. Soboroff. Overview of\nthe TREC-2008 Blog Track. In The Text REtrieval\nConference (TREC 2008) Proc. NIST, 2008.\n[4] Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and\nN. Kando. Overview of multilingual opinion analysis\ntask at NTCIR-7. 2008.\n[5] J. Wiebe, T. Wilson, and C. Cardie. Annotating\nexpressions of opinions and emotions in language.\nLanguage Resources and Evaluation, 1(2):0, 2005.\n",
      "id": 4760700,
      "identifiers": [
        {
          "identifier": "oai:doras.dcu.ie:14779",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.147.6497",
          "type": "OAI_ID"
        },
        {
          "identifier": "147598315",
          "type": "CORE_ID"
        },
        {
          "identifier": "21085589",
          "type": "CORE_ID"
        },
        {
          "identifier": "192641348",
          "type": "CORE_ID"
        },
        {
          "identifier": "143907117",
          "type": "CORE_ID"
        },
        {
          "identifier": "2002121299",
          "type": "MAG_ID"
        },
        {
          "identifier": "10.1145/1571941.1572127",
          "type": "DOI"
        },
        {
          "identifier": "11309106",
          "type": "CORE_ID"
        }
      ],
      "title": "A study of inter-annotator agreement for opinion retrieval",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:doras.dcu.ie:14779",
        "oai:citeseerx.psu:10.1.1.147.6497"
      ],
      "publishedDate": "2009-01-01T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [
        {
          "id": 16581608,
          "title": "Annotating expressions of opinions and emotions in language.",
          "authors": [],
          "date": "2005",
          "doi": "10.1007/s10579-005-7880-9",
          "raw": "J. Wiebe, T. Wilson, and C. Cardie. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2):0, 2005.",
          "cites": null
        },
        {
          "id": 16581596,
          "title": "Answering the call for a standard reliability measure for coding data.",
          "authors": [],
          "date": "2007",
          "doi": "10.1080/19312450709336664",
          "raw": "A. F. Hayes and K. Krippendor®. Answering the call for a standard reliability measure for coding data. In Communication Methods and Measures, 2007.",
          "cites": null
        },
        {
          "id": 16581605,
          "title": "Overview of multilingual opinion analysis task at NTCIR-7.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multilingual opinion analysis task at NTCIR-7. 2008.",
          "cites": null
        },
        {
          "id": 16581601,
          "title": "Overview of the TREC-2008 Blog Track.",
          "authors": [],
          "date": "2008",
          "doi": "10.1145/1842890.1842899",
          "raw": "I. Ounis, C. MacDonald, and I. Soboro®. Overview of the TREC-2008 Blog Track. In The Text REtrieval Conference (TREC 2008) Proc. NIST, 2008.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/14779/1/sigir146-bermingham.pdf",
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.6497"
      ],
      "updatedDate": "2021-12-13T06:11:14",
      "yearPublished": 2009,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11309106.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11309106"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11309106/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11309106/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4760700"
        }
      ]
    },
    {
      "acceptedDate": "2017-10-23T00:00:00",
      "arxivId": "1706.02141",
      "authors": [
        {
          "name": "Alonso-Alonso, Iago"
        },
        {
          "name": "Gómez-Rodríguez, Carlos"
        },
        {
          "name": "Vilares, David"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/243152629"
      ],
      "createdDate": "2017-06-19T02:47:25",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2017-10-23T00:00:00",
      "abstract": "Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence\napplications that need to extract meaning from natural language text or speech.\nSentiment analysis is one example of application for which parsing has recently\nproven useful.\n  In recent years, there have been significant advances in the accuracy of\nparsing algorithms. In this article, we perform an empirical, task-oriented\nevaluation to determine how parsing accuracy influences the performance of a\nstate-of-the-art rule-based sentiment analysis system that determines the\npolarity of sentences from their parse trees. In particular, we evaluate the\nsystem using four well-known dependency parsers, including both current models\nwith state-of-the-art accuracy and more innacurate models which, however,\nrequire less computational resources.\n  The experiments show that all of the parsers produce similarly good results\nin the sentiment analysis task, without their accuracy having any relevant\ninfluence on the results. Since parsing is currently a task with a relatively\nhigh computational cost that varies strongly between algorithms, this suggests\nthat sentiment analysis researchers and users should prioritize speed over\naccuracy when choosing a parser; and parsing researchers should investigate\nmodels that improve speed further, even at some cost to accuracy.Comment: 19 pages. Accepted for publication in Artificial Intelligence Review.\n  This update only adds the DOI link to comply with journal's term",
      "documentType": "research",
      "doi": "10.1007/s10462-017-9584-0",
      "downloadUrl": "http://arxiv.org/abs/1706.02141",
      "fieldOfStudy": null,
      "fullText": "Noname manuscript No.\n(will be inserted by the editor)\nHow Important is Syntactic Parsing Accuracy?\nAn Empirical Evaluation on Rule-Based Sentiment\nAnalysis\nCarlos Go´mez-Rodr´ıguez · Iago\nAlonso-Alonso · David Vilares\nReceived: date / Accepted: date\nThis is the accepted manuscript (final peer-reviewed manuscript) accepted for publication\nin Artificial Intelligence Review, and may not reflect subsequent changes resulting from the\npublishing process such as editing, formatting, pagination, and other quality control mecha-\nnisms. The final publication in Artificial Intelligence Review is available at link.springer.com\nvia http://dx.doi.org/10.1007/s10462-017-9584-0.\nAbstract Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence applica-\ntions that need to extract meaning from natural language text or speech. Sentiment\nanalysis is one example of application for which parsing has recently proven useful.\nIn recent years, there have been significant advances in the accuracy of parsing\nalgorithms. In this article, we perform an empirical, task-oriented evaluation to\ndetermine how parsing accuracy influences the performance of a state-of-the-art\nrule-based sentiment analysis system that determines the polarity of sentences\nfrom their parse trees. In particular, we evaluate the system using four well-known\ndependency parsers, including both current models with state-of-the-art accuracy\nand more innacurate models which, however, require less computational resources.\nThe experiments show that all of the parsers produce similarly good results in\nthe sentiment analysis task, without their accuracy having any relevant influence\non the results. Since parsing is currently a task with a relatively high computational\ncost that varies strongly between algorithms, this suggests that sentiment analysis\nresearchers and users should prioritize speed over accuracy when choosing a parser;\nCarlos Go´mez-Rodr´ıguez has received funding from the European Research Council (ERC),\nunder the European Union’s Horizon 2020 research and innovation programme (FASTPARSE,\ngrant agreement No 714150), Ministerio de Economı´a y Competitividad (FFI2014-51978-C2-\n2-R), and the Oportunius Program (Xunta de Galicia). Iago Alonso-Alonso was funded by an\nOportunius Program Grant (Xunta de Galicia). David Vilares has received funding from the\nMinisterio de Educacio´n, Cultura y Deporte (FPU13/01180) and Ministerio de Economı´a y\nCompetitividad (FFI2014-51978-C2-2-R).\nCarlos Go´mez-Rodr´ıguez · Iago Alonso-Alonso · David Vilares\nFASTPARSE Lab, Grupo LyS, Departamento de Computacio´n, Universidade da Corun˜a\nCampus de A Corun˜a s/n, 15071, A Corun˜a, Spain\nTel.: +34 881 01 1396\nFax: +34 981 167 160\nE-mail: carlos.gomez@udc.es, iago.alonso@udc.es, david.vilares@udc.es\nar\nX\niv\n:1\n70\n6.\n02\n14\n1v\n3 \n [c\ns.C\nL]\n  2\n4 O\nct \n20\n17\n2 Carlos Go´mez-Rodr´ıguez et al.\nand parsing researchers should investigate models that improve speed further, even\nat some cost to accuracy.\nKeywords Syntactic Parsing · Sentiment Analysis · Natural Language Process-\ning · Artificial Intelligence\n1 Introduction\nHaving computers successfully understand the meaning of sentences in human lan-\nguages is a long-standing key goal in artificial intelligence (AI). While full under-\nstanding is still far away, recent advances in the field of natural language processing\n(NLP) have made it possible to implement systems that can successfully extract\nrelevant information from natural language text or speech. Syntactic parsing, the\ntask of finding the internal structure of a sentence, is a key step in that process,\nas the predicate-argument structure of sentences encodes crucial information to\nunderstand their semantics. For example, a text mining system that needs to gen-\nerate a report on customers’ opinions about phones may find statements like “the\niPhone is much better than the HTC 10” and “the HTC 10 is much better than the\niPhone”, which are identical in terms of the individual words that they contain. It\nis the syntactic structure – in this case, the subject and the attribute of the verb\nto be – that tells us which of the phones is preferred by the customer.\nIn recent years, parsing has gone from a merely promising basic research\nfield to see widespread use in useful AI applications such as machine transla-\ntion (Miceli Barone and Attardi 2015; Xiao et al 2016), information extraction\n(Song et al 2015; Yu et al 2015), textual entailment recognition (Pado´ et al 2015),\nlearning for game AI agents (Branavan et al 2012) or sentiment analysis (Joshi and\nPenstein-Rose´ 2009; Vilares et al 2015b,a). Meanwhile, researchers have produced\nimprovements in parsing algorithms and models that have increased their accuracy,\nup to a point where some parsers have achieved levels comparable to agreement\nbetween experts on English newswire text (Berzak et al 2016), although this does\nnot generalize to languages that present extra challenges for parsing (Farghaly and\nShaalan 2009) or to noisy text such as tweets (Kong et al 2014). However, parsers\nconsume significant computational resources, which can be an important concern\nin large-scale applications (Clark et al 2009), and the most accurate models often\ncome at a higher computational cost (Andor et al 2016; Go´mez-Rodr´ıguez 2016).\nTherefore, an interesting question is how much influence parsing accuracy has on\nthe performance of downstream applications, as this can be essential to make an\ninformed choice of a parser to integrate in a given system.\nIn this article, we analyze this issue for sentiment analysis (SA), i.e., the use of\nnatural language processing to extract and identify subjective information (opin-\nions about relevant entities) from natural language texts. Sentiment analysis is\none of the most relevant practical applications of NLP, it has been recently shown\nto benefit from parsing (Socher et al 2013; Vilares et al 2015b) and it is especially\nuseful at a large scale (as millions of texts of potential interest for opinion extrac-\ntion are generated every day in social networks), making the potential accuracy\nvs. speed tradeoff especially relevant.\nFor this purpose, we take a state-of-the-art syntax-based sentiment analysis\nsystem (Vilares et al 2017), which calculates the polarity of a text (i.e., whether it\nHow Important is Syntactic Parsing Accuracy? 3\nexpresses a positive, negative or neutral stance) relying on its dependency parse\ntree; and we test it with a set of well-known syntactic parsers, including mod-\nels with state-of-the-art accuracy and others that are less accurate, but have a\nsmaller computational cost, evaluating how the choice of parser affects the accu-\nracy of the polarity classification. Our results show that state-of-the-art parsing\naccuracy does not provide additional benefit for this sentiment analysis task, as\nall of the parsers tested produce similarly good polarity classification accuracy\n(no statistically significant differences, all p-values ≥ 0.49). Therefore, our results\nsuggest that it makes sense to use the fastest parsers for this task, even if they are\nnot the most accurate.\nThe remainder of this article is organized as follows: we review the state of\nthe art in syntactic parsing and syntax-based sentiment analysis in Section 2, we\ndescribe our experimental setup in Section 3, we report the results in Section 4,\nand discuss their implications in Section 5. Finally, Section 6 draws our conclusion\nand discusses possible avenues for future work.\n2 Background\nWe now provide an overview of research in parsing and sentiment analysis that is\nrelevant to this study.\n2.1 Parsing\nDifferent linguistic theories define different ways in which the syntactic structure\nof a sentence can be described. In particular, the overwhelming majority of natural\nlanguage parsers in the literature adhere to one of two dominant representations.\nIn constituency grammar (or phrase structure grammar), sentences are analyzed by\nbreaking them up into segments called constituents, which are in turn decomposed\ninto smaller constituents, as in the example of Figure 1. In dependency grammar,\nthe syntax of a sentence is represented by directed binary relations between its\nwords, called dependencies, which are most useful when labeled with their syntactic\nroles, such as subject and object, as in Figure 2. Each of these representation\ntypes provides different information about the sentence, and it is not possible to\nfully map constituency to dependency representations or vice versa (Kahane and\nMazziotta 2015).\nIn this paper we will focus on dependency parsing, as it is the predominant\nrepresentation used by most of the downstream AI applications mentioned above\n– with machine translation as an arguable exception, where constituent parsing\nis often used due to the adequacy of phrase structure grammar for modeling re-\nordering of words between languages (DeNeefe and Knight 2009; Xiao et al 2016)\n–, and it is also the alternative used by the syntax-based SA system we will use in\nour experiments.\nMost dependency parsing systems in the literature can be grouped into two\nbroad categories (McDonald and Nivre 2007): graph-based and transition-based\n(shift-reduce) parsers.\nGraph-based parsers use models that score dependency relations or groups of\nthem, and perform a global search for a parse that will maximize the combined\n4 Carlos Go´mez-Rodr´ıguez et al.\nThe    kid    broke    the    red    toy    with    a    hammer\nDET NOUN VERB DET NOUNADJ PREP DET NOUN\nNP\nNP PP\nVP\nS\nS   →  NP VP\nNP→  DET NOUN\nNP→  DET ADJ NOUN\nPP →  PREP DET NOUN\nVP→  VERB NP PP\nFig. 1 A valid constituency parse for the sentence ‘The kid broke the red toy with a hammer’ .\nThe sentence is divided into constituents according to the constituency grammar defined at\nthe left part of the picture\nThe    kid    broke    the    red    toy    with    a    hammer\nsubj\ndobj\nadpmod\ndet det\ndet\namod\nadpobj\nFig. 2 A valid dependency parse for the sentence: ‘The kid broke the red toy with a hammer’ .\nThe sentence is represented as a graph of binary relations between words that represent the\nexisting syntactic relation between them (e.g. ‘kid’ is the subject of the verb ‘broke’)\nscore of all dependencies. Under the assumption of projectivity (i.e., that there are\nno crossing dependencies), there are several dynamic programming algorithms that\nperform exact search in cubic time (Eisner 1996; Go´mez-Rodr´ıguez et al 2008), but\nthis restriction is not realistic in practice (Go´mez-Rodr´ıguez 2016). Unfortunately,\nexact inference has been shown to be intractable for models that support arbitrary\nnon-projectivity, except under strong independence assumptions (McDonald and\nSatta 2007) which enable parsing in quadratic time with maximum spanning tree\nalgorithms (McDonald et al 2005), but severely limit the expressivity of the feature\nmodels that can be used. This restriction can be avoided by using so-called mildly\nnon-projective parsing algorithms, which support the overwhelming majority of\nnon-projective analyses that can be found in real linguistic structures (Go´mez-\nRodr´ıguez et al 2011; Cohen et al 2011; Pitler et al 2013); but they have super-\ncubic complexities that make them too slow for practical use. Another option is\nto forgo exact inference, using approximate inference algorithms with rich feature\nmodels instead. This is the approach taken by TurboParser (Martins et al 2010,\n2013), which currently is the most popular graph-based parser as it can provide\nstate-of-the-art accuracy with a reasonable computational cost.\nTransition-based parsers are based on a state machine that builds syntactic\nanalyses step by step, typically from left to right. A statistical or machine learn-\ning model scores each of the possible transitions to take at each state, and a\nsearch strategy is used to find a high-scoring sequence of transitions. The earlier\napproaches to transition-based parsing, like the MaltParser system (Nivre et al\n2007) used greedy deterministic search for this purpose, which is especially fast,\nbut is prone to obtain suboptimal solutions due to bad decisions at early stages\nthat result in error propagation. This problem is alleviated by instead performing\nbeam search (Zhang and Nivre 2011) or dynamic programming (Huang and Sagae\nHow Important is Syntactic Parsing Accuracy? 5\n2010; Kuhlmann et al 2011) to explore transition sequences, but this increases\nthe computational cost. Other alternatives that provide a good speed-accuracy\ntradeoff are selectional branching, which uses confidence estimates to decide when\nto employ a beam (Choi and McCallum 2013), or dynamic oracles, which reduce\nthe error propagation in greedy search by exploring non-optimal transition se-\nquences during training (Goldberg and Nivre 2012). In the last two years, several\ntransition-based parsers have appeared that use neural networks as their scoring\nmodel (Chen and Manning 2014; Dyer et al 2015; Andor et al 2016), providing\nvery good accuracy.\n2.2 Parsing Evaluation\nThe standard metrics to evaluate the accuracy of a dependency parser are the\nunlabeled attachment score (UAS: the proportion of words that are attached to\nthe correct head word by means of a dependency, regardless of its label), labeled\nattachment score (LAS: the proportion of words that are attached to the correct\nhead by means of a dependency that has the correct label) and label accuracy (LA:\nthe proportion of words that are assigned the correct dependency type). However,\nthe performance of a parser in terms of such scores is not necessarily proportional\nto its usefulness for a given task, as not all dependencies in a syntactic analysis are\nequally useful in practice, or equally difficult to analyze (Nivre et al 2010; Bender\net al 2011). Therefore, LAS, UAS and LA are of limited use for researchers and\npractitioners that work with downstream applications in NLP. For this purpose,\nit is more useful to perform task-oriented evaluation, i.e., to experiment with\nthe parsers in the actual tasks for which they are going to be used (Volokh and\nNeumann 2012).\nSuch evaluations have been performed for some specific NLP tasks, namely in-\nformation extraction (Miyao et al 2008; Buyko and Hahn 2010), textual entailment\nrecognition (Yuret et al 2010; Volokh and Neumann 2012) and machine translation\n(Quirk and Corston-Oliver 2006; Goto et al 2011; Popel et al 2011). However, these\ncomparisons are currently somewhat dated, as they were performed before the ad-\nvent of the major advances in parsing accuracy of the current decade reviewed in\nSection 2.1, such as beam-search transition-based parsing, dynamic oracles, ap-\nproximate variational inference (TurboParser) or neural network parsing. Even\nmore importantly, these analyses provide very different results depending on each\nspecific task and, to our knowledge, no evaluation of parsers has been performed\nfor sentiment analysis, a task where a good speed-accuracy tradeoff is especially\nimportant due to its extensive applications to the Web and social networks.\n2.3 Syntax-based Sentiment Analysis\nA number of state-of-the-art models for SA using different morphological (Khan\net al 2016a,b) and syntactic approaches have proven useful in recent years. Liu\net al (2016) pointed out the benefits of syntactical approaches with respect to\nstatistical models on opinion target extraction, such as domain independence, and\npropose two approaches to select a set of rules, that even being suboptimal, achieve\nbetter results than a state-of-the-art conditional random field supervised method.\n6 Carlos Go´mez-Rodr´ıguez et al.\nWu et al (2009) defined an approach to extract product features through phrase\ndependency parsing: they first combine the output of a shallow and a word-level\ndependency parser to then extract features and feed a support vector machine\n(SVM) with a novel tree kernel function. Their experimental results outperformed\na number of bag-of-words baselines. Jia et al (2009) and Asmi and Ishaya (2012)\ndefined a set of syntax-based rules for identifying and handling negation on nat-\nural language texts represented as dependency trees. They also pointed out the\nadvantage of using this kind of methods with respect to traditional lexicon-based\nperspectives in tasks such as opinion mining or information retrieval. Poria et al\n(2014) posed a set of syntax-based patterns for a concept-level approach to de-\ntermine how the sentiment flows from concept to concept, assuming that such\nconcepts present in texts are represented as nodes of a dependency tree.\nJoshi and Penstein-Rose´ (2009) introduced the concept of generalized triplets,\nusing them as features for a supervised classifier and showing its usefulness for\nsubjectivity detection. Given a dependency triplet, the authors proposed to gen-\neralize the head or the dependent term (or even both at the same time) to its\ncorresponding part-of-speech tag. Thus, the triplet (car, modified, good) could be\ngeneralized as (NOUN, modifier, good), which can be useful to correctly classify\nsimilar triplets that did not appear in the training set (e.g. (bicycle, modifier,\ngood) or (job, modifier, good)). In a similar line, Vilares et al (2015c) enriched the\nconcept of generalized dependency triplets and showed that they can be exploited\nas features to feed a supervised SA system for polarity classification, as long as\nenough labeled data is available. The same authors (Vilares et al 2015b) proposed\nan unsupervised syntax-based approach for polarity classification on Spanish re-\nviews represented as Ancora trees (Taule´ et al 2008). They showed that their\nsystem outperforms the equivalent lexical-based approach (Taboada et al 2011).\nIn this line, however, Taboada et al (2011) pointed out that one of the challenges\nwhen using parsing techniques for sentiment analysis is the need of fast parsers\nthat are able to process in real-time the huge amount of information shared by\nusers in social media.\nWith the recent success of deep learning, Socher et al (2013) syntactically\nannotated a sentiment treebank to then train a recursive neural network that\nlearns how to apply semantic composition for relevant phenomena in SA, such as\nnegation or ‘but’ adversative clauses, over dependency trees. Kalchbrenner et al\n(2014) introduced a convolutional neural network for modeling sentences and used\nit for polarity classification among other tasks. Their approach does not explicitly\nrely on any parser, but the authors argue that one of the strengths of their model\ncomes from the capability of the network to implicitly learn internal syntactic\nrepresentations.\n3 Materials and Methods\nWe now describe the systems, corpora and methods used for our task-oriented\nevaluation.\nHow Important is Syntactic Parsing Accuracy? 7\n3.1 Parsing systems\n– MaltParser: Introduced by Nivre et al (2007), this system can be used to train\ntransition-based parsers with greedy deterministic search. Although its accu-\nracy has fallen behind the state of the art, it is still widely used, probably owing\nto its maturity and solid documentation. Additionally, due to its greedy nature,\nMaltParser is very fast. Following common practice, we use it together with\nthe feature optimization tool MaltOptimizer1 (Ballesteros and Nivre 2012) to\noptimize the parameters and train a suitable model. The trained MaltParser\nmodel uses a standard arc-eager (transition-based) parsing algorithm, where\nat each step the movement to apply is selected among the set of possible tran-\nsitions, previously scored by a linear model, which is faster than using models\nbased on SVMs.\n– TurboParser (Martins et al 2013): A graph-based parser that uses approximate\nvariational inference with non-local features. It has become the most widely\nused graph-based parser, as it provides better speed and accuracy than previous\nalternatives. We use its default configuration, training a second-order non-\nprojective parser with features for arcs, consecutive siblings and grandparents,\nusing the AD3 algorithm as a decoder.\n– YaraParser (Rasooli and Tetreault 2015): A recent transition-based parser,\nwhich uses beam search (Zhang and Nivre 2011) and dynamic oracles (Gold-\nberg and Nivre 2012) to provide state-of-the-art accuracy. Its default configu-\nration is used.\n– Stanford RNN Parser (Chen and Manning 2014): The most popular among\nthe recent wave of transition-based parsers that employ neural networks, it can\nachieve robust accuracy in spite of using greedy deterministic search. We use\npretrained GloVe (Pennington et al 2014) embeddings as input to the parser:\nin particular, 50-dimensional word embeddings2 trained on Wikipedia and the\nEnglish Gigaword (Napoles et al 2012).\n3.2 Parsing corpus\nTo train and evaluate the parsing accuracy of such parsers, we are using the En-\nglish Universal Treebank v2.0 created by McDonald et al (2013). It is a mapping\nfrom the (constituency) Penn treebank (Marcus et al 1993) to a universal de-\npendency grammar annotation. The choice of the treebank is due to the already\nexisting predefined compositional operations in the SA system used for evaluation\n(see §3.3), that are intended for this type of universal guidelines. The corpus con-\ntains 39 833, 1 701 and 2 416 dependency trees for the training, development and\ntest sets, respectively, and it represents one of the largest available treebanks for\nEnglish.\n1 MaltParser often requires feature optimization to obtain acceptable results for the target\nlanguage.\n2 http://nlp.stanford.edu/data/glove.6B.zip\n8 Carlos Go´mez-Rodr´ıguez et al.\n3.3 Sentiment analysis system\nFor the task-oriented evaluation, we will rely on UUUSA, the universal, unsuper-\nvised, uncovered approach for sentiment analysis described by (Vilares et al 2017),\nwhich is based on syntax and the concept of compositional operations. Briefly, given\na text represented as a dependency tree, a compositional operation defines how a\nnode of the tree modifies the semantic orientation (a real value representing a\npolarity and its strength) of a different branch or node, based on features such\nas its word form, part-of-speech tag or dependency type, without any limitation\nin terms of its location inside such tree. The associated system queues operations\nand propagates them through the tree, until the moment they must be dequeued\nand applied to their target. The model has outperformed other state-of-the-art\nlexicon-based methods on a number of corpora and languages, showing the ad-\nvantages of using syntactic information for sentiment analysis. Due to the way\nthe system works, in such a way that the application of the operation relies on\npreviously assigning dependency types and heads correctly, it also constitutes a\nproper environment to test how parsing accuracy affects polarity classification.\nThe system already includes a predefined set of universal syntactic operations,\nthat we are using in this study to determine the importance of parsing accuracy.\nFor the sake of brevity, we are not detailing how the system computes the semantic\norientation of the trees, but we specify which universal dependencies UUUSA is\nrelying on to identify relevant linguistic phenomena that should trigger a compo-\nsitional operation. To apply an operation, usually a dependency type must match\nat the node a branch is rooted at. The existing set of predefined operations that\nwe are considering involve phenomena such as:\n– Intensification: A branch amplifies or decreases the semantic orientation of its\nhead node or other branch (that must be labeled with the acomp (adjectival\ncomplement) dependency type). The intensifier branch must be labeled as one\nof these three dependency types: advmod (adverb modifier), amod (adjective\nmodifier), nmod (noun modifier). Dependencies are relevant in this case be-\ncause they help avoid false positive cases when applying intensification (e.g. in\n‘It is huge’ , ‘huge’ should be (probably) a positive adjective, meanwhile in ‘I\nhave huge problems’ it acts as an intensifier as it is an adjective modifier of the\nnegative word ‘problems’ , and in ‘I have huge exciting news’ it acts again as\nan intensifier, but of a positive term).\n– ‘But’ clauses: To trigger this compositional operation, which decreases the rel-\nevance of the semantic orientation of the main sentence, the dependent branch\nrooted at ‘but’ must be labeled as cc.\n– Negation: The negating terms, that might shift the sentiment of other branches,\nare labeled in a dependency tree with the dependency type neg.\n– ‘If’ : We also include experiments using the proposed rule in Vilares et al (2017)\nfor the ‘if’ clause, which is labeled with the mark dependency type, assuming\nthat the part of the sentence under the scope of influence of the conditional\nclause should be ignored.\nTherefore, the accuracy obtained by UUUSA on the sentiment corpora is re-\nlated to the parsing accuracy: a LAS of zero makes it impossible to trigger any\ncompositional operation, since no dependency type would match; obtaining as\nHow Important is Syntactic Parsing Accuracy? 9\noutput a global polarity which is the result of simply summing the semantic ori-\nentation of individual words.\nFigure 3.a) and Figure 3.b) illustrate two simple examples where part-of-speech\ntags, dependencies and types play a relevant role to accurately capture the seman-\ntic orientation of the sentence. Additionally, Figure 3.c) illustrates with an ad-\nditional example how semantic composition is managed when a negation and an\nintensification appear in the same sentence and affect the same subjective word.\na) 'I do not plan to make you su er'\ndo\nplan\nto\nmake\nyou\nsu erI not\nGreta\nis\nvery\nintelligenta\nperson\nb) 'Greta is a very intelligent person'\nb) 'My dog is not a very clean animal'\ndog\nis\nvery\ncleana\nanimal\nMy\nnot\nattr\n[phrase: you su er]\nSO: -3\n[phrase: I do not plan to make you su er], \nSO= -3 + 4 = 1\n[phrase: a very intelligent person], \nSO: 3.75\n[phrase: very intelligent], \nSO= (1+intensi cation)*3= 3.75\n[phrase: very], \nSO = 0,\nintens cation factor: +0.25\n[phrase: very], \nSO = 0,\nintens cation factor: +0.25\n[phrase: very clean] \nSO= (1+intensi cation)* 2= 2.5\n[phrase: a very clean animal]\nSO: 2.5\n[phrase: My dog is not a very clean animal]\n SO= 2.5 - 4 = -1.5\nneg\nneg\nadjmod\nadjmod\nFig. 3 Analysis of three sentences using the UUUSA approach. The analysis corresponds\nto a post-order recursive traversal. Semantic orientation, intensification and negation values\nare orientative. At each level, we show for the relevant nodes, those playing any role in the\ncomputation of the semantic orientation, the corresponding phrase rooted at that node, and\nits corresponding semantic orientation (SO), once compositional operations have been applied\nat that level. Sentence a) shows an example where the semantic scope of the negation is non-\nlocal, but thanks to dependency parsing and syntactic rules, the system can accurately identify\nsuch scope and shift the semantic orientation coming from that branch. Note that, if either\nthe dependency type neg or attr were assigned incorrectly, the calculation of the SO would\nbe wrong. Sentence b) illustrates how the term ‘very’ increases the semantic orientation of its\nhead. It is important to remark that if the dependency type adjmod were assigned incorrectly,\nthe analysis would be again unaccurate. Sentence c) illustrates a more complex compositional\nexample, where first, the intensifier ‘very’ amplifies the semantic orientation of the word ‘clean’,\nand the negating word ‘not’ shifts the sentiment rooted at the phrase ‘a very clean animal’\n10 Carlos Go´mez-Rodr´ıguez et al.\nWe chose this system among others for three main reasons:\n1. It supports separate compositional operations to address very specific linguistic\nphenomena, which can be enabled or disabled individually. This gives us great\nflexibility to carry out experiments including and excluding a number of lin-\nguistic constructions, allowing us to determine how relevant parsing accuracy\nis to tackle each of them.\n2. It is a modular system where the parser is an independent component that\ncan be swapped with another parser, allowing us to use it for task-oriented\nevaluation of various parsers. This contrasts with Socher et al (2013), a system\nthat also uses syntax, but where the parsing process is tightly woven with the\nsentiment analysis process (a neural network architecture is trained to perform\nboth tasks at the same time) so that it is not possible to use it with the output\nof external parsers.\n3. Symbolic or knowledge-based systems like this perform robustly across different\ndatasets and domains, which we cannot guarantee for the case of many machine\nlearning models, that do not generalize so well (Aue and Gamon 2005; Taboada\net al 2011; Vilares et al 2017).\n3.4 Sentiment analysis corpora\nThree standard corpora for document- and sentence-level sentiment analysis are\nused for the extrinsic evaluation:\n– Taboada and Grieve (2004) corpus: A general-domain dataset composed of 400\nlong reviews (50% positive, 50% negative) about different topics (e.g. washing-\nmachines, books or computers).\n– Pang and Lee (2004) corpus: A collection of 2 000 long movie reviews (50%\npositive, 50% negative).\n– Pang and Lee (2005) corpus: A collection of short (i.e. single-sentence) movie\nreviews. We relied on the test split used by Socher et al (2013), removing the\nneutral ones, as they did, for the binary classification task (1 821 subjective\nsentences: ∼ 49% positive, ∼ 51% negative).\n3.5 Experimental methodology\nThe aim of our experiments is to show how parsing accuracy influences polarity\nclassification, following a task-oriented evaluation. To do so, we first compare the\nperformance of different parsers on a standard treebank test set and metrics. We\nthen extrinsically evaluate the performance of such parsers by parsing sentiment\ncorpora, and using the obtained parse trees to determine the polarity of the texts\nin the corpora by means of a state-of-the-art syntax-based model. The performance\nof this model relies on previous correct assignment of dependency types and heads,\nto be able to handle relevant linguistic phenomena for the purpose at hand (e.g.\nintensification, ‘but’ clauses or negation). This makes it possible to relate parsing\nand syntax-based sentiment performance.\nHow Important is Syntactic Parsing Accuracy? 11\n3.6 Hardware and software used in the experiments\nExperiments were carried in a Dell XPS 8500 Intel Core i7 @ 3.4GHz and 16GB\nof RAM. Operating system was Ubuntu 14.04 64 bits.\n4 Results\nTable 1 shows the performance obtained by the different parsers according to the\nstandard metrics: LAS, UAS and LA. Table 2 illustrates how much time each\nparser consumes to analyze the Pang and Lee (2005) corpus, and the total time\nonce the SA system is run on it.\nTables 3, 4 and 5 show the accuracy obtained by UUUSA on different sentiment\ncorpora, when the output of each of the parsers is used as input to the syntax-based\nsentiment analysis system.3 We take accuracy as the reference metric for the SA\nsystems, because it is the most suitable metric in this case, since the three corpora\nare balanced. In particular, we compare the performance when no syntactic rules\nare used (which would be equivalent to a lexicon-based system that only sums the\nsemantic orientation of individual words), with respect to the one obtained when\ndifferent rules are added. The aim is to determine if different parsers manage\nrelevant linguistic phenomena in a different way.\nFinally, Figure 4 relates the LAS performance on the test set of the universal\ntreebank with respect to the accuracy obtained by UUUSA, when we artificially\nreduce the training set size to simulate a low-accuracy parsing setting, as could\nhappen in low-resource languages.\n3 The results obtained in these corpora are slightly different from the ones reported by\nVilares et al (2017), due to the different tokenization techniques used in this work.\n12 Carlos Go´mez-Rodr´ıguez et al.\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 90\n0 0.1 1 5 10 100\n 0.64\n 0.66\n 0.68\n 0.7\n 0.72\n 0.74\n 0.76\nLA\nS \n(%\n)\nAc\ncu\nra\ncy\n (%\n)\nPercentage of the training treebank used (log scale)\nLAS\nTaboada and Grieve (2004) corpus\nPang and Lee (2004) corpus\nPang and Lee (2005) corpus\nFig. 4 Relationship between LAS (area graphic, left y-axis) and accuracy in different senti-\nment corpora (line graphics, right y-axis), using the Stanford RNN parser (Chen and Manning\n2014) trained with different portions (%) of the training treebank (x-axis). The plot shows that\nusing a larger training treebank improves LAS, but does not necessarily increase the UUUSA\nsentiment accuracy, especially when more than a 5 or 10% of such treebank is used to train\nsuch parser.\n5 Discussion\nThe results illustrated in Tables 1 and 2 indicate the relationship between the\nparsing time and accuracy. The slower parsers (Martins et al 2013; Rasooli and\nTetreault 2015) tend to obtain a better performance, meanwhile the faster ones\n(Nivre et al 2007; Chen and Manning 2014) attain worse LAS, UAS and LA.\nThis fact is expected, as there is a well-known tradeoff between speed and accu-\nracy in the spectrum of parsing algorithms, with one extreme at greedy search\napproaches that scan and parse the sentence in a single pass but are prone to er-\nror propagation, and the other at exact search algorithms that guarantee finding\nthe highest-scoring parse under a rich statistical model, but are prohibitively slow\n(Choi and McCallum 2013; Volokh 2013; Go´mez-Rodr´ıguez 2016). The tendency\nremains when looking at the performance on individual dependency types (where\nalso the head is assigned correctly).\nHowever, a better LAS or UAS does not necessarily translate into a higher\nsentiment accuracy, which is shown in Tables 3, 4 and 5. In most cases, the perfor-\nmance obtained by the different parsers under the same sets of rules is practically\nequivalent. To confirm this statistically, we applied chi-squared significance tests\nto compare the outputs obtained using the different parsers for each given dataset\nand set of sentiment rules. No significant differences in sentiment accuracy were\nfound in any of these experiments, which reinforces our conclusion. The minimum\nHow Important is Syntactic Parsing Accuracy? 13\nTable 1 Performance (LAS, UAS and LA) of the parsers on the English Universal treebank\ntest set. We also detail the performance in terms of Precision (P) and Recall (R) for the\ndependency types that are playing a role in the predefined compositional operations of UUUSA.\nThe subscripts indicate the rank of the parser with respect to the others, given a particular\nmetric\nMetric MaltParser Stanford RNN parser TurboParser YaraParser\nLAS 88.354 88.773 91.362 91.841\nUAS 90.274 90.473 93.292 93.341\nLA 93.014 93.593 95.022 95.721\nP(acomp) 88.663 88.314 88.752 91.031\nR(acomp) 90.293 89.244 91.082 93.181\nP(advmod) 83.183 82.384 84.962 85.851\nR(advmod) 84.043 81.974 85.461 85.222\nP(amod) 95.453 95.014 96.251 96.232\nR(amod) 95.453 95.404 96.302 96.601\nP(attr) 87.173 86.004 89.002 94.881\nR(attr) 88.633 86.294 91.972 92.981\nP(cc) 77.064 77.683 83.561 83.462\nR(cc) 76.954 77.023 83.822 83.971\nP(mark) 83.444 84.213 85.152 88.471\nR(mark) 83.444 88.313 90.262 92.211\nP(neg) 92.992 92.624 94.771 92.683\nR(neg) 94.722 93.484 95.651 94.413\nP(nmod) 80.664 82.173 84.452 85.381\nR(nmod) 79.672 78.664 79.473 80.691\nTable 2 Average, maximum and minimum execution time (seconds) out of 5 runs on the\nPang and Lee (2005) test set. We also include the total execution time, after the SA system\nhas been run on the Pang and Lee (2005) corpus\nParser Average Minimum Maximum Average + UUUSA\ntime (∼ 1.2 sec)\nMaltParser 4.02 3.83 4.23 5.22\nStanford RNN Parser 5.99 5.82 6.23 7.19\nTurbo Parser 97.34 94.79 99.23 98.54\nYara Parser 39.85 38.94 41.23 41.05\nTable 3 Accuracy on the Pang and Lee (2004) corpus considering different subsets of rules\nParser All None Intensification ‘but’ ‘if’ Negation\nMaltParser 72.75 68.05 70.35 67.75 68.20 69.80\nStanford RNN Parser 72.95 68.05 70.60 67.85 68.35 70.30\nTurbo Parser 72.20 68.05 70.35 67.95 68.25 69.60\nYara Parser 72.00 68.05 70.30 67.85 68.55 70.15\nTable 4 Accuracy on the Pang and Lee (2005) corpus considering different subsets of rules\nParser All None Intensification ‘but’ ‘if’ Negation\nMaltParser 74.79 73.75 74.41 73.86 73.75 74.14\nStanford RNN Parser 74.68 73.75 74.35 73.86 73.97 74.19\nTurbo Parser 74.57 73.75 74.41 73.92 73.86 74.19\nYara Parser 74.68 73.75 74.41 73.86 73.97 73.92\n14 Carlos Go´mez-Rodr´ıguez et al.\nTable 5 Accuracy on the Taboada and Grieve (2004) corpus considering different subsets of\nrules\nParser All None Intensification ‘but’ ‘if’ Negation\nMaltParser 74.00 64.75 66.25 64.75 64.00 72.25\nStanford RNN Parser 74.25 64.75 66.25 64.75 64.25 72.75\nTurbo Parser 75.00 64.75 66.50 64.50 63.75 72.75\nYara Parser 73.50 64.75 66.50 64.50 63.75 72.00\np-value obtained was 0.49. It is important to remark that this is very different from\nstating that parsing is not relevant for SA. In the case of UUUSA, Vilares et al\n(2017) already showed that their syntax-based SA approach is able to beat purely\nlexicon-based methods on a number of languages. In this line, Tables 3, 4 and 5\nalso show that the sets of syntactic rules outperform the baseline that does not\nuse any syntactic-based rules (‘None’ column) in almost all cases, proving again\nthat syntax-based rules are useful to handle relevant linguistic phenomena in the\nfield of SA.\nThe specific reasons that explain why the choice of syntactic parsing algorithm\ndoes not significantly affect accuracy lie out of the scope of our empirical work, as\nthey require an exhaustive linguistic analysis. In view of the data, possible factors\nthat may contribute are the following:\n– Low difficulty of some of the most decisive dependencies involved: as can be\nseen in Table 1, even the least accurate parsers analyzed are obtaining well over\n92% precision and recall in adjectival modifiers (amod) and negations (neg),\nwhich are crucial for handling intensification and negation. This is likely be-\ncause these tend to be short-distance dependencies, which are easier to parse\n(McDonald and Nivre 2007), and are common so they do not suffer from train-\ning sparsity problems. Thus, a highly accurate parser is not needed to detect\nthese particular dependencies correctly.\n– Redundancy in sentences: a sentence may include several expressions of sen-\ntiment, so that even if the parse tree contains inaccuracies in a part of the\nsentence, we may still be able to extract the correct sentiment from the rest.\nThis can be especially frequent in long sentences, which are the most difficult\nto parse (McDonald and Nivre 2007).\n– Irrelevance of fine-grained distinctions: in some cases, the parser provides more\ninformation than is strictly needed to evaluate the sentiment of a sentence. For\nexample, the UUUSA rule for intensifiers works in the same way for adverbial\nmodifiers (advmod), adjectival modifiers (amod) or nominal modifiers (nmod).\nThus, if a parser mistakes e.g. an advmod for an amod, this counts as a parsing\nerror, but has no influence in the sentiment output.\nHowever, verifying and quantifying the influence of each of these factors remains\nas an open question, which we would like to explore in the near future.\nAn interesting conclusion that could be extracted from these results is that\nparsing should prioritize speed over accuracy for syntax-based polarity classifica-\ntion. We draw Figure 4 to reinforce this hypothesis. The figure illustrates how LAS\nand sentiment accuracy vary when training the Stanford RNN parser (Chen and\nManning 2014) with different training data size. To do so, we trained a number of\nparsers using the first x% of the training treebank. As expected, it was observed\nthat adding more training data increased the LAS obtained by the parser. How-\nHow Important is Syntactic Parsing Accuracy? 15\never, this same tendency did not remain with respect to sentiment accuracy, which\nremains stable once LAS reaches an acceptable level. Based on empirical evalua-\ntion, sentiment accuracy stops increasing when using the first 5% (82.57% LAS)\nor 10% (84.99% LAS) of the English Universal training treebank, with which it is\npossible to already obtain a performance close to the state of the art (88.77% when\nusing the whole training treebank). On the other hand, there is a clear increasing\ntendency when x < 5, because in those cases the LAS is still not good enough\n(using the first 0.1% and 1% of the training treebank we only are able to achieve\na LAS of 51.39% and 75.58%, respectively).\n6 Conclusions\nIn this article, we have carried out a task-oriented empirical evaluation to deter-\nmine the relevance of parsing accuracy on the primary challenge of sentiment anal-\nysis: polarity classification. We chose English as the target language and trained a\nnumber of standard and freely available parsers on the Universal Dependency Tree-\nbank v2.0 (McDonald et al 2013). The output of such parsers on different standard\nsentiment corpora is then used as input for a state-of-the-art and syntax-based\nsystem that aims to classify the polarity of those texts. Experimental results let\nus draw two interesting and promising conclusions: (1) a better labeled/unlabeled\nattachment score on parsing does not necessarily imply a significantly better accu-\nracy on polarity classification when using syntax-based algorithms and (2) parsing\nfor sentiment analysis should focus on speed instead of accuracy, as a LAS of\naround 80% (which we obtained in the experiments by using only the first 10% of\nthe training treebank) is already good enough to fully take advantage of depen-\ndency trees and exploit syntax-based rules. Using larger training portions produces\nincreases in the labeled attachment score up to the maximum value of close to 92%\nthat we obtained with the most accurate parser, but the performance for senti-\nment accuracy remains stable. Hence, there is no reason to use a slower parser to\nmaximize LAS as long as one is above said “good enough” threshold for sentiment\nanalysis, which is clearly surpassed by all the parsers tested.\nBased on the results, we believe there is room for improvements. We plan to\ndesign algorithms for faster parsing (Volokh 2013), prioritizing speed over accuracy.\nWe also would like to explore the influence of parsing accuracy on other high-level\ntasks analysis, such as aspect extraction (Wu et al 2009) or question answering\n(Rajpurkar et al 2016), where dependencies have played an important role.\nReferences\nAndor D, Alberti C, Weiss D, Severyn A, Presta A, Ganchev K, Petrov S, Collins M (2016)\nGlobally normalized transition-based neural networks. arXiv 1603.06042 [cs.CL], URL\nhttp://arxiv.org/abs/1603.06042\nAsmi A, Ishaya T (2012) Negation identification and calculation in sentiment analysis. In: The\nSecond International Conference on Advances in Information Mining and Management,\npp 1–7\nBallesteros M, Nivre J (2012) Maltoptimizer: A system for maltparser optimization. In: Chair)\nNCC, Choukri K, Declerck T, Dogan MU, Maegaard B, Mariani J, Moreno A, Odijk J,\nPiperidis S (eds) Proceedings of the Eight International Conference on Language Resources\nand Evaluation (LREC’12), European Language Resources Association (ELRA), Istanbul,\nTurkey\n16 Carlos Go´mez-Rodr´ıguez et al.\nBender EM, Flickinger D, Oepen S, Zhang Y (2011) Parser evaluation over local and non-local\ndeep dependencies in a large corpus. In: Proceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, Association for Computational Linguistics, Ed-\ninburgh, Scotland, UK., pp 397–408, URL http://www.aclweb.org/anthology/D11-1037\nBerzak Y, Huang Y, Barbu A, Korhonen A, Katz B (2016) Bias and agreement in syntactic\nannotations. arXiv 1605.04481 [cs.CL], URL https://arxiv.org/abs/1605.04481\nBranavan SRK, Silver D, Barzilay R (2012) Learning to win by reading manuals in a monte-\ncarlo framework. J Artif Int Res 43(1):661–704, URL http://dl.acm.org/citation.cfm?\nid=2387915.2387932\nBuyko E, Hahn U (2010) Evaluating the impact of alternative dependency graph encodings\non solving event extraction tasks. In: Proceedings of the 2010 Conference on Empiri-\ncal Methods in Natural Language Processing, Association for Computational Linguistics,\nCambridge, MA, pp 982–992, URL http://www.aclweb.org/anthology/D10-1096\nChen D, Manning C (2014) A fast and accurate dependency parser using neural networks. In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), Doha, Qatar, pp 740–750, URL http://www.aclweb.org/anthology/D14-1082\nChoi JD, McCallum A (2013) Transition-based dependency parsing with selectional branching.\nIn: Proceedings of the 51st Annual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), Sofia, Bulgaria, pp 1052–1062, URL http://www.aclweb.\norg/anthology/P13-1104\nClark S, Copestake A, Curran JR, Zhang Y, Herbelot A, Haggerty J, Ahn BG, Wyk CV,\nRoesner J, Kummerfeld J, Dawborn T (2009) Large-scale syntactic processing: Parsing\nthe web. Tech. rep., Johns Hopkins University\nCohen SB, Go´mez-Rodr´ıguez C, Satta G (2011) Exact inference for generative probabilistic\nnon-projective dependency parsing. In: Proceedings of the 2011 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP), Association for Computational\nLinguistics, pp 1234–1245, URL http://www.aclweb.org/anthology/D11-1114\nDeNeefe S, Knight K (2009) Synchronous tree adjoining machine translation. In: Proceedings\nof the 2009 Conference on Empirical Methods in Natural Language Processing, Associ-\nation for Computational Linguistics, Singapore, pp 727–736, URL http://www.aclweb.\norg/anthology/D/D09/D09-1076\nDyer C, Ballesteros M, Ling W, Matthews A, Smith NA (2015) Transition-based dependency\nparsing with stack long short-term memory. In: Proceedings of the 53rd Annual Meeting\nof the Association for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1: Long Papers), Association for Computa-\ntional Linguistics, Beijing, China, pp 334–343, URL http://www.aclweb.org/anthology/\nP15-1033\nEisner J (1996) Three new probabilistic models for dependency parsing: An exploration. In:\nProceedings of the 16th International Conference on Computational Linguistics (COLING-\n96), San Francisco, CA, USA, pp 340–345\nFarghaly A, Shaalan K (2009) Arabic natural language processing: Challenges and solutions.\nACM Transactions on Asian Language Information Processing (TALIP) 8(4):14:1–14:22,\nDOI 10.1145/1644879.1644881, URL http://doi.acm.org/10.1145/1644879.1644881\nGoldberg Y, Nivre J (2012) A dynamic oracle for arc-eager dependency parsing. In: Proceedings\nof the 24th International Conference on Computational Linguistics (COLING), Association\nfor Computational Linguistics, pp 959–976, URL http://aclweb.org/anthology/C/C12/\nC12-1059.pdf\nGo´mez-Rodr´ıguez C (2016) Restricted non-projectivity: Coverage vs. efficiency. Comput Lin-\nguist 42(4):809–817, DOI 10.1162/COLI\\ a\\ 00267, URL http://dx.doi.org/10.1162/\nCOLI_a_00267\nGo´mez-Rodr´ıguez C, Carroll J, Weir D (2008) A deductive approach to dependency parsing. In:\nProceedings of the 46th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies (ACL’08:HLT), Association for Computational Linguistics,\npp 968–976, URL http://www.aclweb.org/anthology/P/P08/P08-1110\nGo´mez-Rodr´ıguez C, Carroll JA, Weir DJ (2011) Dependency parsing schemata and mildly\nnon-projective dependency parsing. Computational Linguistics 37(3):541–586\nGoto I, Utiyama M, Onishi T, Sumita E (2011) A comparison study of parsers for patent\nmachine translation. In: Proceedings of the 13th Machine Translation Summit (MT\nSummit XIII), International Association for Machine Translation, pp 448–455, URL\nhttp://www.mt-archive.info/MTS-2011-Goto.pdf\nHow Important is Syntactic Parsing Accuracy? 17\nHuang L, Sagae K (2010) Dynamic programming for linear-time incremental parsing. In:\nProceedings of the 48th Annual Meeting of the Association for Computational Linguis-\ntics, ACL ’10, pp 1077–1086, URL http://portal.acm.org/citation.cfm?id=1858681.\n1858791\nJia L, Yu C, Meng W (2009) The effect of negation on Sentiment Analysis and Retrieval\nEffectiveness. In: CIKM’09 Proceeding of the 18th ACM conference on Information and\nknowledge management, ACM, ACM Press, Hong Kong, pp 1827–1830\nJoshi M, Penstein-Rose´ C (2009) Generalizing dependency features for opinion mining. In:\nProceedings of the ACL-IJCNLP 2009 Conference Short Papers, Association for Compu-\ntational Linguistics, Stroudsburg, PA, USA, ACLShort ’09, pp 313–316\nKahane S, Mazziotta N (2015) Syntactic polygraphs. a formalism extending both constituency\nand dependency. In: Proceedings of the 14th Meeting on the Mathematics of Language\n(MoL 2015), Association for Computational Linguistics, Chicago, USA, pp 152–164, URL\nhttp://www.aclweb.org/anthology/W15-2313\nKalchbrenner N, Grefenstette E, Blunsom P (2014) A Convolutional Neural Network for Mod-\nelling Sentences. In: The 52nd Annual Meeting of the Association for Computational Lin-\nguistics. Proceedings of the Conference. Volume 1: Long Papers, ACL, Baltimore, Mary-\nland, USA, pp 655–665\nKhan FH, Qamar U, Bashir S (2016a) esap: A decision support framework for enhanced\nsentiment analysis and polarity classification. Information Sciences 367:862–873\nKhan FH, Qamar U, Bashir S (2016b) Swims: Semi-supervised subjective feature weighting and\nintelligent model selection for sentiment analysis. Knowledge-Based Systems 100:97–111\nKong L, Schneider N, Swayamdipta S, Bhatia A, Dyer C, Smith NA (2014) A dependency\nparser for tweets. In: Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar,\npp 1001–1012, URL http://www.aclweb.org/anthology/D14-1108\nKuhlmann M, Go´mez-Rodr´ıguez C, Satta G (2011) Dynamic programming algorithms for\ntransition-based dependency parsers. In: Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human Language Technologies (ACL 2011),\nAssociation for Computational Linguistics, Portland, Oregon, USA, pp 673–682, URL\nhttp://www.aclweb.org/anthology/P11-1068\nLiu Q, Gao Z, Liu B, Zhang Y (2016) Automated rule selection for opinion target extraction.\nKonwledge-Based Systems 104:74–88\nMarcus MP, Marcinkiewicz MA, Santorini B (1993) Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics 19(2):313–330\nMartins A, Smith N, Xing E, Aguiar P, Figueiredo M (2010) Turbo parsers: Dependency\nparsing by approximate variational inference. In: Proceedings of the 2010 Conference on\nEmpirical Methods in Natural Language Processing, Association for Computational Lin-\nguistics, Cambridge, MA, pp 34–44, URL http://www.aclweb.org/anthology/D10-1004\nMartins A, Almeida M, Smith NA (2013) Turning on the turbo: Fast third-order non-\nprojective turbo parsers. In: Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, pp 617–622, URL\nhttp://www.aclweb.org/anthology/P13-2109\nMcDonald R, Nivre J (2007) Characterizing the errors of data-driven dependency parsing\nmodels. In: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural Language Learning (EMNLP-CoNLL),\npp 122–131\nMcDonald R, Satta G (2007) On the complexity of non-projective data-driven dependency\nparsing. In: IWPT 2007: Proceedings of the 10th International Conference on Parsing\nTechnologies, pp 121–132\nMcDonald R, Pereira F, Ribarov K, Hajicˇ J (2005) Non-projective dependency parsing using\nspanning tree algorithms. In: HLT/EMNLP 2005: Proceedings of the conference on Human\nLanguage Technology and Empirical Methods in Natural Language Processing, pp 523–530\nMcDonald R, Nivre J, Quirmbach-brundage Y, Goldberg Y, Das D, Ganchev K, Hall K, Petrov\nS, Zhang H, Ta¨ckstro¨m O, Bedini C, Castello´ N, Lee J (2013) Universal Dependency\nAnnotation for Multilingual Parsing. In: Proceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics, Association for Computational Linguistics, pp\n92–97\nMiceli Barone AV, Attardi G (2015) Non-projective dependency-based pre-reordering with\nrecurrent neural network for machine translation. In: Proceedings of the 53rd Annual\n18 Carlos Go´mez-Rodr´ıguez et al.\nMeeting of the Association for Computational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), Association for\nComputational Linguistics, Beijing, China, pp 846–856, URL http://www.aclweb.org/\nanthology/P15-1082\nMiyao Y, Sætre R, Sagae K, Matsuzaki T, Tsujii J (2008) Task-oriented evaluation of syn-\ntactic parsers and their representations. In: Proceedings of ACL-08: HLT, Association\nfor Computational Linguistics, Columbus, Ohio, pp 46–54, URL http://www.aclweb.org/\nanthology/P/P08/P08-1006\nNapoles C, Gormley M, Van Durme B (2012) Annotated gigaword. In: Proceedings of the\nJoint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge\nExtraction, Association for Computational Linguistics, pp 95–100\nNivre J, Hall J, Nilsson J, Chanev A, Eryigˇit G, Ku¨bler S, Marinov S, Marsi E (2007) Malt-\nparser: A language-independent system for data-driven dependency parsing. Natural Lan-\nguage Engineering 13:95–135\nNivre J, Rimell L, McDonald R, Go´mez Rodr´ıguez C (2010) Evaluation of dependency parsers\non unbounded dependencies. In: Proceedings of the 23rd International Conference on Com-\nputational Linguistics (COLING 2010), Association for Computational Linguistics, pp\n833–841, URL http://www.aclweb.org/anthology/C10-1094\nPado´ S, Noh TG, Stern A, Wang R, Zanoli R (2015) Design and realization of a modular\narchitecture for textual entailment. Natural Language Engineering 21(2):167–200\nPang B, Lee L (2004) A sentimental education: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In: Proceedings of the 42nd annual meeting on As-\nsociation for Computational Linguistics, Association for Computational Linguistics, pp\n271–278\nPang B, Lee L (2005) Seeing stars: Exploiting class relationships for sentiment categorization\nwith respect to rating scales. In: Proceedings of the 43rd Annual Meeting on Association\nfor Computational Linguistics, Association for Computational Linguistics, pp 115–124\nPennington J, Socher R, Manning CD (2014) Glove: Global Vectors for Word Representation.\nIn: EMNLP, vol 14, pp 1532–1543\nPitler E, Kannan S, Marcus M (2013) Finding optimal 1-endpoint-crossing trees. Transac-\ntions of the Association of Computational Linguistics 1:13–24, URL http://aclweb.org/\nanthology/Q13-1002\nPopel M, Marecˇek D, Green N, Zabokrtsky Z (2011) Influence of parser choice on dependency-\nbased mt. In: Proceedings of the Sixth Workshop on Statistical Machine Translation,\nAssociation for Computational Linguistics, Edinburgh, Scotland, pp 433–439, URL http:\n//www.aclweb.org/anthology/W11-2153\nPoria S, Cambria E, Winterstein G, Huang GB (2014) Sentic patterns: Dependency-based\nrules for concept-level sentiment analysis. Knowledge-Based Systems 69:45–63\nQuirk C, Corston-Oliver S (2006) The impact of parse quality on syntactically-informed sta-\ntistical machine translation. In: Proceedings of the 2006 Conference on Empirical Methods\nin Natural Language Processing, Association for Computational Linguistics, Sydney, Aus-\ntralia, pp 62–69, URL http://www.aclweb.org/anthology/W06-1608\nRajpurkar P, Zhang J, Konstantin L, Liang P (2016) SQuAD: 100,000+ Questions for Machine\nComprehension of Text. arXiv preprint arXiv:160605250\nRasooli MS, Tetreault JR (2015) Yara parser: A fast and accurate dependency parser. CoRR\nabs/1503.06733, URL http://arxiv.org/abs/1503.06733\nSocher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng A, Potts C (2013) Recursive Deep\nModels for Semantic Compositionality Over a Sentiment Treebank. In: EMNLP 2013.\n2013 Conference on Empirical Methods in Natural Language Processing. Proceedings of\nthe Conference, ACL, Seattle, Washington, USA, pp 1631–1642\nSong M, Kim WC, Lee D, Heo GE, Kang KY (2015) PKDE4J: entity and relation extraction\nfor public knowledge discovery. Journal of Biomedical Informatics 57:320–332, DOI 10.\n1016/j.jbi.2015.08.008, URL http://dx.doi.org/10.1016/j.jbi.2015.08.008\nTaboada M, Grieve J (2004) Analyzing appraisal automatically. In: Proceedings of AAAI\nSpring Symposium on Exploring Attitude and Affect in Text (AAAI Technical Report\nSS0407), Stanford University, CA, AAAI Press, pp 158–161\nTaboada M, Brooke J, Tofiloski M, Voll K, Stede M (2011) Lexicon-based methods for senti-\nment analysis. Computational Linguistics 37(2):267–307\nTaule´ M, Mart´ı MA, Recasens M (2008) AnCora: Multilevel Annotated Corpora for Catalan\nand Spanish. In: Calzolari N, Choukri K, Maegaard B, Mariani J, Odjik J, Piperidis S,\nHow Important is Syntactic Parsing Accuracy? 19\nTapias D (eds) Proceedings of the Sixth International Conference on Language Resources\nand Evaluation (LREC’08), Marrakech, Morocco, pp 96–101\nVilares D, Alonso MA, Go´mez-Rodr´ıguez C (2015a) A linguistic approach for determining the\ntopics of Spanish Twitter messages. Journal of Information Science 41(02):127–145\nVilares D, Alonso MA, Go´mez-Rodr´ıguez C (2015b) A syntactic approach for opinion mining\non Spanish reviews. Natural Language Engineering 21(01):139–163\nVilares D, Alonso MA, Go´mez-Rodr´ıguez C (2015c) On the usefulness of lexical and syntactic\nprocessing in polarity classification of Twitter messages. Journal of the Association for\nInformation Science Science and Technology 66(9):1799–1816\nVilares D, Go´mez-Rodr´ıguez C, Alonso MA (2017) Universal, unsupervised (rule-based), un-\ncovered sentiment analysis. Knowledge-Based Systems 118:45–55, DOI https://doi.org/10.\n1016/j.knosys.2016.11.014\nVolokh A (2013) Performance-oriented dependency parsing. Doctoral dissertation, Saarland\nUniversity, Saarbru¨cken, Germany\nVolokh A, Neumann G (2012) Task-oriented dependency parsing evaluation methodology. In:\nIEEE 13th International Conference on Information Reuse & Integration, IRI 2012, Las\nVegas, NV, USA, August 8-10, 2012, pp 132–137, DOI 10.1109/IRI.2012.6303001, URL\nhttp://dx.doi.org/10.1109/IRI.2012.6303001\nWu Y, Zhang Q, Huang X, Wu L (2009) Phrase Dependency Parsing for Opinion Mining. In:\nProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,\nACL, Singapore, pp 1533–1541\nXiao T, Zhu J, Zhang C, Liu T (2016) Syntactic skeleton-based translation. In: Proceedings of\nthe Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA., pp 2856–2862, URL http://www.aaai.org/ocs/index.php/AAAI/AAAI16/\npaper/view/11933\nYu M, Gormley MR, Dredze M (2015) Combining word embeddings and feature embeddings\nfor fine-grained relation extraction. In: Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Association for Computational Linguistics, Denver, Colorado, pp 1374–1379,\nURL http://www.aclweb.org/anthology/N15-1155\nYuret D, Han A, Turgut Z (2010) Semeval-2010 task 12: Parser evaluation using tex-\ntual entailments. In: Proceedings of the 5th International Workshop on Semantic Eval-\nuation, Association for Computational Linguistics, Uppsala, Sweden, pp 51–56, URL\nhttp://www.aclweb.org/anthology/S10-1009\nZhang Y, Nivre J (2011) Transition-based dependency parsing with rich non-local features. In:\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies: short papers - Volume 2, pp 188–193, URL http://dl.\nacm.org/citation.cfm?id=2002736.2002777\n",
      "id": 42871895,
      "identifiers": [
        {
          "identifier": "2765558935",
          "type": "MAG_ID"
        },
        {
          "identifier": "1706.02141",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "10.1007/s10462-017-9584-0",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1706.02141",
          "type": "OAI_ID"
        },
        {
          "identifier": "83867333",
          "type": "CORE_ID"
        },
        {
          "identifier": "info:doi/10.1007%2fs10462-017-9584-0",
          "type": "OAI_ID"
        },
        {
          "identifier": "243152629",
          "type": "CORE_ID"
        }
      ],
      "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on\n  Rule-Based Sentiment Analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1706.02141",
        "info:doi/10.1007%2fs10462-017-9584-0"
      ],
      "publishedDate": "2017-10-23T00:00:00",
      "publisher": "'Springer Science and Business Media LLC'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1706.02141"
      ],
      "updatedDate": "2024-02-27T12:10:48",
      "yearPublished": 2017,
      "journals": [
        {
          "title": "Artificial Intelligence Review",
          "identifiers": [
            "0269-2821",
            "1573-7462"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1706.02141"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/42871895"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Ferdinand Graf"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/6234553"
      ],
      "createdDate": "2012-07-06T03:40:29",
      "dataProviders": [
        {
          "id": 153,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/153",
          "logo": "https://api.core.ac.uk/data-providers/153/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "I analyze company news from Reuters with the 'General Inquirer' and relate measures of positive sentiment, negative sentiment and disagreement to abnormal stock returns, stock and option trading volume, the volatility spread and the CDS spread. I test hypotheses derived from market microstructure models. Consistent with these models, sentiment and disagreement are strongly related to trading volume. Moreover, sentiment and disagreement might be used to predict stock returns, trading volume and volatility. Trading strategies based on positive and negative sentiment are profitable if the transaction costs are moderate, indicating that stock markets are not fully efficient.Content Analysis, Company News, Market Microstructure",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/pdf/6234553.pdf",
      "fieldOfStudy": null,
      "fullText": " \n \n \n \n \n \nUniversity of Konstanz \nDepartment of Economics \n \nMechanically Extracted Company \nSignals and their Impact on Stock \nand Credit Markets \n \n \nFerdinand Graf \n \n \nhttp://www.wiwi.uni-konstanz.de/workingpaperseries \n                           Working Paper Series  \n2011-18 Mechanically Extracted Company Signals\nand their Impact on Stock and Credit\nMarkets\nFerdinand Graf1\nUniversity of Konstanz\nMay 31, 2011\nAbstract: I analyze company news from Reuters with the `General In-\nquirer' and relate measures of positive sentiment, negative sentiment and\ndisagreement to abnormal stock returns, stock and option trading vol-\nume, the volatility spread and the CDS spread. I test hypotheses derived\nfrom market microstructure models. Consistent with these models, sen-\ntiment and disagreement are strongly related to trading volume. More-\nover, sentiment and disagreement might be used to predict stock returns,\ntrading volume and volatility. Trading strategies based on positive and\nnegative sentiment are pro\ftable if the transaction costs are moderate,\nindicating that stock markets are not fully e\u000ecient.\nKeywords: Content Analysis, Company News, Market Microstructure\nJEL-classi\fcation: G12, G14\n1Department of Economics, Box D-147, 78457 Konstanz, Germany, Phone: +49 7531 88 3620,\nE-Mail: ferdinand.graf@uni-konstanz.de1 Introduction\nInvestors read daily newspapers, internet articles, watch TV news and listen to\nthe radio. The information obtained might a\u000bect their trading decision and, hence,\nmarket prices, trading volume and volatility. Barber and Odean (2008) show that the\nnumber of news releases by Dow Jones News Service is related to the trading behavior\nof individual investors, but not institutional investors. Engelberg and Parsons (2011)\n\fnd a causal relationship between \fnancial news articles in local newspapers and\nthe trading volume of local retail investors. However, news have many dimensions.\nThe number of relevant news articles for a company is a very restrictive measure and\nignores much information that might be important for \fnancial markets, e.g. the\nsentiment. Tetlock (2007) and Gro\u0019-Klu\u0019mann and Hautsch (2011) \fnd that the\nsentiment of news articles predicts daily index returns, and intraday liquidity and\nvolatility, respectively. The sentiment of chat-room postings, which could contain\nnews as well, may have predictive power for \fnancial markets too, see Antweiler\nand Frank (2004), Das, Matinez-Jeres and Tufano (2005) and Das and Chen (2007).\nI build on these studies and construct a \nexible content analysis algorithm and\nanalyze company news from Reuters.\nReuters company news usually describe and interpret a wide range of facts and\nevents which might be relevant for companies. The author's interpretation and her\nword choice may provide valuable information for \fnancial markets. The author's\nview might account for the economic environment, the \frm's industry position,\nthe management quality and much more aspects which are rather hard to measure\nquantitatively. If the author concludes that some fact is positive news for a company,\nshe will use friendly and positive words to write the news story. If the facts are\nconsidered as negative, alarmed and sad words will probably characterize the news\nstory. Of course, the quality of the author's comments depends on her background.\nThis makes the analysis of chat-room postings and their impact on the market\ndi\u000ecult, since everybody can post her opinion, rumors or lies without reputation\ndamage. Another advantage of Reuters company news is that it allows to study the\nimpact of heterogeneous events on \fnancial markets simultaneously.\nI use the `General Inquirer' to measure the sentiment of a news story with respect to a\ncompany and disagreement among news stories mechanically. The `General Inquirer'\nassigns words to word categories. The word categories `positive' and `negative' are\nused to measure positive and negative sentiment of news stories. Also, I use the\nword categories `strong' and `weak' to measure the uncertainty of a news story. I\ntest if positive sentiment, negative sentiment and disagreement of Reuters company\nnews articles impact \fnancial markets. The data cover 62 large U.S. companies\nlisted at the NYSE or the Nasdaq with liquid stock option and CDS markets for the\ntime period June 01, 2007 to December 31, 2010.\nFirst, I investigate the impact of sentiment and disagreement on the abnormal stock\nreturn derived from the three factor Fama-French model, stock and option trading\nvolume, the volatility spread and the CDS spread using daily data. This analysis\n1allows to test implications given by market microstructure models where investors\ninterpret public signals individually. My results are consistent with these models.\nSecond, I show that sentiment and disagreement have predictive power for (abnor-\nmal) stock returns, stock trading volume and the volatility spread. Positive senti-\nment is frequently followed by positive (abnormal) returns and disagreement tends\nto lower the (abnormal) return on the following day. The volatility spread increases\nafter negative sentiment and disagreement. Stock trading volume is signi\fcantly\nhigher after news with positive sentiment, but disagreement reduces stock trading\nvolume at the following day. The latter \fnding is surprising and might be due to\nimmediate execution of scheduled orders, giving contradicting news articles.\nFinally, I test the economic relevance of positive and negative sentiment by analyzing\ntrading strategies based on sentiment. Even with realistic transaction costs of 10\nbps per round-trip, the trading strategies are comparable to approximate arbitrage\nopportunities, indicating that the stock market is not fully e\u000ecient. For transaction\ncosts of 20 bps, the trading strategies are on average still pro\ftable, but bear a\nsubstantial loss potential. The strategies cannot compensate transaction costs of 30\nbps and more.\nThe contribution of this paper is manyfold. (1) I consider a large number of compa-\nnies with liquid stock and derivative markets and analyze the relationship between\nnews articles and abnormal stock returns, stock and option trading volume, the\nvolatility spread and the CDS spread company individually. Hence, I do not aggre-\ngate returns, etc., at the same day across companies. This distinguishes this study\nfrom Tetlock (2007), who considers index returns, and from Das et. al. (2005),\nwho analyze four representative companies individually. (2) I analyze a comprehen-\nsive and hand-collected dataset of news stories, downloaded from the homepage of\nReuters with a \nexible procedure, and extend Gro\u0019-Klu\u0019man and Hautsch (2011),\nwho relate pre-calculated dummy variables for positive and negative sentiment to\nthe stock market, using a continuous sentiment score. (3) The Reuters company\nnews are highly credible. This distinguishes this analysis from Antweiler and Frank\n(2004) and Das and Chen (2007), who study chat-room postings. Das et. al. (2005)\nanalyze chat-room postings, too, claiming that these postings disseminate public\ninformation. This paper analysis might contribute to those study since I analyze\nnews articles which might be closer to public information and, hence, less noisy. (4)\nTo my best knowledge, this study is the \frst that analyzes the relationship between\nsentiment respectively disagreement of general news articles and the CDS spread.\nThe rest of the paper is organized as follows. Section 2 gives a literature review.\nSection 3 derives testable hypotheses from market microstructure models. There-\nafter, I explain how market activity is measured. I describe my hand-collected news\ndatabase in section 4. Section 5 describes the content analysis and de\fnes measures\nfor sentiment and disagreement. Thereafter, I relate these measures to the market\nvariables and develop trading strategies based on sentiment. Section 8 concludes\nand gives an outlook for further research.\n22 Related Literature\nSeveral papers investigate the relationship between a company's publicity and the\nstock market. Publicity often refers to the number of news articles on the company.\nIn an early study, Mitchell and Mulherin (1994) relate the number of news releases by\nDow Jones & Company to the absolute value of the market return, the absolute value\nof \frm-speci\fc return and the trading volume. By controlling for macroeconomic\nannouncements and weekday e\u000bects, the study documents a signi\fcant relationship\nbetween news activity and market activity. Barber and Odean (2008) de\fne atten-\ntion grabbing stocks as stocks with high abnormal trading volume, extreme returns\nor news coverage. They show that individual investors are more likely to purchase\nattention-grabbing stocks than other stocks. Engelberg and Parsons (2011) address\nthe causality between news articles and investors' behavior. They identify articles\non earnings announcement in local newspapers. Local news coverage predicts trad-\ning volume of local investors and gives strong support to a causal relationship from\nnews coverage to trading. Fang and Peress (2009) study the cross-section of stock\nreturns. They \fnd that stocks with media coverage, measured by the number of\narticles on the company in the four major US newspapers (New York Times, USA\nToday, Wall Street Journal, Washington Post), underperform stocks without media\ncoverage.\nOf course, the number of news per day ignores the content of the news article.\nTetlock (2007) identi\fes weak or negative words in the daily article 'Abreast of the\nMarket' in the Wall Street Journal with a content analysis algorithm, the `General\nInquirer'. He \fnds that the number of negative or weak words predicts the return of\nthe Dow Jones Industrial Average on the following day. This e\u000bect is o\u000bset within the\nsubsequent \fve days and disappears after one week. Gro\u0019-Klu\u0019mann and Hautsch\n(2011) show that the sentiment of news articles and their relevance for stocks listed\nat the LSE predict high frequency returns, volatility and liquidity. The sentiment\nof a news article is calculated by Reuters and can take on only the values +1, 0 and\n-1. The relevance of the news story determines the sensitivity of the market with\nrespect to the news article. Tetlock et. al. (2008) show that print news can predict\nfundamental value as well as market value. However, trading strategies based on\nthese forecasts generate pro\fts only if transaction costs are excluded. Carretta et.\nal. (2010) study the Italian stock market and its reaction to corporate governance\nnews. News stories are analyzed with respect to content and tone, revealing that\nthe content of news on pro\ftable corporations is important to explain stock returns.\nSeveral studies use a more general de\fnition of news and analyze chat-room post-\nings. However, this kind of information is presumably more noisy and, hence, less\ncredible than regular news articles. Antweiler and Frank (2004) relate measures for\nbullishness and disagreement in chat-room postings and chat-room activity to mar-\nket activity. Their main \fnding is that chat-room postings predict realized volatility\nand trading volume, given high frequency data. Das, Martinez-Jerez and Tufano\n(2005) analyze chat-room postings of four representative companies from di\u000berent\nindustries and \fnd a contemporaneous relationships between the sentiment in in-\n3vestors' conversations and market returns, but no predictive power. This motivates\ntheir conclusion that investors \frst trade and then talk. Das and Chen (2007) apply\na wide spectrum of text analysis algorithms to chat-rooms postings and develop\nmeasures for sentiment and disagreement. Relating these measures to the stock\nmarket return of a company shows that market activity is related to small investors'\nsentiment. Tumarkin and Whitelaw (2001) analyze chat-room postings, too. How-\never, their \fndings on the interdependence between market observations and posted\nnews are inconclusive.\nBy using a narrow de\fnition of news / events, the number of articles might be re-\nduced signi\fcantly and a mechanical content analysis might be redundant. Brooks,\nPatel and Su (2003) analyze stock responses to rare, negative surprises like the\nExxon Valdes catastrophe, plane crashes or the sudden death of a CEO. They \fnd\nthat stocks respond with a delay to fully unanticipated news, but overreact, see also\nBrourn and Derwall (2010), who study terrorist attacks and earthquakes, respec-\ntively. Yu (2011) uses the dispersion in analyst forecasts to measure disagreement.\nA portfolio of stocks with high disagreement underperforms compared to a portfolio\nwith low disagreement. Boyd, Hu and Jagannathan (2005) do not focus on \frm spe-\nci\fc news, they analyze unemployment reports and \fnd that stock markets respond\nto unemployment news conditional on the state of the economy.\nNot only stock prices seem to respond to textual information, there is evidence that\nthe price of credit derivatives and \fxed income securities do so as well. Norden (2008)\nstudies the relationship between the credit spread of credit default swaps and rating\nannouncements. He \fnds that the rating downgrade of a company is anticipated\nby the company's major lenders, concluding that information spills over from the\nmajor lenders to the market, see also Hull, Predrescu and White (2004). Hess et. al.\n(2008) study the impact of macroeconomic news on commodity future price indices.\nThe index return responds to news about the in\nation rate or real activity only in\na recession. Hautsch and Hess (2002) analyze the U.S. employment report impact\non the mean and the volatility of T-bond futures returns. The mean's reaction\nis related to surprises and the volatility's reaction is related to uncertainty in the\nannouncements. Besides of liquidity patterns, the study documents asymmetries in\nthe T-bond future price reaction to positive and negative news. Coval and Shumway\n(2001) propose a very remarkable measure of information arrival, the ambient noise\nin the CBOT trading pit. This measure predicts returns, liquidity and the customer\norder \now of the 30 year U.S. treasury bond for several minutes.\n3 Market Reactions\n3.1 Hypotheses\nThe e\u000ecient market hypothesis says that market prices adjust immediately to public\ninformation. I test this hypothesis. Hence\n4Hypothesis 1: Market prices adjust immediately to public information,\nleaving no predictive power for public company news.\nAssuming homogeneous beliefs, the absence of private information and homogeneous\npreferences, investors do not trade if new information becomes public, starting at an\nequilibrium, see Milgrom and Stokey (1982). However, this is inconsistent with the\nempirical studies cited before. Harris and Raviv (1993) and Kandel and Pearson\n(1995) drop the assumption of homogeneous beliefs. They assume that investors\nobserve noisy, public signals and update their beliefs consistent with their individual\ninterpretation. Di\u000berent levels of con\fdence with respect to the noisy, public signal\nacross investors (di\u000berence of opinion) might cause heterogenous changes in the\ndemand for risky assets and, hence, trading. Furthermore, Cao and Ou-Yang (2009)\nextend this framework and show that public signals and heterogeneous priors may\nalso cause trading in stock options. Banerjee and Kremer (2010) relate a time-\nvarying magnitude of di\u000berence of opinion to trading volume and price volatility\nand \fnd that `periods of major disagreement are periods of higher volume and also\nof higher absolute price changes'. The latter might be used as measure for volatility.\nCompany news might be closely related to public signals. Therefore, I approximate\nthe intensity of public company signals by the sentiment of relevant news stories.\nThe degree of di\u000berences of opinion is approximated by the variation in the sentiment\nof relevant news articles within on trading day, hereafter called disagreement. Hence\nHypothesis 2: Trading volume of stocks and options increases with\npositive sentiment and negative sentiment.\nHypothesis 3: An increase in disagreement raises trading volume of\nstocks and options.\nHypothesis 4: The stock return volatility increases with disagreement.\nAccording to Hong and Stone (2007), heterogenous priors of investors are one ex-\nplanation why disagreement a\u000bects the stock market. Others are limited attention\nor gradual information \now. However, these explanations have similar implications\non the relationship between the stock market and disagreement.\nAnother strand of literature explains trading patterns by information asymmetries\nacross investors. Blume, Easley and O'Hara (1994) show theoretically that trading\nvolume might contain valuable information to determine the precision of noisy, pri-\nvate information and might be useful for stock pricing, see also Suominen (2001).\nTetlock (2010) analyzes market data around company announcements and \fnds pat-\ntern which are consistent with information asymmetries. Sarwar (2005) and Kyr-\niacou and Sarno (1999) study option trading volume and market volatility. Both\nstudies \fnd a strong predictive power of option trading volume for volatility and\nvice versa. Adjusting hedged portfolios to changes in volatility might explain why\nvolatility predicts option trading volume. Also, investors with private information\nmight exploit their informational advantage aggressively with options and use the\n5leverage e\u000bect or bet on volatility via derivatives. Hence, trading volume might\npredict volatility. By analyzing the ratio of traded put and call options, Pan and\nPoteshman (2006) \fnd that stocks with low ratios signi\fcantly outperform stocks\nwith high ratios. Again, this indicates that informed traders use derivates to bene\ft\nfrom their informational advantage.\nEmpirically, it is likely that evidence for - at least parts of - both strands of literature,\ni.e. di\u000berence of opinion and asymmetric information, appears jointly. I test the\nimplications given by the di\u000berence of opinion theory, but allow for inter-temporal\ndependencies between trading volume, stock volatility and returns to account for\ninformation asymmetries. Furthermore, I include the CDS spread of a company for\ntwo reasons: (1) Structure models for credit derivatives like Merton (1974) imply\nthat the equity market and the credit market are closely linked. Cremers et. al.\n(2006) and Zhang, Zhou and Zhu (2009) document a close relationship between\ncredit markets and equity markets. Hence, I control for information spillovers from\ndebt to equity markets and vice versa. (2) I test if the CDS spread is related to the\ndegree of di\u000berence of opinion and to public signals. Since equity volatility and the\nunobservable asset volatility in structural models are positively related, the CDS\nspread might also respond to a change in di\u000berence of opinion, given that the equity\nvolatility reacts. Therefore\nHypothesis 5: The CDS spread increases with disagreement.\nFurthermore, the CDS spread represents the market price of a traded derivative.\nPredictability of the CDS spread might be related to market ine\u000eciency and to\nHypothesis 1.\n3.2 Measures of Market Reactions\nThe daily close-to-close excess stock return of company i at day t, denoted ri;t,\nmight be used as a measure for the stock market's response to news releases. More\nappropriate and in line with many other studies is the abnormal stock return, mea-\nsured by the residuum in the three factor Fama-French model (hereafter FF model\n/ factors / residuum), see Fama and French (1993). The residual measures the\nstock price movements that are not due to common market risk factors but might\nbe due to \frm-speci\fc risk respectively news. The FF factors and the risk-free\ninterest rate are downloaded from the homepage of Kenneth French (see http:\n//mba.tuck.dartmouth.edu/pages/faculty/ken.french/index.html), the divi-\ndend adjusted stock prices are downloaded from Thomson Reuters Datastream. I\nestimate\nri;t = \u000bi + \fi;MarketXMarket;t + \fSMB;iXSMB;t + \fHML;iXHML;t + \"i;t; (1)\nwhere \fi;\u0001 denotes the factor loadings of the corresponding factor X\u0001;t (Market, Small\nMinus Big market capitalization, High Minus Low book to market ratio). The\n6estimated residuum is de\fned by ^ \"i;t := ri;t \u0000 ^ ri;t, where ^ ri;t is the explained stock\nreturn. If ^ \"i;t is large in absolute value, it is likely that important \frm-speci\fc\ninformation arrives. However, a residuum close to zero does not necessarily indicate\na calm trading day.\nI measure the stock trading volume by the daily turnover volume, divided by the\naverage turnover volume in the preceding 3 months. This measure is denoted Ti;t.\nTo address the study of Sarwar (2005) and Kyriacou and Sarno (1999), and to test\nthe model of Cao and Ou-Yang (2009), I also include the cumulated option trading\nvolume of all outstanding options on stock i at day t, divided by its 3-month moving\naverage. This measure is denoted Oi;t. All time series are provided by Thomson\nReuters Datastream. I use the 3-month volatility spread, de\fned by\nVi;t = IVi;t \u0000 RVi;t;\nto measure the investor expectations on volatility relative to realized volatility. IVi;t\nis the at-the-money implied volatility of 3-month constant maturity options, calcu-\nlated by Thomson Reuters Datastream. According to Martens and van Dijk (2007),\nthe 3-month realized volatility is well approximated by\nRVi;t =\nv u u\nt1\n2\nt X\ns=t\u000060\n[(lnHi;s \u0000 lnLi;s)2 \u0000 (2ln2 \u0000 1)(lnRi;s)2];\nwhere Hi;s is the highest intraday stock price within day s and Li;s is the lowest\nintraday stock price. Ri;s is the close-to-close gross stock return of day s2. Finally, I\nuse the 5-year CDS spread on senior debt as an indicator for the company's default\nrisk. The CDS spreads is denoted Ci;t, the data are provided by CMA.\n4 Company News\n4.1 Data Description\nMy hand-collected database consists of mainly fundamental and unscheduled news\nstories on companies in the S&P500, FTSE100 or EuroStoxx50 for the time period\nJune 01, 2007 to December 31, 2010. Given a date (mmddyyyy) and a company,\nidenti\fed with its RIC (= Reuters instrument code), the domain Reuters.com\nreturns a list of up to ten news articles on the uniform resource locator http://www.\nreuters.com/finance/stocks/companyNews?symbol=RIC&date=mmddyyyy. All\ncompany news stories are downloaded mechanically. Long news stories might span\n2The volatility spread measures the expected excess volatility relative to the realized volatility.\nIt might measure the risk aversion of the market, too. However, the time series V is non-stationary\nfor many companies and is, hence, di\u000berentiated. Since the realized volatility moves very slowly,\nit has only little impact on the \frst di\u000berence of V such that the results do not depend on the\nrealized volatility.\n7over more than one internet page. However, the download routine recognizes this\nand controls for it.\nA news story consists of a headline, the full text or body, a time stamp (date and\ntime), keywords and a list of companies indicating for whom the news story might\nbe important. In the following, this list is called 'related RICs'. The assignment of\nkeywords and related RICs to a news story is done by Reuters. Keywords provide\na rough, standardized categorization of the news story (e.g. Major Breaking News,\nDebt ratings news, Corporate Results, Mergers and Acquisitions). In a nutshell,\ncompany news inform about rating adjustments, analyst reports and changes for\nthe stock price target, give summarizing \fgures on quarterly and annual reports\nand general news (e.g. macro-economic indicators, political events, articles in the\nWashington Post, New York Times, Wall Street Journal, etc.). Corrected or updated\nnews are not excluded to capture the information \now correctly. Even though the\nlist of company news on the homepage of Reuters is limited to ten, the number\nof daily news articles per company, e.g. identi\fed by searching for the company's\nRIC in `related RICs', is not bounded in my database because there are many news\narticles that mention a company or have it in the \feld `related RICs' and do not\nappear in the list for the company on the homepage of Reuters. For the observation\nperiod June 01, 2007 to December 31, 2010, there are more than 350,000 unique\nnews stories with respect to the url. The average news article consists of 301.29\nwords (including numerical expressions and symbols) with a standard deviation of\n239.64 words. The median of words per news article is 272 and indicates that the\ndistribution is skewed to the right. The 99% quantile is 961 words. On average,\na news article consists of 14.02 sentences with a standard deviation of 33.20. The\nmedian is 11, again, indicating that the distribution of sentences per news story is\nskewed to the right, and the 99% quantile is 44.\nTable 1 provides descriptive statistics for the number of news articles per day for all\nS&P500 companies jointly, for the index components of the Dow Jones Industrial\nAverage by January 01, 2011 and for some frequently used keywords. I have 210,495\nnews articles for all S&P500 companies on 1311 days. Hence, the daily, average\nnumber of news articles for all S&P500 companies is 160.56 with a standard deviation\nof 94.01. Ignoring Saturdays and Sundays, the average number of news releases per\nday increases to 212.63 with standard deviation 52.94. On October 22, 2009, 354\nnews stories were published, this is the maximum number of news stories per day\nin the observation period. There are 17,525 news stories labeled with the keywords\n`Corporate Result', `Result Forecast' or `Warnings', this gives a daily average of\n13.36 with a standard deviation of 20.13. The total number of news stories with\n`Broker research and recommendation' is 1,867, the daily average is, hence, 1.42\nand the standard deviation is 2.26. News stories on e.g. Bank of America (BAC.N),\nidenti\fed by searching for `BAC.N' in `related RICs', sum up to 11,974, with a mean\nof 9.13 news stories per day and standard deviation of 8.23.\nFigure 1, upper plot, shows the time series of the daily number of news stories with\nthe keywords `Bankruptcy' or `Insolvency' (blue curve) and its 3 day moving average\n(red curve). Since there are no such news stories prior to November 23, 2007, the\n8plot excludes the period June 1, 2007 to November 23, 2007. The large number of\nnews stories in the middle of September 2008 indicates the bankruptcy of Lehman\nBrothers and the peaks in 2009 and 2010 are mainly due to the sovereign debt crisis\nin Europe. The lower plot shows the time series of the daily number of news stories\nfor the Bank of America and the corresponding 3-day moving average. This time\nseries starts on June 1, 2007. The time series displays a weekly cyclicality caused by\nthe low number of news articles during the weekend. Again, the default of Lehman\nBrothers at September 15, 2008 can be clearly identi\fed. The peak in January 2009\nis caused by the arranged acquisition of Merrill Lynch by Bank of America.\n[Table 1 about here.]\n[Figure 1 about here.]\n4.2 News Coverage\nTo improve the understanding of the news database, in the following I investigate\nthe company characteristics that expose a company to news coverage. I consider\n62 large companies in the S&P500 with liquid option and CDS market. Table 9\nlists these companies. The news exposure of a company is measured by its average\nnumber of news articles per day, identi\fed with the company's RIC in `related\nRICs'. This measure is denoted Qi. Alternatively, news coverage is measured by\nthe number of days with at least one news story. This measure is denoted Yi.\nCompanies are characterized by the average market capitalization in the observation\nperiod, CAPi, the average price-to-book ratio, P2Bi, the stock return during the\nobservation period, Reti, and the corresponding realized volatility, \u001b(Reti).\nThe average company has an average market capitalization during June 01, 2007\nto December 31, 2010 of 8,3223 billions USD and an average price-to-book ratio of\n2.67. The average stock market performance in this period and across companies is\n-9.41% and the average stock return volatility is 42.63. I estimate an ordinary linear\nregression model, i.e.\nQi = \u000b + \f1P2Bi + \f2 ln(CAPi) + \f3Reti + \f4\u001b(Reti) + \u0011i: (2)\nTable 2 shows the regression estimates for (2) and some straightforward adjustments.\na indicates signi\fcance at the 1% con\fdence level, b at the 5% level and c at the\n10% level. Even though this analysis excludes small and mid-sized companies, the\ncompany size is still a signi\fcant, positive determinant of the news coverage. The\nprice-to-book ratio is signi\fcant and negative in all regressions. This indicates that\ncompanies with high ratios are less often in the news than companies with low\nratios. One reason for this pattern might be that the latter companies have a\nhigher potential for stock price increases. The stock return is weakly signi\fcant and\nnegative. The stock return volatility is signi\fcant, too, and positive. Both indicate\n9that troubled companies are frequently in the media. However, this result might\nalso be due to the \fnancial crisis.\nAll results are qualitatively the same if Yi is considered instead of Qi. Hence, large\ncompanies with low price-to-book ratios and volatile stock returns have a high media\ncoverage, or conversely, companies with a high media coverage are large, have a low\nprice-to-book ratio and their stock price is rather volatile.\n[Table 2 about here.]\n5 Content Analysis and Variable Construction\nA company, a news article is relevant for a company if\n(a) contains the company's RIC in the \feld 'related RICs', or\n(b) mentions the company name or its nickname in the headline and has the com-\npany's RIC in the \feld `related RICs'.\nDe\fnition (a) is, of course, a broader de\fnition than (b), and sensitive to news\nregarding the whole industry or direct competitors. The term company name in\nde\fnition (b) and in the following refers to the shortest fraction of the full company\nname that clearly identi\fes the company, e.g. `Disney' instead of `Walt Disney Co' or\n`Conoco' instead of `ConocoPhillips'. For most companies I am able to identify the\ncompany's nickname very accurately. For example, Bank of America is frequently\ncalled BofA, Johnson & Johnson is called J&J and American Express is AmEx.\nTexas Instruments, often called TI, and General Electrics, shortened GE, can only\nbe identi\fed with a small error rate. Filtering for related RICs in de\fnition (b)\nensures that a news story with a headline such as 'BofA cuts Google price target' is\nassigned to Google, but not to Bank of America.\nEven though a news article is relevant for a company, it is unlikely that all words in\nthe full text are important for the company as well. Hence, I de\fne which passages\nin the full text of a relevant news story have to be analyzed. Given a relevant news\nstory according to de\fnition (a) or (b), I de\fne the relevant text by:\n(c) All words in a sentence are relevant if the company name or nickname is men-\ntioned within the same sentence, or\n(d) All words with a distance of at most 5 words to the company name or nickname\nare relevant.\nWords that contain numerical expressions (e.g. `B787', `A330-200', `$35') are not\ncounted for the word distance since they are not related to the sentiment of a news\nstory.\n10Given a company, I analyze the content of the relevant text of a relevant news story\nand assign a numerical value to it. The approach relies on the `General Inquirer'\n(http://www.webuse.umd.edu:9090/). The `General Inquirer' is a dictionary based\ncontent analysis algorithm. It assigns words to word categories and reports the\nnumber of hits in each cluster, relative to all analyzed words, see Stone et. al. (1966).\nThere are more than 80 word categories. However, I restrict myself to the categories\n`positive', `negative', `strong' and `weak'. Even though being very popular, the\n`General Inquirer' is not perfect. Many words have more than one meaning and\nmight be incorrectly assigned to a word category, see for example Loughran and\nMcDonald (2011), who test the performance of the `General Inquirer' by analyzing\nannual and 10-K reports, and \fnd that a substantial fraction of negative words\n(about 60%) is misinterpreted. However, the content of the Reuters news articles is\nvery general and hardly comparable to annual reports, hence I expect a low error\nrate.\nConsider, for example, the following news stories:\nFeb. 29, 2008, Northrop-EADS beats Boeing to built U.S. tanker\nWASHINGTON, Feb. 29, 2008 - The U.S. Air Force said on Friday it\nhad picked a transatlantic team led by Northrop Grumman, instead of\nBoeing, to start building a new aerial refueling \neet in a surprise choice\nworth about $35 billion. Northrop Grumman Corp (NOC.N) and its\nEuropean partner, Airbus parent EADS (EAD.PA), \"clearly provided\nthe best value to the government,\" Sue Payton, the Air Force's chief\nweapons buyer, told reporters at a brie\fng. The contract is to supply up\nto 179 tanker aircraft in a deal valued at about $35 billion over the next\n15 years, the Air Force said in a statement. The aircraft will replace [...].\nSept. 29, 2009, Kenya Airways eyes Airbus A330-200s sources\nNAIROBI, Sept. 29, 2009 - Kenya Airways (KQNA.NR) is in talks with\nAirbus (EAD.PA) about buying several A330-200 planes after delays to\nBoeing's (BA.N) much-anticipated B787 Dreamliner jet, senior o\u000ecials\nat the airline said on Tuesday. The carrier's Chief Executive O\u000ecer\nTitus Naikuni said on Friday the company was in talks with Airbus [...].\nClearly, the news stories are rather positive for Northrop and EADS, respectively,\nand negative for Boeing. According to the 'General Inquirer' dictionary, there are\nseveral positive words in the second sentence of the \frst news story (`clearly', `pro-\nvide', `best'). There, Northrop and EADS are mentioned, but not Boeing. Re-\ngarding the second news story, approach (c) might fail since Airbus and Boeing are\nmentioned in the same sentence. However, the \fve word distance around Boing\ncovers the word `delay', which clearly signals negative sentiment for Boeing, but\nthere are no negative words within the \fve word distance around EADS. Of course,\nthe performance of both approaches depends on the structure of the news story. If\n11a news article describes complex events where many companies interact, both ap-\nproaches might fail to measure the correct sentiment. However, both approaches\nperform very well for simple or well structured news. Furthermore, the company\nname is sometimes replaced by a synonym, e.g. `planemaker' instead of `Boeing'.\nSuch cases are not recognized.\nTo homogenize market data and news stories, I assign news stories which were\nreleased after 4 p.m. New York time to the following trading day. News stories\npublished between Friday, 4 p.m. and Monday, 4 p.m. are assigned to Monday.\nAssume there are Qi;t 2 N news stories for company i on day t according to de\fnition\n(a) or (b). Given the relevant text following de\fnition (c) or (d), let Posi;t;j [Negi;t;j]\ndenote the number of positive [negative] words relative to the total number of words\nin the relevant text of news story j = 1;:::;Qi;t. Then, the average, relative number\nof positive words and the average, relative number of negative words are used to\nmeasure positive signals, Pi;t, and negative signals, Ni;t, at t and for company i, i.e.\nPi;t = max\n8\n<\n:\n1\nQi;t\nQi;t X\nj=1\n(Posi;t;j \u0000 Negi;t;j);0\n9\n=\n;\n;\nNi;t = max\n8\n<\n:\n1\nQi;t\nQi;t X\nj=1\n(Negi;t;j \u0000 Posi;t;j);0\n9\n=\n;\n: (3)\nPi;t and Ni;t might be interpreted as positive and negative public signals in the style\nof Harris and Raviv (1993). High values of Pi;t or Ni;t indicate strong signals.\nIt is likely that there is a monotone relationship between the average of net sentiment\nof a day, i.e. 1\nQi;t\nPQi;t\nj=1(Posi;t;j\u0000Negi;t;j), and the abnormal stock returns or the CDS\nspread, but trading volume and volatility are presumably not monotonically related\nto the net sentiment. Therefore, positive and negative signals are disentangled. I do\nnot exclude days without news releases since these days are important as well and\nmight indicate `neutral' or `calm' days. The sentiment for these days is set to zero.\nFurthermore, I de\fne two disagreement scores. Usually, the investors' view on a\ncompany is in\nuenced, perhaps driven, by public information. If news stories dis-\nagree heavily, it is likely that investors disagree as well. Hence, the degree of di\u000ber-\nence of opinion among investors might well be approximated by the variation in the\nsentiment of news stories. I de\fne\nDstd\ni;t = \u001b\n\u0000\n(Posi;t;j \u0000 Negi;t;j)j2Qi;t\n\u0001\n; (4)\nwhere \u001b(\u0001) is the standard deviation. I set Dstd\ni;t = 0 if Qi;t \u0014 1.\nInspired by Das and Chen (2007), I construct a second measure for disagreement.\nDe\fne the auxiliary variable Ai;t;j := 1(Negi;t;j < Posi;t;j) \u0000 1(Negi;t;j > Posi;t;j).\n1(\u0001) is the indicator function. It is one if and only if the argument is true. Hence,\nAi;t;j = 1 if the net sentiment of news j is positive, it is zero if Posi;t;j = Negi;t;j\nand \u00001 otherwise. A might be interpreted as a buy- or sell-signal for investors.\n12Disagreement is alternatively measured by\nD\npol\ni;t =\nmax\nnPQi;t\nj=1 jAi;t;jj;1\no\nmax\nn\f \f \f\nPQi;t\nj=1 Ai;t;j\n\f \f \f;1\no: (5)\nIf all news stories on day t and for company i have a positive sentiment or all news\nstories have a negative sentiment, D\npol\ni;t = 1. This might indicate no disagreement.\nFor days without news stories (Qi;t = 0) I set D\npol\ni;t = 1, too. For all other days\nD\npol\ni;t > 1. D\npol\ni;t is high if there are many news stories with positive or negative\nsentiment (numerator is large) and the number of positive and negative news sto-\nries is balanced (denominator is small). These days might be associated with high\ndisagreement across investors. Whereas Dpol measures the polarity of (Ai;t;j)j2Qi;t\nand ignores the magnitude of the net sentiment, Dstd is sensitive to variations in\nthe net sentiment even though the sign of the net sentiment might be the same for\nall news stories.\n6 Regression Results\n6.1 Contemporaneous Analysis\nI analyze the contemporaneous relationship between sentiment respectively disagree-\nment and the \fnancial market. This analysis is motivated by Das, Martinez-Jeres\nand Tufano (2005) and the literature on di\u000berence of opinion. This analysis allows\nto test Hypotheses 2 to 5 on the co-movement of market variables and public signals\nrespectively the degree of di\u000berence of opinion. The analysis does not allow to con-\nclude on market e\u000eciency and the predictability of market returns. Even though\nthe news stories are unscheduled, a signi\fcant relationship between sentiment re-\nspectively disagreement and market prices or returns on a daily frequency might be\nconsistent with e\u000ecient markets if the market anticipates the news.\nAccording to Gro\u0019-Klu\u0019mann and Hautsch (2011), the relevance of a news article\nfor a company determines the strength of the relationship between sentiment of the\nnews and the market. Hence, I apply the more restrictive de\fnition of relevance,\ni.e. de\fnition (b), and use de\fnition (c) to identify the relevant words. The other\nde\fnitions are used for robustness tests.\n6.1.1 Company Individual Analysis\nAs shown in Blume, Easley and O'Hara (1994), volatility and historical stock prices\nmight be valuable information for future stock returns. Pan and Poteshman (2006)\ndocument that option trading contains relevant information for stock returns, too,\n13and according to Sarwar (2006) and Kyriacou and Sarno (1999), option trading vol-\nume and volatility interact. Cremers et. al. (2008) report a signi\fcant relationship\nbetween equity markets and credit markets. Chordia, Sarkar and Subrahmanyam\n(2005) study the intertemporal association between liquidity, volatility and returns\nby applying a vector autoregressive model. Also, the di\u000berence of opinion liter-\nature implies positive autocorrelation in trading volume, negative autocorrelation\nin returns and positive correlation between trading volume and volatility. To con-\ntrol for these associations and to determine the relationship between the \fnancial\nmarket and sentiment and disagreement, respectively, accurately, I choose the most\nparsimonious regression model that allow for the aforementioned pattern, a vector\nautoregressive process with one lag. I estimate\nh\n^ \"i;t Ti;t \u0001Vi;t Oi;t \u0001Ci;t\ni0\n(6)\n= \u0003i\nh\n^ \"i;t\u00001 Ti;t\u00001 \u0001Vi;t\u00001 Oi;t\u00001 \u0001Ci;t\u00001\ni0\n+ \fi[Pi;t Ni;t Di;t]\n0 + KiUt + \u0011i;t:\nD stands for the disagreement score and refers to Dstd or Dpol. Frequently, the\naugmented Dicky-Fuller test cannot reject the unit-root hypothesis for the CDS\nspread and for the volatility spread. Hence, I replace these time series by the\n\frst di\u000berence for all companies. \u0001V and \u0001C denote the \frst di\u000berence of the\nvolatility spread and the CDS spread, respectively. ^ \"i;t, Ti;t and Oi;t are always\nstationary. \u0003i is a 5 \u0002 5 matrix and captures possible inter-temporal associations\nbetween the abnormal returns, trading volume in stock and options and the change\nin the volatility spread and the CDS spread, respectively. \fi is a 5 \u0002 3 matrix\nand measures the association between sentiment respectively disagreement and the\nmarket. Ut is 5 \u0002 1 vector with weekday dummies and Ki's dimension is 5 \u0002 5. \u0011i;t\nis white noise.\n[Table 3 about here.]\n[Table 4 about here.]\n[Table 5 about here.]\nTables 3, 4 and 5 show the estimates for \fi. The companies listed in these tables\nare chosen because I have a su\u000ecient number of daily observations for all time\nseries jointly to calculate reliable regression coe\u000ecients and p-values. A list of\ncompany names and RIC is provided in the appendix. The option data is available\nfor most companies since June 2008 and determine the beginning of the estimation\nperiod, whereas the CDS spread is available until October 2010 and determine the\nend. With the exception of Intel Technology (INTC.O) and Travelers Companies\n(TRV.N), I have 597 days without missing observations for each company. There are\n479 observations for Intel and 660 observations for Travelers. The FF residuum is\nestimated in-sample using the time span June 01, 2008 to September 30, 2010. The\n14average FF-R2 across all companies is about 54%, indicating that the general market\nmovements explain a substantial fraction of the variation in the stock returns. The\naverage correlation between the abnormal return and the change in the volatility\nspread across all companies is -0.2859. Stock and option trading volume are on\naverage correlated by 0.3201 and the change in the volatility spread and the CDS\nspread are on average correlated by 0.2219. All other correlations between the\nmarket variables are close to zero. On average, positive and negative sentiment\nare correlated by -0.0725, positive sentiment and Dstd are correlated by 0.2613 and\nnegative sentiment and Dstd by 0.1727. The average correlation between Dpol and\nP respectively N is insigni\fcantly higher.\nTable 3 gives the estimated, contemporaneous relationship between positive sen-\ntiment and abnormal returns, stock trading volume, the change in the volatility\nspread, option trading volume and the change in the CDS spread (this is the \frst\ncolumn of ^ \fi), as well as the number of days with positive sentiment, #(P > 0), the\nmean of positive sentiment, given all days with positive sentiment, m(PjP > 0), and\nthe standard deviation, \u001b(PjP > 0). Table 4 shows the second column of ^ \fi, this is\nthe estimated relationship between negative sentiment and the market, and the cor-\nresponding descriptive statistics. Table 5 shows the estimated regression coe\u000ecients\nof disagreement. In all tables, a indicates signi\fcance at the 1% con\fdence level, b\nat the 5% level and c at the 10% level. I do not show the regression estimates for\n\u0003i and Ki.\nAs can be seen in the \frst column of Tables 3 and 4, positive sentiment and nega-\ntive sentiment are frequently signi\fcant for the FF residuum. Often, the coe\u000ecient\nof positive sentiment is positive and the coe\u000ecient of negative sentiment is nega-\ntive, indicating that positive news are associated with positive abnormal returns\nand negative news with negative abnormal returns. This suggests that the General\nInquirer and the relevant text identi\fcation procedure approximate the `true' sen-\ntiment or signal accurately. Disagreement is frequently signi\fcant, but the sign of\nthe signi\fcant coe\u000ecients varies among companies. There are 9 signi\fcant, posi-\ntive coe\u000ecients and 10 signi\fcant, negative coe\u000ecients. Hence, it is unclear which\ne\u000bect dominates on average. The average R2 across all companies with respect to\nthe abnormal return is 4.57%. Compared to an average R2 of 2.99% in regression\nmodel (6) and omitting \fi[Pi;tNi;tDi;t], shows that positive and negative sentiment\nand disagreement account on average for 1.58 percentage points in the R2. This\nsigni\fcant increase by more than 50% is exclusively due to the content analysis and\nhighlights its accuracy. Even though the news articles are usually unscheduled and\nfundamental, the signi\fcant link between returns and news does not allow conclusion\non market e\u000eciency.\nThe average R2 of regression model (6) with respect to stock trading volume is\n38.48% and the average R2 of (6) and without the regressors [Pi;tNi;tDi;t] is 34.92%,\nindicating that the content analysis explains about 4 percentage points. Regarding\noption trading volume, the average R2 increases from 15.43% without the content\nanalysis to 16.46%. To test Hypothesis 2, I use positive sentiment and negative\nsentiment to approximate the public signal's intensity and study its association\n15with trading volume on the same day. As shown in the second column of Tables\n3 and 4, the estimated coe\u000ecient of positive sentiment on stock trading volume\nis positive and signi\fcant for 19 out of 62 companies. The coe\u000ecient of negative\nsentiment is positive and signi\fcant for 12 companies. Option trading volume shows\nsimilar patterns, but the dependencies are less pronounced. However, the signal's\nintensity seems to be positively related to trading volume, as stated in Hypothesis\n2. Hypothesis 3 relates stock and option trading volume to disagreement. Table 5,\ncolumns 2 and 4, show the estimated relationship between disagreement across news\nand trading volume. High disagreement is associated with signi\fcantly higher stock\ntrading volume for 49 companies out of 62. There is no company with a signi\fcant,\nnegative regression coe\u000ecient. Regarding the relationship between option trading\nvolume and disagreement, I \fnd 23 out of 62 positive and signi\fcant relationships.\nHence, I have strong support for Hypotheses 3. Investors seem to trade on public\nsignals and the degree of disagreement accelerates the trading volume.\nThe relationship between the change in the volatility spread and disagreement is in-\nconclusive, see Table 5, column 3. The number of signi\fcant regression coe\u000ecients\nis low, and the number of signi\fcantly negative regression coe\u000ecients and signif-\nicantly positive regression coe\u000ecients are almost balanced. Hence, it is infeasible\nto draw robust conclusions on the relationship between volatility and disagreement.\nHowever, Table 4 indicates that the volatility spread widens subsequent to days\nwith negative sentiment (12 positive and signi\fcant coe\u000ecients in Table 4, column\n3). This \fnding is consistent with evidence on negative correlation between index\nreturns and volatility, since days with negative sentiment are also associated with\nnegative abnormal returns. The average R2 of the full regression model with respect\nto the change in the volatility spread is 6.80% and the contribution of the content\nanalysis in terms of average R2 is 1.13 percentage points. Nevertheless, Hypothesis\n4 is not supported.\nThe change in the CDS spread is often negatively correlated with positive sentiment\nand positively correlated with negative sentiment. This is consistent with the rela-\ntionship between the abnormal stock returns and sentiment, and with the response\nof the volatility spread. Given a negative signal, the value of equity decreases and the\nequity volatility goes up. Consistent with structural models, the distance to default\nis reduced and the expected default loss, measured by the CDS spread, increases. As\ncan be seen in Table 5, column 5, disagreement has frequently a signi\fcant, positive\nregression coe\u000ecient and supports Hypothesis 5. The content analysis increases the\naverage R2 of the change in the CDS spread from 5.91% to 7.11%\nMost results remain qualitatively unchanged if I consider Dpol as a measure of dis-\nagreement instead of Dstd. Therefore, the results are not shown but only discussed\nbrie\ny. The relationship between abnormal returns and positive sentiment becomes\nslightly stronger and the coe\u000ecient of disagreement is now frequently signi\fcant,\nnegative for abnormal returns. This is consistent with Yu (2011), who shows that\nstocks with high analyst forecast dispersion underperform relative to stocks with low\nforecast dispersion. Another noteworthy change is that the CDS spread increases\nwith the alternative disagreement measure for many companies. This gives further\n16support to Hypothesis 5.\n6.1.2 Pooled Analysis\nNext, I analyze all companies jointly. The purpose of this analysis is to investigate\nthe dominating e\u000bects between the \fnancial market and sentiment and disagreement,\nrespectively, for all companies. It simpli\fes the interpretation of the regression\ncoe\u000ecients. I estimate\nh\ns^ \"i;t sTi;t s\u0001Vi;t sOi;t s\u0001Ci;t\ni0\n(7)\n= \u0003\nh\ns^ \"i;t\u00001 sTi;t\u00001 s\u0001Vi;t\u00001 sOi;t\u00001 s\u0001Ci;t\u00001\ni0\n+ \f[sPi;t sNi;t sDi;t]\n0 + KUt + \u0011i;t:\nThe regression coe\u000ecients \u0003, \f and K are now independent of the company index\ni. Hence, I make the strong assumption that the relationship between the market\nvariables, measured by \u0003, and between the market variables and the information\nextracted from company news, measured by \f, is described by the same coe\u000ecients\nfor all \frms. I standardize and pool all time series (with the exception of the weekday\ndummies, which are pooled without further manipulation) by subtracting the time\nseries' individual mean and dividing by the time series' standard deviation. The\nstandardized, pooled time series are marked with the pre\fx s. As an example, the\npooled, standardized stock trading volume is given by the vectors\nsT\u00001 =\n\"\u0014\nT1;t \u0000 m(T1;\u0001)\n\u001b(T1;\u0001)\n\u0015\nt=1;:::;G1\u00001\n;:::;\n\u0014\nTL;t \u0000 m(TL;\u0001)\n\u001b(TL;\u0001)\n\u0015\nt=1;:::;GL\u00001\n#0\nand\nsT =\n\"\u0014\nT1;t \u0000 m(T1;\u0001)\n\u001b(T1;\u0001)\n\u0015\nt=2;:::;G1\n;:::;\n\u0014\nTL;t \u0000 m(TL;\u0001)\n\u001b(TL;\u0001)\n\u0015\nt=2;:::;GL\n#0\n;\nwhere m(\u0001) denotes the mean, \u001b(\u0001) is the standard deviation, L is the number of\ncompanies and Gi is the number of observations for company i. Then, the estimates\nin the pooled regression model are given by\nf^ \u0003; ^ \f; ^ Kg = argmin\u0003;\f;K\nn\n11\u0002G\n\u0010\n[s^ \" sT s\u0001V sO s\u0001C] (8)\n\u0000 [s^ \"\u00001 sT\u00001 s\u0001V\u00001 sO\u00001 s\u0001C\u00001]\u0003 \u0000 [sP sN sD]\f \u0000 UK\n\u00112\n15\u00021\no\n;\nwhere G =\nPL\ni=1(Gi \u0000 1), 1a\u0002b is a matrix of dimension a \u0002 b with 1s everywhere\nand U is the pooled matrix of weekday dummies. The square symbol in (8) refers\nto each component in the vector of residuals and is not a matrix operator.\nPooling all company-speci\fc observations gives in total 36,229 company-day obser-\nvations. Table 6 shows ^ \u0003, ^ \f and ^ K. Disagreement is measured by Dstd in the upper\npanel and by Dpol in the lower panel. The regression estimates of \u0003 and K are very\nsimilar for Dstd and Dpol. Stock trading volume displays positive autocorrelation,\nwhich is consistent with the models of Harris and Raviv (1993) and Banerjee and\n17Kremer (2010). Furthermore, there are several pattern which are consistent with in-\nformation asymmetry. Trading volume predicts abnormal stock returns, as discussed\nin Blume, Easley and O'Hara (1994). Also option trading volume predicts abnormal\nreturns, which might be related to the results of Pan and Poteshman (2006), even\nthough I do not study the ratio of traded put and call options, but the sum.\n[Table 6 about here.]\nAbnormal stock returns, the change in the volatility spread (which is closely related\nto the absolute return) and the change in the CDS spread positively predict stock\ntrading volume. This \fnding is consistent with Barber and Odean (2008), who\nidentify attention-grabbing stocks also with large stock price movements, and \fnd\nthat these stocks have a higher turnover volume than stocks that do not attract\nattention. However, attention might also be gained by large movements in the CDS\nspread and an increase in volatility. Consistent with structural models on credit\nderivatives, the CDS spread increases given an increase in volatility. Surprisingly,\nit also increases given a large abnormal return. This might be due to analyzing\nabnormal returns instead of gross returns. The weekday dummies are frequently\nsigni\fcant, indicating the presence of weekday e\u000bects.\nPositive and negative sentiment are still highly signi\fcant for abnormal returns.\nConsistent with the results in the previous section, positive sentiment is positively\nrelated to abnormal returns and negative sentiment negatively. The coe\u000ecient of\nDstd is insigni\fcant, see upper panel. This does not necessarily mean that disagree-\nment is not relevant for the abnormal return. The insigni\fcance might be due to\nthe heterogeneous relationship between stock prices and disagreement among news\narticles, e.g. Table 5 shows 9 signi\fcant, positive and 10 signi\fcant, negative re-\nlationships. Hence, both e\u000bects are likely to cancel out in the pooled regression.\nFurthermore, the alternative disagreement score Dpol detects a signi\fcant, negative\nrelationship between abnormal returns and disagreement in the pooled analysis, see\nTable 6, lower panel. Again, this is consistent with Yu (2011) and Das et. al. (2005).\nThe R2s in Table 6 with respect to the abnormal return are lower than the average R2\nof the \frm individual regressions. So, the R2 of s^ \" is 0.4% and 0.51%, respectively,\nwhereas the average R2 of ^ \"i is 4.57%. This decrease might be due to the restrictive\nassumption of identical regression coe\u000ecients for all companies. The contribution\nof the content analysis to the R2 of s^ \" is about 0.2 percentage point and doubles\nthe explained variation in abnormal returns.\nThe average R2 of stock trading volume and allowing for company individual regres-\nsion coe\u000ecients is 38.48% and reduces to 34.99% respectively 35.52% in the pooled\nanalysis. The R2 of standardized option trading volume is 10.67% and 10.97%, re-\nspectively, whereas the average R2 of the company individual analysis is 16.46%.\nThis moderate decrease in terms of R2 might indicate that the assumption of iden-\ntical regression coe\u000ecients is not too restrictive for trading volume. Investors' trad-\ning behavior seems to be similarly related to information such as sentiment, lagged\n18volatility, etc. for all companies. The contribution of the text analysis to the R2 is\nabout 3 percentage points for stock trading volume and about 0.65 percentage points\nfor option trading volume. In the upper panel, standardized stock and option trad-\ning volume increase with positive and negative sentiment and disagreement. The\nrelationship is highly signi\fcant and consistent with the company individual anal-\nysis and Hypotheses 2 and 3. Surprisingly, negative sentiment is negatively related\nto trading volume in the lower panel of Table 6. As discussed in Barber and Odean\n(2008), investors might be subject to investment restrictions such as short-selling re-\nstrictions. Then, negative signals are only relevant for investors who already own the\nstock. On the other hand, positive signals are relevant for all investors. Therefore,\nnegative signals might reduce trading whereas positive signals increase trading.\nThe volatility spread narrows with positive sentiment and it increases with negative\nsentiment. However, the estimated relationship between the volatility spread and\ndisagreement is inconclusive. Whereas the coe\u000ecient of Dstd is insigni\fcant in the\nupper panel of Table 6, the coe\u000ecient of Dpol is negative and weakly signi\fcant.\nBoth results are inconsistent with Hypothesis 4. Nevertheless, and consistent with\nHypothesis 5, the CDS spread increases with high disagreement. This increase is\npresumably due to the decrease in the equity value, given high disagreement, and not\ndue to an increase in the equity volatility and asset volatility, respectively. Moreover,\nthe CDS spread increases with negative sentiment and, at least in the lower panel\nof Table 6, decreases with positive sentiment as expected.\nThe regression results in this section and the previous sections show that simple mea-\nsures of sentiment and disagreement based on company news articles of Reuters add\nuseful information to standard factors which are frequently used to explain market\nactivity. However, the predictive power of sentiment and disagreement is ambiguous,\neven though the news articles may be fundamental news and unscheduled.\n6.2 Predicting Market Activity\nNow, I use the pooled regression model to study the predictive power of senti-\nment and disagreement. Hence, I do not analyze the contemporaneous relationship\nbetween market activity and sentiment respectively disagreement, but the relation-\nship between the market and sentiment respectively disagreement from the previous\ntrading day. Hence, the regression model changes to\nh\ns^ \"i;t sTi;t s\u0001Vi;t sOi;t s\u0001Ci;t\ni0\n= \u0003\nh\ns^ \"i;t\u00001 sTi;t\u00001 s\u0001Vi;t\u00001 sOi;t\u00001 s\u0001Ci;t\u00001\ni0\n(9)\n+ \f[sPi;t\u00001 sNi;t\u00001 sDi;t\u00001]\n0 + KUt + \u0011i;t:\nNow, the residual in the objective function (8) is\nh\ns^ \" sT s\u0001V sO s\u0001C\ni\n\u0000\nh\ns^ \"\u00001 sT\u00001 s\u0001V\u00001 sO\u00001 s\u0001C\u00001\ni\n^ \u0003\n\u0000\nh\nsP\u00001 sN\u00001 sD\u00001\ni\n^ \f \u0000 U ^ K: (10)\n19Table 7 shows the regression estimates for \u0003, \f and K. The results in the upper\npanel are based on the disagreement measure Dstd and the results in the lower panel\non Dpol. The estimates for \u0003 and for K are very similar to the contemporaneous\nanalysis. The R2s decrease compared to the contemporaneous analysis of sentiment,\ndisagreement and market activity. Positive sentiment is still highly signi\fcant and\npredicts positive abnormal returns on the following day. Both disagreement mea-\nsures predict negative abnormal returns on the following trading day. Negative\nsentiment is insigni\fcant. This might be due to incorrect assignments of words to\nthe word category `negative' by the `General Inquirer', see Loughran and McDonald\n(2011). However, the relationship between abnormal stock returns and sentiment\nand disagreement, respectively, are still unexpected and might be inconsistent with\nHypothesis 1. Assuming e\u000ecient markets, prices should respond to new information\nimmediately. However, the signi\fcance of positive sentiment and disagreement hints\ntowards market ine\u000eciencies even on a daily frequency. These results become even\nstronger if I consider the excess stock return instead of the abnormal stock return.\nThen, the R2 of the excess return is 1.16%, positive sentiment is positive, signi\fcant\nand negative sentiment and disagreement are signi\fcant, negative. The results are\nnot shown.\nThere is no signi\fcant relationship between the change in the CDS spread and the\none day lagged sentiment and disagreement, respectively. Hence, the credit market\nseems to be e\u000ecient with respect to the information extracted from the Reuters com-\npany news and in this framework. However, the company individual analysis (the\nresults are not shown) indicates that negative sentiment and disagreement predict\nthe change in the CDS spread for several companies. The sign of the company-\nindividual regression coe\u000ecients varies across \frms and might destroy a signi\fcant\nrelationship in the pooled regression model.\n[Table 7 about here.]\nFurthermore, positive sentiment predicts stock trading volume on the following day,\nindicating that positive signals have a long-lasting impact on trading volume. How-\never, negative sentiment is insigni\fcant. This heterogeneity might be due to in-\nvestment restrictions, see Barber and Odean (2008). Surprisingly, the regression\ncoe\u000ecients of both disagreement measures are signi\fcantly negative. One possible\nexplanation might be that investors tend to over-react to disagreement. Then, the\nstock trading volume might be lower during the following days.\nThe volatility spread increases signi\fcantly after negative sentiment and after dis-\nagreement, measured by Dstd. Dpol is insigni\fcant. Compared to the contem-\nporaneous relationship between disagreement and the volatility spread, which is\ninconclusive, the result for the one day lagged Dstd is more consistent with Hypoth-\nesis 4. The delayed response of the volatility spread could be due to a rather slow\ninformation processing and might also hint at market ine\u000eciencies. Positive and\nnegative sentiment and disagreement have only little predictive power for option\ntrading volume. These results are con\frmed by the company individual analysis.\n206.3 Robustness\nA further extension of these simple analyses is to weight the sentiment with its degree\nof uncertainty. I measure uncertainty with two approaches. (1) Uncertainty in news\narticles might be measured with the `General Inquirer' word categories `strong' and\n`weak'. However, there is a substantial overlap between the categories `positive' and\n`strong' respectively `negative' and `weak'. This might bias the results. Nevertheless,\nI measure the uncertainty attached to a news article by\nH\n(1)\ni;t;j =\nZi;t;j + #\nWi;t;j + Zi;t;j + 2#\n;\nwhere Zi;t;j denotes the number of strong words and Wi;t;j is the number of weak\nwords. # is a small, positive constant that ensures the existence of Hi;t;j even though\nthere are neither strong nor weak words in the news story j. Then, Hi;t;j = 0:5. If\nthere are only strong words, Hi;t;j \u0019 1 and if there are only weak words Hi;t;j is close\nto zero. (2) Alternatively, if the author of a news article uses many positive words\nand negative words in the relevant text for a company, she might be unsure about\nthe \fnal consequences. Therefore, uncertainty is measured by\nH\n(2)\ni;t;j =\njPosi;t;j \u0000 Negi;t;jj\nmaxfPosi;t;j + Negi;t;j;#g\n:\nIf positive and negative words are almost balanced, H\n(2)\ni;t;j is close to zero. If either\npositive words clearly dominate negative words or negative words clearly dominate\npositive words, H\n(2)\ni;t;j is close to 1.\nNow, by multiplying the net sentiment Posi;t;j \u0000 Negi;t;j in (3) with H\n(1)\ni;t;j or H\n(2)\ni;t;j,\nthe uncertainty that is related to a news article can be taken into account. The\nresults with respect to both measures of uncertainty stay qualitatively the same\ncompared to the results discussed above and, hence, are not shown.\nMoreover, zooming into the news story and analyzing words within the close neigh-\nborhood of the company name or nickname, as described in de\fnition (d), gives very\nsimilar results. The results are not shown, too, but indicate that a small fraction of\nthe full news text already contains valuable information for the \fnancial market.\n7 Trading Strategies\nAccording to the previous section, positive sentiment and disagreement are statis-\ntically signi\fcant to predict abnormal stock returns and excess returns. However,\nthis does not allow to conclude on the economic signi\fcance and on market e\u000e-\nciency. Therefore, I trading strategies based on positive and negative sentiment to\ngain insights on the economic signi\fcance of news articles for the stock market.\n21Assume that an investor trades in the 62 stocks simultaneously. The investor has\nno initial endowment. She observes the signal\nXi;t = 1(Pi;t > Ni;t) \u0000 1(Pi;t < Ni;t):\nXi;t can take on the value +1;0 and \u00001. Xi;t = 1 might be interpreted as a buy-signal\nand Xi;t = \u00001 as a sell-signal3. Xi;t = 0 might indicate a neutral position. Since\nXi;t incorporates news from 4 p.m. at t\u00001 to 4 p.m. at day t, trading on Xi;t might\nimply a substantially delayed response to new information. Whenever Xi;t = 1, the\ninvestor borrows one USD at the risk-free rate and purchases (a fraction of) stock i\nat the closing price of day t. At the following day, the stock is sold at the closing\nprice of day t + 1 and the loan is repaid, if the signal changes to neutral or to sell.\nOtherwise, the position is not closed until the buy-signal disappears. If Xi;t = \u00001,\nthe investor short-sells one USD in stock i, invests this one USD at the risk-free\nrate Rf and holds the position until the signal changes. Pro\fts and losses, due to\ntrading are collected in her money account, which is grossed up with the risk-free\nrate. Furthermore, the investor has to pay transaction costs for one round-trip. For\nsimplicity, I assume that the risk-free rate for lending and borrowing is the same\nand that the transaction costs are payed when the position is closed.\nMore precisely, let Mt denote the value of the money account at day t, Si;t the closing\nprice of stock i at day t and R\nf\nt is the daily gross risk-free interest rate, taken from\nthe data library of Kenneth French. By assumption, M0 = 0. The money account\nat t is given by\nMt = Mt\u00001R\nf\nt +\n62 X\ni=1\n\u0012\nLongi;t + Shorti;t\n\u0013\n; (11)\nLongi;t =\n0\n@\nt Y\ns=\u001c(t)+1\nSi;s\nSi;s\u00001\n\u0000\nt Y\ns=\u001c(t)+1\nR\nf\ns \u0000 TC\n1\nA1(Xi;t\u00001 = 1 _ Xi;t 6= 1); (12)\nShorti;t =\n0\n@\nt Y\ns=\u001a(t)+1\nR\nf\ns \u0000\nt Y\ns=\u001a(t)+1\nSi;s\nSi;s\u00001\n\u0000 TC\n1\nA1(Xi;t\u00001 = \u00001 _ Xi;t 6= \u00001); (13)\nwhere \u001c(t) = maxfs < tjXi;s\u00001 6= 1 _ Xi;s = 1g denote the most recent `buy'-signal\nand \u001a(t) = maxfs < tjXi;s\u00001 6= \u00001 _ Xi;s = \u00001g the most recent `sell'-signal. TC\ndenotes the transaction costs. The indicator function in (12) and (13) is one if and\nonly if a position is closed. Then, the pro\ft is assigned to the money account.\nAlternatively, I test this trading strategy against the market. This means that the\ninvestor does not \fnance trades at the risk-free rate and invest at the risk free-rate\nif a stock is short-sold, respectively, but at the market return. Then, Rf in (12) and\n(13) is replaced by RMarket, both benchmarks are downloaded from the homepage\nof Kenneth French.\n3The variables X and A di\u000ber since A is de\fned for each news story individually whereas X\nrefers to the average net sentiment of a trading day.\n22In addition to Xi;t, I consider trading strategies that are based on the signals X\n+\ni;t =\nmaxfXi;t;0g and X\n\u0000\ni;t = minfXi;t;0g. Whereas X\n+\ni;t consists only of buy-signals, X\n\u0000\ni;t\nincorporates only sell-signals. I do not consider trading strategies that are based\non disagreement to avoid con\nicting signals between sentiment and disagreement.\nFurthermore, I do not incorporate the signal intensity, i.e. Pi;t\u0000Ni;t, nor the trading\nvolume in the corresponding stock, the stock volatility or the company's CDS spread.\nThose trading strategies might depend on parameter values and, hence, require an\nin-sample optimization and an out-of-sample performance evaluation. However, the\nshort time span of my data sample is insu\u000ecient for this approach.\nThe full observation period June 01, 2007 to December 31, 2010 covers 56110\ncompany-day observations (62 companies \u0002 905 days). Using de\fnition (b) and\n(c) to calculate Pi;t and Ni;t results in 8757 buy-signals and 3816 sell-signals. This\nyields 6062 long-positions and 3042 short positions with an average duration of\n1.44 days and 1.25 days, respectively. Excluding transaction costs and re\fnancing\ncosts, the average gain of a long-position is 29 bps with a standard deviation of\n272 bps and the average gain of a short-position is 51 bps with standard deviation\n365 bps. Hence, trades on sell-signals are more pro\ftable and less frequent. The\nlower number of sell-signals and their shorter duration compared to buy-signals is\nsomewhat surprising since the observation period covers the \fnancial crisis. Further-\nmore, transaction and \fnancing costs of 30 bps and more would render trading on\nbuy-signals, on average, non-pro\ftable. Sell-signals seem to be more robust against\ntransaction costs. Moreover, the pro\fts of the daily, aggregated long and short\ntrades are correlated by -0.45. Therefore, the trading strategy on X\n+\ni;t might be an\ne\u000ecient hedge for the strategy on X\n\u0000\ni;t.\nTable 8 shows summary statistics for the money accounts of the trading strategies\nbased on the signal Xi;t, X\n+\ni;t and X\n\u0000\ni;t and for the benchmarks risk-free rate and mar-\nket return, assuming di\u000berent levels of transaction costs. Without transaction costs\nand by benchmarking against the risk-free rate, the money account of the trading\nstrategy that incorporates buy- and sell-signals increases from 0 USD by June 01,\n2007, to 33.32485 USD by December 31, 2010. The money account's minimum is\n-0.0406 USD and it turns negative only for one day. Hence, there is almost no risk of\nlosing money, indicating that the strategy might be interpreted as an approximate\narbitrage opportunity. The trading strategies based on buy- respectively sell-signals\nexclusively have similar gain-loss pro\fles and might be seen as approximate arbitrage\nopportunities as well. The gain-loss pro\fles of the trading strategies are almost un-\nchanged if the market return is used as a benchmark. However, the short-positions\nsu\u000ber slightly presumably due to long-investments in the poorly performing stock\nmarket during the \fnancial crisis.\nBy assuming 10 bps transaction costs4 per round-trip, the terminal values of the\nmoney account of the joint trading strategy on buy- and sell-signals are 24.0594 USD\nrespectively 19.8684 USD, depending on the benchmark, and the gain-loss ratios\n4The transaction costs might also cover the bid-ask spread and di\u000berent rates for borrowing\nand lending.\n23are still very attractive and comparable to an approximate arbitrage opportunity.\nThe 5% quantile, q0:05(Mt), is positive for both strategies, and the money accounts\nturn negative for only 3 respectively 4 days with a minimum value of -0.1286 USD\nrespectively -0.1429 USD. However, trading on buy-signals only, \fnanced at the risk-\nfree rate becomes quite risky compared to the scenario without transaction costs.\nThe 5% quantile of the money account is -0.5732 USD and the money account is\nnegative for 155 days. The reason might be that long-signals generate only little\npro\fts in the \fnancial crisis. These pro\fts hardly cover the transaction costs and\nincrease the probability that the money account turns negative. Also, trading on\nsell-signals only and investing into the stock market bear some shortfall risk now.\nFigure 2 depicts the value of the money accounts of the three strategies when the\nrisk-free rate is used as benchmark. The blue, solid curve shows the money account\nof trading on Xi;t, the green, dashed curve is the money account of trading on\nX\n+\ni;t and the red, dotted curve of X\n\u0000\ni;t. The money account of Xi;t increases almost\nmonotonically. During the heydays of the \fnancial crisis (June 2007 to April 2010),\nthe trading strategy on buy-signals generates signi\fcant losses, but the performance\nof trades on sell-signals is excellent and compensates the losses of the buy-signals\nfully. However, in spring 2009, governments and central banks successfully calmed\ndown the \fnancial markets and the stock market recovered. In the aftermath, the\ntrading strategy on sell-signals fails to generate pro\fts and becomes unpro\ftable.\nAt the same time, buy-signals work very well. This underlines the hedging quality\nof trading on both, buy- and sell-signals, jointly. Furthermore, Figure 2 shows the\nstrongest decrease in the value of the money account of Xi;t (black line, 1.81 USD\nin May and June 2009) and the longest waiting period to establish a new high\nwatermark (light blue line, 112 days during Spring and Summer 2010). Both \fgures\nare moderate5.\nIncreasing the transaction costs to more than 10 bps reduces the performance of all\ntrading strategies and increases the likelihood of a negative money account value\nsigni\fcantly. The assumption of 20 bps transaction costs per round-trip reduces\nthe terminal value of the money account of the joint trading strategy on buy- and\nsell-signals to 14.8703 USD, including 197 days with a negative value and a mini-\nmum of -1.5261 USD. This trading strategy might be still an attractive investment\nopportunity, but it now bears a substantial shortfall risk. Transaction costs of 30\nbps and more imply that the investor looses money on almost every buy-signal and\non many sell-signals. Hence, the terminal values of the money accounts of Xi;t and\nX\n+\ni;t are negative. However, X\n\u0000\ni;t might still be pro\ftable.\n[Table 8 about here.]\n[Figure 2 about here.]\n5The worst case, i.e. the strongest downturn and the longest waiting period to exceed the high\nwatermark appear jointly at day zero, might be an indication for the minimum equity bu\u000ber in\nthe approximate arbitrage portfolio.\n248 Conclusion and Outlook\nDas et. al. (2005) analyzed chat-room postings and conclude that `investors \frst\ntrade and then talk'. I analyze company news of Reuters. These news are more\nreliable than chat room postings which, at best, disseminate company news. Simple\ndictionary based content analysis algorithms with rather high error rates might be\napplied to measure sentiment and disagreement of those news articles. Both contain\nvaluable information for \fnancial markets.\nMy results are mostly consistent with models on di\u000berence of opinion, i.e. investors\nare more likely to trade stocks and options after observing public signals. Dis-\nagreement across news articles is also positively correlated with stock and option\ntrading volume and expected stock volatility. Moreover, sentiment and disagreement\nare statistically signi\fcant to predict returns, volatility and trading volume. With\nmoderate transaction costs, it might be possible to exploit market ine\u000eciencies by\ntrading on buy- and sell-signals based on the mechanical evaluation of company\nnews. However, transaction costs of more than 10 bps destroy this approximate\narbitrage opportunity. Therefore, only institutional investors might be able to take\nadvantage of this ine\u000eciency. For transaction costs in the range of 10 bps up to 30\nbps, the expected pro\fts of the trading strategies are still positive, but the strategies\nbecome risky, i.e. there might be a substantial probability that the terminal value\nof money account is negative. Even higher transaction costs render the strategies\nuseless.\nI consider the following extensions. (1) Classi\fer: The 'General Inquirer' dictionary\nis very general. The dictionary is not designed for analyzing \fnancial news. Hence,\nit is likely that the results will improve signi\fcantly if I adjust the dictionary to\naccount for important characteristics in \fnancial and economic news, which might\nbe misinterpreted right now. Furthermore, it might be interesting to determine\nthe sentiment / disagreement by applying di\u000berent methods that are not based on\na dictionary approach. Bayesian classi\fers, adjective-adverb classi\fers and vector\ndistance classi\fers could be used as well. The grammar, text lengths or readability\nmight be incorporated to evaluate the sentiment. (2) Industry Portfolios: Compa-\nnies within the same industry might have a similar exposures to news. Hence, by\nstudying industry portfolios instead of all companies separately or the overall pooled\nsample, the results might become even stronger. Furthermore, it might be possible\nto compare information processing among industries.\nAppendix\n[Table 9 about here.]\n25References\n[1] Antweiler, Werner and Murry Z. Frank, 2004, \"Is all that just Noise? The\nInformation Content of internet Stock Message Boards\", Journal of Finance 59,\n1259-1294.\n[2] Banerjee, Snehal and Ilan Kremer, 2010, \"Disagreement and Learning: Dynamic\nPattern of Trading Volume\", Journal of Finance 65, 1269-1302.\n[3] Barber, Brad M. and Terrance Odean, 2008, \"All That Glitters: The E\u000bect\nof Attention and News on the Buying Behavior of Individual and Institutional\nInvestors\", Review of Financial Studies 21, 785-818.\n[4] Blume, Lawrence, David Easley, Maureen O'Hara, 1994, \"Market Statistics and\nTechnical Analysis: The Role of Volume\", Journal of Finance 49, 153-181.\n[5] Boyd, John H., Jian Hu and Ravi Jagannathan, 2005, \"The Stock Market's\nReaction to Unemployment News: Why Bad News Is Usually Good for Stocks\"\nJournal of Finance 60, 649-672.\n[6] Brooks, Raymond M., Ajay Patel and Tie Su, 2003, \"How the Equity Market\nResponds to Unanticipated Events\", Journal of Business 76, 109-133.\n[7] Brounrn, Dirk and Jeroen Derwall, 2010, \"The Impact of Terrorist Attacks on\nInternational Stock Markets\", European Financial Management 16, 585-598.\n[8] Carretta, Alessandro, Vincenzo Farina, Duccio Martelli, Franco Fiordelisi and\nPaola Schwizer, 2010, \"The Impact of Corporate Governance Press News on\nStock Market Return\", European Financial Management 17, 100-119.\n[9] Cao, H. Henry and Hui Ou-Yang, 2009, \"Di\u000berences of Opinion of Public In-\nformation and Speculative Trading in Stocks and Options\", Review of Financial\nStudies 22, 299-335.\n[10] Charida, Tarum, Asani Sarkar and Avanidhar Subrahmanyam, 2005, \"An Em-\npirical Analysis of Stock and Bond Market Liquidity\", Review of Financial Stud-\nies 18, 85-129.\n[11] Coval, Joshua D. and Tyler Shumway, 2001, \"Is Sound Just Noise?\", Journal\nof Finance 56, 1887-1910.\n[12] Cremers, Martijn, Joost Driessen, Pascal Maenhout and David Weinbaum,\n2008, \"Individual stock-option prices and credit spreads\", Journal of Banking\nand Finance 32, 2706-2715.\n[13] Das, Sanjiv R. and Mike Y. Chen, 2007, \"Yahoo! for Amazon; Sentiment\nExtraction form Small Talk on the Web\", Management Science 53, 1375-1388.\n26[14] Das, Sanjiv R., Asis Martinez-Jerez and Peter Tufano, 2005, \"e-Information:\nA Clinical Study of Investor Discussion and Sentiment\", Financial Management\n34, 103-137.\n[15] Engelberg, Joseph E. and Christopher A. Parsons, 2011, \"The Causal Impact\nof Media in Financial Markets\", Journal of Finance 66, 67-98.\n[16] Fama, Eugene F., 1998, \"Market E\u000eciency, Long-term Returns, and Behavioral\nFinance\", Journal of Financial Economics 49, 283-306.\n[17] Fama, Eugene F. and Kenneth French, 1993, \"Common Risk Factors in the\nReturns on Stocks and Bonds\", Journal of Financial Economics 33, 3-56.\n[18] Fang, Lily and Joel Peress, 2009, \"Media Coverage and the Cross-section of\nStock Returns\", Journal of Finance 64, 2023-2052.\n[19] Gro\u0019-Klu\u0019mann, Axel and Nikolas Hautsch, 2011, \"When machines read the\nnews: Using automated text analysis to quantify high frequency news-implied\nmarket reactions\", Journal of Empirical Finance 18, 321-340.\n[20] Harris, Milton and Artur Raviv, 1993, \"Di\u000berences of Opinion Make a Horse\nRace\", Review of Financial Studies 6, 473-506.\n[21] Hautsch, Nikolaus and Dieter Hess, 2002, \"The Processing of Non-Anticipated\nInformation in Financial Markets: Analyzing the Impact of Surprises in the\nEmployment Report\", European Financial Review 6, 133-161.\n[22] Hess, Dieter, 2008, \"How Do Commodity Futures Respond to Macroeconomic\nNews?\", Financial Markets and Portfolio Management 22, 127-146.\n[23] Hong, Harrison and Jeremy C. Stein, 2007, \"Disagreement and the Stock Mar-\nket\", Journal of Economic Perspectives 21, 109-128.\n[24] Hull, John, Mirela Predescu and Alan White, 2004, \"The relationship between\ncredit default swap spreads, bond yields, and credit rating announcements\",\nJournal of Banking and Finance 28, 2789-2811.\n[25] Kandel, Eugene and Neil D. Pearson, 1995, \"Interpretation of Public Signals\nand Trade in Speculative Makets, Journal of Political Economy 103, 831-872.\n[26] Kyriacou, Kyriaos and Lucio Sarno, 1999, \"The Temporal Relationship between\nDerivatives Trading and Spot Market Volatility in the U.K.: Empirical Analysis\nand Monte Carlo Evidence\", Journal of Futures Markets 19, 245-270.\n[27] Loughran, Tim and Bill McDonald, 2011, \"When Is a Liability Not a Liability?\nTextual Analysis, Dictionaries, and 10-Ks\", Journal of Finance 66, 35-66.\n[28] Milgrom, Paul and Nancy Stokey, 1982, \"Information, Trade and Common\nKnowledge\", Journal of Economic Theory 26, 17-27.\n27[29] Mitchell, Mark L. and J. Harold Mulherin, 1994, \"The Impact of Public Infor-\nmation on the Stock Market\", Journal of Finance 49, 923-950.\n[30] Merton, Robert C., 1974, \"On the Pricing of Corporate Debt: The Risk Struc-\nture of Interest Rates\", Journal of Finance 29, 449-70\n[31] Norden, Lars, 2009, \"Credit Derivatives, Corporate News, and Credit Ratings\",\nworking paper.\n[32] Pan, Jun and Allen M. Poteshman, 2006, \"The Information in Option Volume\nfor Future Stock Prices\", Review of Financial Studies 19, 871-908.\n[33] Sarwar, Ghulam, 2005, \"The Informational Role of Option Trading Volume in\nEquity Index Options Markets\", Review of Quantitative Finance and Accounting\n24, 159-176.\n[34] Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith, Daniel M. Ogilvie, and\nassociates. \"The General Inquirer: A Computer Approach to Content Analysis,\"\nThe MIT Press, 1966.\n[35] Suominen, Matti, (2001), \"Trading Volume and Information Revelation in\nStock Markets\", Journal of Financial and Quantitative Analysis 36, 545-565.\n[36] Tetlock, Paul C., 2007, \"Giving Content to Investor Sentiment: The Role of\nMedia in the Stock Market\", Journal of Finance 62, 1139-1167.\n[37] Tetlock, Paul C., 2010, \"Does Public Financial News Resolve Asymmetric In-\nformation?\", Review of Financial Studies 23, 3520-3557.\n[38] Tetlock, Paul C., Maytal Saar-Tsechansky and Sofus Macskassy, 2008, \"More\nthan Words: Quantifying Language to Measure Firms' Fundamentals\", Journal\nof Finance 63, 1437-1467.\n[39] Tumarkin, Robert and Robert F. Whitelaw, 2001, \"News or Noise? Internet\nPosting and Stock Prices\", Financial Analysts Journal 57, 41-51.\n[40] Yu, Jialin, 2011, \"Disagreement and return predictability of stock portfolios\",\nJournal of Financial Economics 99, 162-183.\n[41] Zhang, Benjamin Yibin, Hou Zhou and Haibin Zhu, 2009, \"Explaining Credit\nDefault Swap Spreads with the Equity Volatility and Jump Risks of Individual\nFirms\", Review of Financial Studies 22, 5099-5131.\n28Figure 1: The upper \fgure shows the daily number of news stories with keywords\n`Bankruptcy' or `Insolvency' (blue curve) and the 3 day moving average (red curve),\nstarting in June 01, 2007 to December 31, 2010. The lower \fgure shows the daily\nnumber of news stories for the Bank of America.\n29Figure 2: The \fgure shows the value of the money accounts of trading on buy- and\nsell-signals (blue, solid curve), buy-signals (green, dashed curve) and sell-signals\n(red, dotted curve), assuming 10 bps transaction costs per round-trip and the risk-\nfree interest rate as benchmark. For the trading strategy on buy- and sell-signals,\nthe black line marks the strongest downturn, realized in May and June 2009, and the\nlight blue line marks the longest waiting period to establish a new high watermark,\nobserved in Summer 2010.\n30Summary statistics for Reuters news Sum Mean Std. Max.\nAll 210495 160.56 94.01 354\nAll w/o Saturdays / Sundays 199238 220.64 52.91 354\nEconomic news / Macroeconomics 56531 43.12 38.61 196\nGeneral News 28085 21.42 21.00 118\nDebt ratings / Credit Market News 2461 1.88 2.65 22\nSociety / Science / Nature 2426 1.85 4.17 30\nMajor Breaking News 5042 3.85 9.82 65\nBankruptcy / Insolvency 671 0.51 1.21 11\nBroker Research and Recommendation 1867 1.42 2.26 17\nCorporate Results / Results Forecasts / Warnings 17525 13.37 20.13 150\nMergers / Acquisitions / Takeovers 13598 10.37 11.47 60\nAA.N 2770 2.11 4.16 49\nAXP.N 2341 1.79 3.55 40\nBA.N 4163 3.18 3.65 21\nBAC.N 11974 9.13 8.23 77\nCAT.N 2442 1.86 3.65 50\nCSCO.O 3085 2.35 3.83 35\nCVX.N 5687 4.34 3.67 29\nDD.N 980 0.75 1.86 22\nDIS.N 5326 4.06 3.86 26\nGE.N 10236 7.81 5.98 42\nHD.N 1546 1.18 2.99 34\nHPQ.N 4593 3.50 4.31 31\nIBM.N 4790 3.65 4.64 40\nINTC.O 5219 3.98 5.26 40\nJNJ.N 2860 2.18 3.01 29\nJPM.N 11723 8.94 7.62 45\nKFT.N 2171 1.66 3.43 35\nKO.N 2080 1.59 2.54 18\nMCD.N 2120 1.62 3.02 38\nMMM.N 1043 0.80 2.32 28\nMRK.N 3223 2.46 3.40 41\nMSFT.O 10495 8.01 7.21 68\nPG.N 2096 1.60 2.86 42\nPFE.N 3803 2.90 3.61 52\nT.N 4559 3.47 3.95 33\nTRV.N 407 0.31 1.22 19\nUTX.N 2026 1.55 2.59 20\nVZ.N 3435 2.62 3.32 28\nWMT.N 6676 5.09 5.19 45\nXOM.N 8096 6.18 4.98 33\nTable 1: This table gives summary statistics (sum, mean, standard deviation and\nmaximum) for the number of news articles per day. The upper panel classi\fes news\non the S&P500 companies by keywords and the lower panel show the statistics for all\nmembers of the Dow Jones Industrial Average separately. A news story is considered\nas relevant for a company if the company's RIC is mentioned in the \feld `related\nRICs'.\n31Qi Yi\nconstant \u000020.8872a \u000023.0588a \u00002.0450a \u00002.0569a\nP2B \u00000.3051a \u00000.2101a \u00000.0251a \u00000.0225a\nln(CAP) 2.2701a 2.3374a 0.2481a 0.2501a\nRet - \u00001.9734b - \u00000.1547b\n\u001b(Ret) - 6.9538a - 0.2916b\nR2 38.73% 68.26% 54.58% 65.89%\n# Obs. 61 61 61 61\nTable 2:\nThe table shows the regression estimates for model (2). The subscript a, b and c indicate\nsigni\fcance at the 1%, 5% and 10% con\fdence level. Qi denotes the average number of\nnews per day of company i, and Yi is the average number of days with at least on news\nstory. A news story is relevant for a company if the company's RIC is mentioned in the\n\feld `related RICs'.\n32Optimism ^ \" T V O C #(P > 0) m(PjP > 0) \u001b(PjP > 0)\nAA.N 0.0017b 0.0182 \u00000.0002 0.0343b \u00000.0609 79.0 3.2504 2.4254\nABT.N 0.0007 0.0249b 0.0007 0.0046 \u00000.0322 64.0 2.9495 1.9430\nAIG.N \u00000.0002 0.0011 0.0001 \u00000.0107 \u00000.3763 173.0 3.6032 2.3316\nAMGN.O \u00000.0001 0.0125 0.0003 0.0773a \u00000.0121 60.0 2.4562 1.4674\nAPC.N 0.0002 0.0164 \u00000.0004 \u00000.0057 \u00000.2079 56.0 4.2889 2.6812\nAXP.N \u00000.0002 0.0011 0.0001 0.0285a \u00000.4751 64.0 3.8703 4.7347\nBA.N 0.0002 \u00000.0057 0.0004 0.0087 \u00000.1926 172.0 2.3197 1.7603\nBAC.N \u00000.0007 0.0104 0.0003 \u00000.0031 0.0503 229.0 2.6885 2.0884\nBAX.N 0.0004 0.0296 \u00000.0010 \u00000.0017 \u00000.0543 27.0 3.1350 2.3361\nBMY .N 0.0010b 0.0273c \u00000.0009 0.0177 \u00000.0914c 68.0 3.5177 2.1955\nBSX.N 0.0017 \u00000.0332 \u00000.0001 \u00000.0070 \u00000.1993 28.0 2.4508 1.9893\nC.N \u00000.0003 0.0132 \u00000.0038c 0.0059 \u00000.5522 302.0 3.1279 2.0583\nCAT.N 0.0031a 0.0587a \u00000.0008 0.0223 \u00000.6997b 52.0 3.0266 1.8953\nCOP.N 0.0004 \u00000.0024 \u00000.0002 0.0037 \u00000.1246 130.0 2.9440 1.5714\nCSC.N \u00000.0009 0.0238 \u00000.0010 0.0358 \u00000.0119 7.0 5.2307 3.1978\nCSCO.O 0.0004 \u00000.0006 0.0002 \u00000.0071 \u00000.1287c 120.0 3.6380 2.6329\nCV X.N \u00000.0006c 0.0031 \u00000.0002 \u00000.0244 0.0150 136.0 2.7571 2.0611\nDD.N 0.0001 \u00000.0094 \u00000.0007 \u00000.0253 \u00000.0287 42.0 3.4745 2.2462\nDELL.O 0.0006 0.0122 0.0006 0.0468b 0.0704 99.0 2.7777 2.1504\nDIS.N \u00000.0002 \u00000.0023 0.0009 0.0082 \u00000.0449 123.0 3.1999 2.1781\nDOW.N 0.0010 0.0029 0.0011 0.0137 \u00000.5548 28.0 4.2514 2.3947\nDV N.N 0.0005 0.0191 0.0008 \u00000.0012 \u00000.1463 32.0 3.4291 2.1876\nF.N 0.0008 0.0268b \u00000.0022 \u00000.0008 1.1325 198.0 3.0296 2.2643\nFDX.N 0.0040a 0.0758a \u00000.0001 0.1048a \u00000.5887b 40.0 2.7887 2.2183\nGE.N \u00000.0008 0.0266c 0.0033a 0.0092 0.7432 59.0 3.8617 2.7574\nGLW.N 0.0018b 0.0226c \u00000.0008 0.0625b \u00000.5179 19.0 5.2515 3.9589\nGR.N \u00000.0015a 0.1334a \u00000.0012 0.0872c \u00000.2023b 20.0 4.7729 5.7485\nGS.N 0.0003 \u00000.0049 \u00000.0003 \u00000.0003 \u00000.3873 235.0 2.9275 2.1619\nHON.N 0.0000 0.0282b 0.0014 0.0273 \u00000.0772 45.0 3.3022 2.1268\nHPQ.N 0.0010b 0.0023 0.0012c 0.0029 0.2170a 119.0 3.0873 2.2698\nIBM.N 0.0002 0.0023 \u00000.0004 0.0024 0.0542 138.0 3.6778 2.9647\nINTC.O 0.0001 \u00000.0014 0.0003 \u00000.0013 \u00000.0736 112.0 2.9896 2.2001\nJNJ.N 0.0000 0.0196b 0.0009c \u00000.0046 \u00000.0076 102.0 3.1333 2.6229\nJPM.N 0.0010c 0.0041 \u00000.0001 0.0002 0.1774 183.0 2.7744 2.2258\nKFT.N 0.0004 0.0223 \u00000.0002 \u00000.0271 0.0710 94.0 2.6039 1.6123\nKO.N 0.0009c 0.0328b \u00000.0001 0.0166 \u00000.0389 50.0 3.3672 2.3124\nMCD.N 0.0010b 0.0310b \u00000.0006 0.0001 \u00000.0065 70.0 3.4556 2.3593\nMDT.N 0.0003 \u00000.0236 0.0000 \u00000.0129 0.0866 38.0 3.5020 2.7540\nMO.N 0.0002 0.0080 \u00000.0022a 0.0058 \u00000.1456 27.0 2.9978 2.6200\nMON.N 0.0010c \u00000.0034 0.0006 0.0109 0.0525 65.0 3.9448 3.6470\nMMM.N 0.0022a 0.0532a \u00000.0010 0.0687 \u00000.0954 27.0 3.5051 2.1892\nMRK.N 0.0001 0.0282a \u00000.0003 \u00000.0122 0.0518 99.0 3.4270 2.6114\nMS.N 0.0008 0.0167 \u00000.0036c 0.0259c 0.7892 201.0 2.8148 2.1083\nMSFT.O 0.0005 0.0056 0.0003 0.0038 \u00000.0363 226.0 2.6009 1.6957\nLLY .N \u00000.0013b 0.0223 0.0005 \u00000.0185 0.0591 56.0 2.9219 1.7841\nLMT.N 0.0002 0.0122 \u00000.0008c 0.0413c 0.0365 105.0 3.5112 2.4568\nORCL.O 0.0006 0.0599a \u00000.0010 0.0971a \u00000.0725 70.0 2.8478 2.2948\nOXY .N \u00000.0002 0.0172 \u00000.0006 0.0324b 0.2416b 23.0 4.5061 3.3075\nPFE.N 0.0002 \u00000.0021 0.0036 \u00000.0088 0.0384 122.0 3.0920 2.3710\nPG.N 0.0001 0.0289c \u00000.0016 \u00000.0161 0.0033 52.0 2.5772 1.7435\nSLB.N \u00000.0018b 0.0143 \u00000.0018 0.0229 0.1169 34.0 3.2163 2.5419\nT.N 0.0003 0.0104 0.0003 0.0066 0.0119 102.0 3.0401 1.8108\nTRV .N 0.0105a 0.1612b \u00000.0140c 0.1683 0.1582 9.0 1.8596 0.9999\nTWX.N 0.0000 0.0091 0.0010 0.0239 \u00000.0975 75.0 3.0055 2.1811\nTXN.N 0.0006 0.0124 0.0003 0.0119 0.0478 21.0 5.6115 4.4692\nUTX.N 0.0021a 0.0053 \u00000.0018 \u00000.0035 \u00000.2170 8.0 5.0200 3.0939\nV Z.N 0.0004 \u00000.0003 0.0003 0.0057 0.1712 121.0 2.8905 2.0383\nWFC.N 0.0011 0.0387b \u00000.0066c 0.0086 \u00000.2366 84.0 3.4648 2.4066\nWMT.N 0.0003 0.0094 \u00000.0003 \u00000.0408 \u00000.0443 180.0 2.7180 2.0239\nWLP.N \u00000.0011 0.0387b \u00000.0003 0.0257 0.4564b 36.0 3.7594 2.3134\nXOM.N \u00000.0002 0.0006 \u00000.0001 \u00000.0216 \u00000.0209 160.0 3.2283 2.1963\nPos. & sig. 13 19 3 11 3\nNeg. & sig. 4 0 6 0 5\nTable 3: The table shows the estimated, company-individual co-movement of posi-\ntive sentiment Pi;t\u00001 and abnormal returns (^ \"), stock trading volume (T), the \frst\ndi\u000berence of the volatility spread (V ), cumulated option trading volume (O) and\nthe \frst di\u000berence of the CDS spread (C), according to regression model (6). The\nsubscript a, b and c indicate signi\fcance at the 1%, 5% and 10% con\fdence level.\nColumn 7, 8 and 9 show the number of days with positive sentiment, the conditional\nmean of positive sentiment and its standard deviation.\n33Pessimism ^ \" T V O C #(N > 0) m(NjN > 0) \u001b(NjN > 0)\nAA.N \u00000.0035a \u00000.0356c 0.0088a \u00000.0501c 2.9180a 48.0 2.7442 2.0826\nABT.N 0.0001 \u00000.0258 \u00000.0010 0.0271 \u00000.0018 20.0 2.3062 1.1289\nAIG.N \u00000.0008 \u00000.0327c \u00000.0048 0.0035 \u00000.0403 136.0 3.9970 3.0133\nAMGN.O \u00000.0007 0.1162a 0.0048a 0.1479a 0.3611c 27.0 1.9520 1.8325\nAPC.N \u00000.0030b 0.0184 0.0050a 0.0395b 3.0280a 25.0 3.7985 2.7814\nAXP.N \u00000.0001 0.0115 0.0022 0.0160 2.1222a 65.0 3.1979 2.4588\nBA.N \u00000.0001 0.0162 \u00000.0004 \u00000.0045 \u00000.1733 138.0 2.4733 2.0844\nBAC.N \u00000.0010 0.0154 \u00000.0026 0.0108 0.0427 107.0 2.4113 2.1768\nBAX.N \u00000.0240a 1.2724a 0.0108b 0.2341 0.0491 6.0 1.3261 1.0626\nBMY .N 0.0012 0.0125 \u00000.0025c \u00000.0037 0.0375 20.0 2.0438 1.6211\nBSX.N \u00000.0039 0.7366a \u00000.0073 0.0303 0.8928 17.0 1.3504 0.9238\nC.N \u00000.0015 \u00000.0022 0.0010 \u00000.0014 \u00001.4546 99.0 1.9581 1.6824\nCAT.N \u00000.0011 \u00000.0276 \u00000.0009 \u00000.0301 \u00000.6743 16.0 1.7822 1.3524\nCOP.N \u00000.0002 0.0016 0.0004 0.0067 \u00000.1752c 84.0 2.8550 2.0177\nCSC.N 0.0091a 0.1801b \u00000.0080c \u00000.0537 \u00000.7654 2.0 2.8250 2.5809\nCSCO.O \u00000.0010 0.0783a \u00000.0034 0.0588c \u00000.1064 19.0 2.0095 1.6628\nCV X.N \u00000.0000 0.0117 \u00000.0002 \u00000.0156 0.0176 94.0 3.0215 1.9161\nDD.N \u00000.0008 0.0323c \u00000.0015 \u00000.0388 1.8261a 23.0 2.8427 1.9362\nDELL.O \u00000.0045a 0.0706a 0.0008 0.0472 0.5376c 42.0 1.8228 1.7795\nDIS.N \u00000.0024b \u00000.0076 0.0040 0.0174 0.1655 31.0 1.9420 1.3118\nDOW.N 0.0000 \u00000.0144 \u00000.0001 \u00000.0133 \u00000.0967 31.0 3.9294 2.7804\nDV N.N \u00000.0088a 0.0188 0.0220a 0.0137 \u00000.6530 8.0 1.8642 1.5859\nF.N \u00000.0020 \u00000.0082 0.0017 \u00000.0090 11.9571 88.0 2.4773 2.0080\nFDX.N 0.0005 \u00000.0053 \u00000.0001 \u00000.0261 0.1863 30.0 3.2106 3.6588\nGE.N 0.0007 0.0529b 0.0000 0.0107 0.9876 37.0 3.0648 1.9041\nGLW.N \u00000.0073a 0.2269a 0.0026 0.0348 0.6894 15.0 2.6617 3.9609\nGR.N 0.0159c \u00000.0328 \u00000.0025 1.4221c 0.0063 2.0 1.4533 0.5704\nGS.N \u00000.0019b 0.0480b 0.0034b 0.0338c 0.1399 116.0 2.3064 1.9284\nHON.N 0.0005 0.0029 \u00000.0033 \u00000.0353 \u00000.4228 9.0 2.5706 1.5175\nHPQ.N \u00000.0022a 0.0652a 0.0012 0.0419c 0.3178b 52.0 2.1668 1.8391\nIBM.N \u00000.0003 0.0054 0.0014c 0.0092 0.1860 37.0 2.7328 2.5395\nINTC.O \u00000.0012 \u00000.0199 0.0021 \u00000.0652 \u00000.7197a 48.0 2.2750 1.5690\nJNJ.N \u00000.0001 0.0025 0.0000 \u00000.0145 0.0012 46.0 2.8465 2.7153\nJPM.N 0.0003 0.0017 0.0032b 0.0040 0.1679 106.0 2.9178 2.6319\nKFT.N \u00000.0019c 0.0219 \u00000.0005 0.1888 \u00000.1957 14.0 2.4609 1.9463\nKO.N 0.0001 0.0169 0.0005 \u00000.0139 0.0210 20.0 5.2119 4.8020\nMCD.N 0.0006 0.0104 0.0005 0.0140 0.0247 31.0 2.0148 1.5340\nMDT.N \u00000.0019 0.1235a 0.0051b 0.0897 0.4595 21.0 1.6765 1.1224\nMO.N 0.0026 \u00000.0224 \u00000.0024 \u00000.0074 \u00000.0677 6.0 1.0153 1.2205\nMON.N \u00000.0003 \u00000.0204 0.0006 \u00000.0103 \u00000.0741 30.0 2.3710 2.2904\nMMM.N 0.0019 \u00000.0005 0.0050 \u00000.0135 \u00000.3744 5.0 2.2973 0.9381\nMRK.N \u00000.0023a 0.0267 0.0003 0.0119 0.1090 48.0 2.3156 1.9083\nMS.N \u00000.0028c 0.0269 0.0043 \u00000.0106 9.7139 75.0 2.4378 1.9433\nMSFT.O \u00000.0004 0.0088 \u00000.0009 \u00000.0615c 0.0550 72.0 1.8498 1.4563\nLLY .N \u00000.0003 0.0022 0.0002 \u00000.0225 \u00000.0258 25.0 3.2295 2.9857\nLMT.N 0.0013 \u00000.0065 0.0005 \u00000.0117 \u00000.0058 44.0 2.1761 1.8434\nORCL.O \u00000.0001 0.0128 0.0001 0.0119 0.0585 51.0 2.9248 2.5158\nOXY .N \u00000.0019 \u00000.0035 \u00000.0135 0.2157 \u00000.7786 2.0 1.3525 0.6824\nPFE.N \u00000.0007 0.0185 0.0015 \u00000.0478 0.0943 62.0 2.3397 1.5395\nPG.N 0.0002 \u00000.0186 \u00000.0023c \u00000.1008c 0.0270 48.0 2.2890 1.4934\nSLB.N \u00000.0026 \u00000.0102 0.0084a 0.0284 0.5508b 12.0 2.5346 1.9957\nT.N \u00000.0002 0.0007 \u00000.0010 \u00000.0013 \u00000.1212 36.0 2.0412 1.8221\nTRV .N 0.0075a 0.0632 \u00000.0072 0.0580 0.3904 8.0 3.3083 2.2270\nTWX.N \u00000.0010 0.0136 \u00000.0006 \u00000.0571 0.0371 21.0 1.8660 2.7775\nTXN.N 0.0028 0.0238 0.0011 \u00000.0238 4.5247a 5.0 2.4740 1.1307\nUTX.N \u00000.0019 0.0468 \u00000.0006 \u00000.0560 0.1638 7.0 3.9548 0.7271\nV Z.N \u00000.0021 0.0348 0.0045 0.0133 \u00000.3960 26.0 1.5650 0.8817\nWFC.N \u00000.0033b 0.0730b 0.0019 0.0207 0.1401 54.0 2.3091 1.6347\nWMT.N \u00000.0012b 0.0121 0.0016b \u00000.0899 0.0103 62.0 2.6808 2.1770\nWLP.N \u00000.0001 0.0251 \u00000.0008 \u00000.0011 \u00001.2920 9.0 1.3725 1.2788\nXOM.N \u00000.0002 \u00000.0002 \u00000.0003 0.0032 0.0288 89.0 2.6779 1.9955\nPos. & sig. 3 12 11 6 9\nNeg. & sig. 14 2 3 2 2\nTable 4: The table shows the estimated, company-individual co-movement of nega-\ntive sentiment Pi;t\u00001 and abnormal returns (^ \"), stock trading volume (T), the \frst\ndi\u000berence of the volatility spread (V ), cumulated option trading volume (O) and\nthe \frst di\u000berence of the CDS spread (C), according to regression model (6). The\nsubscript a, b and c indicate signi\fcance at the 1%, 5% and 10% con\fdence level.\nColumn 7, 8 and 9 show the number of days with negative sentiment, the conditional\nmean of negative sentiment and its standard deviation.\n34Disagreement ^ \" T V O C #(D > 0) m(DjD > 0) \u001b(DjD > 0)\nAA.N \u00000.0001 0.1077a \u00000.0028 0.0469c \u00000.5025 54.0 2.4188 2.2592\nABT.N \u00000.0022b 0.0682b \u00000.0005 0.0195 0.1882 31.0 1.6257 1.6121\nAIG.N 0.0017 0.0653a 0.0029 0.0437b \u00002.3837 209.0 3.2710 2.1547\nAMGN.O 0.0085a 0.2571a \u00000.0051a 0.1350a \u00000.6805a 33.0 1.7816 1.6824\nAPC.N 0.0027 0.1083a \u00000.0009 0.0215 \u00000.3629 32.0 1.8993 1.7249\nAXP.N \u00000.0004 0.0749a \u00000.0019 0.0277c \u00001.0286 54.0 2.7427 2.8239\nBA.N \u00000.0018a 0.0514a 0.0013b 0.0091 0.2428 202.0 2.0917 1.7702\nBAC.N 0.0000 0.0628a 0.0028 0.0367a \u00000.2434 216.0 2.2327 1.9952\nBAX.N \u00000.0024c 0.3327a \u00000.0022 0.2572a \u00000.0154 13.0 2.8396 2.5411\nBMY .N 0.0002 0.0427 \u00000.0006 0.0091 0.0492 43.0 1.7008 1.7800\nBSX.N \u00000.0124a 0.3925a 0.0169a 0.0894 3.3971a 19.0 1.9882 2.4059\nC.N 0.0003 0.0515a 0.0021 0.0234b 0.5848 276.0 2.2967 1.6895\nCAT.N 0.0002 0.1059a \u00000.0024 0.0315 \u00000.4019 27.0 2.1223 2.3512\nCOP.N \u00000.0004 0.0207c 0.0016 0.0008 0.2143c 103.0 2.3616 1.5052\nCSC.N 0.0128b 0.3420b 0.0217a \u00000.4354 7.2457a 2.0 1.8420 0.9650\nCSCO.O \u00000.0013b 0.0349b 0.0001 0.0169 \u00000.0436 73.0 1.7396 2.1665\nCV X.N 0.0008c 0.0021 0.0002 0.0041 0.1201 133.0 2.1836 1.8328\nDD.N 0.0007 0.1190a 0.0026 \u00000.0062 \u00000.4010 21.0 1.1783 1.1597\nDELL.O \u00000.0021 0.0898a 0.0013 0.1456a 0.2407 78.0 1.3372 1.3688\nDIS.N 0.0017b 0.0797a \u00000.0068 0.0144 \u00000.3150c 86.0 1.3702 1.4605\nDOW.N 0.0002 0.2729a 0.0011 0.1078a 0.7003 29.0 3.1736 2.5388\nDV N.N 0.0061a 0.0848a \u00000.0015 0.1263a \u00000.0593 20.0 2.4095 2.5574\nF.N \u00000.0001 0.0440b 0.0028 0.0411a \u000010.0795 177.0 1.6024 1.5340\nFDX.N \u00000.0029b 0.2532a \u00000.0009 0.0986a 0.9380a 31.0 2.4819 2.4811\nGE.N 0.0021c 0.0072 \u00000.0058b 0.0584 \u00001.1426 42.0 2.3322 1.9647\nGLW.N \u00000.0017 0.0610a \u00000.0006 \u00000.0232 0.3987 17.0 3.2953 3.3540\nGR.N 0.0096a 0.3587a \u00000.0019 0.4344c 0.3461 8.0 1.8870 1.7545\nGS.N 0.0006 0.0229 0.0008 0.0098 1.0095c 207.0 2.1838 1.7548\nHON.N 0.0001 0.0832b \u00000.0046c \u00000.0144 0.2443 27.0 1.2727 1.2025\nHPQ.N \u00000.0002 0.0813a \u00000.0025b 0.0591a 0.1156 88.0 2.1268 1.8925\nIBM.N 0.0002 0.0592a \u00000.0009 0.0982a \u00000.0697 111.0 1.6985 2.2262\nINTC.O 0.0007 0.0745a \u00000.0015 0.0864b 0.2383c 94.0 1.8811 1.9041\nJNJ.N \u00000.0006 0.0272c \u00000.0003 0.0177 0.0037 72.0 2.1668 1.9909\nJPM.N \u00000.0010c 0.0080 \u00000.0025 \u00000.0056 \u00000.2332 162.0 2.4532 2.5032\nKFT.N \u00000.0013 0.0557 0.0003 \u00000.5110 \u00000.0836 82.0 1.3556 1.1954\nKO.N 0.0007 0.0599a \u00000.0022b 0.0502 0.1193 23.0 3.1491 2.7196\nMCD.N 0.0006 0.0846a 0.0008 0.0246 0.0502 53.0 1.8974 1.3850\nMDT.N \u00000.0045a 0.2090a 0.0001 0.0732 \u00000.2749 32.0 1.7002 1.7557\nMO.N \u00000.0033c 0.1571a 0.0023 0.0858 0.8789 13.0 1.7484 0.9833\nMON.N \u00000.0003 0.0819a \u00000.0006 0.0666a 0.1395 35.0 2.3683 3.0473\nMMM.N \u00000.0020 0.1856a \u00000.0028 0.3179c 0.6947c 12.0 1.4302 1.3572\nMRK.N 0.0009 0.0479b 0.0019c 0.0110 0.0390 69.0 2.2193 1.8914\nMS.N \u00000.0009 0.0284 0.0037 \u00000.0044 \u00001.4121 151.0 2.1024 1.8497\nMSFT.O \u00000.0004 0.0423a \u00000.0013 0.0135 \u00000.0051 212.0 1.8450 1.5528\nLLY .N \u00000.0008 0.0413b 0.0017 \u00000.0208 0.2304b 33.0 2.3060 1.8998\nLMT.N \u00000.0016 0.0861a \u00000.0010 0.0193 0.0244 55.0 1.6729 1.3771\nORCL.O 0.0009 0.0151 0.0012 0.0388 \u00000.0426 65.0 2.2632 2.4857\nOXY .N 0.0022 0.0646 0.0012 0.1249b \u00000.2937 7.0 2.1770 1.7105\nPFE.N \u00000.0009 0.0358a 0.0038 0.0817 0.3807a 87.0 1.9404 1.7370\nPG.N \u00000.0011 0.0588b 0.0007 0.1997a \u00000.1966 49.0 1.8380 1.7458\nSLB.N 0.0064a 0.0920b \u00000.0071b 0.0708c \u00000.8166a 22.0 1.7485 1.7624\nT.N \u00000.0000 0.0526a \u00000.0019 0.0084 0.5071b 77.0 1.2798 1.3578\nTRV .N \u00000.0081a 0.0590 0.0027 \u00000.0531 \u00000.7709 8.0 2.2566 2.0838\nTWX.N \u00000.0016 0.0070 \u00000.0008 \u00000.0466 \u00000.1464 59.0 1.1914 1.5567\nTXN.N \u00000.0026 0.2319a \u00000.0001 0.2729a \u00000.1736 15.0 2.6765 1.2774\nUTX.N 0.0052 \u00000.0015 \u00000.0026 0.1113 \u00000.5210 3.0 1.4775 1.5095\nV Z.N 0.0019b 0.0550a 0.0006 0.0204 \u00000.0501 76.0 1.4279 1.4429\nWFC.N 0.0019 0.1155a 0.0016 0.0590c 0.0182 62.0 1.6387 1.5371\nWMT.N 0.0007 0.0763a \u00000.0014c 0.3018a 0.0805 137.0 1.7030 1.5085\nWLP.N 0.0023 0.1351a \u00000.0017 \u00000.0064 1.1201c 18.0 1.7603 1.6106\nXOM.N \u00000.0007 0.0050 0.0000 0.0645 0.0096 149.0 2.0730 1.6049\nPos. & sig. 9 49 4 23 11\nNeg. & sig. 10 0 7 0 3\nTable 5: The table shows the estimated, company-individual co-movement of dis-\nagreement, measured by Dstd, and abnormal returns (^ \"), stock trading volume (T),\nthe \frst di\u000berence of the volatility spread (V ), cumulated option trading volume\n(O) and the \frst di\u000berence of the CDS spread (C), according to regression model\n(6). The subscript a, b and c indicate signi\fcance at the 1%, 5% and 10% con\f-\ndence level. Column 7, 8 and 9 show the number of days with disagreement, the\nconditional mean of disagreement and its standard deviation.\n35Pooled Analysis s^ \" sT sV sO sC\ns^ \"\u00001 \u00000.0010 0.0088b \u00000.0034 0.0063 0.0151a\nsT\u00001 0.0221a 0.5380a \u00000.0528a 0.0854a \u00000.0049\nsV\u00001 0.0349a 0.0734a \u00000.1591a 0.0184a 0.1232a\nsO\u00001 \u00000.0190a 0.0262a 0.0037 0.2701a \u00000.0163a\nsC\u00001 \u00000.0082 0.0274a 0.0035 \u00000.0018 0.0876a\nsP 0.0273a 0.0455a \u00000.0111a 0.0237a \u00000.0069\nsN \u00000.0321a 0.0374a 0.0128a 0.0088c 0.0194a\nsDstd 0.0008 0.1187a \u00000.0058 0.0599a 0.0142a\nMonday 0.0493a \u00000.2202a 0.1149a \u00000.0089 \u00000.1259a\nTuesday 0.0349b 0.0616a \u00000.0621a 0.0489a \u00000.1059a\nWednesday \u00000.0085 \u00000.0541a 0.0538a \u00000.0091 \u00000.0880a\nThursday 0.0203 0.0095 0.1196a 0.0184 \u00000.0408b\nConstant \u00000.0199 0.0387a \u00000.0445a \u00000.0097 0.0713a\nR2 0.0041 0.3499 0.0340 0.1067 0.0297\ns^ \"\u00001 \u00000.0017 0.0103b \u00000.0033 0.0063 0.0159a\nsT\u00001 0.0237a 0.5345a \u00000.0522a 0.0849a \u00000.0043\nsV\u00001 0.0353a 0.0731a \u00000.1605a 0.0175a 0.1220a\nsO\u00001 \u00000.0169a 0.0228a 0.0044 0.2683a \u00000.0159a\nsC\u00001 \u00000.0077 0.0250a 0.0046 \u00000.0034 0.0870a\nsP 0.0429a 0.0124a \u00000.007 0.0016 \u00000.0125b\nsN \u00000.0146b \u00000.0137a 0.0176a \u00000.0230a 0.0115b\nsDpol \u00000.0388a 0.1650a \u00000.0131b 0.0969a 0.0237a\nMonday 0.0472a \u00000.2201a 0.1138a \u00000.0120 \u00000.1259a\nTuesday 0.0359b 0.0568a \u00000.0616a 0.0454a \u00000.1043a\nWednesday \u00000.0058 \u00000.0561a 0.0528a \u00000.0119 \u00000.0893a\nThursday 0.0206 0.0091 0.1199a 0.0170 \u00000.0416b\nConstant \u00000.0194 0.0402a \u00000.0443a \u00000.0076 0.0716a\nR2 0.0051 0.3552 0.0341 0.1097 0.0299\n#Obs. 36229 36229 36229 36229 36229\nTable 6: The table shows the regression estimates for \u0003, \f and K in the pooled\nregression model with contemporaneous relationships between the market variables\nand sentiment and disagreement, respectively, i.e. (7). The upper panel measures\ndisagreement with Dstd and the lower panel with sDpol. a denotes signi\fcance at\nthe 1% con\fdence level, b at the 5% con\fdence level and c at the 10% level.\n36Pooled Analysis s^ \" sT sV sO sC\ns^ \"\u00001 \u00000.0005 0.0079c \u00000.0028 0.0054 0.0155a\nsT\u00001 0.0239a 0.5475a \u00000.0548a 0.0910a \u00000.0044\nsV\u00001 0.0347a 0.0751a \u00000.1605a 0.0187a 0.1224a\nsO\u00001 \u00000.0182a 0.0350a 0.0033 0.2749a \u00000.0147a\nsC\u00001 \u00000.0081 0.0272a 0.0041 \u00000.0022 0.0871a\nsP\u00001 0.0105a 0.0093b \u00000.0058 0.0043 \u00000.0011\nsN\u00001 \u00000.0042 \u00000.0027 0.0149a 0.0070 0.0074\nsDstd\n\u00001 \u00000.0163a \u00000.0248a 0.0097c \u00000.0104a 0.0071\nMonday 0.0501a \u00000.2338a 0.1152a \u00000.0190 \u00000.1275a\nTuesday 0.0342b 0.0687a \u00000.0615a 0.0524a \u00000.1021a\nWednesday \u00000.0064 \u00000.0366a 0.0513a \u00000.0013 \u00000.0874a\nThursday 0.0200 0.0248c 0.1189a 0.0252 \u00000.0401b\nConstant \u00000.0196 0.0331a \u00000.0440a \u00000.0116 0.0707a\nR2 0.0025 0.3291 0.0340 0.1018 0.0291\ns^ \"\u00001 \u00000.0010 0.0069 \u00000.0026 0.0052 0.0154a\nsT\u00001 0.0242a 0.5487a \u00000.0544a 0.0906a \u00000.0033\nsV\u00001 0.0345a 0.0746a \u00000.1605a 0.0187a 0.1222a\nsO\u00001 \u00000.0179a 0.0356a 0.0033 0.2749a \u00000.0144a\nsC\u00001 \u00000.0080 0.0275a 0.0041 \u00000.0022 0.0872a\nsP\u00001 0.0131b 0.0151a \u00000.0055 0.0042 0.0015\nsN\u00001 0.0007 0.0067 0.0142b 0.0082 0.0096\nsD\npol\n\u00001 \u00000.0179a \u00000.0319a 0.0058 \u00000.0069 \u00000.0020\nMonday 0.0500 \u00000.2342a 0.1150a \u00000.0188 \u00000.1279a\nTuesday 0.0337b 0.0676a \u00000.0616a 0.0524 \u00000.1026a\nWednesday \u00000.0064 \u00000.0367a 0.0513a \u00000.0012 \u00000.0874a\nThursday 0.0201 0.0251b 0.1189a 0.0252 \u00000.0399a\nConstant \u00000.0193 0.0334a \u00000.0440a \u00000.0116 0.0708a\nR2 0.0025 0.3292 0.0340 0.1017 0.0291\n#Obs. 36229 36229 36229 36229 36229\nTable 7: The table shows the regression estimates for \u0003, \f and K in the pooled\nregression model without contemporaneous relationships, i.e. (9). The upper panel\nmeasures disagreement with Dstd and the lower panel with sDpol. a denotes signif-\nicance at the 1% con\fdence level, b at the 5% con\fdence level and c at the 10%\nlevel.\n37Risk-free rate Market rate\nTrading Strategy Xi;t X\n+\ni;t X\n\u0000\ni;t Xi;t X\n+\ni;t X\n\u0000\ni;t\nNo transaction costs\nMT 33.2485 17.7226 15.5405 29.0575 15.4238 13.6477\nmaxt2[0;T]fMtg 33.3755 17.7738 16.3134 29.1761 15.6800 13.7278\nmint2[0;T]fMtg -0.0406 -0.1168 -0.0917 -0.0549 0.0000 -0.2398\nq0:05(Mt) 0.8345 0.4743 0.2734 1.1774 1.0912 0.0000 P\nt 1(Mt < 0) 1 1 12 1 0 43\n10 bps\nMT 24.0594 11.6034 12.4646 19.8684 9.3046 10.5718\nmaxt2[0;T]fMtg 24.2044 11.6657 14.7080 20.2108 10.5371 10.6630\nmint2[0;T]fMtg -0.1286 -2.2423 -0.1940 -0.1429 -0.0399 -0.3421\nq0:05(Mt) 0.2535 -0.5732 0.0557 0.3913 0.4594 -0.1355 P\nt 1(Mt < 0) 3 155 22 4 1 134\n20 bps\nMT 14.8703 5.4843 9.3888 10.6793 3.1855 7.4959\nmaxt2[0;T]fMtg 15.3437 5.0716 13.1026 14.0891 6.6094 8.6155\nmint2[0;T]fMtg -1.5261 -4.8499 -0.3238 -1.6210 -0.9771 -0.6888\nq0:05(Mt) -0.9640 -3.0753 -0.1363 -0.8408 -0.3894 -0.4805 P\nt 1(Mt < 0) 197 413 82 156 116 212\n30 bps\nMT 5.6812 -0.6349 6.3129 1.4902 -2.9337 4.4201\nmaxt2[0;T]fMtg 8.954 1.1072 11.4972 9.1693 3.2732 7.0032\nmint2[0;T]fMtg -3.4651 -7.6135 -0.6654 -3.5721 -3.1743 -1.2930\nq0:05(Mt) -2.6450 -5.7681 -0.3723 -2.4114 -2.7774 -0.9527 P\nt 1(Mt < 0) 321 687 176 311 510 285\n\u001b(\u0001Mt) 0.1667 0.1819 0.1224 0.1774 0.2018 0.1340\nNumber of trades 9104 6062 3042 9104 6062 3042\nAverage duration 1.39 days 1.46 days 1.26 days 1.39 days 1.46 days 1.26 days\nTable 8: The table shows the terminal value, the maximum and the minimum value,\nthe 5% quantile and the number of days with a negative value of the money account\nfor trading strategies on company signals and di\u000berent levels of transaction costs.\nXi;t incorporates of buy- and sell-signals, X\n+\ni;t only buy-signals and X\n\u0000\ni;t only sell-\nsignals. The benchmark in the left half is the risk-free rate. The benchmark in the\nright half is the stock market. The lower panel shows the volatility of the change in\nthe value of the money account, the number of trades and the average duration per\ntrade.\n38RIC Company Name\nAA.N Alcoa Incorporated\nABT.N Abbott Laboratories\nAIG.N American International Group Inc\nAMGN.O Amgen Inc\nAPC.N Anadarko Petroleum Corp\nAXP.N American Express Co\nBA.N The Boeing Company\nBAC.N Bank of America Corp\nBAX.N Baxter International Inc\nBMY .N Bristol Myers Squibb Co\nBSX.N Boston Scienti\fc Corp\nC.N Citigroup Inc\nCAT.N Caterpillar Inc\nCOP.N ConocoPhillips\nCSC.N Computer Sciences Corp\nCSCO.O Cisco Systems Inc\nCV X.N Chevron Corp\nDD.N E I Du Pont De Nemours And Company\nDELL.O Dell Inc\nDIS.N Walt Disney Co\nDOW.N The Dow Chemical Co\nDV N.N Devon Energy Corp\nF.N Ford Motor Co\nFDX.N Fedex Corp\nGE.N General Electric Co\nGLW.N Corning Inc\nGR.N Goodrich Corp\nGS.N The Goldman Sachs Group Inc\nHD.N The Home Depot Inc\nHON.N Honeywell International Inc\nHPQ.N Hewlett Packard Co\nIBM.N International Business Machines Corp\nINTC.O Intel Corp\nJNJ.N Johnson & Johnson\nJPM.N Jpmorgan Chase & Co\nKFT.N Kraft Foods Inc\nKO.N The Coca Cola Co\nMCD.N McDonald's Corp\nMDT.N Medtronic Inc\nMO.N Altria Group Inc\nMON.N Monsanto Co\nMMM.N 3m Co\nMRK.N Merck and Co Inc\nMS.N Morgan Stanley\nMSFT.O Microsoft Corp\nLLY .N Eli Lilly And Co\nLMT.N Lockheed Martin Corp\nORCL.O Oracle Corp\nOXY .N Occidental Petroleum Corp\nPFE.N P\fzer Inc\nPG.N Procter & Gamble Co\nSLB.N Schlumberger NV\nT.N AT&T Inc\nTRV .N Travelers Companies Inc\nTWX.N Time Warner Inc\nTXN.N Texas Instruments Inc\nUTX.N United Technologies Corp\nV Z.N Verizon Communications Inc\nWFC.N Wells Fargo and Co\nWMT.N Wal Mart Stores Inc\nWLP.N WellPoint Inc\nXOM.N Exxon Mobil Corp\nTable 9: The table gives the list of companies that are included in the analyses and\nmatches the company name with the company's RIC (= Reuters instrument code)\n39",
      "id": 2459847,
      "identifiers": [
        {
          "identifier": "6234553",
          "type": "CORE_ID"
        }
      ],
      "title": "Mechanically Extracted Company Signals and their Impact on Stock and Credit Markets",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [],
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://www.wiwi.uni-konstanz.de/workingpaperseries/WP_18-11-Graf.pdf"
      ],
      "updatedDate": "2014-10-24T12:19:59",
      "yearPublished": null,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/pdf/6234553.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/6234553"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/6234553/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/6234553/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/2459847"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/11309727",
        "https://api.core.ac.uk/v3/outputs/143909190",
        "https://api.core.ac.uk/v3/outputs/147599699"
      ],
      "createdDate": "2013-07-10T11:53:34",
      "dataProviders": [
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The advent of the real-time web is proving both challeng-\n\ning and at the same time disruptive for a number of areas of research,\n\nnotably information retrieval and web data mining. As an area of research reaching maturity, sentiment analysis oers a promising direction for modelling the text content available in real-time streams. This paper reviews the real-time web as a new area of focus for sentiment analysis\n\nand discusses the motivations and challenges behind such a direction",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/11309727.pdf",
      "fieldOfStudy": null,
      "fullText": "Crowdsourced Real-world Sensing:\nSentiment Analysis and the Real-Time Web\nAdam Bermingham and Alan F. Smeaton\nCLARITY: Centre for Sensor Web Technologies,\nSchool of Computing,\nDublin City University\n{abermingham, asmeaton}@computing.dcu.ie\nAbstract. The advent of the real-time web is proving both challeng-\ning and at the same time disruptive for a number of areas of research,\nnotably information retrieval and web data mining. As an area of re-\nsearch reaching maturity, sentiment analysis offers a promising direction\nfor modelling the text content available in real-time streams. This paper\nreviews the real-time web as a new area of focus for sentiment analysis\nand discusses the motivations and challenges behind such a direction.\nKeywords: sentiment analysis, real-time web, microblog\n1 Introduction\nIn the last 10 years, user-generated content has come to dominate a large por-\ntion of the web. Reviews, blogs, social networks, discussion forums and wikis are\nall familiar concepts to the average Internet user. User-generated content has\nnow earned respect as a credible source for exploring both factual and subjec-\ntive information. This has inspired research in the area of automatic sentiment\nanalysis: methods for automatic detection of negative and positive emotions,\nopinions and other evaluations in text.\nThe real-time web refers to the portion of the web where information is\navailable shortly after it is created and where it is connected in some way with\nevents that are happening in the real world either at, or close to that time. In\nterms of user-generated content, the information takes the form of blog posts,\nmicroblog posts, news feeds and social network content amongst others. This\ncontent is often reactionary in nature, disseminating news of real-world events\nin real-time and expressing associated opinion and commentary. Just as events\nin the real-world can happen at specific times and are scheduled, or can be\nunpredicted and occur spontaneously, so too does user-generated content have\na prominent time component. Examples of scheduled real-world events would\nbe sporting contests and TV programs and spontanous real-word events would\ninclude riots and civil disturbances.\nThe microblogging service, Twitter, is a good example of information making\nup the real-time web. Twitter allows users to publish short text messages and\nthese messages then appear in their followers’ feeds and may appear in searches.\nTwitter users write about a wide variety of topics including both scheduled\nand spontaneous real-world and real-time events. The diversity of content, the\nlarge volume and the availability of data mean that Twitter provides us with a\nunique opportunity to mine sentiment in real-time in a way not possible before.\nThroughout this paper we use Twitter as a case study for sentiment in the\nreal-time web.\nThe task of applying sentiment analysis to the real-time web however has a\nnumber of challenges. Due to the dynamic nature of the real-time web, topics\nof interest are constantly evolving. There are also infrastructural challenges as\nstatic indexes and latency in computation are no longer acceptable. This also\nposes difficulties for experimental methodology as methods for evaluating tech-\nniques for real-time knowledge discovery are not well established.\nThis paper offers a review of sentiment analysis in the real-time web as a\nfuture direction for research. The rest of this paper is laid out as follows: in\nSection 2, background to sentiment analysis is presented followed by a motivation\nfor pursuing this line of research in Section 3. Challenges are discussed in Sections\n4 and we conclude in Section 5.\nPUBLIC\nPRIVATE\nARCHIVAL REAL-TIME\nBooks\nInstant \nMessaging\nBreaking News \nBlogs\nEditorial \nNews Microblogs\nEmail\nSocial \nNetworks\nResearch \nPapers\nREAL-TIME WEB\nDiscussion \nForums\nSMS\nReviews\nCompany \nDocuments\nPersonal \nArchives\nFig. 1. A conceptualization of the the real-time web in terms of digital content.\n2 Background\nMuch sentiment analysis research concerns the study of static data. A number of\npublicly available collections have been widely used in research such as Pang and\nLee’s movie review corpus [6], Wilson at al.’s MPQA corpus [10], Blogs06 from\nthe TREC Blog Track [4] and the data provided as part of the NTCIR opinion\nfinding task [8] to name a few of the more common examples. These corpora\nmay all be thought of as static corpora: all of the topics are predefined and the\nlabels are independent of any temporal aspect. This has many advantages, the\nreplicability of experiments being one. However, in the real-time web, documents,\ntopics and sentiment all have an inherent temporal component.\nRecently there have been research works based on Twitter. Bollen et al. [1]\nmodelled trends in mood on Twitter in the 6 dimensions defined by the psy-\nchometric test, the Profile of Mood States (POMS): tension, depression, fatigue,\nvigour, anger and confusion. They developed a term-based emotional rating sys-\ntem by extending the 65 adjectives defined in POMS to a lexicon of 793 terms.\nThey then use the variance of these terms in Twitter posts over a series of 153\ndays to model the trends in mood along the 6 dimensions. They found that\nsentiment on Twitter correlated with real-world values such as stock prices and\ncoincides with events. They conclude:\n“events in the social, political, cultural and economical sphere do\nhave a significant, immediate and highly specific effect on the various\ndimensions of public mood.”\nSimilarly, Diakapolous and Shamma analysed sentiment in Twitter posts\nto characterise the reaction to various issues in a US Presidential election de-\nbate in 2008 [2]. Rather than use an automated sentiment analysis approach,\nthey crowdsourced the annotations using Amazon Mechanical Turk (AMT). The\nAMT annotators were asked to annotate documents which discuss the debate as\npositive, negative, mixed or other towards each of the presidentatial candidates.\nDiakopolous notes significant shift in sentiment as the debate moves between\nspeakers and topics. The also suggest that by correlating the positive and neg-\native sentiment they can identify controversial issues, though they note that\nthis requires further investigation. They conclude by offering two caveats sur-\nrounding modelling sentiment in real-time streams: (i) the relationship between\nsentiment and a real-world occurence is inferred based on document timestamp\nand relevant terms and is not necessarily accurate in all cases and (ii) the au-\nthors of the documents in the stream are not necessarily representative of the\nwider population.\nThese works support the assumption that the sentiment in the document\nstream is indicative of people’s reaction to real-world events. This is encouraging\nas it demonstrates that real-time social content, such as Twitter, is a valid data\nsource for gauging public sentiment.\nAnother promising avenue of research is in market research. One such work\nis Jansen et al. who studied the Word-Of-Mouth effect on Twitter, focusing on\nhow and why positive and negative sentiment towards brands spreads on Twitter\n[3]. They use supervised learning to classify twitter posts which mention brands\nfor sentiment towards that brand using n-gram features. Interestingly, they find\nautomated sentiment analysis accuracy comparable to manual classification. Of\nthe documents they analyse, 19% contain a reference to a product, company or\nservice and of these, 20% contained sentiment towards that product, company\nor service. They also observe large temporal swings in sentiment and suggest\nthat marketing companies are required to continually monitor their streams of\ndocuments over time. Again the results demonstrate that Twitter data provides\na means to sense the collective sentiment towards topics of interest.\nThese exploratory works hint at the potential of researching automatic tech-\nniques to model the sentiment in the real-time web. There remains much work\nto be done to explore fully the possibilities presented by the real-time web.\n3 Motivation and Applications\nIn applying sentiment analysis to the real-time web, and in specific user-generated\ncontent, we are in essence crowd-sourcing our sensing of the real world in real-\ntime. The online conversation becomes a sea of data from which we can infer\nsentiment and extrapolate information about the real-world around us. This is\nnot something that has been possible until now in any meaningful way and so\nwe are presented with a unique avenue for research.\nFor some time there have been methods of near-instantaneous computer-\nmediated communication. Instant messaging (IM) and text messaging on mobile\nphones (SMS) are two such examples. Each of these types of communication\nhowever are intrinsically private and obtaining and publishing datasets based on\nthe private correspondence of users is problematic at best. The public nature of\nthe Internet means that no such privacy restriction exists in terms of mining the\ninformation in online content, real-time or otherwise. The standardised way in\nwhich this content is made available not only encourages developers and users to\nbetter use the content, but also us as researchers to efficiently construct datasets\nand data streams to be used for study.\nThe recent growth in the volume in the real-time web, specifically on Twitter,\nis staggering. At least one website has recently measured the rate of Twitter posts\nbeing published is 2 billion per month, or 64 million per day, and increasing1.\nThis is undoubtedly a large volume of information to analyse, even given the\nshort length of twitter posts. But what portion of this deluge are relevant to\na given topic interest? In the recent Soccer World Cup in South Africa, even\nthe early matches saw activity in the region of hundreds of thousands of tweets\nper match. Similar activity was seen during the NBA play-offs. High levels of\nactivity are also seen in relation to unfolding news stories and live television.\nThus the need for automated analytical and aggregation techniques is clear.\nSearch on Twitter2 is dominated by inverse chronologically ordered results, fil-\n1 http://royal.pingdom.com/2010/06/08/twitter-now-2-billion-tweets-per-month/\n2 http://search.twitter.com\ntered by keyword. In this model, the assumption is that recency is the single\nmost important measure of relevance. With many relevant documents being\nproduced, there will be many more before a user has time to finish reading the\nsearch results. This simple model does not scale well. The problem of search\nin the real-time web is still an unsolved problem. Perhaps real-time streams of\nuser generated content are destined to be passively observed rather than ac-\ntively searched. The problem definition and methodologies are still in flux. By\nenriching the documents with sentiment information, the opportunity is there\nto employ more sophisticated methods to help users find useful information. For\nexample, ensuring a level of diversity and representativeness of sentiment in the\nresults list.\nAs well as helping users find documents of interest, there is also value in being\nable to determine the aggregate sentiment in a real-time stream of documents.\nBeing able to quantify sentiment for a given topic over time permits us to use\nsentiment as we would a stream of any other source of data: stock prices, sensor\ndata. A real-time sentiment trend then allows us to find events that trigger\ndeviations in sentiment and to integrate this with other data feeds both from the\nonline and oﬄine world. This type of aggregation and high performance analytics\nis of obvious benefit to many areas of industry, government and research.\nSentiment analysis is an area of research reaching maturity (see [7] for a de-\ntailed history of the field). There are now established methodologies, in particular\nfor machine learning techniques, for obtaining accuracies comparable with the\ntraditionally easier task of topical classification. It is the intersection between (i)\nthe abundance and availability of data, (ii) the maturity and of sentiment anal-\nysis as an area of research and (iii) the dearth of research into sentiment-based\nstrategies for real-time information analysis that motivate this area of research.\n4 Challenges\nThe primary challenge in the real-time web is understanding the information\nneeds and interaction patterns. We have already seen how real-time services\nsuch as Twitter are both a disruptive and a challenging and opportunistic tech-\nnology. As of now it is unclear what are the common interaction patterns and\nperhaps more importantly, which ones will prove to be beneficial as the technol-\nogy matures to a significant degree of productivity. Again taking Twitter as an\nexample, perhaps search will prove to be the most valuable way of interacting\nwith Twitter, as it has been for the traditional web. On the other hand, perhaps\nthe real value is being able to navigate the people you personally follow, a more\nsocial network oriented perspective. Perhaps focus will shift from journal style\ncontent to more topical content, as has arguably happened in the blogosphere.\nPerhaps the real-time web in the context of an event behaves quite differently to\nthat at another arbitrary time. Or most likely, perhaps the overall picture is one\nwhich is more complex and which warrants careful thought and consideration.\nFrom a sentiment analysis point of view, the real-time web means pushing\nsentiment analysis beyond review classification. Review classification serves as a\ngood constrained experiment to evaluate sentiment analysis techniques but is less\nrelevant in a real-world ad-hoc domain. In review classification often the topics\nare homogenous and there is little or no topic drift in the documents. Twitter by\nit’s nature is dynamic and unpredictable, even the sentiment topics of interest\nmay themselves may only become apparent as a real-world event unfolds and not\nbe conceived beforehand. These types of ad-hoc scenarios can be troublesome.\nAdd to this the variability in topic nature and the temporal dependency of\ntraining data used and there are a lot of challenges in approaching the accuracy\nenjoyed in classifying reviews.\nComputational efficiency is also a consideration. Some of the higher perform-\ning sentiment analysis systems (for example [5]) have relied on computationally\nexpensive feature extraction techniques such as parsing and dependency extrac-\ntion to achieve their results. Without extensive computing resources, this would\nlikely be unfeasible for the forseeable future with a required throughput of many\ndocuments per second. These problems can be mitigated by sampling strategies\nor, for aggregate sentiment, by allowing for a latency or less granular sentiment\nreading.\nAs an informal communication platform, Twitter exhibits characteristics of\nnoisy text. Twitter posts often contain spelling errors, grammatical contrac-\ntions, non-standard punctuation, emoticons etc. Tagliamonte analysed English\nlanguage use in Instant Messenger (IM) by teenagers and adolescents and found\nthat although the text exhibited features of noisy text, these patterns were not\nprevalent in his older participants and that this type of text was not as com-\nmon as construed in the media [9]. In any case, with sufficient training data\nn-gram models (or extensions thereof) should robustly handle arbitrary tokens.\nThis could degrade performance of approaches who rely on parsing to extract\nfeatures from the text. Such approaches may benefit from a step of language\nnormalisation where the text is amended, either heuristically or using a machine\ntranslation approach, to a more standard form. However, non-standard language\nusage may even prove to be beneficial to learning approaches. Often an author\nmay do this to express themselves in a more concise way where they may be\nconstrained by length of by the modality they are using for input. For example,\nthe following punctuation sequences all add tone to the content of a document:\n“...”, “:-)”, “?!”, “!!!!”.\nEvaluating methods for sentiment analysis in real-time also poses a signifi-\ncant challenge for research methodologies. It is likely that a true measure of the\neffectiveness of improving information discovery using sentiment analysis is not\npossible to determine outside of real-time. In real-time, perhaps our only option\nis the expensive task of evaluating users’ interactions with systems by inferring\neffectiveness from their use of the system. Another option is to prompt users for\nreal-time system feedback. This type of evaluation can borrow from the field of\nuser interface evaluation where contrasting methods of interaction are evaluated.\nIn either case, this is not as scalable or as reproducible as some common eval-\nuation methodologies such as multiple fold cross validation in machine learning\nor the Cranfield model for evaluating information retrieval systems.\nThese challenges collectively are not insurmountable and form a number of\ninteresting research questions for the field of sentiment analysis to pursue.\n5 Conclusion\nThe real-time web has disrupted and challenged our methods for managing in-\nformation retrieval and knowledge discovery on the web. Our research method-\nologies for analysing static corpora, independent of time, need to be rethought\nand we need to adapt our infrastructures for dealing with such time dependent\ninformation. At its core this is an exciting time for user-generated content. In a\nshort space of time, the Internet has a become a place where information and\nthoughts about a vast array of topics can be gathered nearly instantaneously.\nWithout a well-designed method for organising this information and allowing\nusers to access and leverage it, the data becomes redundant, and we are not able\nto take advantage of the opportunities presented by the advent of the real-time\nweb.\nAcknowledgments\nThis work is supported by Science Foundation Ireland under grant 07/CE/I1147\nReferences\n1. J. Bollen, A. Pepe, and H. Mao. Modeling public mood and emotion: Twitter\nsentiment and socio-economic phenomena. CoRR, abs/0911.1583, 2009.\n2. N. A. Diakopoulos and D. A. Shamma. Characterizing debate performance via\naggregated Twitter sentiment. In Conference on Human Factors in Computing\nSystems (CHI 2010), 2010.\n3. B. Jansen, M. Zhang, K. Sobel, and A. Chowdury. Twitter power: Tweets as\nelectronic word of mouth. Journal of the American Society for Information Science\nand Technology, 2009.\n4. C. Macdonald and I. Ounis. The TREC Blogs06 collection : Creating and analysing\na blog test collection. Technical report, University of Glasgow, Department of\nComputing Science, 2006.\n5. S. Matsumoto, H. Takamura, and M. Okumura. Sentiment classification using word\nsub-sequences and dependency sub-trees. In Proceedings of PAKDD’05, the 9th\nPacific-Asia Conference on Advances in Knowledge Discovery and Data Mining,\n2005.\n6. B. Pang and L. Lee. A sentimental education: sentiment analysis using subjectivity\nsummarization based on minimum cuts. In ACL ’04: Proceedings of the 42nd An-\nnual Meeting on Association for Computational Linguistics, page 271, Morristown,\nNJ, USA, 2004. Association for Computational Linguistics.\n7. B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundation and\nTrends in Information Retrieval, 2(1-2):1–135, 2008.\n8. Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multi-\nlingual opinion analysis task at NTCIR-7. 2008.\n9. S. A. Tagliamonte and D. Denis. LINGUISTIC RUIN? LOL! INSTANT MESSAG-\nING AND TEEN LANGUAGE. American Speech, 83(1):3–34, 2008.\n10. T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase-\nlevel sentiment analysis. Proceedings of the 2005 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 347–354, 2005.\n",
      "id": 4763956,
      "identifiers": [
        {
          "identifier": "oai:doras.dcu.ie:15585",
          "type": "OAI_ID"
        },
        {
          "identifier": "147599699",
          "type": "CORE_ID"
        },
        {
          "identifier": "143909190",
          "type": "CORE_ID"
        },
        {
          "identifier": "11309727",
          "type": "CORE_ID"
        }
      ],
      "title": "Crowdsourced real-world sensing: sentiment analysis and the real-time web",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:doras.dcu.ie:15585"
      ],
      "publishedDate": "2010-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 16549230,
          "title": "A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.",
          "authors": [],
          "date": "2004",
          "doi": "10.3115/1218955.1218990",
          "raw": "B. Pang and L. Lee. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 271, Morristown, NJ, USA, 2004. Association for Computational Linguistics.",
          "cites": null
        },
        {
          "id": 16549214,
          "title": "Characterizing debate performance via aggregated Twitter sentiment.",
          "authors": [],
          "date": "2010",
          "doi": "10.1145/1753326.1753504",
          "raw": "N. A. Diakopoulos and D. A. Shamma. Characterizing debate performance via aggregated Twitter sentiment. In Conference on Human Factors in Computing Systems (CHI 2010), 2010.",
          "cites": null
        },
        {
          "id": 16549209,
          "title": "Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "J. Bollen, A. Pepe, and H. Mao. Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena. CoRR, abs/0911.1583, 2009.",
          "cites": null
        },
        {
          "id": 16549235,
          "title": "Opinion mining and sentiment analysis. Foundation and Trends in Information Retrieval,",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundation and Trends in Information Retrieval, 2(1-2):1{135, 2008.",
          "cites": null
        },
        {
          "id": 16549236,
          "title": "Overview of multilingual opinion analysis task at NTCIR-7.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multilingual opinion analysis task at NTCIR-7. 2008.9. S. A. Tagliamonte and D. Denis. LINGUISTIC RUIN? LOL! INSTANT MESSAGING AND TEEN LANGUAGE. American Speech, 83(1):3{34, 2008.",
          "cites": null
        },
        {
          "id": 16549241,
          "title": "Recognizing contextual polarity in phraselevel sentiment analysis.",
          "authors": [],
          "date": "2005",
          "doi": "10.3115/1220575.1220619",
          "raw": "T. Wilson, J. Wiebe, and P. Homann. Recognizing contextual polarity in phraselevel sentiment analysis. Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 347{354, 2005.",
          "cites": null
        },
        {
          "id": 16549226,
          "title": "Sentiment classi using word sub-sequences and dependency sub-trees.",
          "authors": [],
          "date": null,
          "doi": "10.1007/11430919_37",
          "raw": "S. Matsumoto, H. Takamura, and M. Okumura. Sentiment classication using word sub-sequences and dependency sub-trees. In Proceedings of PAKDD'05, the 9th Pacic-Asia Conference on Advances in Knowledge Discovery and Data Mining,",
          "cites": null
        },
        {
          "id": 16549222,
          "title": "The TREC Blogs06 collection : Creating and analysing a blog test collection.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "C. Macdonald and I. Ounis. The TREC Blogs06 collection : Creating and analysing a blog test collection. Technical report, University of Glasgow, Department of Computing Science, 2006.",
          "cites": null
        },
        {
          "id": 16549219,
          "title": "Twitter power: Tweets as electronic word of mouth.",
          "authors": [],
          "date": "2009",
          "doi": "10.1002/asi.21149",
          "raw": "B. Jansen, M. Zhang, K. Sobel, and A. Chowdury. Twitter power: Tweets as electronic word of mouth. Journal of the American Society for Information Science and Technology, 2009.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/15585/1/aics2010.pdf"
      ],
      "updatedDate": "2021-12-13T06:11:25",
      "yearPublished": 2010,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11309727.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11309727"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11309727/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11309727/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4763956"
        }
      ]
    },
    {
      "acceptedDate": "2016-07-07T00:00:00",
      "arxivId": "1512.01818",
      "authors": [
        {
          "name": "A Abbasi"
        },
        {
          "name": "A Esuli"
        },
        {
          "name": "A Hannak"
        },
        {
          "name": "A Tamersoy"
        },
        {
          "name": "A Tumasjan"
        },
        {
          "name": "AB Warriner"
        },
        {
          "name": "ADI Kramer"
        },
        {
          "name": "B Liu"
        },
        {
          "name": "B Pang"
        },
        {
          "name": "B Pang"
        },
        {
          "name": "C Biever"
        },
        {
          "name": "C Hutto"
        },
        {
          "name": "C Levallois"
        },
        {
          "name": "C Strapparava"
        },
        {
          "name": "D Garcia"
        },
        {
          "name": "D Tang"
        },
        {
          "name": "D Watson"
        },
        {
          "name": "DH Wolpert"
        },
        {
          "name": "E Cambria"
        },
        {
          "name": "E Cambria"
        },
        {
          "name": "E Kouloumpis"
        },
        {
          "name": "GA Miller"
        },
        {
          "name": "H Wang"
        },
        {
          "name": "J Reis"
        },
        {
          "name": "J Reis"
        },
        {
          "name": "J Wiebe"
        },
        {
          "name": "JR Landis"
        },
        {
          "name": "M Araujo"
        },
        {
          "name": "M Brysbaert"
        },
        {
          "name": "M Cha"
        },
        {
          "name": "M Hu"
        },
        {
          "name": "M Taboada"
        },
        {
          "name": "M Taboada"
        },
        {
          "name": "M Taboada"
        },
        {
          "name": "M Tsytsarau"
        },
        {
          "name": "ML Berenson"
        },
        {
          "name": "N Godbole"
        },
        {
          "name": "N Kalchbrenner"
        },
        {
          "name": "N Oliveira"
        },
        {
          "name": "N Pappas"
        },
        {
          "name": "N Pappas"
        },
        {
          "name": "NA Diakopoulos"
        },
        {
          "name": "P Gonçalves"
        },
        {
          "name": "P Nakov"
        },
        {
          "name": "PJ Stone"
        },
        {
          "name": "PS Dodds"
        },
        {
          "name": "PS Dodds"
        },
        {
          "name": "R Feldman"
        },
        {
          "name": "R Jain"
        },
        {
          "name": "R Johnson"
        },
        {
          "name": "R Plutchik"
        },
        {
          "name": "R Snow"
        },
        {
          "name": "R Socher"
        },
        {
          "name": "R Valitutti"
        },
        {
          "name": "S Baccianella"
        },
        {
          "name": "S Mohammad"
        },
        {
          "name": "S Mohammad"
        },
        {
          "name": "S Mohammad"
        },
        {
          "name": "S Narr"
        },
        {
          "name": "SM Mohammad"
        },
        {
          "name": "T Smedt De"
        },
        {
          "name": "T Wilson"
        },
        {
          "name": "T Wilson"
        },
        {
          "name": "YR Tausczik"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Filipe N"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/202415923",
        "https://api.core.ac.uk/v3/outputs/194925623",
        "https://api.core.ac.uk/v3/outputs/646587179"
      ],
      "createdDate": "2016-08-03T02:33:42",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 2612,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2612",
          "logo": "https://api.core.ac.uk/data-providers/2612/logo"
        },
        {
          "id": 7959,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/7959",
          "logo": "https://api.core.ac.uk/data-providers/7959/logo"
        }
      ],
      "depositedDate": "2016-07-07T00:00:00",
      "abstract": "In the last few years thousands of scientific papers have investigated\nsentiment analysis, several startups that measure opinions on real data have\nemerged and a number of innovative products related to this theme have been\ndeveloped. There are multiple methods for measuring sentiments, including\nlexical-based and supervised machine learning methods. Despite the vast\ninterest on the theme and wide popularity of some methods, it is unclear which\none is better for identifying the polarity (i.e., positive or negative) of a\nmessage. Accordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, \\textit{as they are\nused in practice}, across multiple datasets originated from different data\nsources. Such a comparison is key for understanding the potential limitations,\nadvantages, and disadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-four popular sentiment\nanalysis methods (which we call the state-of-the-practice methods). Our\nevaluation is based on a benchmark of eighteen labeled datasets, covering\nmessages posted on social networks, movie and product reviews, as well as\nopinions and comments in news articles. Our results highlight the extent to\nwhich the prediction performance of these methods varies considerably across\ndatasets. Aiming at boosting the development of this research area, we open the\nmethods' codes and datasets used in this article, deploying them in a benchmark\nsystem, which provides an open API for accessing and comparing sentence-level\nsentiment analysis methods",
      "documentType": "research",
      "doi": "10.1140/epjds/s13688-016-0085-1",
      "downloadUrl": "https://core.ac.uk/download/478016225.pdf",
      "fieldOfStudy": null,
      "fullText": "SentiBench - a benchmark comparison of\nstate-of-the-practice sentiment analysis methods\nFilipe N Ribeiro\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfiliperibeiro@dcc.ufmg.br\nMatheus Arau´jo\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nPollyanna Gonc¸alves\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nMarcos Andre´ Gonc¸alves\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nFabrı´cio Benevenuto\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nABSTRACT\nIn the last few years thousands of scientific papers have in-\nvestigated sentiment analysis, several startups that measure\nopinions on real data have emerged and a number of inno-\nvative products related to this theme have been developed.\nThere are multiple methods for measuring sentiments, inclu-\nding lexical-based and supervised machine learning methods.\nDespite the vast interest on the theme and wide popularity\nof some methods, it is unclear which one is better for iden-\ntifying the polarity (i.e., positive or negative) of a message.\nAccordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, as\nthey are used in practice, across multiple datasets originated\nfrom different data sources. Such a comparison is key for\nunderstanding the potential limitations, advantages, and di-\nsadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-\nfour popular sentiment analysis methods (which we call the\nstate-of-the-practice methods). Our evaluation is based on a\nbenchmark of eighteen labeled datasets, covering messages\nposted on social networks, movie and product reviews, as\nwell as opinions and comments in news articles. Our results\nhighlight the extent to which the prediction performance of\nthese methods varies considerably across datasets. Aiming at\nboosting the development of this research area, we open the\nmethods’ codes and datasets used in this article, deploying\nthem in a benchmark system, which provides an open API for\naccessing and comparing sentence-level sentiment analysis\nmethods.\nIntroduction\nSentiment analysis has become an extremely popular tool, ap-\nplied in several analytical domains, especially on the Web and\nsocial media. To illustrate the growth of interest in the field,\nFigure 1 shows the steady growth on the number of searches\non the topic, according to Google Trends1, mainly after the\npopularization of online social networks (OSNs). More than\n7,000 articles have been written about sentiment analysis and\nvarious startups are developing tools and strategies to extract\nsentiments from text [20].\nThe number of possible applications of such a technique is\nalso considerable. Many of them are focused on monitoring\nthe reputation or opinion of a company or a brand with the\nanalysis of reviews of consumer products or services [27].\nSentiment analysis can also provide analytical perspectives\nfor financial investors who want to discover and respond to\nmarket opinions [46, 8]. Another important set of applicati-\nons is in politics, where marketing campaigns are interested\nin tracking sentiments expressed by voters associated with\ncandidates [67].\nFigure 1: Searches on Google for the Query: “Sentiment\nAnalysis”.\nDue to the enormous interest and applicability, there has been\na corresponding increase in the number of proposed sentiment\nanalysis methods in the last years. The proposed methods rely\non many different techniques from different computer science\nfields. Some of them employ machine learning methods that\n1https://www.google.com/trends/explore#q=sentiment%\n20analysis\nar\nX\niv\n:1\n51\n2.\n01\n81\n8v\n5 \n [c\ns.C\nL]\n  1\n4 J\nul \n20\n16\noften rely on supervised classification approaches, requiring\nlabeled data to train classifiers [48]. Others are lexical-based\nmethods that make use of predefined lists of words, in which\neach word is associated with a specific sentiment. The lexical\nmethods vary according to the context in which they were\ncreated. For instance, LIWC [64] was originally proposed to\nanalyze sentiment patterns in formally written English texts,\nwhereas PANAS-t [25] and POMS-ex [9] were proposed as\npsychometric scales adapted to the Web context.\nOverall, the above techniques are acceptable by the research\ncommunity and it is common to see concurrent important\npapers, sometimes published in the same computer science\nconference, using completely different methods. For example,\nthe famous Facebook experiment [33] which manipulated\nusers feeds to study emotional contagion, used LIWC [64].\nConcurrently, Reis et al. used SentiStrength [65] to measure\nthe negativeness or positiveness of online news headlines [53,\n52], whereas Tamersoy [62] explored VADER’s lexicon [28]\nto study patterns of smoking and drinking abstinence in social\nmedia.\nAs the state-of-the-art has not been clearly established, resear-\nchers tend to accept any popular method as a valid methodo-\nlogy to measure sentiments. However, little is known about the\nrelative performance of the several existing sentiment analysis\nmethods. In fact, most of the newly proposed methods are\nrarely compared with all other pre-existing ones using a large\nnumber of existing datasets. This is a very unusual situation\nfrom a scientific perspective, in which benchmark compari-\nsons are the rule. In fact, most applications and experiments\nreported in the literature make use of previously developed\nmethods exactly how they were released with no changes and\nadaptations and with none or almost none parameter setting.\nIn other words, the methods have been used as a black-box,\nwithout a deeper investigation on their suitability to a particular\ncontext or application.\nTo sum up, existing methods have been widely deployed for\ndeveloping applications without a deeper understanding regar-\nding their applicability in different contexts or their advanta-\nges, disadvantages, and limitations in comparison with each\nanother. Thus, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, as\nthey are used in practice, across multiple datasets originated\nfrom different data sources.\nThis state-of-the-practice situation is what we propose to inves-\ntigate in this article. We do this by providing a thorough bench-\nmark comparison of twenty-four state-of-the-practice methods\nusing eighteen labeled datasets. In particular, given the recent\npopularity of online social networks and of short texts on the\nWeb, many methods are focused in detecting sentiments at\nthe sentence-level, usually used to measure the sentiment of\nsmall sets of sentences in which the topic is known a priori.\nWe focus on such context – thus, our datasets cover messa-\nges posted on social networks, movie and product reviews,\nand opinions and comments in news articles, Ted talks, and\nblogs. We survey an extensive literature on sentiment analysis\nto identify existing sentence-level methods covering several\ndifferent techniques. We contacted authors asking for their\ncodes when available or we implemented existing methods\nwhen they were unavailable but could be reproduced based on\ntheir descriptions in the original published paper. We should\nemphasize that our work focus on off-the-shelf methods as\nthey are used in practice. This excludes most of the supervised\nmethods which require labeled sets for training, as these are\nusually not available for practitioners. Moreover, most of the\nsupervised solutions do not share the source code or a trained\nmodel to be used with no supervision.\nOur experimental results unveil a number of important fin-\ndings. First, we show that there is no single method that\nalways achieves the best prediction performance for all dif-\nferent datasets, a result consistent with the “there is no free\nlunch theorem” [74]. We also show that existing methods vary\nwidely regarding their agreement, even across similar datasets.\nThis suggests that the same content could be interpreted very\ndifferently depending on the choice of a sentiment method.\nWe noted that most methods are more accurate in correctly\nclassifying positive than negative text, suggesting that current\napproaches tend to be biased in their analysis towards positi-\nvity. Finally, we quantify the relative prediction performance\nof existing efforts in the field across different types of datasets,\nidentifying those with higher prediction performance across\ndifferent datasets.\nBased on these observations, our final contribution consists\non releasing our gold standard dataset and the codes of the\ncompared methods2. We also created a Web system through\nwhich we allow other researchers to easily use our data and\ncodes to compare results with the existing methods 3. More\nimportantly, by using our system one could easily test which\nmethod would be the most suitable to a particular dataset\nand/or application. We hope that our tool will not only help\nresearchers and practitioners for accessing and comparing a\nwide range of sentiment analysis techniques, but can also help\ntowards the development of this research field as a whole.\nThe remainder of this paper is organized as follows. In Section\n2, we briefly describe related efforts. Then, in Section 3 we\ndescribe the sentiment analysis methods we compare. Section\n4 presents the gold standard data used for comparison. Section\n5 summarizes our results and findings. Finally, Section 6\nconcludes the article and discusses directions for future work.\nBackground and Related Work\nNext we discuss important definitions and justify the focus of\nour benchmark comparison. We also briefly survey existing\nrelated efforts that compare sentiment analysis methods.\nFocus on Sentence-Level Sentiment Analysis\nSince sentiment analysis can be applied to different tasks, we\nrestrict our focus on comparing those efforts related to de-\ntect the polarity (i.e. positivity or negativity) of a given short\ntext (i.e. sentence-level). Polarity detection is a common\nfunction across all sentiment methods considered in our work,\n2Except for paid methods\n3http://www.ifeel.dcc.ufmg.br\nproviding valuable information to a number of different appli-\ncations, specially those that explore short messages that are\ncommonly available in social media [20].\nSentence-level sentiment analysis can be performed with su-\npervision (i.e. requiring labeled training data) or not. An\nadvantage of supervised methods is their ability to adapt and\ncreate trained models for specific purposes and contexts. A\ndrawback is the need of labeled data, which might be highly\ncostly, or even prohibitive, for some tasks. On the other hand,\nthe lexical-based methods make use of a pre-defined list of\nwords, where each word is associated with a specific senti-\nment. The lexical methods vary according to the context in\nwhich they were created. For instance, LIWC [64] was origi-\nnally proposed to analyze sentiment patterns in English texts,\nwhereas PANAS-t [25] and POMS-ex [9] are psychometric\nscales adapted to the Web context. Although lexical-based\nmethods do not rely on labeled data, it is hard to create a\nunique lexical-based dictionary to be used for all different\ncontexts.\nWe focus our effort on evaluating unsupervised efforts as\nthey can be easily deployed in Web services and applicati-\nons without the need of human labeling or any other type of\nmanual intervention. As described in Section 3, some of the\nmethods we consider have used machine learning to build\nlexicon dictionaries or even to build models and tune specific\nparameters. We incorporate those methods in our study, since\nthey have been released as black-box tools that can be used in\nan unsupervised manner.\nExisting Efforts on Comparison of Methods\nDespite the large number of existing methods, only a limited\nnumber of them have performed a comparison among senti-\nment analysis methods, usually with restricted datasets. Ove-\nrall, lexical methods and machine learning approaches have\nbeen evolving in parallel in the last years, and it comes as no\nsurprise that studies have started to compare their performance\non specific datasets and use one or another strategy as baseline\nfor comparison. A recent survey summarizes several of these\nefforts [66] and conclude that a systematic comparative study\nthat implements and evaluates all relevant algorithms under\nthe same framework is still missing in the literature. As new\nmethods emerge and compare themselves only against one, at\nmost two other methods, using different evaluation datasets\nand experimental methodologies, it is hard to conclude if a\nsingle method triumphs over the remaining ones, or even in\nspecific scenarios. To the best of our knowledge, our effort\nis the first of kind to create a benchmark that provides such\nthorough comparison.\nAn important effort worth mentioning consists of an annual\nworkshop – The International Workshop on Semantic Evalu-\nation (SemEval). It consists of a series of exercises grouped\nin tracks, including sentiment analysis, text similarity, among\nothers, that put several together competitors against each other.\nSome new methods such as Umigon [35] have been proposed\nafter obtaining good results on some of these tracks. Although,\nSemEval has been playing an important role for identifying\nrelevant methods, it requires authors to register for the chal-\nlenge and many popular methods have not been evaluated in\nthese exercises. Additionally, SemEval labeled datasets are\nusually focused on one specific type of data, such as tweets,\nand do not represent a wide range of social media data. In our\nevaluation effort, we consider one dataset from SemEval 2013\nand two methods that participated in the competition in that\nsame year.\nAhmadi et al. [1] performed a comparision of twitter-based\nsentiment analysis tools. They selected twenty tools and tested\nthem across five twitter datasets. This benchmark is the work\nthat most approximate from ours, but it is different in some\nmeaningful aspects. Firstly, we embraced distinct contexts\nsuch as reviews, comments and social networks aiming at pro-\nviding a broader evaluation of the tools. Secondly, the methods\nthey selected included supervised and unsupervised approa-\nches which, in our view, could be unfair for the unsupervised\nones. Although the results have been presented separately,\nthe supervised methods, as mentioned by authors, required\nextensive parameter tuning and validation in a training environ-\nment. Therefore, supervised approaches tend to adapt to the\ncontext they were applied to. As previously highlighted, our\nfocus is on off-the-shelf tools as they have been extensively\nand recently used. Many researchers and practitioners have\nalso used supervised approaches but this is out of scope of\nour work. Finally, most of the unsupervised methods selected\nin the Twitter Benchmark are paid tools, except from two of\nthem, both of which were developed as a result of published\nacademic research. Oppositely we made an extensive biblio-\ngraphy review to include relevant academic outcomes without\nexcluding the most used commercial options.\nFinally, in a previous effort [24], we compared eight sentence-\nlevel sentiment analysis methods, based on one public dataset\nused to evaluate SentiStrength [65]. This article largely ex-\ntends our previous work by comparing a much larger set of\nmethods across many different datasets, providing a much\ndeeper benchmark evaluation of current popular sentiment\nanalysis methods. The methods used in this paper were also\nincorporated as part of an existing system, namely iFeel [4].\nSentiment Analysis Methods\nThis section provides a brief description of the twenty-four\nsentence-level sentiment analysis methods investigated in this\narticle. Our effort to identify important sentence-level senti-\nment analysis methods consisted of systematically search for\nthem in the main conferences in the field and then checking for\npapers that cited them as well as their own references. Some\nof the methods are available for download on the Web; others\nwere kindly shared by their authors under request; and a small\npart of them were implemented by us based on their descripti-\nons in the original paper. This usually happened when authors\nshared only the lexical dictionaries they created, letting the\nimplementation of the method that use the lexical resource to\nourselves.\nTable 1 and Table 2 present an overview of these methods, pro-\nviding a description of each method as well as the techniques\nthey employ (L for Lexicon Dictionary and ML for Machine\nLearning), their outputs (e.g. -1,0,1, meaning negative, neutral,\nand positive, respectively), the datasets they used to validate,\nthe baseline methods used for comparison and finally lexicon\ndetails, as well as the Lexicon size column describing the num-\nber of terms contained in the method’s lexicon. The methods\nare organized in chronological order to allow a better over-\nview of the existing efforts over the years. We can note that\nthe methods generate different outputs formats. We colored\nin blue the positive outputs, in black the neutral ones, and\nin red those that are negative. Note that we included LIWC\nand LIWC15 entries in Table 2, which represents the former\nversion, launched in 2007, and the latest version, from 2015,\nrespectively. We considered both versions because the first\none was extensively used in the literature. This also allows to\ncompare the improvements between both versions.\nAdapting Lexicons for the Sentence Level Task\nSince we are comparing sentiment analysis methods on a\nsentence-level basis, we need to work with mechanisms that\nare able to receive sentences as input and produce polarities\nas output. Some of the approaches considered in this paper,\nshown in Table 2, are complex dictionaries built with great\neffort. However, a lexicon alone has no natural ability to infer\npolarity in sentence level tasks. The purpose of a lexicon goes\nbeyond the detection of polarity of a sentence [20, 37], but it\ncan also be used with that purpose [23, 32].\nSeveral existing sentence-level sentiment analysis methods,\nlike VADER [28] and SO-CAL [61], combine a lexicon and\nthe processing of the sentence characteristics to determine a\nsentence polarity. These approaches make use of a series of\nintensifiers, punctuation transformation, emoticons, and many\nother heuristics.\nThus, to evaluate each lexicon dictionaries as the basis for a\nsentence-level sentiment analysis method, we considered the\nVADER’s implementation. In other words, we used VADER’s\ncode for determining if a sentence is positive or not conside-\nring different lexicons. The reasons for choosing VADER are\ntwofold: (i) the fact it is an open source tool, allowing easy re-\nplication of the procedures we performed in our study; and (ii)\nVADER’s expressive results observed in previous experiments.\nVADER’s heuristics were proposed based on qualitative analy-\nses of textual properties and characteristics which affect the\nperceived sentiment intensity of the text. VADER’s author\nidentified five heuristics based on grammatical and syntactical\ncues to convey changes to sentiment intensity that go beyond\nthe bag-of-words model. The heuristics include treatments\nfor: 1) punctuation (e.g number of ‘!’s); 2) capitalization (e.g\n”I HATE YOU”is more intense than ”i hate you”); 3) degree\nmodifiers (e.g ”The service here is extremely good”is more\nintense than ”The service here is good”); 4) constructive con-\njunction ”but”to shift the polarity; 5) Tri-gram examination\nto identify negation (e.g ”The food here isn’t really all that\ngreat.”). We choose VADER as a basis for such heuristics\nas it is one of the most recent methods among those we con-\nsidered. Moreover, it is becoming widely used, being even\nimplemented as part of the well-known NLTK python library4.\nWe applied such heuristics to the following lexicons:\nANEW SUB, AFINN, Emolex, EmoticonsDS, NRC Hash-\ntag, Opinion Lexicon, PANAS-t, Sentiment 140 Lexicon and\n4http://www.nltk.org/ modules/nltk/sentiment/vader.html\nSentiWordNet. We notice that those strategies drastically im-\nproved most of the results of the lexicons for sentence-level\nsentiment analysis in comparison with a simple baseline ap-\nproach that averages the occurrence of positive and negative\nwords to classify the polarity of a sentence. The results for\nthe simplest usage of the above lexicons as plain methods are\navailable in the last four Tables in the Additional File 1 of the\nelectronic version of the manuscript. LIWC dictionary was\nnot included in these adaptations due to its very restrictive\nlicense, which does not allow any derivative work based on\nthe original application and lexicon. Table 2 has also a column\n(Lexicon size) that describes the number of terms contained in\nthe the proposed dictionary.\nOutput Adaptations\nIt is worth noticing that the output of each method varies\ndrastically depending on the goal it was developed for and the\napproach it employs. PANAS-t, for instance, associates each\nword with eleven moods as described in Table 1 and it was\ndesigned to track any increase or decrease in sentiments over\ntime. Emolex lexicon provides the association of each word\nwith eight sentiments. The word ’unhappy’ for example is\nrelated to anger, disgust, and sadness and it is not related to joy,\nsurprise, etc. SentiWordNet links each word with a synset (i.e.\na set of synonyms) characterized by a positive and a negative\nscore, both of them represented with a value between 0 and 1.\nThe aforementioned lexicons were used as dictionary input\nto VADER’s code. We had to adapt the way the words are\nprocessed as follows. For PANAS-t we assumed that joviality,\nassurance, serenity, and surprise are positive affect. Fear, sad-\nness, guilt, hostility, shyness, and fatigue are negative affect.\nAttentiveness was considered neutral. In the case of Emolex,\nwe considered two other entries released by the authors. The\nfirst one defines the positiviy of a word (0 or 1) and the second\ncharacterizes the negativity (0 or 1). For SentiWordNet we cal-\nculate an overall score to the word by subtracting the positive\nvalue from negative value defined to that word. For example,\nthe positive value for the word faithful is 0.625 while its nega-\ntive score is 0.0. Then the overall score score is 0.625. Finally,\nfor ANEW SUB we employed only the valence emotion of\neach word. This metric ranges from 1 to 9 and indicates the\nlevel of pleasantness of a specific word – we considered the\nvalues one to four as negative, five as neutral, and six to nine\nas positive.\nOther lexicons included in our evaluation already provide po-\nsitive and negative scores such as SentiWordNet or an overall\nscore ranging from a negative to a positive value. After ap-\nplying VADER’s heuristics for each one of these lexicons we\nget scores in the same way VADER’s output (see Table 2).\nOther methods also required some output handling. The availa-\nble implementation of OpinionFinder 5, for instance, generates\npolarity outputs (-1,0, or 1) for each sentiment clue found in\na sentence so that a single sentence can have more than one\nclue. We considered the polarity of a single sentence as the\nsum of the polarities of all the clues.\n5http://mpqa.cs.pitt.edu/opinionfinder/\nTable 1: Overview of the sentence-level methods available in the literature.\nName Description L ML\nEmoticons [24] Messages containing positive/negative emoticons are positive/negative. Messages without emoticons are notclassified. X\nOpinion Lexicon [27] Focus on Product Reviews. Builds a Lexicon to predict polarity of product features phrases that are summarized toprovide an overall score to that product feature. X\nOpinion Finder (MPQA)\n[72] [73]\nPerforms subjectivity analysis trough a framework with lexical analysis former and a machine learning approach\nlatter. X X\nSentiWordNet [19] [5] Construction of a lexical resource for Opinion Mining based on WordNet [38]. The authors grouped adjectives, nouns,etc in synonym sets (synsets) and associated three polarity scores (positive, negative and neutral) for each one. X X\nLIWC [64]\nAn acronym for Linguistic Inquiry and Word Count, LIWC is a text analysis paid tool to evaluate emotional,\ncognitive, and structural components of a given text. It uses a dictionary with words classified into categories (anxiety,\nhealth, leisure, etc). An updated version was launched in 2015.\nX\nSentiment140 [22]\nSentiment140 (previously known as ”Twitter Sentiment”) was proposed as an ensemble of three classifiers (Naive\nBayes, Maximum Entropy, and SVM) built with a huge amount of tweets containing emoticons collected by the\nauthors. It has been improved and transformed into a paid tool.\nX\nSenticNet [12] Uses dimensionality reduction to infer the polarity of common sense concepts and hence provide a resource formining opinions from text at a semantic, rather than just syntactic level. X\nAFINN [45] - A new\nANEW\nBuilds a Twitter based sentiment Lexicon including Internet slangs and obscene words. AFINN can be considered as\nan expansion of ANEW [10], a dictionary created to provides emotional ratings for English words. ANEW dictionary\nrates words in terms of pleasure, arousal and dominance.\nX\nSO-CAL [61]\nCreates a new Lexicon with unigrams (verbs, adverbs, nouns and adjectives) and multi-grams (phrasal verbs and\nintensifiers) hand ranked with scale +5 (strongly positive) to -5 (strongly negative). Authors also included part of\nspeech processing, negation and intensifiers.\nX\nEmoticons DS (Distant\nSupervision)[26]\nCreates a scored lexicon based on a large dataset of tweets. Its based on the frequency each lexicon occurs with\npositive or negative emotions. X\nNRC Hashtag [39]\nBuilds a lexicon dictionary using a Distant Supervised Approach. In a nutshell it uses known hashtags (i.e #joy,\n#happy etc) to “classify” the tweet. Afterwards, it verifies frequency each specific n-gram occurs in a emotion and\ncalculates its Strong of Associaton with that emotion.\nX\nPattern.en [15] Python Programming Package (toolkit) to deal with NLP, Web Mining and Sentiment Analysis. Sentiment analysis isprovided through averaging scores from adjectives in the sentence according to a bundle lexicon of adjective. X\nSASA [69] Detects public sentiments on Twitter during the 2012 U.S. presidential election. It is based on the statistical modelobtained from the classifier Naı¨ve Bayes on unigram features. It also explores emoticons and exclamations. X\nPANAS-t [25]\nDetects mood fluctuations of users on Twitter. The method consists of an adapted version (PANAS) Positive Affect\nNegative Affect Scale [70], well-known method in psychology with a large set of words, each of them associated with\none from eleven moods such as surprise, fear, guilt, etc .\nX\nEmoLex [41]\nBuilds a general sentiment Lexicon crowdsourcing supported. Each entry lists the association of a token with 8 basic\nsentiments: joy, sadness, anger, etc defined by [51]. Proposed Lexicon includes unigrams and bigrams from\nMacquarie Thesaurus and also words from GI and Wordnet.\nX\nUSent [49] Infer additional reviews user ratings by performing sentiment analysis (SA) of user comments and integrating itsoutput in a nearest neighbor (NN) model that provides multimedia recommendations over TED Talks. X X\nSentiment140 Lexicon [42]\nA lexicon dictionary based on the same dataset used to train the Sentiment140 Method. The lexicon was built in a\nsimilar way to [39] but authors used the occurrency of emoticons to classify the tweet as positive or negative. Then,\nthe n-gram score was calculated based on the frequency of occurrence in each class of tweets.\nX\nSentiStrength [65] Builds a lexicon dictionary annotated by humans and improved with the use of Machine Learning. X X\nStanford Recursive Deep\nModel [56]\nProposes a model called Recursive Neural Tensor Network (RNTN) that processes all sentences dealing with their\nstructures and compute the interactions between them. This approach is interesting since RNTN take into account the\norder of words in a sentence, which is ignored in most of methods.\nX X\nUmigon [35] Disambiguates tweets using lexicon with heuristics to detect negations plus elongated words and hashtags evaluation. X\nANEW SUB [3]\nAnother extension of the ANEW dictionary [10] including the most common words from the SubtlexUS corpus [11].\nSubtlexUS was an effort to propose a different manner to calculate word frequencies considering film and TV\nsubtitles.\nX\nVADER [28] It is a human-validated sentiment analysis method developed for twitter and social media contexts. VADER wascreated from a generalizable, valence-based, human-curated gold standard sentiment lexicon. X\nSemantria [36]\nIt is a paid tool that employs multi-level analysis of sentences. Basically it has four levels: part of speech, assignment\nof previous scores from dictionaries, application of intensifiers and finally machine learning techniques to delivery a\nfinal weight to the sentence.\nX X\nTable 2: Overview of the sentence-level methods available in the literature.\nName Output Validation Compared To Lexiconsize\nEmoticons -1, 1 - - 79\nOpinion\nLexicon Provides polarities for lexicons\nProduct Reviews from\nAmazon and CNet - 6,787\nOpinion\nFinder\n(MPQA)\nNegative, Objective, Positive MPQA [71] Compared to itself in different versions 20,611\nSentiWordNet Provides positive, negative and objectivescores for each word (0.0 to 1.0 ) - General Inquirer (GI)[57] 117,658\nSentiment140 0, 2, 4\nTheir own datasets -\n359 tweets\n(Tweets STF, presented\nat Table 3)\nNaive Bayes, Maximum Entropy, and SVM\nclassifiers as described in [48] -\nLIWC07 negEmo, posEmo - Their previous dictionary (2001) 4,500\nSenticNet Negative, Positive Patient Opinions(Unavailable) SentiStrength [65] 15,000\nAFINN Provides polarity score for lexicons (-5 to 5). Twiter [7] OpinonFinder [72], ANEW [10], GI [57] andSentistrength [65] 2,477\nSO-CAL [<0), 0, (>0]\nEpinion [59],\nMPQA[71],\nMyspace[65],\nMPQA[71], GI[57], SentiWordNet\n[19],”Maryland”Dict [40], Google Generated Dict\n[60]\n9,928\nEmoticons DS\n(Distant\nSupervision)\nProvides polarity score for lexicons\nValidation with\nunlabeled twitter data\n[14]\n- 1,162,894\nNRC Hashtag Provides polarities for lexicons\nTwitter (SemEval-2007\nAffective Text\nCorpus) [58]\nWordNet Affect [58] 679,468\nPattern.en Objective, [<0.1, ≥0.1] Product reviews, but thesource was not specified - 2,973\nSASA [69] Negative, Neutral, Unsure, Positive\n“Political” Tweets\nlabeled by “turkers”\n(AMT) (unavailable)\n- -\nPANAS-t\nProvides association for each word with\neleven moods (joviality, attentiveness, fear,\netc )\nValidation with\nunlabeled twitter data\n[14]\n- 50\nEmoLex Provides polarities for lexicons - Compared with existing gold standard data but itwas not specified 141,820\nUSent neg, neu, pos Their own dataset - TedTalks\nComparison with other multimedia\nrecommendation approaches\nMPQA\n(8,226) /\nTheir own\n(9,176)\nSentiment140\nLexicon Provides polarity scores for lexicon\nTwitter and SMS from\nSemeval 2013, task 2\n[43]\nOther Semeval 2013, task 2 approaches 1,220,176\nSentiStrength -1,0,1\nTheir own datasets -\nTwitter, Youtube, Digg,\nMyspace, BBC Forums\nand Runners World\nThe best of nine Machine Learning techniques for\neach test 2,698\nStanford\nRecursive\nDeep Model\nvery negative, negative, neutral, positive,\nvery positive Movie Reviews [47]\nNaı¨ve Bayes and SVM with bag of words features\nand bag of bigram features 227,009\nUmigon Negative, Neutral, Positive\nTwitter and SMS from\nSemeval 2013, task 2\n[43]\n[42] 1,053\nANEW SUB\nProvides ratings for words in terms of\nValence, Arousal and Dominance. Results\ncan also be grouped by gender, age and\neducation\n-\nCompared to similar works, including\ncross-language studies, by means of correlations\nbetween emotional dimensions\n13,915\nVADER [<-0,05), (-0,05..0,05), (>0,05]\nTheir own datasets -\nTwitter, Movie Reviews,\nTechnical Product\nReviews, NYT User’s\nOpinions\n(GI)[57], LIWC, [64], SentiWordNet [19], ANEW\n[10], SenticNet [13] and some Machine Learning\nApproaches\n7,517\nLIWC15 negEmo, posEmo - Their previous dictionary (2007) 6,400\nSemantria negative, neutral, positive not available not available notavailable\nThe outputs from the remaining methods were easily adap-\nted and converted to positive, negative or neutral. SO-CAL\nand Pattern.en delivery float numbers greater than a threshold,\nindicating positive, and lesser than the threshold, indicating\nnegative. LIWC, SenticNet, SASA, USent, SentiStrength,\nUmigon, VADER and Semantria already provide fixed out-\nputs indicating one of three desired classes while Stanford\nRecursive Deep Model yields very negative and very positive\nwhich in our experiments are handled as negative and positive,\nrespectively.\nPaid Softwares\nSeven out of the twenty-four methods evaluated in this work\nare closed paid softwares: LIWC (2007 and 2015), Semantria,\nSenticNet 3.0, Sentiment140 and SentiStrength. Although\nSentiStrength is paid, it has a free of charge academic license.\nSenticNet’s authors kindly processed all datasets with the com-\nmercial version and return the polarities for us. For SentiS-\ntrenght we used the Java version from May 2013 in a package\nwith all features of the commercial version. For LIWC we ac-\nquired the licenses from 2007 (LIWC07) and 2015 (LIWC15)\nversions. Finally, for Semantria and Sentiment140 we used a\ntrial account free of charge for a limited number of sentences,\nwhich was sufficient to run our experiments.\nMethods not included\nDespite our effort to include in our comparison most of the\nhighly cited and important methods we could not include a\nfew of them for different reasons. Profile of Mood States\n(POMS-ex) [9] is not available on the Web or under request\nand could not be re-implemented based on their descriptions\nin the original papers. The same situation occurs with the\nLearning Sentiment-Specific Word Embedding for Twitter\nSentiment Classification [63]. NRC SVM [42] is not available\nas well, although the lexical resources used by the authors are\navailable and were considered in our evaluation resulting in the\nmethods: NRC Hashtag and Sentiment140. The authors of the\nConvolutional Neural Network for Modeling Sentences [31]\nand of the Effective Use of Word Order for Text Categorization\nwith Convolutional Neural Networks [30] have made their\nsource code available but the first one lacks the train files\nand the second one requires a GPU to execute. There are a\nfew other methods for sentiment detection proposed in the\nliterature and not considered here. Most of them consists of\nvariations of the techniques used by the above methods, such\nas WordNet-Affect[68] and Happiness Index [18].\nDatasets and Comparison Among Methods\nFrom Table 2 we can note that the validation strategy, the\ndatasets used, and the comparison with baselines performed\nby these methods vary greatly, from toy examples to large\nlabeled datasets. PANAS-t and Emoticons DS used manually\nunlabeled twitter data to validate their methods, by presenting\nevaluations of events in which some bias towards positivity\nand negativity would be expected. PANAS-t is tested with\nunlabeled Twitter data related to Michael Jackson’s death and\nthe release of a Harry Potter movie whereas Emoticons DS\nverified the influence of weather and time on the aggregate\nsentiment from Twitter. Lexical dictionaries were validated\nin very different ways. AFINN[45] compared its Lexicon\nwith other dictionaries. Emoticon Distance Supervised [26]\nused Pearson Correlation between human labeling and the\npredicted value. SentiWordNet [19] validates the proposed\ndictionary with comparisons with other dictionaries, but it also\nused human validation of the proposed lexicon. These efforts\nattempt to validate the created lexicon, without comparing the\nlexicon as a sentiment analysis method by itself. VADER [28]\ncompared results with lexical approaches considering labeled\ndatasets from different social media data. SenticNet [13] was\ncompared with SentiStrength [65] with a specific dataset re-\nlated to patient opinions, which could not be made available.\nStanford Recursive Deep Model [56] and SentiStrength [65]\nwere both compared with standard machine learning approa-\nches, with their own datasets.\nThis scenario, where every new developed solution compares\nitself with different solutions using different datasets, happens\nbecause there is no standard benchmark for evaluating new\nmethods. This problem is exacerbated because many methods\nhave been proposed in different research communities (e.g.\nNLP, Information Science, Information Retrieval, Machine Le-\narning), exploiting different techniques, with low knowledge\nabout related efforts in other communities. Next, we describe\nhow we created a large gold standard to properly compare all\nthe considered sentiment analysis methods.\nGold Standard Data\nA key aspect in evaluating sentiment analysis methods consists\nof using accurate gold standard labeled datasets. Several exis-\nting efforts have generated labeled data produced by experts\nor non-experts evaluators. Previous studies suggest that both\nefforts are valid as non-expert labeling may be as effective\nas annotations produced by experts for affect recognition, a\nvery related task [55]. Thus, our effort to build a large and\nrepresentative gold standard dataset consists of obtaining labe-\nled data from trustful previous efforts that cover a wide range\nof sources and kinds of data. We also attempt to assess the\n“quality” of our gold standard in terms of the accuracy of the\nlabeling process.\nTable 3 summarizes the main characteristics of the eighteen ex-\nploited datasets, such as number of messages and the average\nnumber of words per message in each dataset. It also defines\na simpler nomenclature that is used in the remainder of this\npaper. The table also presents the methodology employed in\nthe classification. Human labeling was implemented in almost\nall datasets, usually done with the use of non-expert reviewers.\nReviews I dataset relies on five stars rates, in which users\nrate and provide a comment about an entity of interest (e.g. a\nmovie or an establishment).\nLabeling based on Amazon Mechanical Turk (AMT) was used\nin seven out of the eighteen datasets, while volunteers and\nother strategies that involve non-expert evaluators were used\nin ten datasets. Usually, an agreement strategy (i.e. majority\nvoting) is applied to ensure that, in the end, each sentence\nhas an agreed-upon polarity assigned to it. The number of\nannotators used to build the datasets is also shown in Table 3.\nTable 3: Labeled datasets.\nDataset Nomeclature # # # # Average # Average # Annotators # of CK\nMsgs Pos Neg Neu of phrases of words Expertise Annotators\nComments (BBC) [65] Comments BBC 1,000 99 653 248 3.98 64.39 Non Expert 3 0.427\nComments (Digg) [65] Comments Digg 1,077 210 572 295 2.50 33.97 Non Expert 3 0.607\nComments (NYT) [28] Comments NYT 5,190 2,204 2,742 244 1.01 17.76 AMT 20 0.628\nComments (TED) [50] Comments TED 839 318 409 112 1 16.95 Non Expert 6 0.617\nComments (Youtube) [65] Comments YTB 3,407 1,665 767 975 1.78 17.68 Non Expert 3 0.724\nMovie-reviews [47] Reviews I 10,662 5,331 5,331 - 1.15 18.99 User Rating - 0.719\nMovie-reviews [28] Reviews II 10,605 5,242 5,326 37 1.12 19.33 AMT 20 0.555\nMyspace posts [65] Myspace 1,041 702 132 207 2.22 21.12 Non Expert 3 0.647\nProduct reviews [28] Amazon 3,708 2,128 1,482 98 1.03 16.59 AMT 20 0.822\nTweets (Debate) [16] Tweets DBT 3,238 730 1249 1259 1.86 14.86 AMT + Expert Undef. 0.419\nTweets (Random) [65] Tweets RND I 4,242 1,340 949 1953 1.77 15.81 Non Expert 3 0.683\nTweets (Random) [28] Tweets RND II 4,200 2,897 1,299 4 1.87 14.10 AMT 20 0.800\nTweets (Random) [44] Tweets RND III 3,771 739 488 2,536 1.54 14.32 AMT 3 0.824\nTweets (Random) [2] Tweets RND IV 500 139 119 222 1.90 15.44 Expert Undef. 0.643\nTweets (Specific domains w/ emot.) [22] Tweets STF 359 182 177 - 1.0 15.1 Non Expert Undef. 1.000\nTweets (Specific topics) [54] Tweets SAN 3737 580 654 2503 1.60 15.03 Expert 1 0.404\nTweets (Semeval2013 Task2) [43] Tweets Semeval 6,087 2,223 837 3027 1.86 20.05 AMT 5 0.617\nRunners World forum [65] RW 1,046 484 221 341 4.79 66.12 Non Expert 3 0.615\nTweets DBT was the unique dataset built with a combination\nof AMT Labeling with Expert validation [16]. They selected\n200 random tweets to be classified by experts and compared\nwith AMT results to ensure accurate ratings. We note that\nthe Tweets Semeval dataset was provided as a list of Twitter\nIDs, due to the Twitter policies related to data sharing. While\ncrawling the respective tweets, a small part of them could not\nbe accessed, as they were deleted. We plan to release all gold\nstandard datasets in a request basis, which is in agreement\nwith Twitter policies.\nIn order to assess the extent to which these datasets are trustful,\nwe used a strategy similar to the one used by Tweets DBT.\nOur goal was not to redo all the performed human evaluation,\nbut simply inspecting a small sample of them to infer the level\nof agreement with our own evaluation. We randomly select\n1% of all sentences to be evaluated by experts (two of the\nauthors) as an attempt to assess if these gold standard data are\nreally trustful. It is important to mention that we did not have\naccess to the instructions provided by the authors. We also\ncould not get access to small amount of the raw data in a few\ndatasets, which was discarded. Finally, our manual inspection\nunveiled a few sentences in idioms other than English in a few\ndatasets, such as Tweets STA and TED, which were obviously\ndiscarded.\nColumn CK from Table 3 exhibits the level of agreement of\neach dataset in our evaluation by means of Cohen’s Kappa,\nan extensively used metric to calculate inter-anotator agree-\nment. After a close look in the cases of disagreement with\nthe evaluations in the Gold standard, we realized that other\ninterpretations could be possible for the given text, finding\ncases of sentences with mixed polarity. Some of them are\nstrongly linked to original context and are very hard to evalu-\nate. Some NYT comments, for instance, are directly related to\nthe news they were inserted to. We can also note that some of\nthe datasets do not contain neutral messages. This might be\na characteristic of the data or even a result of how annotators\nwere instructed to label their pieces of text. Most of the cases\nof disagreement involve neutral messages. Thus, we conside-\nred these cases, as well as the amount of disagreement we had\nwith the gold standard data, reasonable and expected, specially\nwhen taking into account that Landis and Koch [34] suggest\nthat Kappa values between 0.4 and 0.6 indicate moderate agre-\nement and values amid 0.60 and 0.8 correspond to substantial\nagreements.\nComparison Results\nNext, we present comparison results for the twenty-four\nmethods considered in this paper based on the eighteen consi-\ndered gold standard datasets.\nExperimental details\nAt least three distinct approaches have been proposed to deal\nwith sentiment analysis of sentences. The first of them, ap-\nplied by OpinionFinder and Pattern.en, for instance, splits this\ntask into two steps: (i) identifying sentences with no senti-\nment, also named as objective vs. neutral sentences and then\n(ii) detecting the polarity (positive or negative), only for the\nsubjective sentences. Another common way to detect sentence\npolarity considers three distinct classes (positive, negative and\nneutral) in a single task, an approach used by VADER, SO-\nCAL, USent and others. Finally, some methods like SenticNet\nand LIWC, classify a sentence as positive or negative only,\nassuming that only polarized sentences are presented, given\nthe context of a given application. As an example, reviews of\nproducts are expected to contain only polarized opinion.\nAiming at providing a more thorough comparison among these\ndistinct approaches, we perform two rounds of tests. In the first\nwe consider the performance of methods to identify 3-class\n(positive, negative and neutral). The second considers only po-\nsitive and negative as output and assumes that a previous step\nof removing the neutral messages needs to be executed firstly.\nIn the 3-class experiments we used only datasets containing\na considerable number of neutral messages (which excludes\nTweets RND II, Amazon, and Reviews II). Despite being 2-\nclass methods, as highlighted in Table 2, we decided to include\nLIWC, Emoticons and SenticNet in the 3-class experiments to\npresent a full set of comparative experiments. LIWC, Emoti-\ncons, and SenticNet cannot define, for some sentences, their\npositive or negative polarity, considering it as undefined. It\noccurs due to the absence in the sentence of emoticons (in\nthe case of Emoticons method) or of words belonging to the\nmethods’ sentiment lexicon. As neutral (objective) sentences\ndo not contain sentiments, we assumed, in the case of these\n2-class methods, that sentences with undefined polarities are\nequivalent to neutral sentences.\nThe 2-class experiments, on the other hand, were performed\nwith all datasets described in Table 3 excluding the neutral\nsentences. We also included all methods in these experiments,\neven those that produce neutral outputs. As discussed before,\nwhen 2-class methods cannot detect the polarity (positive or\nnegative) of a sentences they usually assign it to an undefined\npolarity. As we know all sentences in the 2-class experiments\nare positive or negative, we create the coverage metric to\ndetermine the percentage of sentences a method can in fact\nclassify as positive or negative. For instance, suppose that\nEmoticons’ method can classify only 10% of the sentences in\na dataset, corresponding to the actual percentage of sentences\nwith emoticons. It means that the coverage of this method\nin this specific dataset is 10%. Note that, the coverage is\nquite an important metric for a more complete evaluation in\nthe 2-class experiments. Even though Emoticons presents\nhigh accuracy for the classified phrases, it was not able to\nmake a prediction for 90% of the sentences. More formally,\ncoverage is calculated as the number of total sentences minus\nthe number of undefined sentences, all of this divided by the\ntotal of sentences, where the number of undefined sentences\nincludes neutral outputs for 3-class methods.\nCoverage=\n#Sentences−#Unde f ined\n#Sentences\nComparison Metrics\nConsidering the 3-class comparison experiments, we used\nthe traditional Precision, Recall, and F1 measures for the\nautomated classification.\nPredicted\nPositive Neutral Negative\nPositive a b c\nActual Neutral d e f\nNegative g h i\nEach letter in the above table represents the number of ins-\ntances which are actually in class X and predicted as class\nY, where X;Y ∈ positive; neutral; negative. The recall (R)\nof a class X is the ratio of the number of elements correctly\nclassified as X to the number of known elements in class X .\nPrecision (P) of a class X is the ratio of the number of elements\nclassified correctly as X to the total predicted as the class X .\nFor example, the precision of the negative class is computed as:\nP(neg) = i/(c+ f + i); its recall, as: R(neg) = i/(g+h+ i);\nand the F1 measure is the harmonic mean between both preci-\nsion and recall. In this case, F1(neg) = 2P(neg)·R(neg)P(neg)+R(neg) .\nWe also compute the overall accuracy as: A =\na+e+i\na+b+c+d+e+ f+g+h+i . It considers equally important the\ncorrect classification of each sentence, independently of the\nclass, and basically measures the capability of the method\nto predict the correct output. A variation of F1, namely,\nMacro-F1, is normally reported to evaluate classification\neffectiveness on skewed datasets. Macro-F1 values are\ncomputed by first calculating F1 values for each class\nin isolation, as exemplified above for negative, and then\naveraging over all classes. Macro-F1 considers equally\nimportant the effectiveness in each class, independently of\nthe relative size of the class. Thus, accuracy and Macro-F1\nprovide complementary assessments of the classification\neffectiveness. Macro-F1 is especially important when the\nclass distribution is very skewed, to verify the capability of\nthe method to perform well in the smaller classes.\nThe described metrics can be easily computed for the 2-class\nexperiments by just removing neutral columns and rows as in\nthe table below.\nPredicted\nPositive Negative\nPositive a b\nActual Negative c d\nIn this case, the precision of positive class is computed as:\nP(pos) = a/(a+ c); its recall as: R(pos) = a/(a+b); while\nits F1 is F1(pos) = 2P(pos)·R(pos)P(pos)+R(pos)\nAs we have a large number of combinations among the base\nmethods, metrics and datasets, a global analysis of the per-\nformance of all these combinations is not an easy task. We\npropose a simple but informative measure to assess the overall\nperformance ranking. The Mean Ranking is basically the sum\nof ranks obtained by a method in each dataset divided by the\ntotal number of datasets, as below:\nMR=\nnd\n∑\nj=1\nri\nnd\nwhere nd is the number of datasets and ri is the rank of the\nmethod for dataset i. It is important to notice that the rank was\ncalculated based on Macro F1.\nThe last evaluation metric we exploit is the Friedman’s Test [6].\nIt allows one to verify whether, in a specific experiment, the\nobserved values are globally similar. We used this test to tell\nif the methods present similar performance across different\ndatasets. More specifically, suppose that k expert raters evalu-\nated n item – the question that arises is: are rates provided by\njudges consistent with each other or do they follow comple-\ntely different patterns? The application in our context is very\nsimilar: the datasets are the judges and the Macro-F1 achieved\nby a method is the rating from the judges.\nThe Friedman’s Test is applied to rankings. Then, to proceed\nwith this statistical test, we sort the methods in decreasing\norder of Macro-F1 for each dataset. More formally, the Fried-\nman’s rank test in our experiment is defined as:\nFR = (\n12\nrc(c+1)\nc\n∑\nj=1\nR2j)−3r(c+1)\nwhere\nR2j = square of the sum of rank positions of method j (j =\n1,2,..,c)\nr = number of datasets\nc = number of methods\nAs the number of datasets increases, the statistical test can be\napproximated by using the chi-square distribution with c−1\ndegrees of freedom [29]. Then, if the FR computed value is\nlarger than the critical value for the chi-square distribution the\nnull hypothesis is rejected. This null hypothesis states that\nranks obtained per dataset are globally similar. Accordingly,\nrejecting the null hypothesis means that there are significant\ndifferences in the ranks across datasets. It is important to note\nthat, in general, the critical value is obtained with significance\nlevel α = 0.05. Synthesizing, the null hypothesis should be\nrejected if FR > X2α , where X\n2\nα is the critical value verified in\nthe chi-square distribution table with c−1 degrees of freedom\nand α equals 0.05.\nComparing Prediction Performance\nWe start the analysis of our experiments by comparing the\nresults of all previously discussed metrics for all datasets. Ta-\nble 4 and Table 5 present accuracy, precision, and Macro-F1\nfor all methods considering four datasets for the 2-class and\n3-class experiments, respectively. For simplicity, we choose to\ndiscuss results only for these datasets as they come from diffe-\nrent sources and help us to illustrate the main findings from\nour analysis. Results for all the other datasets are presented in\nthe Additional File 1. There are many interesting observations\nwe can make from these results, summarized next.\nMethods prediction performance varies considerably\nfrom one dataset to another: First, we note the same so-\ncial media text can be interpreted very differently depending\non the choice of a sentiment method. Overall, we note that all\nthe methods yielded with large variations across the different\ndatasets. By analyzing Table 4 we can note that VADER works\nwell for Tweets RND II, appearing in the first place, but it\npresents poor performance in Tweets STF, Comments BBC,\nand Comments DIGG, achieving the eleventh, thirteenth and\ntenth place respectively. Although the first two datasets con-\ntain tweets, they belong to different contexts, which affects\nthe performance of some methods like VADER. Another im-\nportant aspect to be analyzed in this Table is the coverage.\nAlthough SentiStrength has presented good Macro-F1 values,\nits coverage is usually low as this method tends to classify a\nhigh number of instances as neutral. Note that some datasets\nprovided by the SentiStrength’s authors, as shown in Table 3,\nspecially the Twitter datasets, have more neutral sentences\nthan positive and negative ones. Another expected result is the\ngood Macro-F1 values obtained by Emoticons, specially in\nthe Twitter datasets. It is important to highlight that, in spite\nof achieving high accuracy and Macro-F1, the coverage of\nmany methods, such as PANAS, VADER, and SentiStrength,\nis low (e.g. below 30%) as they only infer the polarity of part\nof the input sentences.Thus, the choice of a sentiment analysis\nis highly dependent on the data and application, suggesting\nthat researchers and practitioners need to take into account\nthis tradeoff between prediction performance and coverage.\nThe same high variability regarding the methods’s prediction\nperformance can be noted for the 3-class experiments, as pre-\nsented in Table 5. Umigon, the best method in five Twitter\ndatasets, felt to the eighteenth place in the Comments NYT\ndataset. We can also note the lower Macro-F1 values for some\nmethods like Emoticons are due to the high number of senten-\nces without emoticons in the datasets. Methods like Emoticons\nDS and PANAS tend do classify only a small part of instances\nas neutral and also presented a poor performance in the 3-class\nexperiments. Methods like SenticNet and LIWC were not\noriginally developed for detecting neutral sentences and also\nachieved low values of Macro-F1. However, they also do not\nappear among the best methods in the 2-class experiments,\nwhich is the task they were originally designed for. This ob-\nservation about LIWC is not valid for the newest version, as\nLIWC15 appears among the top five methods for 2-class and\n3-class experiments (see Table 6).\nFinally, Table 7 presents the Friedman’s test results showing\nthat there are significant differences in the mean rankings\nobserved for the methods across all datasets. It statistically\nindicates that in terms of accuracy and Macro-F1 there is no\nsingle method that always achieves a consistent rank position\nfor different datasets, which is something similar to the well-\nknown “no-free lunch theorem” [74]. So, overall, before using\na sentiment analysis method in a novel dataset, it is crucial\nto test different methods in a sample of data before simply\nchoose one that is acceptable by the research community.\nTable 7: Friedman’s Test Results\n2-class experiments 3-class experiments\nFR 275.59 FR 197.52\nCritical Value 35.17 Critical Value 35.17\nReject null hypothesis Reject null hypothesis\nThis last results suggests that, even with the good insights\nprovided by this work about which methods perform better\nin each context, a preliminary investigation needs to be per-\nformed when sentiment analysis is used in a new dataset in\norder to guarantee a reasonable prediction performance. In the\ncase in which prior tests are not feasible, this benchmark pre-\nsents valuable information for researchers and companies that\nare planning to develop research and solutions on sentiment\nanalysis.\nExisting methods let space for improvements: We can note\nthat the performance of the evaluated methods are ok, but\nthere is a lot of space for improvements. For example, if\nwe look at the Macro-F1 values only for the best method on\neach dataset (See Table 4 and Table 5), we can note that the\noverall prediction performance of the methods is still low –\ni.e. Macro-F1 values are around 0.9 only for methods with\nlow coverage in the 2-class experiments and only 0.6 for the\n3-class experiment. Considering that we are looking at the\nperformance of the best methods out of 24 unsupervised tools,\nthese numbers suggest that current sentence-level sentiment\nanalysis methods still let a lot of space for improvements.\nTable 4: 2-classes experiments results with 4 datasets\nDataset Method Accur. Posit. Sentiment Negat. Sentiment MacroF1 CoverageP R F1 P R F1\nAFINN 96.37 97.66 96.94 97.30 93.75 95.19 94.47 95.88 80.77\nANEW SUB 81.36 80.52 96.38 87.74 85.44 47.64 61.17 74.45 93.35\nEmolex 86.06 89.82 89.11 89.47 78.77 80.00 79.38 84.42 63.58\nEmoticons 97.75 97.90 99.42 98.65 96.97 89.72 93.20 95.93 14.82\nEmoticons DS 71.04 70.61 99.90 82.74 95.83 5.43 10.28 46.51 99.09\nNRC Hashtag 67.37 83.76 65.43 73.47 48.17 71.69 57.62 65.55 91.94\nLIWC07 66.47 74.46 78.81 76.58 44.20 38.31 41.04 58.81 73.93\nLIWC15 96.44 97.09 98.04 97.56 94.68 92.23 93.44 95.50 77.05\nOpinion Finder 78.32 93.86 71.11 80.92 63.42 91.50 74.92 77.92 41.23\nOpinion Lexicon 93.45 97.03 93.14 95.04 86.93 94.11 90.38 92.71 70.64\nTweets PANAS-t 90.71 96.95 88.19 92.36 82.11 95.12 88.14 90.25 5.39\nRND II Pattern.en 91.76 92.94 96.19 94.54 87.86 79.06 83.23 88.88 70.85\nSASA 70.06 82.81 72.81 77.49 49.05 63.39 55.30 66.40 63.04\nSemantria 91.61 96.94 90.55 93.64 82.25 93.88 87.68 90.66 63.61\nSenticNet 73.64 90.74 68.45 78.03 55.41 84.88 67.05 72.54 82.82\nSentiment140 94.75 97.10 95.71 96.40 88.64 92.13 90.35 93.37 49.95\nSentiment140 L 78.05 88.68 78.31 83.17 61.32 77.47 68.45 75.81 93.28\nSentiStrength 96.97 98.92 96.43 97.66 93.54 98.01 95.72 96.69 34.65\nSentiWordNet 78.57 87.88 80.91 84.25 61.09 72.87 66.46 75.36 61.49\nSO-CAL 87.76 94.25 86.99 90.47 77.34 89.32 82.90 86.68 67.18\nStanford DM 60.46 94.48 44.87 60.84 44.06 94.30 60.06 60.45 88.89\nUmigon 88.63 97.73 85.92 91.45 73.64 95.17 83.03 87.24 70.83\nUSent 84.46 89.28 87.67 88.47 74.77 77.63 76.17 82.32 38.94\nVADER 99.04 99.16 99.45 99.31 98.77 98.12 98.45 98.88 94.40\nAFINN 84.42 80.62 91.49 85.71 89.66 77.04 82.87 84.29 76.88\nANEW SUB 68.05 63.08 93.18 75.23 84.62 40.74 55.00 65.11 94.15\nEmolex 79.65 76.09 88.98 82.03 85.23 69.44 76.53 79.28 62.95\nEmoticons 85.42 80.65 96.15 87.72 94.12 72.73 82.05 84.89 13.37\nEmoticons DS 51.96 51.41 100.00 67.91 100.00 2.27 4.44 36.18 99.72\nNRC Hashtag 71.30 73.05 70.93 71.98 69.51 71.70 70.59 71.28 92.20\nLIWC07 64.29 63.75 76.12 69.39 65.22 50.85 57.14 63.27 70.39\nLIWC15 89.22 84.18 97.08 90.17 96.40 81.06 88.07 89.12 74.93\nOpinion Finder 80.77 81.16 76.71 78.87 80.46 84.34 82.35 80.61 43.45\nOpinion Lexicon 86.10 83.67 91.11 87.23 89.29 80.65 84.75 85.99 72.14\nTweets PANAS-t 94.12 88.89 100.00 94.12 100.00 88.89 94.12 94.12 4.74\nSTF Pattern.en 79.55 74.86 94.48 83.54 90.12 61.34 73.00 78.27 73.54\nSASA 68.52 65.65 78.90 71.67 72.94 57.94 64.58 68.12 60.17\nSemantria 88.45 89.15 88.46 88.80 87.70 88.43 88.07 88.43 69.92\nSenticNet 70.49 71.31 63.50 67.18 69.88 76.82 73.19 70.18 80.22\nSentiment140 93.29 91.36 94.87 93.08 95.18 91.86 93.49 93.29 45.68\nSentiment140 L 79.12 81.48 76.30 78.81 76.97 82.04 79.42 79.11 94.71\nSentiStrength 95.33 95.18 96.34 95.76 95.52 94.12 94.81 95.29 41.78\nSentiWordNet 72.99 73.17 78.95 75.95 72.73 65.98 69.19 72.57 58.77\nSO-CAL 87.36 82.89 93.33 87.80 92.80 81.69 86.89 87.35 77.16\nStanford DM 66.56 87.69 36.31 51.35 61.24 95.18 74.53 62.94 89.97\nUmigon 86.99 91.73 81.88 86.52 83.02 92.31 87.42 86.97 81.34\nUSent 73.21 69.35 82.69 75.44 78.82 63.81 70.53 72.98 58.22\nVADER 84.44 80.23 92.21 85.80 90.40 76.35 82.78 84.29 84.12\nAFINN 70.94 47.01 81.82 59.72 91.17 67.05 77.27 68.49 74.81\nANEW SUB 43.25 30.98 92.31 46.39 90.13 25.46 39.71 43.05 93.73\nEmolex 61.71 34.60 75.83 47.52 88.93 57.53 69.87 58.69 67.14\nEmoticons 73.08 72.22 86.67 78.79 75.00 54.55 63.16 70.97 3.32\nEmoticons DS 28.24 27.30 100.00 42.89 100.00 1.77 3.48 23.19 98.72\nNRC Hashtag 74.69 51.01 40.64 45.24 80.80 86.48 83.54 64.39 92.97\nLIWC07 46.15 27.44 58.40 37.34 72.49 41.52 52.79 45.07 58.18\nLIWC15 70.67 49.81 90.91 64.36 94.35 62.36 75.09 69.72 62.79\nOpinion Finder 71.14 43.04 64.76 51.71 86.88 73.13 79.42 65.56 56.27\nOpinion Lexicon 71.82 47.45 86.43 61.27 93.40 66.75 77.86 69.56 69.44\nComments PANAS-t 68.00 12.50 50.00 20.00 94.12 69.57 80.00 50.00 3.20\nDigg Pattern.en 60.05 43.73 92.14 59.31 92.57 45.21 60.75 60.03 56.65\nSASA 65.54 40.26 66.91 50.27 84.82 65.06 73.64 61.95 68.29\nSemantria 82.46 62.72 88.33 73.36 94.81 80.25 86.93 80.14 56.14\nSenticNet 69.40 46.30 72.46 56.50 86.77 68.25 76.40 66.45 96.55\nSentiment140 85.06 62.50 78.95 69.77 93.65 86.76 90.08 79.92 33.38\nSentiment140 L 67.76 42.07 73.45 53.50 88.01 65.84 75.33 64.41 89.64\nSentiStrength 92.09 78.69 92.31 84.96 97.40 92.02 94.64 89.80 27.49\nSentiWordNet 62.17 36.86 77.68 50.00 88.84 57.18 69.58 59.79 58.82\nSO-CAL 76.55 52.86 77.08 62.71 90.65 76.37 82.90 72.81 71.99\nStanford DM 69.16 35.29 20.27 25.75 75.21 86.68 80.54 53.15 78.90\nUmigon 83.37 66.22 75.38 70.50 90.72 86.23 88.42 79.46 63.04\nUSent 55.98 36.06 80.65 49.83 86.67 46.80 60.78 55.31 43.86\nVADER 69.05 45.48 85.88 59.47 92.55 63.00 74.97 67.22 82.23\nAFINN 66.56 23.08 81.08 35.93 96.32 64.66 77.38 56.65 85.11\nANEW SUB 31.37 15.48 95.79 26.65 97.18 21.73 35.52 31.08 97.07\nEmolex 59.64 21.52 89.04 34.67 97.38 55.62 70.80 52.73 80.72\nEmoticons 33.33 0.00 0.00 0.00 100.00 33.33 50.00 25.00 0.40\nEmoticons DS 13.33 13.10 100.00 23.17 100.00 0.31 0.61 11.89 99.73\nNRC Hashtag 84.45 33.33 25.27 28.75 89.76 92.83 91.27 60.01 97.47\nLIWC07 50.10 15.38 58.33 24.35 88.00 48.78 62.77 43.56 69.55\nLIWC15 63.21 25.86 90.67 40.24 97.55 58.86 73.42 56.83 73.01\nOpinion Finder 74.43 21.74 62.50 32.26 94.93 75.72 84.24 58.25 76.46\nOpinion Lexicon 74.14 29.81 84.93 44.13 97.24 72.66 83.17 63.65 80.72\nComments PANAS-t 58.73 20.00 75.00 31.58 93.94 56.36 70.45 51.02 8.38\nBBC Pattern.en 41.75 19.73 93.55 32.58 96.61 32.57 48.72 40.65 54.79\nSASA 61.61 23.50 66.20 34.69 90.80 60.77 72.81 53.75 61.30\nSemantria 83.43 40.00 84.75 54.35 97.64 83.26 89.88 72.11 67.42\nSenticNet 66.07 24.44 74.16 36.77 94.24 64.83 76.81 56.79 88.96\nSentiment140 68.51 24.00 69.77 35.71 94.04 68.33 79.15 57.43 45.61\nSentiment140 L 56.85 18.52 69.15 29.21 92.35 55.03 68.97 49.09 97.07\nSentiStrength 93.93 64.29 78.26 70.59 97.72 95.54 96.61 83.60 32.85\nSentiWordNet 57.49 20.00 88.06 32.60 97.13 53.45 68.96 50.78 76.33\nSO-CAL 75.28 28.93 80.28 42.54 96.71 74.64 84.25 63.40 82.85\nStanford DM 89.45 63.16 40.91 49.66 91.81 96.52 94.11 71.88 92.02\nUmigon 79.37 39.13 61.02 47.68 92.10 82.72 87.15 67.42 50.93\nUSent 52.60 18.33 80.49 29.86 94.56 48.60 64.20 47.03 43.48\nVADER 62.76 22.68 85.54 35.86 96.75 59.60 73.76 54.81 90.69\nTable 5: 3-classes experiments results with 4 datasets\nDataset Method Accur. Posit. Sentiment Negat. Sentiment Neut. Sentiment MacroF1P R F1 P R F1 P R F1\nAFINN 62.36 61.10 70.09 65.28 44.08 55.56 49.15 71.43 58.57 64.37 59.60\nANEW SUB 39.51 38.79 96.31 55.31 43.50 23.18 30.24 57.38 2.31 4.45 30.00\nEmolex 48.74 48.15 62.71 54.47 31.27 38.59 34.55 57.90 41.30 48.21 45.74\nEmoticons 52.88 72.83 11.34 19.62 55.56 5.38 9.80 34.05 96.53 50.34 26.59\nEmoticons DS 36.59 36.55 100.00 53.53 75.00 0.36 0.71 100.00 0.03 0.07 18.10\nNRC Hashtag 36.95 42.04 75.03 53.88 24.57 56.03 34.16 53.33 3.70 6.92 31.65\nLIWC07 39.54 36.52 42.33 39.21 15.14 13.02 14.00 48.64 44.83 46.66 33.29\nLIWC15 62.56 59.77 71.03 64.91 49.04 42.65 45.62 68.90 61.84 65.18 58.57\nOpinion Finder 57.63 67.57 27.94 39.53 40.75 33.69 36.89 58.20 86.06 69.44 48.62\nOpinion Lexicon 60.37 62.09 62.71 62.40 41.19 52.81 46.28 66.41 60.75 63.46 57.38\nTweets PANAS-t 53.08 90.95 9.04 16.45 51.56 3.94 7.33 51.65 99.01 67.89 30.55\nSemeval Pattern.en 57.99 57.97 68.74 62.89 34.83 35.24 35.04 65.55 56.39 60.63 52.85\nSASA 50.63 46.34 47.77 47.04 33.07 20.31 25.17 56.39 61.12 58.66 43.62\nSemantria 61.54 67.28 57.35 61.92 39.57 52.81 45.24 65.98 67.03 66.50 57.89\nSenticNet 49.68 51.85 1.26 2.46 29.79 1.67 3.17 49.82 98.51 66.17 23.93\nSentiment140 60.42 63.87 51.37 56.94 50.96 37.87 43.45 60.35 73.31 66.20 55.53\nSentiment140 L 39.44 43.52 74.72 55.00 27.67 65.35 38.88 65.87 6.38 11.63 35.17\nSentiStrength 57.83 78.01 27.13 40.25 47.80 23.42 31.44 55.49 89.89 68.62 46.77\nSentiWordNet 48.33 55.54 53.44 54.47 19.67 37.51 25.81 61.22 47.57 53.54 44.61\nSO-CAL 58.83 58.89 59.02 58.95 40.39 54.24 46.30 39.89 59.96 47.91 51.05\nStanford DM 22.54 72.14 18.17 29.03 14.92 90.56 25.61 47.19 6.94 12.10 22.25\nUmigon 65.88 75.18 56.14 64.28 39.66 55.91 46.41 70.65 75.78 73.13 61.27\nUSent 52.13 49.86 32.88 39.63 39.96 22.82 29.05 54.33 74.36 62.79 43.82\ns VADER 60.21 56.46 79.04 65.87 44.30 59.02 50.61 76.02 46.71 57.87 58.12\nAFINN 64.41 40.81 72.12 52.13 49.67 62.50 55.35 85.95 62.54 72.40 59.96\nANEW SUB 28.03 21.89 92.29 35.38 44.30 34.22 38.61 74.82 8.18 14.74 29.58\nEmolex 54.76 31.67 59.95 41.44 40.14 47.54 43.53 77.48 54.64 64.08 49.68\nEmoticons 70.22 70.06 16.78 27.07 65.62 8.61 15.22 41.29 97.56 58.02 33.44\nEmoticons DS 20.34 19.78 99.46 33.00 62.07 3.69 6.96 53.85 0.55 1.09 13.68\nNRC Hashtag 30.47 28.25 77.40 41.39 24.18 72.54 36.27 79.08 8.77 15.78 31.15\nLIWC 46.88 21.85 38.43 27.86 19.18 18.24 18.70 69.51 54.83 61.31 35.95\nLIWC15 67.75 44.78 78.35 56.99 57.49 57.38 57.44 85.18 66.67 74.80 63.07\nOpinion Finder 71.55 57.48 32.75 41.72 49.85 34.63 40.87 75.95 89.90 82.34 54.98\nOpinion Lexicon 63.86 40.65 66.17 50.36 48.84 56.15 52.24 81.96 64.66 72.29 58.30\nTweets PANAS-t 68.79 79.49 8.39 15.18 48.57 3.48 6.50 68.75 98.86 81.10 34.26\nRND III Pattern.en 59.56 36.20 77.00 49.24 52.87 45.29 48.79 81.75 57.23 67.33 55.12\nSASA 55.37 29.42 54.53 38.22 42.46 47.34 44.77 78.30 57.15 66.08 49.69\nSemantria 68.89 48.86 63.73 55.31 49.82 55.53 52.52 82.02 72.96 77.22 61.68\nSenticNet 29.97 31.08 74.83 43.92 20.98 73.98 32.68 79.70 8.49 15.35 30.65\nSentiment140 76.40 64.42 51.69 57.36 74.75 45.49 56.56 79.04 89.50 83.94 65.95\nSentiment140 L 31.32 25.83 77.13 38.70 30.05 78.69 43.49 79.37 8.92 16.04 32.74\nSentiStrength 73.80 70.94 41.95 52.72 57.53 25.82 35.64 75.35 92.26 82.95 57.10\nSentiWordNet 55.85 37.42 58.19 45.55 24.04 35.86 28.78 79.25 59.00 67.64 47.33\nSO-CAL 66.51 43.06 68.88 52.99 51.84 60.66 55.90 45.77 66.94 54.37 54.42\nStanford DM 31.90 64.48 38.57 48.26 15.58 85.04 26.33 75.64 19.77 31.35 35.32\nUmigon 74.12 57.67 70.23 63.33 48.83 68.44 57.00 88.80 76.34 82.10 67.47\nUSent 66.06 40.60 36.81 38.61 44.87 28.69 35.00 74.54 81.72 77.97 50.53\nVADER 60.14 37.69 81.60 51.56 48.56 65.57 55.80 88.96 52.87 66.32 57.89\nAFINN 50.10 16.22 60.61 25.59 82.62 56.05 66.79 40.11 30.24 34.48 42.29\nANEW SUB 24.30 11.38 91.92 20.24 84.15 21.13 33.78 38.89 5.65 9.86 21.30\nEmolex 44.10 15.51 65.66 25.10 83.19 45.48 58.81 35.27 31.85 33.47 39.13\nEmoticons 24.60 0.00 0.00 0.00 33.33 0.15 0.30 19.77 98.79 32.95 11.09\nEmoticons DS 10.00 9.85 98.99 17.92 66.67 0.31 0.61 0.00 0.00 0.00 6.18\nNRC Hashtag 64.00 20.72 23.23 21.90 70.20 91.27 79.36 52.50 8.47 14.58 38.62\nLIWC07 33.00 11.11 42.42 17.61 67.69 33.69 44.99 22.90 27.42 24.95 29.18\nLIWC15 43.70 17.94 68.69 28.45 85.06 42.73 56.88 30.72 36.29 33.27 39.53\nOpinion Finder 51.80 14.96 35.35 21.02 78.76 60.18 68.23 33.71 36.29 34.95 41.40\nOpinion Lexicon 55.00 20.67 62.63 31.08 85.27 59.42 70.04 40.82 40.32 40.57 47.23\nComments PANAS-t 27.10 16.67 6.06 8.89 75.61 4.75 8.93 25.35 94.35 39.97 19.26\nBBC Pattern.en 28.70 14.25 58.59 22.92 82.61 17.46 28.82 25.27 46.37 32.72 28.16\nSASA 38.20 17.03 47.47 25.07 70.75 36.29 47.98 25.19 39.52 30.77 34.60\nSemantria 56.00 28.90 50.51 36.76 83.82 57.12 67.94 35.86 55.24 43.49 49.40\nSenticNet 47.10 17.74 66.67 28.03 72.87 57.58 64.33 25.89 11.69 16.11 36.16\nSentiment140 40.00 17.75 30.30 22.39 79.77 31.39 45.05 28.75 66.53 40.15 35.86\nSentiment140 L 43.10 13.32 65.66 22.15 73.84 53.60 62.11 42.11 6.45 11.19 31.82\nSentiStrength 44.20 47.37 18.18 26.28 86.64 32.77 47.56 29.37 84.68 43.61 39.15\nSentiWordNet 42.40 14.90 59.60 23.84 81.63 41.50 55.03 34.56 37.90 36.15 38.34\nSO-CAL 55.50 20.88 57.58 30.65 80.47 63.09 70.73 28.57 34.68 31.33 44.23\nStanford DM 65.50 43.37 36.36 39.56 71.01 89.28 79.10 37.50 14.52 20.93 46.53\nUmigon 45.70 28.35 36.36 31.86 76.35 41.04 53.39 29.31 61.69 39.74 41.66\nUSent 33.80 13.75 33.33 19.47 82.25 21.29 33.82 28.09 66.94 39.57 30.95\nVADER 49.40 16.36 71.72 26.64 83.02 54.67 65.93 48.53 26.61 34.38 42.31\nAFINN 42.45 64.81 41.79 50.81 80.29 39.82 53.24 7.89 77.87 14.32 39.46\nANEW SUB 51.12 48.35 88.57 62.55 79.65 24.69 37.69 7.92 9.84 8.78 36.34\nEmolex 42.97 55.12 53.72 54.41 75.35 33.33 46.22 7.22 54.10 12.74 37.79\nEmoticons 4.68 0.00 0.00 0.00 0.00 0.00 0.00 4.47 99.59 8.56 2.85\nEmoticons DS 42.58 42.55 99.77 59.66 78.57 0.40 0.80 0.00 0.00 0.00 20.15\nNRC Hashtag 54.84 55.38 45.74 50.10 61.55 65.68 63.55 8.33 15.16 10.76 41.47\nLIWC07 24.35 42.88 27.72 33.67 53.42 19.07 28.11 4.67 53.28 8.58 23.45\nLIWC15 36.49 65.29 40.29 49.83 81.50 29.25 43.05 7.17 83.61 13.20 35.36\nOpinion Finder 29.38 68.77 18.78 29.51 76.52 32.68 45.80 6.29 88.11 11.75 29.02\nOpinion Lexicon 44.57 65.95 43.15 52.17 79.81 43.11 55.98 7.94 73.77 14.34 40.83\nComments PANAS-t 5.88 69.23 1.23 2.41 62.07 1.31 2.57 4.75 99.18 9.07 4.68\nNYT Pattern.en 31.60 55.23 45.05 49.63 72.80 17.76 28.55 5.88 65.57 10.79 29.66\nSASA 30.04 49.92 30.13 37.58 59.11 27.21 37.26 5.74 61.07 10.49 28.44\nSemantria 44.59 70.60 41.83 52.54 80.54 44.24 57.11 7.53 73.36 13.65 41.10\nSenticNet 61.85 58.19 59.48 58.83 65.01 69.26 67.07 0.00 0.00 0.00 41.97\nSentiment140 13.58 77.32 6.81 12.51 75.40 11.96 20.65 4.98 93.03 9.45 14.20\nSentiment140 L 54.61 54.72 59.12 56.84 67.00 54.41 60.05 6.70 15.98 9.44 42.11\nSentiStrength 18.17 78.51 8.62 15.54 81.12 18.96 30.74 5.41 95.49 10.24 18.84\nSentiWordNet 32.20 57.35 34.53 43.10 70.31 26.95 38.97 6.08 70.08 11.19 31.09\nSO-CAL 50.79 64.36 51.13 56.99 77.25 49.16 60.08 8.68 65.98 15.34 44.14\nStanford DM 51.93 73.39 21.14 32.83 59.48 77.90 67.46 9.65 38.11 15.40 38.56\nUmigon 24.08 68.76 16.38 26.46 68.78 24.51 36.14 5.88 88.93 11.04 24.54\nUSent 27.44 56.61 28.95 38.31 77.69 21.59 33.79 5.88 79.51 10.94 27.68\nVADER 48.03 62.67 51.63 56.62 79.91 43.07 55.97 9.18 71.31 16.26 42.95\nTable 6: Mean Rank Table for All Datasets\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 VADER 4.00(4.17) 1 SentiStrength 2.33(3.00) 29.30(28.91)\n2 LIWC15 4.62 2 Sentiment140 3.44 39.29\n3 AFINN 4.69 3 Semantria 4.61 62.34\n4 Opinion Lexicon 5.00 4 Opinion Lexicon 6.72 69.50\n5 Semantria 5.31 5 LIWC15 7.33 68.28\n6 Umigon 5.77 6 SO-CAL 7.61 72.64\n7 SO-CAL 7.23 7 AFINN 8.11 73.05\n8 Pattern.en 9.92 8 VADER 9.17(9.79) 82.20(83.18)\n9 Sentiment140 10.92 9 Umigon 9.39 64.11\n10 Emolex 11.38 10 PANAS-t 10.17 5.10\n11 Opinion Finder 13.08 11 Emoticons 10.39 10.69\n12 SentiWordNet 13.38 12 Pattern.en 12.61 65.02\n13 Sentiment140 L 13.54 13 SenticNet 13.61 84.00\n14 SenticNet 13.62 14 Emolex 14.50 66.12\n15 SentiStrength 13.69(13.71) 15 Opinion Finder 14.72 46.63\n16 SASA 14.77 16 USent 14.89 44.00\n17 Stanford DM 15.85 17 Sentiment140 L 14.94 93.36\n18 USent 15.92 18 NRC Hashtag 17.17 93.52\n19 NRC Hashtag 16.31 19 Stanford DM 17.39 87.32\n20 LIWC 16.46 20 SentiWordNet 17.50 61.77\n21 ANEW SUB 18.54 21 SASA 18.94 60.12\n22 Emoticons 21.00 22 LIWC 19.67 61.82\n23 PANAS-t 21.77 23 ANEW SUB 21.17 94.20\n24 Emoticons DS 23.23 24 Emoticons DS 23.61 99.36\nAdditionally, we also noted that the best method for each\ndataset varies considerably from one dataset to another. This\nmight indicate that each method complements the others in\ndifferent ways.\nMost methods are better to classify positive than negative\nor neutral sentences: Figure 2 presents the average F1 Score\nfor the 3-class experiments. It is easier to notice that twelve out\nof twenty-four methods are more accurate while classifying\npositive than negative or neutral messages, suggesting that\nsome methods may be more biased towards positivity. Neutral\nmessages showed to be even harder to detect by most methods.\nInterestingly, recent efforts show that human language have\na universal positivity bias ([21] and [17]). Naturally, part\nof the bias is observed in sentiment prediction, an intrinsic\nproperty of some methods due to the way they are designed.\nFor instance, [26] developed a lexicon in which positive and\nnegative values are associated to words, hashtags, and any sort\nof tokens according to the frequency with which these tokens\nappear in tweets containing positive and negative emoticons.\nThis method showed to be biased towards positivity due to the\nlarger amount of positivity in the data they used to build the\nlexicon. The overall poor performance of this specific method\nis credited to its lack of treatment of neutral messages and the\nfocus on Twitter messages.\nSome methods are consistently among the best ones:\nTable 6 presents the mean rank value, detailed before, for 2-\nclass and 3-class experiments. The elements are sorted by the\noverall mean rank each method achieved based on Macro-F1\nfor all datasets. The top nine methods based on Macro-F1\nfor the 2-class experiments are: SentiStrength, Sentiment140,\nSemantria, OpinionLexicon, LIWC15, SO-CAL, AFINN and\nVADER and Umigon. With the exception of Sentistrength,\nreplaced by Pattern.en, the other eight methods produce the\nbest results across several datasets for both, 2 and 3-class tasks.\nThese methods would be preferable in situations in which any\nsort of preliminary evaluation is not possible to be done. The\nmean rank for 2-class experiments is accompanied by the cove-\nrage metric, which is very important to avoid misinterpretation\nof the results. Observe that SentiStrength and Sentiment140\nexhibited the best mean ranks for these experiments, however\nboth present very low coverage, around 30% and 40%, a very\npoor result compared with Semantria and OpinionLexicon\nthat achieved a worse mean rank (4,61 and 6,62 respectively)\nbut an expressive better coverage, above 60%. Note also that\nSentiStrength and Sentiment140 present poor results in the\n3-class experiments which can be explained by their bias to\nthe neutral class as mentioned before.\nAnother interesting finding is the fact that VADER, the best\nmethod in the 3-class experiments, did not achieve the first\nposition for none of the datasets. It reachs the second place\nfive times, the third place twice, the seventh three times, and\nthe fourth, sixth and fifth just once. It was a special case of\nconsistency across all datasets. Tables 8 and 8 present the best\nmethod for each dataset in the 2-class and 3-class experiments,\nrespectively.\nMethods are often better in the datasets they were origi-\nnally evaluated: We also note those methods perform better\nin datasets in which they were originally validated, which is\nsomewhat expected due to fine tuning procedures. We could\ndo this comparison only for SentiStrength and VADER, which\nkindly allowed the entire reproducibility of their work, sharing\nboth methods and datasets. To understand this difference, we\ncalculated the mean rank for these methods without their ‘ori-\nTable 8: Best Method for each Dataset - 2-class experiments\nDataset Method F1-Pos F1-Neg Macro-F1 Coverage\nComments BBC SentiStrength 70.59 96.61 83.60 32.85\nComments Digg SentiStrength 84.96 94.64 89.80 27.49\nComments NYT SentiStrength 70.11 86.52 78.32 17.63\nComments TED Emoticons 85.71 94.12 89.92 1.65\nComments YTB SentiStrength 96.94 89.62 93.28 38.24\nReviews I SenticNet 97.39 93.66 95.52 69.41\nReviews II SenticNet 94.15 93.87 94.01 94.25\nMyspace SentiStrength 98.73 88.46 93.6 31.53\nAmazon SentiStrength 93.85 79.38 86.62 19.58\nTweets DBT Sentiment140 72.86 83.55 78.2 18.75\nTweets RND I SentiStrength 95.28 90.6 92.94 27.13\nTweets RND II VADER 99.31 98.45 98.88 94.4\nTweets RND III Sentiment140 97.57 95.9 96.73 50.77\nTweets RND IV Emoticons 94.74 86.76 88.6 58.27\nTweets STF SentiStrength 95.76 94.81 95.29 41.78\nTweets SAN SentiStrength 90.23 88.59 89.41 29.61\nTweets Semeval SentiStrength 93.93 83.4 88.66 28.66\nRW SentiStrength 90.04 75.79 82.92 23.12\nTable 9: Best Method for each Dataset - 3-class experiments\nDataset Method F1-Pos F1-Neg F1-Neu Macro-F1\nComments BBC Semantria 36.76 67.94 43.49 49.40\nComments Digg Umigon 49.62 62.04 44.27 51.98\nComments NYT SO-CAL 56.99 60.08 15.34 44.14\nComments TED Opinion Lexicon 64.95 56.59 30.77 50.77\nComments YTB LIWC15 73.68 49.72 48.79 57.4\nMyspace LIWC15 78.83 41.74 43.76 54.78\nTweets DBT Opinion Lexicon 43.44 47.71 48.84 46.66\nTweets RND I Umigon 60.53 51.39 65.22 59.05\nTweets RND III Umigon 63.33 57.00 82.10 67.47\nTweets RND IV Umigon 75.86 76.33 71.54 74.58\nTweets SAN Umigon 44.16 45.95 70.45 53.52\nTweets Semeval Umigon 64.28 46.41 73.13 61.27\nRW Sentiment140 62.24 51.17 42.66 52.02\nFigure 2: Average F1 Score for each class.\nginal’ datasets and put the results in parenthesis. Note that, in\nsome cases the rank order changes towards a lower value but it\ndoes not imply in major changes. We also note those methods\noften perform better in datasets in which they were originally\nvalidated, which is somewhat expected due to fine tuning pro-\ncedures. We could do this comparison only for SentiStrength\nand VADER, which kindly allowed the entire reproducibility\nof their work, sharing both methods and datasets. To unders-\ntand this difference, we calculated the mean rank for these\nmethods without their ‘original’ datasets and put the results in\nparenthesis. Note that, in some cases the rank order slightly\nchanges but it does not imply in major changes. Overall, these\nobservations suggest that initiatives like SemEval are key for\nthe development of the area, as they allow methods to compete\nin a contest for a specific dataset. More important, it highlight\nthat a standard sentiment analysis benchmark is needed and\nit needs to be constantly updated. We also emphasize that is\npossible that other methods, such as paid softwares, make use\nof some of the datasets used in this benchmark to improve\ntheir performance as most of gold standard used in this work\nis available in the Web or under request to authors.\nSome methods showed to be better for specific contexts:\nIn order to better understand the prediction performance of\nmethods in types of data, we divided all datasets in three\nspecific contexts – Social Networks, Comments, and Reviews\n– and calculated mean rank of the methods for each of them.\nTable 10 presents the contexts and the respective datasets.\nTables 11, 12 and 13 present the mean rank for each context\nseparately. In the context of Social Networks the best method\nfor 3-class experiments was Umigon, followed by LIWC15\nand VADER. In the case of 2-class the winner was SentiS-\ntrength with a coverage around 30% and the third and sixth\nplace were Emoticons and Panas-t with about 18% and 6%\nof coverage, respectively. This highlights the importance to\nTable 10: Contexts’ Groups\nContext Groups\nSocial Networks\nMyspace, Tweets DBT,\nTweets RND I, Tweets RND II,\nTweets RND III, Tweets RND IV,\nTweets STF, Tweets SAN,\nTweets Semeval\nComments\nComments BBC, Comments DIGG,\nComments NYT, Comments TED,\nComments YTB, RW\nReviews Reviews I, Reviews I, Amazon\nanalyze the 2-class results together with the coverage. Ove-\nrall, when there is an emoticon on the text or a word from the\npsychometric scale PANAS, these methods are able to tell the\npolarity of the sentences, but they are not able to identify the\npolarity of the input text for the large majority of the input\ntext. Recent efforts suggest these properties are useful for\ncombination of methods [24]. Sentiment140, LIWC15, Se-\nmantria, OpinionLexicon and Umigon showed to be the best\nalternatives for detecting only positive and negative polarities\nin social network data due to the high coverage and prediction\nperformance. It is important to highlight that LIWC 2007 ap-\npears on the 16th and 21th position for the 3-class and 2-class\nmean rank results for the social network datasets and it is a\nvery popular method in this community. On the other side,\nthe newest version of LIWC (2015) presented a considerable\nevolution obtaining the second and the fourth place in the same\ndatasets.\nSimilar analyses can be performed for the contexts Comments\nand Reviews. Sentistregth, VADER, Semantria, AFINN, and\nOpinion Lexicon showed to be the best alternatives for 2-class\nTable 11: Mean Rank Table for Datasets of Social Networks\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 Umigon 2.57 1 SentiStrength 2.22(2.57) 31.54(32.18)\n2 LIWC15 3.29 2 Sentiment140 3.00 46.98\n3 VADER 4.57(4.57) 3 Emoticons 5.11 18.04\n4 AFINN 5.00 4 LIWC15 5.67 71.73\n5 Opinion Lexicon 5.57 5 Semantria 5.89 61.98\n6 Semantria 6.00 6 PANAS-t 6.33 5.87\n7 Sentiment140 7.00 7 Opinion Lexicon 7.56 66.56\n8 Pattern.en 7.57 8 Umigon 8.00 71.67\n9 SO-CAL 9.00 9 AFINN 8.67 73.37\n10 Emolex 12.29 10 SO-CAL 8.78 67.81\n11 SentiStrength 12.43(11.60) 11 VADER 8.78(9.75) 83.29(81.90)\n12 Opinion Finder 13.00 12 Pattern.en 11.22 69.47\n13 SentiWordNet 13.57 13 Sentiment140 L 14.00 94.61\n14 SenticNet 14.14 14 Opinion Finder 14.33 39.58\n15 SASA 14.86 15 Emolex 14.56 62.63\n16 LIWC 15.43 16 USent 15.22 38.60\n17 Sentiment140 L 15.43 17 SenticNet 17.22 75.46\n18 USent 16.00 18 SentiWordNet 18.44 61.41\n19 ANEW SUB 19.14 19 NRC Hashtag 19.11 94.20\n20 Emoticons 19.14 20 SASA 19.44 58.57\n21 Stanford DM 19.43 21 LIWC 19.56 61.24\n22 NRC Hashtag 20.00 22 ANEW SUB 20.56 93.51\n23 PANAS-t 20.86 23 Stanford DM 22.56 89.06\n24 Emoticons DS 23.71 24 Emoticons DS 23.78 99.28\nTable 12: Mean Rank Table for Datasets of Comments\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 VADER 3.33(3.60) 1 SentiStrength 1.17(1.50) 28.29(24.02)\n2 AFINN 4.33 2 Semantria 2.83 61.02\n3 Opinion Lexicon 4.33 3 Sentiment140 4.17 36.49\n4 Semantria 4.50 4 Opinion Lexicon 6.50 71.59\n5 SO-CAL 5.17 5 LIWC15 6.67 65.80\n6 LIWC15 6.17 6 AFINN 7.00 74.21\n7 Umigon 9.50 7 SO-CAL 7.50 74.59\n8 Emolex 10.33 8 VADER 9.50(9.60) 81.98(85.34)\n9 Sentiment140 L 11.33 9 Umigon 10.50 57.87\n10 Stanford DM 11.67 10 Emoticons 11.83 4.99\n11 NRC Hashtag 12.00 11 Opinion Finder 13.00 55.66\n12 Pattern.en 12.67 12 SenticNet 13.00 95.28\n13 SenticNet 13.00 13 USent 14.00 45.66\n14 Opinion Finder 13.17 14 NRC Hashtag 14.67 93.43\n15 SentiWordNet 13.17 15 Emolex 15.00 69.69\n16 SASA 14.67 16 PANAS-t 15.50 5.10\n17 SentiStrength 15.17(19.00) 17 Stanford DM 15.67 84.43\n18 Sentiment140 15.50 18 Pattern.en 15.83 59.00\n19 USent 15.83 19 Sentiment140 L 15.83 92.30\n20 LIWC 17.67 20 SentiWordNet 17.00 63.32\n21 ANEW SUB 17.83 21 SASA 17.50 61.91\n22 Emoticons DS 22.67 22 LIWC 19.67 62.24\n23 PANAS-t 22.83 23 ANEW SUB 22.00 94.31\n24 Emoticons 23.17 24 Emoticons DS 23.67 99.31\nTable 13: Mean Rank Table for Datasets of Reviews\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 - - 1 Sentiment140 3.33 21.82\n2 - - 2 SenticNet 4.00 87.05\n3 - - 3 Semantria 4.33 66.04\n4 - - 4 SO-CAL 4.33 83.20\n5 - - 5 Opinion Lexicon 4.67 74.14\n6 - - 6 SentiStrength 5.00(5.00) 24.56(24.56)\n7 - - 7 Stanford DM 5.33 87.89\n8 - - 8 AFINN 8.67 69.77\n9 - - 9 VADER 9.67(11.00) 79.39(82.70)\n10 - - 10 Pattern.en 10.33 63.70\n11 - - 11 PANAS-t 11.00 2.80\n12 - - 12 Umigon 11.33 53.90\n13 - - 13 Emolex 13.33 69.47\n14 - - 14 LIWC15 13.67 62.90\n15 - - 15 USent 15.67 56.85\n16 - - 16 SentiWordNet 15.67 59.73\n17 - - 17 Sentiment140 L 16.00 91.71\n18 - - 18 NRC Hashtag 16.33 91.64\n19 - - 19 Opinion Finder 19.33 49.73\n20 - - 20 LIWC 20.00 62.75\n21 - - 21 SASA 20.33 61.22\n22 - - 22 ANEW SUB 21.33 96.05\n23 - - 23 Emoticons DS 23.00 99.71\n24 - - 24 Emoticons 23.33 0.04\nand 3-class experiments on datasets of comments whereas\nSentiment140, SenticNet, Semantria and SO-CAL showed\nto be the best for the 2-class experiments for the datasets\ncontaining short reviews. Note that for the last one, the 3-\nclass experiments have no results since datasets containing\nreviews have no neutral sentences nor a representative number\nof sentences without subjectivity.\nWe also calculated the Friedman’s value for each of these speci-\nfic contexts. Even after grouping the datasets, we still observe\nthat there are significant differences in the observed ranks\nacross the datasets. Although the values obtained for each\ncontext were quite smaller than Friedman’ global value, they\nare still above the critical value. Table 14 presents the results\nof Friedman’s test for the individual contexts in both experi-\nments, 2 and 3-class. Recall that for the 3-class experiments,\ndatasets with no neutral sentences or with an unrepresentative\nnumber of neutral sentences were not considered. For this rea-\nson, Friedman’s results for 3-class experiments in the Reviews\ncontext presents no values.\nConcluding Remarks\nRecent efforts to analyze the moods embedded in Web 2.0 con-\ntent have adopted various sentiment analysis methods, which\nwere originally developed in linguistics and psychology. Seve-\nral of these methods became widely used in their knowledge\nfields and have now been applied as tools to quantify mo-\nods in the context of unstructured short messages in online\nsocial networks. In this article, we present a thorough compa-\nrison of twenty-four popular sentence-level sentiment analysis\nmethods using gold standard datasets that span different types\nof data sources. Our effort quantifies the prediction perfor-\nmance of the twenty-four popular sentiment analysis methods\nacross eighteen datasets for two tasks: differentiating two clas-\nTable 14: Friedman’s Test Results per Contexts\nContext: Social Networks\n2-class experiments 3-class experiments\nFR 175.94 FR 124.16\nCritical Value 35.17 Critical Value 35.17\nReject null hypothesis Reject null hypothesis\nContext: Comments\n2-class experiments 3-class experiments\nFR 95.59 FR 96.41\nCritical Value 35.17 Critical Value 35.17\nReject null hypothesis Reject null hypothesis\nContext: Reviews\n2-class experiments 3-class experiments\nFR 60.52 FR -\nCritical Value 35.17 Critical Value -\nReject null hypothesis Reject null hypothesis\nses (positive and negative) and three classes (positive, negative,\nand neutral).\nAmong many findings, we highlight that although our results\nidentified a few methods able to appear among the best ones for\ndifferent datasets, we noted that the overall prediction perfor-\nmance still left a lot of space for improvements. More impor-\ntant, we show that the prediction performance of methods vary\nlargely across datasets. For example, LIWC 2007, is among\nthe most popular sentiment methods in the social network\ncontext and obtained a bad rank position in comparison with\nother datasets. This suggests that sentiment analysis methods\ncannot be used as “off-the-shelf” methods, specially for novel\ndatasets. We show that the same social media text can be inter-\npreted very differently depending on the choice of a sentiment\nmethod, suggesting that it is important that researchers and\ncompanies perform experiments with different methods before\napplying a method.\nAs a final contribution we open the datasets and codes used in\nthis paper for the research community. We also incorporated\nthem in a Web service from our research team called iFeel [4]\nthat allow users to easily compare the results of various senti-\nment analysis methods. We hope our effort can not only help\nresearchers and practitioners to compare a wide range of senti-\nment analysis techniques, but also help fostering new relevant\nresearch in this area with a rigorous scientific approach.\nREFERENCES\n1. Ahmed Abbasi, Ammar Hassan, and Milan Dhar. 2014.\nBenchmarking Twitter Sentiment Analysis Tools. In\nProceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC’14) (26-31),\nNicoletta Calzolari (Conference Chair), Khalid Choukri,\nThierry Declerck, Hrafn Loftsson, Bente Maegaard,\nJoseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios\nPiperidis (Eds.). European Language Resources\nAssociation (ELRA), Reykjavik, Iceland.\n2. Fotis Aisopos. 2014. Manually Annotated Sentiment\nAnalysis Twitter Dataset NTUA. (2014).\nwww.grid.ece.ntua.gr.\n3. Marc Brysbaert Amy Beth Warriner, Victor Kuperman.\n2013. Norms of valence, arousal, and dominance for\n13,915 English lemmas. Behavior research methods 45, 4\n(Dec. 2013), 1191–1207.\n4. Matheus Araujo, Joa˜o P. Diniz, Lucas Bastos, Elias\nSoares, Manoel Ju´nior, Miller Ferreira, Filipe Ribeiro,\nand Fabrı´cio Benevenuto. 2016. iFeel 2.0: A Multilingual\nBenchmarking System for Sentence-Level Sentiment\nAnalysis. In Proceedings of the International AAAI\nConference on Web-Blogs and Social Media. Cologne,\nGermany.\n5. Stefano Baccianella, Andrea Esuli, and Fabrizio\nSebastiani. 2010. SentiWordNet 3.0: An Enhanced\nLexical Resource for Sentiment Analysis and Opinion\nMining.. In LREC (2010-06-02), Nicoletta Calzolari,\nKhalid Choukri, Bente Maegaard, Joseph Mariani, Jan\nOdijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias\n(Eds.).\n6. Mark L. Berenson, David M. Levine, and Kathryn A.\nSzabat. 2014. Basic Business Statistics - Concepts and\nApplications (13 ed.). Pearson, USA. 840 pages.\n7. Celeste Biever. 2010. Twitter mood maps reveal\nemotional states of America. The New Scientist 207\n(2010). Issue 2771.\n8. Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2010.\nTwitter Mood Predicts the Stock Market. CoRR\nabs/1010.3003 (2010).\n9. Johan Bollen, Alberto Pepe, and Huina Mao. 2009.\nModeling Public Mood and Emotion: Twitter Sentiment\nand Socio-Economic Phenomena. CoRR abs/0911.1583\n(2009).\n10. M. M. Bradley and P. J. Lang. 1999. Affective norms for\nEnglish words (ANEW): Stimuli, instruction manual, and\naffective ratings. Technical Report. Center for Research\nin Psychophysiology, University of Florida, Gainesville,\nFlorida.\n11. Marc Brysbaert and Boris New. 2009. Moving beyond\nKucera and Francis: A Critical Evaluation of Current\nWord Frequency Norms and the Introduction of a New\nand Improved Word Frequency Measure for American\nEnglish. Behavior research methods 41, 4 (2009),\n977–990.\n12. E. Cambria, D. Olsher, and D. Rajagopal. 2014. Senticnet\n3: A common and common-sense knowledge base for\ncognition-driven sentiment analysis. In AAAI. Quebec\nCity, 1515–1521.\n13. Erik Cambria, Robert Speer, Catherine Havasi, and Amir\nHussain. 2010. SenticNet: A Publicly Available Semantic\nResource for Opinion Mining. In AAAI Fall Symposium\nSeries.\n14. Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto,\nand Krishna P. Gummadi. 2010. Measuring User\nInfluence in Twitter: The Million Follower Fallacy. In\nInternational AAAI Conference on Weblogs and Social\nMedia (ICWSM).\n15. Tom De Smedt and Walter Daelemans. 2012. Pattern for\npython. The Journal of Machine Learning Research 13, 1\n(2012), 2063–2067.\n16. N.A. Diakopoulos and D.A. Shamma. 2010.\nCharacterizing debate performance via aggregated twitter\nsentiment. In Proceedings of the 28th international\nconference on Human factors in computing systems.\nACM, 1195–1198.\n17. Peter Sheridan Dodds, Eric M. Clark, Suma Desu,\nMorgan R. Frank, Andrew J. Reagan, Jake Ryland\nWilliams, Lewis Mitchell, Kameron Decker Harris,\nIsabel M. Kloumann, James P. Bagrow, Karine\nMegerdoomian, Matthew T. McMahon, Brian F. Tivnan,\nand Christopher M. Danforth. 2015. Human language\nreveals a universal positivity bias. Proceedings of the\nNational Academy of Sciences 112, 8 (2015), 2389–2394.\nDOI:http://dx.doi.org/10.1073/pnas.1411678112\n18. Peter Sheridan Dodds and Christopher M Danforth. 2009.\nMeasuring the happiness of large-scale written\nexpression: songs, blogs, and presidents. Journal of\nHappiness Studies 11, 4 (2009), 441–456. DOI:\nhttp://dx.doi.org/10.1007/s10902-009-9150-9\n19. Esuli and Sebastiani. 2006. SentiWordNet: A Publicly\nAvailable Lexical Resource for Opinion Mining. In\nInternational Conference on Language Resources and\nEvaluation (LREC). 417–422.\n20. Ronen Feldman. 2013. Techniques and Applications for\nSentiment Analysis. Commun. ACM 56, 4 (April 2013),\n82–89. DOI:http://dx.doi.org/10.1145/2436256.2436274\n21. David Garcia, Antonios Garas, and Frank Schweitzer.\n2012. Positive words carry less information than negative\nwords. EPJ Data Science 1, 1 (2012), 1–12.\n22. Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter\nSentiment Classification using Distant Supervision.\nProcessing - (2009), 1–6.\n23. Namrata Godbole, Manjunath Srinivasaiah, and Steven\nSkiena. 2007. Large-Scale Sentiment Analysis for News\nand Blogs. In Proceedings of the International\nConference on Weblogs and Social Media (ICWSM).\n24. Pollyanna Gonc¸alves, Matheus Araujo, Fabrı´cio\nBenevenuto, and Meeyoung Cha. 2013a. Comparing and\nCombining Sentiment Analysis Methods. In Proceedings\nof the 1st ACM Conference on Online Social Networks\n(COSN’13). 12.\n25. Pollyanna Gonc¸alves, Fabrı´cio Benevenuto, and\nMeeyoung Cha. 2013b. PANAS-t: A Pychometric Scale\nfor Measuring Sentiments on Twitter. abs/1308.1857v1\n(2013).\n26. Aniko Hannak, Eric Anderson, Lisa Feldman Barrett,\nSune Lehmann, Alan Mislove, and Mirek Riedewald.\n2012. Tweetin’ in the Rain: Exploring societal-scale\neffects of weather on mood. In Int’l AAAI Conference on\nWeblogs and Social Media (ICWSM).\n27. Minqing Hu and Bing Liu. 2004. Mining and\nsummarizing customer reviews (KDD ’04). 168–177.\nhttp://doi.acm.org/10.1145/1014052.1014073\n28. CJ Hutto and Eric Gilbert. 2014. Vader: A parsimonious\nrule-based model for sentiment analysis of social media\ntext. In Eighth International AAAI Conference on\nWeblogs and Social Media (ICWSM).\n29. Raj Jain. 1991. The art of computer systems performance\nanalysis - techniques for experimental design,\nmeasurement, simulation, and modeling. (1 ed.). Wiley,\nCanada. 1–685 pages.\n30. Rie Johnson and Tong Zhang. 2015. Effective Use of\nWord Order for Text Categorization with Convolutional\nNeural Networks. In NAACL HLT 2015, The 2015\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Denver, Colorado, USA, May 31\n- June 5, 2015. 103–112.\n31. Nal Kalchbrenner, Edward Grefenstette, and Phil\nBlunsom. 2014. A Convolutional Neural Network for\nModelling Sentences. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Linguistics.\n32. Efthymios Kouloumpis, Theresa Wilson, and Johanna\nMoore. 2011. Twitter Sentiment Analysis: The Good the\nBad and the OMG!. In Int’l AAAI Conference on Weblogs\nand Social Media (ICWSM).\n33. Adam D I Kramer, Jamie E Guillory, and Jeffrey T\nHancock. 2014. Experimental evidence of massive-scale\nemotional contagion through social networks.\nProceedings of the National Academy of Sciences of the\nUnited States of America 111, 24 (June 2014), 8788–90.\nDOI:http://dx.doi.org/10.1073/pnas.1320040111\n34. J. Richard Landis and Gary G. Koch. 1977. The\nMeasurement of Observer Agreement for Categorical\nData. Biometrics 33, 1 (1977).\n35. Clement Levallois. 2013. Umigon: sentiment analysis for\ntweets based on terms lists and heuristics. In Second Joint\nConference on Lexical and Computational Semantics\n(*SEM), Volume 2: Proceedings of the Seventh\nInternational Workshop on Semantic Evaluation\n(SemEval 2013). Association for Computational\nLinguistics, Atlanta, Georgia, USA, 414–417.\nhttp://www.aclweb.org/anthology/S13-2068\n36. Lexalytics. 2015. Sentiment Extraction - Measuring the\nEmotional Tone of Content. Technical Report. Lexalytics.\n37. Bing Liu. 2012. Sentiment Analysis and Opinion Mining.\nSynthesis Lectures on Human Language Technologies 5, 1\n(May 2012), 1–167. DOI:\nhttp://dx.doi.org/10.2200/s00416ed1v01y201204hlt016\n38. George A. Miller. 1995. WordNet: a lexical database for\nEnglish. Commun. ACM 38, 11 (1995), 39–41.\n39. Saif Mohammad. 2012. #Emotional Tweets. In The First\nJoint Conference on Lexical and Computational\nSemantics - Volume 1: Proceedings of the main\nconference and the shared task, and Volume 2:\nProceedings of the Sixth International Workshop on\nSemantic Evaluation (SemEval 2012). Association for\nComputational Linguistics, Montre´al, Canada, 246–255.\nhttp://www.aclweb.org/anthology/S12-1033\n40. Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.\nGenerating High-coverage Semantic Orientation\nLexicons from Overtly Marked Words and a Thesaurus.\nIn Proceedings of the 2009 Conference on Empirical\nMethods in Natural Language Processing: Volume 2 -\nVolume 2 (EMNLP ’09). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 599–608.\nhttp://dl.acm.org/citation.cfm?id=1699571.1699591\n41. Saif Mohammad and Peter D. Turney. 2013.\nCrowdsourcing a Word-Emotion Association Lexicon.\nComputational Intelligence 29, 3 (2013), 436–465.\n42. Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan\nZhu. 2013. NRC-Canada: Building the State-of-the-Art\nin Sentiment Analysis of Tweets. In Proceedings of the\nseventh international workshop on Semantic Evaluation\nExercises (SemEval-2013). Atlanta, Georgia, USA.\n43. Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara\nRosenthal, Veselin Stoyanov, and Theresa Wilson. 2013.\nSemEval-2013 Task 2: Sentiment Analysis in Twitter.\n(2013).\n44. Sascha Narr, Michael Hu¨lfenhaus, and Sahin Albayrak.\n2012. Language-independent Twitter sentiment analysis.\nKnowledge Discovery and Machine Learning (KDML)\n(2012), 12–14.\n45. Finn rup Nielsen. 2011. A new ANEW: Evaluation of a\nword list for sentiment analysis in microblogs. arXiv\npreprint arXiv:1103.2903 (2011).\n46. Nuno Oliveira, Paulo Cortez, and Nelson Areal. 2013. On\nthe Predictability of Stock Market Behavior Using\nStockTwits Sentiment and Posting Volume. 8154 (2013),\n355–365.\n47. Bo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. In In Proceedings of the ACL.\n271–278.\n48. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up?: sentiment classification using\nmachine learning techniques. In ACL Conference on\nEmpirical Methods in Natural Language Processing.\n79–86.\n49. Nikolaos Pappas, Georgios Katsimpras, and Efstathios\nStamatatos. 2013. Distinguishing the Popularity Between\nTopics: A System for Up-to-date Opinion Retrieval and\nMining in the Web. In 14th International Conference on\nIntelligent Text Processing and Computational\nLinguistics.\n50. Nikolaos Pappas and Andrei Popescu-Belis. 2013.\nSentiment analysis of user comments for one-class\ncollaborative filtering over TED talks. In Proceedings of\nthe 36th international ACM SIGIR conference on\nResearch and development in information retrieval. ACM,\n773–776.\n51. R. Plutchik. 1980. A general psychoevolutionary theory\nof emotion. Academic press, New York, 3–33.\n52. Julio Reis, Fabricio Benevenuto, Pedro Vaz de Melo,\nRaquel Prates, Haewoon Kwak, and Jisun An. 2015.\nBreaking the News: First Impressions Matter on Online\nNews. In Proceedings of the 9th International AAAI\nConference on Web-Blogs and Social Media (ICWSM).\n53. Julio Reis, Pollyanna Goncalves, Pedro Vaz de Melo,\nRaquel Prates, and Fabricio Benevenuto. 2014. Magnet\nNews: You Choose the Polarity of What you Read. In\nInternational AAAI Conference on Web-Blogs and Social\nMedia.\n54. Niek Sanders. 2011. Twitter Sentiment Corpus by Niek\nSanders. (2011).\nhttp://www.sananalytics.com/lab/twitter-sentiment/.\n55. Rion Snow, Brendan O’Connor, Daniel Jurafsky, and\nAndrew Y. Ng. 2008. Cheap and Fast—but is It Good?:\nEvaluating Non-expert Annotations for Natural Language\nTasks. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP ’08).\n56. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,\nChristopher D. Manning, Andrew Y. Ng, and Christopher\nPotts. 2013. Recursive Deep Models for Semantic\nCompositionality Over a Sentiment Treebank. In 2013\nConference on Empirical Methods in Natural Language\nProcessing. 1631–1642.\n57. Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,\nand Daniel M. Ogilvie. 1966. The General Inquirer: A\nComputer Approach to Content Analysis. MIT Press,\nUSA.\n58. Carlo Strapparava and Rada Mihalcea. 2007.\nSemEval-2007 Task 14: Affective Text. In Proceedings of\nthe 4th International Workshop on Semantic Evaluations\n(SemEval ’07). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 70–74.\nhttp://dl.acm.org/citation.cfm?id=1621474.1621487\n59. Maite Taboada, Caroline Anthony, and Kimberly Voll.\n2006a. Methods for Creating Semantic Orientation\nDictionaries. In Conference on Language Resources and\nEvaluation (LREC). 427–432.\n60. Maite Taboada, Caroline Anthony, and Kimberly Voll.\n2006b. Methods for Creating Semantic Orientation\nDictionaries. In Conference on Language Resources and\nEvaluation (LREC). 427–432.\n61. Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly\nVoll, and Manfred Stede. 2011. Lexicon-based Methods\nfor Sentiment Analysis. Comput. Linguist. 37, 2 (June\n2011), 267–307.\n62. Acar Tamersoy, Munmun De Choudhury, and\nDuen Horng Chau. 2015. Characterizing Smoking and\nDrinking Abstinence from Social Media. In Proceedings\nof the 26th ACM Conference on Hypertext and Social\nMedia (HT).\n63. Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,\nand Bing Qin. 2014. Learning Sentiment-Specific Word\nEmbedding for Twitter Sentiment Classification. In\nProceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2014,\nJune 22-27, 2014, Baltimore, MD, USA, Volume 1: Long\nPapers. 1555–1565.\n64. Yla R. Tausczik and James W. Pennebaker. 2010. The\nPsychological Meaning of Words: LIWC and\nComputerized Text Analysis Methods. Journal of\nLanguage and Social Psychology 29, 1 (2010), 24–54.\n65. Mike Thelwall. 2013. Heart and soul: Sentiment strength\ndetection in the social web with SentiStrength. (2013).\nhttp://sentistrength.wlv.ac.uk/documentation/\nSentiStrengthChapter.pdf.\n66. Mikalai Tsytsarau and Themis Palpanas. 2012. Survey on\nMining Subjective Data on the Web. Data Min. Knowl.\nDiscov. 24, 3 (May 2012), 478–514. DOI:\nhttp://dx.doi.org/10.1007/s10618-011-0238-6\n67. Andranik Tumasjan, Timm O. Sprenger, Philipp G.\nSandner, and Isabell M. Welpe. 2010. Predicting\nElections with Twitter: What 140 Characters Reveal\nabout Political Sentiment. In International AAAI\nConference on Weblogs and Social Media (ICWSM).\n68. Ro Valitutti. 2004. WordNet-Affect: an Affective\nExtension of WordNet. In In Proceedings of the 4th\nInternational Conference on Language Resources and\nEvaluation. 1083–1086.\n69. Hao Wang, Dogan Can, Abe Kazemzadeh, Franc¸ois Bar,\nand Shrikanth Narayanan. 2012. A system for real-time\nTwitter sentiment analysis of 2012 U.S. presidential\nelection cycle. In ACL System Demonstrations. 115–120.\n70. D. Watson and L. Clark. 1985. Development and\nvalidation of brief measures of positive and negative\naffect: the PANAS scales. Journal of Personality and\nSocial Psychology 54, 1 (1985), 1063–1070.\n71. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating Expressions of Opinions and Emotions in\nLanguage. Language Resources and Evaluation 1, 2\n(2005), 0. http://www.cs.pitt.edu/˜\n72. Theresa Wilson, Paul Hoffmann, Swapna Somasundaran,\nJason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,\nEllen Riloff, and Siddharth Patwardhan. 2005a.\nOpinionFinder: a system for subjectivity analysis. In\nHLT/EMNLP on Interactive Demonstrations. 34–35.\n73. Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005b. Recognizing Contextual Polarity in Phrase-Level\nSentiment Analysis. In ACL Conference on Empirical\nMethods in Natural Language Processing. 347–354.\n74. David H. Wolpert and William G. Macready. 1997. No\nfree lunch theorems for optimization. IEEE Transactions\non Evlutionary Computation 1, 1 (1997), 67–82.\n",
      "id": 24749806,
      "identifiers": [
        {
          "identifier": "oai:repositorio.ufop.br:123456789/9265",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1512.01818",
          "type": "OAI_ID"
        },
        {
          "identifier": "194925623",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.1140/epjds/s13688-016-0085-1",
          "type": "DOI"
        },
        {
          "identifier": "42661665",
          "type": "CORE_ID"
        },
        {
          "identifier": "1512.01818",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "2471350540",
          "type": "MAG_ID"
        },
        {
          "identifier": "478016225",
          "type": "CORE_ID"
        },
        {
          "identifier": "646587179",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:localhost:123456789/9265",
          "type": "OAI_ID"
        },
        {
          "identifier": "202415923",
          "type": "CORE_ID"
        }
      ],
      "title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment\n  analysis methods",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:repositorio.ufop.br:123456789/9265",
        "oai:localhost:123456789/9265",
        "oai:arxiv.org:1512.01818"
      ],
      "publishedDate": "2016-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "file:///data/remote/core/dit/data/Springer-OA/pdf/43c/aHR0cDovL2xpbmsuc3ByaW5nZXIuY29tLzEwLjExNDAvZXBqZHMvczEzNjg4LTAxNi0wMDg1LTEucGRm.pdf",
        "http://www.repositorio.ufop.br/jspui/bitstream/123456789/9265/1/ARTIGO_SentiBenchBencmark.pdf",
        "http://arxiv.org/abs/1512.01818"
      ],
      "updatedDate": "2025-06-14T06:02:30",
      "yearPublished": 2016,
      "journals": [
        {
          "title": "EPJ Data Science",
          "identifiers": [
            "issn:2193-1127",
            "2193-1127"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/478016225.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/478016225"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/478016225/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/478016225/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24749806"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Bagheri, A"
        },
        {
          "name": "de Jong, F"
        },
        {
          "name": "Saraee, MH"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Human Media Interaction",
        "The Pennsylvania State University CiteSeerX Archives",
        "Faculty of Electrical Engineering, Mathematics & Computer Science",
        "Pervasive Systems"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/572108859",
        "https://api.core.ac.uk/v3/outputs/92026839"
      ],
      "createdDate": "2013-09-18T15:15:22",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 130,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/130",
          "logo": "https://api.core.ac.uk/data-providers/130/logo"
        },
        {
          "id": 708,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/708",
          "logo": "https://api.core.ac.uk/data-providers/708/logo"
        },
        {
          "id": 363,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/363",
          "logo": "https://api.core.ac.uk/data-providers/363/logo"
        }
      ],
      "depositedDate": "2013-09-06T13:52:00",
      "abstract": "In recent years probabilistic topic models have gained tremendous attention in data mining and natural language processing research areas. In the field of information retrieval for text mining, a variety of probabilistic topic models have been used to analyse content of documents. A topic model is a generative model for documents, it specifies a probabilistic procedure by which documents can be generated. All topic models share the idea that documents are mixture of topics, where a topic is a probability distribution over words. In this paper we describe Latent Dirichlet Markov Allocation Model (LDMA), a new generative probabilistic topic model, based on Latent Dirichlet Allocation (LDA) and Hidden Markov Model (HMM), which emphasizes on extracting multi-word topics from text data. LDMA is a four-level hierarchical Bayesian model where topics are associated with documents, words are associated with topics and topics in the model can be presented with single- or multi-word terms. To evaluate performance of LDMA, we report results in the field of aspect detection in sentiment analysis, comparing to the basic LDA model",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/16412155.pdf",
      "fieldOfStudy": null,
      "fullText": "Latent Dirichlet Markov Allocation for Sentiment Analysis \nAyoub Bagheri Mohamad Saraee \n  \nIsfahan University of Technology, Isfahan, Iran \nUniversity of Salford, Manchester, \nUK \nIntelligent Database, Data Mining and Bioinformatics \nLab, Electrical and Computer Engineering Department \nSchool of Computing, Science and \nEngineering \na.bagheri@ec.iut.ac.ir m.saraee@salford.ac.uk \n  \n  \nFranciska de Jong \n \nUniversity of Twente, Enschede, Netherlands \nHuman Media Interaction, P.O. Box 217, 7500 AE \nf.m.g.dejong@utwente.nl \n \n \nAbstract. In recent years probabilistic topic models have gained tremendous attention in \ndata mining and natural language processing research areas. In the field of information \nretrieval for text mining, a variety of probabilistic topic models have been used to ana-\nlyse content of documents. A topic model is a generative model for documents, it speci-\nfies a probabilistic procedure by which documents can be generated. All topic models \nshare the idea that documents are mixture of topics, where a topic is a probability distri-\nbution over words. In this paper we describe Latent Dirichlet Markov Allocation Model \n(LDMA), a new generative probabilistic topic model, based on Latent Dirichlet Alloca-\ntion (LDA) and Hidden Markov Model (HMM), which emphasizes on extracting multi-\nword topics from text data. LDMA is a four-level hierarchical Bayesian model where \ntopics are associated with documents, words are associated with topics and topics in the \nmodel can be presented with single- or multi-word terms. To evaluate performance of \nLDMA, we report results in the field of aspect detection in sentiment analysis, comparing \nto the basic LDA model. \nKeywords: topic model, latent Dirichlet allocation (LDA), hidden markov model (HMM), Latent \nDirichlet Markov Allocation (LDMA), sentiment analysis. \n1 INTRODUCTION \nWith the explosion of web 2.0, user generated content in online reviews present a wealth of infor-\nmation that can be very helpful for manufactories, companies and other customers. Mining these \nonline reviews to extract and summarize users’ opinions is a challenging task in the field of data min-\ning and natural language processing. One main task in sentiment review analysis is to find aspects that \nusers evaluate in their reviews. Aspects are topics on which opinion are expressed about. In the field \nof sentiment analysis, other names for aspect are: features, product features or opinion targets [1-10]. \nAspects are important because without knowing them, the opinions expressed in a sentence or a re-\nview are of limited use. For example, in the review sentence “after using iPhone, I found the size to be \nperfect for carrying in a pocket”, “size” is the aspect for which an opinion is expressed. Likewise as-\npect detection is critical to sentiment analysis, because its effectiveness dramatically affects the per-\nformance of opinion word detection and sentiment orientation identification. Therefore, in this study \nwe concentrate on aspect detection for sentiment analysis. \nDifferent approaches have been proposed for aspect detection from reviews. Previous works like \ndouble propagation [4] and supervised learning methods have the limitation that they do not group \nsemantically related aspect expressions together [2]. Supervised methods, additionally, are not often \npractical due to the fact that building sufficient labeled data is often expensive and needs much human \nlabor. In contrast, the effectiveness of unsupervised topic modeling approaches has been shown in \nidentifying aspect words. Probabilistic topic models are a suite of algorithms whose aim is to extract \nlatent structure from large collection of documents. These models all share the idea that documents \nare mixtures of topics and each topic is a distribution over words [11-15]. We follow this promising \nline of research to extend existing topic models for aspect detection in sentiment analysis. Current \ntopic modeling approaches are computationally efficient and also seem to capture correlations be-\ntween words and topics but they have two main limitations: the first limitation is that they assume that \nwords are generated independently to each other, i.e. the bag of words assumption. In other words, \ntopic models only extract unigrams for topics in a corpus. We believe that a topic model considering \nunigrams and phrases is more realistic and would be more useful in applications. The second limit for \ncurrent topic modeling approaches is that of the assumption that the order of words can be ignored is \nan unrealistic oversimplification. Topic models assume the subsequent words in a document or a sen-\ntence have different topics, which is not a true assumption. In our model, in addition to extracting \nunigrams and phrases for topics we assume that the topics of words in a sentence form a Markov \nchain and that subsequent words are more likely to have the same topic. Therefore, in this paper we \npropose a new topic modeling approach that can automatically extract topics or aspects in sentiment \nreviews. We call the proposed model: Latent Dirichlet Markov Allocation Model (LDMA), which is a \ngenerative probabilistic topic model based on Latent Dirichlet Allocation (LDA) and Hidden Markov \nModel (HMM), which emphasizes on extracting multi-word topics from text data. In addition LDMA \nrelaxes the “bag of words” assumption from topic modeling approaches to yield to a better model in \nterms of extracting latent topics from text. \nWe proceed by reviewing the formalism of LDA. We then propose the LDMA model and its infer-\nence procedure to estimate parameters. We demonstrate the effectiveness of our model in our experi-\nments by comparing to basic LDA model for aspect detection. Finally, we conclude our work and \noutline future directions. \n2 LDA \nLDA, introduced by David Blei et al. [12], is a probabilistic generative topic model based on the \nassumption that each document is a mixture of various topics and each topic is a probability distribu-\ntion over different words.  \nA graphical model of LDA is shown in Figure 1, wherein nodes are random variables and edges in-\ndicate the dependence between nodes [12, 13]. As a directed graph, shaded and unshaded variables \nindicate observed and latent (i.e., unobserved) variables respectively, and arrows indicate conditional \ndependencies between variables while plates (the boxes in the figure) refer to repetitions of sampling \nsteps with the variable in the lower right corner referring to the number of samples. \nGiven a corpus with a collection of D documents, each document in the corpus is a sequence of W \nwords, each word in the document is an item from a vocabulary index with V distinct terms, and T is \nthe total number of topics. The procedure for generating a word in a document by LDA is as follows: \n1. for each topic z = 1…K, \nDraw z ~ Dirichlet(); \n2. for each document d = 1…D, \n(a) Draw topic distribution d ~ Dirichlet(); \n(b) for each word wi in document d, \ni. Draw a topic     \n \n~ d; \nii. Draw a word    \n \n~      ; \nThe goal of LDA is therefore to find a set of model parameters, topic proportions and topic-word \ndistributions. Standard statistical techniques can be used to invert the generative process of LDA, in-\nferring the set of topics that were responsible for generating a collection of documents. The exact in-\nference in LDA is generally intractable, and we have to appeal to approximate inference algorithms \nfor posterior estimation. The most common approaches that are used for approximate inference are \nEM, Gibbs Sampling and Variational method [12, 13, 15]. \n \n \n \nFigure 1 LDA Topic Model \n3 LDMA \nLDA model makes an assumption that words are generated independently to each other in a docu-\nment. Also LDA ignores the order or positions of words or simplicity. Here we propose LDMA, a \ngenerative probabilistic topic model based on Latent Dirichlet Allocation (LDA) and Hidden Markov \nModel (HMM), to relax the “bag of words” assumption from LDA to yield to a better model in terms \nof extracting latent topics from text. Figure 2 shows the graphical model corresponding to the LDMA \ngenerative model. \nLDMA, in addition to the two sets of random variables z and w, introduces a new set of variables x \nto detect an n-gram phrase from the text. LDMA assumes that the topics in a sentence form a Markov \nchain with a transition probability that depends on , a distribution zw, a random variable xi and the \ntopic of previous word zi-1. Random variable x denotes whether a bigram can be formed with previous \nterm or not. Therefore LDMA has the power to decide whether to generate a unigram, a bigram, a \ntrigram or etc. Here we only consider generating unigrams and bigrams from LDMA. If the model \nsets xi equal to one, it means that wi-1 and wi form a bigram and if it is equal to zero they do not.  \nThe generative process of the LDMA model can be described as follows: \n3. for z = 1…K, \nDraw multinomial z ~ Dirichlet(); \n4. for z = 1…K, \nfor w = 1…W, \n(a) Draw binomial zw ~ Dirichlet(); \n(b) Draw multinomial zw ~ Beta(); \n5. for d = 1…D, \n(c) Draw multinomial d ~ Dirichlet(); \n(d) for s = 1…S in document d, \n    for each word wi in sentence s, \n   i. Draw   \n  from binomial      \n     \n ; \n   ii. if(  \n  = 0) Draw   \n  from multinomial d; \n       else   \n      \n ; \n   iii. if(  \n  = 1) Draw  \n  from multinomial    \n     \n ; \n        else draw  \n  from multinomial    \n ; \n     \n \n \n\nD\nz\nW\n \nK\nw\n \n \nFigure 2 A graphical view of LDMA Topic Model \n \nLDMA model is sensitive to the order of the words, which it is not assumes that a document is a bag \nof words, i.e. successive words in LDMA tend to have the same topics. Therefore unlike LDA ap-\nproach, LDMA will not give the same topic to all appearances of the same word within a sentence, a \ndocument or corpus. \n4 INFERENCE \nThe exact inference of learning parameters in LDMA is intractable due to the large number of param-\neters in the model [13, 15]. Therefore approximate techniques are needed. We use Gibbs sampling to \napproximate the latent variables in the model. At each transition of Markov chain, the aspect (topic) \nof ith sentence, zi, and the n-gram variable xi are drawn from the following conditional probability: \n \nIf      then: \n  (     |                 )  \n               \n∑ (             \n)    \n(         ) (\n         \n∑ (       )\n \n   \n) \nIf      then: \n  (     |                 )  \n               \n∑ (             \n)    \n(         ) (\n             \n∑ (           )\n \n   \n) \n \nWhere     denotes the bigram status for all words except   ,     represents the topic (aspect) assign-\nments for all words except   ,     represents how many times word w is assigned to aspect z as a \nunigram,      represents how many times word v is assigned to aspect z as a second term of a bi-\ngram word given the previous word w,      denotes how many times the status variable x is set to k \ngiven the previous word w and the previous word’s aspect z, and     represents how many times a \nword is assigned to aspect z in sentence s of document d. \n\nzi - 1 zi ...\n\nzi + 1\nwi - 1 wi wi + 1\n\n...\nS\n\nK\n...\n...\nxi xi + 1\nD\n......\n\nKW\n \n5 EXPERIMENTS \nIn this section, we apply the proposed LDMA model to customer review datasets of digital cameras \nand compare the results with the original LDA model. These datasets of customer reviews [1] contain \ntwo different products: Canon G3 and Nikon Coolpix 4300. Table 1 shows the number of review sen-\ntences and the number of manually tagged product aspects for each product in the dataset. \nTable 1 \nSummary of customer review datasets. \nDataset Number of review sentences Number of \nmanual aspects \nCanon 597 100 \nNikon 346 74 \n \nWe first start by preprocessing review document from the datasets. We extract the sentences accord-\ning to the delimiters ‘.’, ‘,’, ‘!’, ‘?’, ‘;’. And then by removing Stopwords and words with frequency \nless than three we extract a feature vector to represent review documents. By applying LDMA and the \noriginal LDA models examples of most probable aspects (topics) extracted are shown in Tables 2 and \nTable 3. \nTable 2 \nExampled aspects discovered by LDMA and LDA form Canon reviews \nCanon \nLDMA LDA \ncanon \ncanon powershot \ncanon \npurchased \ndurability durability \nphotos \nbattery \ndigital zoom \neasy \nlife \nbattery \npicture \nsize \npicture quality \noptical zoom \nquality \npowershot \nzoom \nsize \nphotos quality picture \nbattery life digital \nTable 3 \nExampled aspects discovered by LDMA and LDA form Nikon reviews \nNikon \nLDMA LDA \neasy use \nauto mode \nBattery \nperfect \ncamera transfer \npictures camera \nnikon nikon \ntransfer cable Manual \nbattery \nmanual mode \nprint quality \nmacro \nsmall \nmode \n \nFrom the tables we can find that the LDMA model discovered more informative words for aspects or \ntopics. In addition to the unigrams, LDMA can extract phrases, hence the unigram and bigram list of \naspects are more pure in LDMA. Also LDMA associates words together to detect the multi-word as-\npects which are only highly probable in this model. Based on the results, LDMA can successfully find \naspects that consist of words that are consecutive in a review document. \n6 CONCLUSIONS \nManaging the explosion of digital content on the internet requires new tools for automatically mining, \nsearching, indexing and browsing large collections of text data. Recent research in data mining and \nstatistics has developed a new brand of techniques call probabilistic topic modeling for text. In this \npaper, we proposed LDMA model, a model which extends the original topic modeling approach, \nLDA, by considering the underlying structure of a document and order of words in document. LDMA \nignores the bag of words assumption of LDA to extract multi-word and n-gram aspects and topics \nfrom text data. We find that ignoring this basic assumption allows the model to learn more coherent \nand informatics topics. \nREFERENCES \n[1] Hu, M., Liu, B. (2004). Mining opinion features in customer reviews. In: Proceedings of 19th Na-tional Conference on Artificial \nIntelligence AAAI. \n[2] B. Liu, L. Zhang, (2012). A survey of opinion mining and sentiment analysis, Mining Text Data, 415-463. \n[3] C. Lin, Y. He, R. Everson, S. Ruger, (2012). Weakly supervised joint sentiment-topic detection from text, IEEE Trans. Knowl. Data \nEng. 24, no. 6, 1134-1145. \n[4] G. Qiu, B. Liu, J. Bu, C. Chen. (2011). Opinion word expansion and target extraction through double propagation, Computational \nlinguistics, 37(1), 9-27. \n[5] I. Titov, R. McDonald, (2008). A joint model of text and aspect ratings for sentiment summarization, in: Proceedings of the Annual \nMeeting on Association for Computational Linguistics and the Human Language Technology Conference (ACL-HLT). pp. 308–316. \n[6] S. Brody, N. Elhadad, (2010). An unsupervised aspect-sentiment model for online reviews, in: Proceedings of Annual Conference of \nthe North American Chapter of the Association for Computational Linguistics. Publishing, Association for Computational Linguistics, \npp. 804-812. \n[7] S. Moghaddam, M. Ester, (2011). ILDA: interdependent LDA model for learning latent aspects and their ratings from online product \nreviews, in: Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. \nPublishing, ACM, pp. 665-674. \n[8] X. Fu, G. Liu, Y. Guo, Z. Wang, (2013). Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling \nand HowNet lexicon, Knowledge-Based Systems,  37, 186-195. \n[9] Z. Zhai, B. Liu, H. Xu, P. Jia, (2011). Constrained LDA for grouping product features in opinion mining, in: Proceedings of 15th \nPacific-Asia Conference, Advances in Knowledge Discovery and Data Mining, pp 448-459. \n[10] Jo, Y., & Oh, A. H. (2011). Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM \ninternational conference on Web search and data mining (pp. 815-824). ACM. \n[11] C. Zhai, J. Lafferty, (2001). Model-based feedback in the language modeling approach to information retrieval, in: Proceedings of 10th \nInternational Conference on Information and knowledge management. Publishing, pp. 403-410. \n[12] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, \n3:993–1022. \n[13] Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. (2005). Integrating topics and syntax. In Lawrence K. \nSaul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press, \nCambridge, MA. \n[14] Hanna M. Wallach. (2006). Topic modeling: Beyond bag-of-words. In ICML ’06: 23rd International Conference on Machine Learn-\ning, Pittsburgh, Pennsylvania, USA. \n[15] Xuerui Wang and Andrew McCallum. (2005). A note on topical n-grams. Technical Report UM-CS-071, Department of Computer \nScience University of Massachusetts Amherst. \n",
      "id": 6795298,
      "identifiers": [
        {
          "identifier": "18296113",
          "type": "CORE_ID"
        },
        {
          "identifier": "16412155",
          "type": "CORE_ID"
        },
        {
          "identifier": "92026839",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:doc.utwente.nl:87440",
          "type": "OAI_ID"
        },
        {
          "identifier": "23567148",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:usir.salford.ac.uk:29460",
          "type": "OAI_ID"
        },
        {
          "identifier": "572108859",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ris.utwente.nl:publications/531eadb2-76dc-4734-81bd-30774e99a480",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.388.8641",
          "type": "OAI_ID"
        }
      ],
      "title": "Latent dirichlet markov allocation for sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:usir.salford.ac.uk:29460",
        "oai:doc.utwente.nl:87440",
        "oai:ris.utwente.nl:publications/531eadb2-76dc-4734-81bd-30774e99a480",
        "oai:citeseerx.psu:10.1.1.388.8641"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "The OR Society , Birmingham, UK",
      "pubmedId": null,
      "references": [
        {
          "id": 7223179,
          "title": "A joint model of text and aspect ratings for sentiment summarization, in:",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "I. Titov, R. McDonald, (2008). A joint model of text and aspect ratings for sentiment summarization, in: Proceedings of the Annual Meeting on Association for Computational Linguistics and the Human Language Technology Conference (ACL-HLT). pp. 308–316.",
          "cites": null
        },
        {
          "id": 7223192,
          "title": "A note on topical n-grams.",
          "authors": [],
          "date": "2005",
          "doi": "10.1109/icdm.2007.86",
          "raw": "Xuerui Wang and Andrew McCallum. (2005). A note on topical n-grams. Technical Report UM-CS-071, Department of Computer Science University of Massachusetts Amherst.",
          "cites": null
        },
        {
          "id": 7223175,
          "title": "A survey of opinion mining and sentiment analysis, Mining Text Data,",
          "authors": [],
          "date": "2012",
          "doi": "10.1007/978-1-4614-3223-4_13",
          "raw": "B. Liu, L. Zhang, (2012). A survey of opinion mining and sentiment analysis, Mining Text Data, 415-463.",
          "cites": null
        },
        {
          "id": 7223180,
          "title": "An unsupervised aspect-sentiment model for online reviews, in:",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "S. Brody, N. Elhadad, (2010). An unsupervised aspect-sentiment model for online reviews, in: Proceedings of Annual Conference of the North American Chapter of the Association for Computational Linguistics. Publishing, Association for Computational Linguistics, pp. 804-812.",
          "cites": null
        },
        {
          "id": 7223187,
          "title": "Aspect and sentiment unification model for online review analysis.",
          "authors": [],
          "date": "2011",
          "doi": "10.1145/1935826.1935932",
          "raw": "Jo, Y., & Oh, A. H. (2011). Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 815-824). ACM.",
          "cites": null
        },
        {
          "id": 7223185,
          "title": "Constrained LDA for grouping product features in opinion mining, in:",
          "authors": [],
          "date": "2011",
          "doi": "10.1007/978-3-642-20841-6_37",
          "raw": "Z. Zhai, B. Liu, H. Xu, P. Jia, (2011). Constrained LDA for grouping product features in opinion mining, in: Proceedings of 15th Pacific-Asia Conference, Advances in Knowledge Discovery and Data Mining, pp 448-459.",
          "cites": null
        },
        {
          "id": 7223182,
          "title": "ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews, in:",
          "authors": [],
          "date": "2011",
          "doi": "10.1145/2009916.2010006",
          "raw": "S. Moghaddam, M. Ester, (2011). ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews, in: Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. Publishing, ACM, pp. 665-674.",
          "cites": null
        },
        {
          "id": 7223190,
          "title": "Integrating topics and syntax. In",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": "Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. (2005). Integrating topics and syntax. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press, Cambridge, MA.",
          "cites": null
        },
        {
          "id": 7223189,
          "title": "Latent Dirichlet allocation.",
          "authors": [],
          "date": "2003",
          "doi": null,
          "raw": "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.",
          "cites": null
        },
        {
          "id": 7223174,
          "title": "Mining opinion features in customer reviews. In:",
          "authors": [],
          "date": "2004",
          "doi": "10.1145/1014052.1014073",
          "raw": "Hu, M., Liu, B. (2004). Mining opinion features in customer reviews. In: Proceedings of 19th Na-tional Conference on Artificial Intelligence AAAI.",
          "cites": null
        },
        {
          "id": 7223188,
          "title": "Model-based feedback in the language modeling approach to information retrieval, in:",
          "authors": [],
          "date": "2001",
          "doi": "10.1145/502585.502654",
          "raw": "C. Zhai, J. Lafferty, (2001). Model-based feedback in the language modeling approach to information retrieval, in: Proceedings of 10th International Conference on Information and knowledge management. Publishing, pp. 403-410.",
          "cites": null
        },
        {
          "id": 7223184,
          "title": "Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling and HowNet lexicon,",
          "authors": [],
          "date": "2013",
          "doi": "10.1016/j.knosys.2012.08.003",
          "raw": "X. Fu, G. Liu, Y. Guo, Z. Wang, (2013). Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling and HowNet lexicon, Knowledge-Based Systems,  37, 186-195.",
          "cites": null
        },
        {
          "id": 7223178,
          "title": "Opinion word expansion and target extraction through double propagation,",
          "authors": [],
          "date": "2011",
          "doi": "10.1162/coli_a_00034",
          "raw": "G. Qiu, B. Liu, J. Bu, C. Chen. (2011). Opinion word expansion and target extraction through double propagation, Computational linguistics, 37(1), 9-27.",
          "cites": null
        },
        {
          "id": 7223191,
          "title": "Topic modeling: Beyond bag-of-words.",
          "authors": [],
          "date": "2006",
          "doi": "10.1145/1143844.1143967",
          "raw": "Hanna M. Wallach. (2006). Topic modeling: Beyond bag-of-words. In ICML ’06: 23rd International Conference on Machine Learning, Pittsburgh, Pennsylvania, USA.",
          "cites": null
        },
        {
          "id": 7223176,
          "title": "Weakly supervised joint sentiment-topic detection from text,",
          "authors": [],
          "date": "2012",
          "doi": "10.1109/tkde.2011.48",
          "raw": "C. Lin, Y. He, R. Everson, S. Ruger, (2012). Weakly supervised joint sentiment-topic detection from text, IEEE Trans. Knowl. Data Eng. 24, no. 6, 1134-1145.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.388.8641",
        "http://www.loc.gov/mods/v3",
        "http://doc.utwente.nl/87440/1/IMSIO_Latent_Dirichlet_Markov_Allocation.pdf",
        "https://ris.utwente.nl/ws/files/5511169/IMSIO_Latent_Dirichlet_Markov_Allocation.pdf"
      ],
      "updatedDate": "2023-07-14T13:54:28",
      "yearPublished": 2013,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/16412155.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/16412155"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/16412155/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/16412155/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/6795298"
        }
      ]
    },
    {
      "acceptedDate": "2018-01-08T00:00:00",
      "arxivId": "1707.02657",
      "authors": [
        {
          "name": "Bertaglia, Thales F. C."
        },
        {
          "name": "Brum, Henrico B."
        },
        {
          "name": "Corrêa Jr, Edilson A."
        },
        {
          "name": "Marinho, Vanessa Q."
        },
        {
          "name": "Santos, Leandro B. dos"
        },
        {
          "name": "Treviso, Marcos V."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/466414045"
      ],
      "createdDate": "2017-07-23T18:37:55",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2017-10-01T00:00:00",
      "abstract": "The enormous amount of texts published daily by Internet users has fostered\nthe development of methods to analyze this content in several natural language\nprocessing areas, such as sentiment analysis. The main goal of this task is to\nclassify the polarity of a message. Even though many approaches have been\nproposed for sentiment analysis, some of the most successful ones rely on the\navailability of large annotated corpus, which is an expensive and\ntime-consuming process. In recent years, distant supervision has been used to\nobtain larger datasets. So, inspired by these techniques, in this paper we\nextend such approaches to incorporate popular graphic symbols used in\nelectronic messages, the emojis, in order to create a large sentiment corpus\nfor Portuguese. Trained on almost one million tweets, several models were\ntested in both same domain and cross-domain corpora. Our methods obtained very\ncompetitive results in five annotated corpora from mixed domains (Twitter and\nproduct reviews), which proves the domain-independent property of such\napproach. In addition, our results suggest that the combination of emoticons\nand emojis is able to properly capture the sentiment of a message.Comment: Accepted for publication in BRACIS 201",
      "documentType": "research",
      "doi": "10.1109/bracis.2017.45",
      "downloadUrl": "http://arxiv.org/abs/1707.02657",
      "fieldOfStudy": null,
      "fullText": "PELESent: Cross-domain polarity classification\nusing distant supervision\nEdilson A. Correˆa Jr, Vanessa Q. Marinho, Leandro B. dos Santos,\nThales F. C. Bertaglia, Marcos V. Treviso, Henrico B. Brum\nInstitute of Mathematics and Computer Science\nUniversity of Sa˜o Paulo (USP)\nSa˜o Carlos, Sa˜o Paulo, Brazil\nEmail: {edilsonacjr,vanessaqm,leandrobs,thales.bertaglia,marcostreviso,henrico.brum}@usp.br\nAbstract—The enormous amount of texts published daily by\nInternet users has fostered the development of methods to analyze\nthis content in several natural language processing areas, such\nas sentiment analysis. The main goal of this task is to classify the\npolarity of a message. Even though many approaches have been\nproposed for sentiment analysis, some of the most successful ones\nrely on the availability of large annotated corpus, which is an\nexpensive and time-consuming process. In recent years, distant\nsupervision has been used to obtain larger datasets. So, inspired\nby these techniques, in this paper we extend such approaches to\nincorporate popular graphic symbols used in electronic messages,\nthe emojis, in order to create a large sentiment corpus for\nPortuguese. Trained on almost one million tweets, several models\nwere tested in both same domain and cross-domain corpora.\nOur methods obtained very competitive results in five annotated\ncorpora from mixed domains (Twitter and product reviews),\nwhich proves the domain-independent property of such approach.\nIn addition, our results suggest that the combination of emoticons\nand emojis is able to properly capture the sentiment of a message.\nI. INTRODUCTION\nIn the last few years, Sentiment Analysis has become a\nprominent field in natural language processing (NLP), mostly\ndue to its direct application in several real-world scenarios [1],\nsuch as product reviews, government intelligence, and the\nprediction of the stock markets. One of the main tasks in Sen-\ntiment Analysis is the polarity classification, i.e., classifying\ntexts into categories according to the emotions expressed on\nthem. In general, the classes are positive, negative and neutral.\nA popular application of polarity classification is in social\nmedia content. Microblogging and social networks websites,\nsuch as Twitter, have been used to express personal thoughts.\nAccording to Twitter’s website1, more than 500 million short\nmessages, known as tweets, are posted each day. The analysis\nof this type of content is particularly challenging due to its\nspecific language, which is mostly informal, with spelling\nerrors, out of the vocabulary words, as well as the usage of\nemoticons and emojis to express ideas and sentiments.\nMachine learning methods have been widely applied to\npolarity classification tasks in the context of social networks.\nThis is particularly evident in shared tasks such as the SemEval\n1https://business.twitter.com/en/basics.html\nSentiment Analysis tasks [2], [3], where these methods usually\noutperform lexical-based approaches. However, a major draw-\nback of machine learning is its high dependency on large an-\nnotated corpora, and since manual annotation usually is time-\nconsuming and expensive [4], many non-English languages\nlack this type of resource or, when existing, are very limited\nand specific, as it is the case for Portuguese.\nIn this paper, we adapt a distant supervision approach [5] to\nannotate a large number of tweets in Portuguese and use them\nto train state-of-the-art methods for polarity classification. We\napplied these methods in manually annotated corpora from the\nsame domain (Twitter) and cross-domain (product reviews).\nThe obtained results indicate that the proposed approach is\nwell suited for both: same domain and cross-domain. More-\nover, it is a powerful alternative to produce sentiment analysis\ncorpora with less effort than manual annotation.\nThis paper is organized as follows. Section II gives a\nbrief overview of some approaches for sentiment analysis and\npresents some works that have applied distant supervision\nto this task. Our approach is described in Section III. The\nevaluation corpora, machine learning algorithms and results\nare given in Section IV. Finally, our conclusions are drawn in\nSection V.\nII. RELATED WORK\nCurrently, methods devised to perform sentiment analysis\nand, more specifically, polarity classification range from ma-\nchine learning to lexical-based approaches. While machine\nlearning methods have proved useful in scenarios where a large\namount of training data is available along with top quality\nNLP resources (such as taggers, parsers and others), they\nusually have low performance in opposite scenarios. Since\nmost non-English languages face resource limitations, for\nexample Portuguese, lexical-based approaches have become\nvery popular. Some works following this line are [6]–[8].\nAnother alternative for languages with fewer resources is\nthe use of hybrid systems, which combine machine learning\nand lexical-based methods. Avanc¸o et al. [9] showed that\nthis combination outperforms both individual approaches. This\nmay imply that the development of better individual elements\nwill lead to better results in the final combination.\nar\nX\niv\n:1\n70\n7.\n02\n65\n7v\n1 \n [c\ns.C\nL]\n  9\n Ju\nl 2\n01\n7\nMachine learning approaches rely on document representa-\ntions, normally vectorial ones with features like n-grams [1],\na simple example is the bag-of-words model. Once a repre-\nsentation has been chosen, several classification methods are\navailable, such as Support Vector Machines (SVM), Naive\nBayes (NB), Maximum Entropy (MaxEnt), Conditional Ran-\ndom Fields (CRF), and ensembles of classifiers [3].\nApart from the traditional features, such as n-grams, some\nresearchers have taken advantage of word embeddings, which\nare known to capture some linguistic properties, such as se-\nmantic and syntactic features. A well-known example of word\nembeddings is Word2Vec [10], [11]. Algebraic operations, such\nas sum or average, can be applied to convert word vectors\ninto a sentence or document vector [12], [13]. However, this\nrepresentation does not consider the order of the words in the\nsentence.\nParagraph vectors [14] (also known as Doc2Vec) can be\nunderstood as a generalization of Word2Vec for larger blocks\nof text, such as paragraphs or documents. This technique has\nobtained state-of-the-art results on sentiment analysis for two\ndatasets of movie reviews [14]. The main goal of these dense\nrepresentations is to predict the words in those blocks. Two\nmodels were proposed by Le and Mikolov [14], in which one\nof them accounts for the word order.\nIn addition, deep neural networks also consider the word\norder. Their methods have achieved good results in sentiment\nanalysis, as shown in [15]–[17] and in the SemEval Sentiment\nAnalysis Tasks [2], [3]. Nevertheless, these approaches need\nlarge datasets for training. Distant Supervision is a good\nalternative to obtain these datasets for the training/pre-training\nof deep neural networks [16], [18], [19].\nDistant supervision is an alternative to create large datasets\nwithout the need of manual annotation. Some works have\nreported the use of emoticons as semantic indicator for sen-\ntiment [5], [18], [20], [21], while others use emoticons and\nhashtags for the same purpose [22], [23]. Go et al. [5], the first\nwork to apply distant supervision to Twitter data, collected\napproximately 1.6 million of tweets containing positive and\nnegative emoticons – e.g. “:)” and “:(” – equally distributed\ninto two classes. They combined sets of features – unigrams,\nbigrams, part-of-speech (POS) tags – in order to train machine\nlearning algorithms (NB, MaxEnt and SVM) and evaluate\nthose in manually annotated datasets. The best accuracy was\nachieved using unigram and bigram as features for a MaxEnt\nclassifier.\nSeveryn and Moschitti [18] used Distant Supervision to pre-\ntrain a Convolutional Neural Network (CNN). An architecture\nsimilar to the one proposed by Kim [17]. The network is\ncomposed of a first layer to convert words in dense vec-\ntors, following a single convolutional layer with a non-linear\nactivation function, max pooling and soft-max. Deriu et al.\n[19] used a combination of 2 CNNs with a Random Forest\nclassifier. However, this approach did not obtain improvements\nwith distant supervision.\nDespite the numerous studies and investigations of different\ntechniques and methods for polarity classification, the problem\nTABLE I\nALL EMOTICONS USED TO REPRESENT EMOTION.\nPositive Negative\n:) :-) :D =) :( :-(\nof relying on large annotated corpora remains open and the\ndifficulty is intensified in non-English languages. In this paper,\nour contributions are the adapted framework for building\npolarity classification corpus to Portuguese, the built corpus\nitself and an evaluation on different state-of-the-art methods\nusing this corpus, for same domain and cross-domain corpora.\nIII. APPROACH\nFollowing the approach of Go et al. [5], we initially col-\nlected a large amount of tweets in order to create the distant\nsupervision corpus. Only tweets in Portuguese were crawled,\nand no specific queries were employed. In total, 41 million of\ntweets were collected.\nAfter collecting the tweets, the next step was to split them\ninto positive and negative classes. In order to do so, we\nused lists of emojis and emoticons selected according to the\nsentiment conveyed by them. Therefore, the polarity of a tweet\nis determined by the presence of emojis and emoticons in\nit – if it only contains positive ones (from the positive list),\nits polarity is assigned as positive. If a tweet contains both\npositive and negative elements, it is discarded since it is likely\nto be ambiguous. Following this idea, we used the same list of\nemoticons used by Go et al. [5], which is presented in Table I.\nGo et al. [5] did not use emojis, but these graphic symbols\nare also employed to convey ideas and sentiments [24]. In\ncontrast to the small set of emoticons, there are hundreds of\npossible emojis. Therefore, we selected a representative list\nwith positive and negative emojis. All the emojis conveying\npositive emotion are presented in Fig. 1. Fig. 2 illustrates the\nselected ones with negative emotion.\nAfter filtering the tweets by the aforementioned criteria, we\nobtained a labeled corpus comprising 554,623 positive tweets\nand 425,444 negative ones. This corpus was used to train the\nmachine learning methods. It is important to highlight that\nemojis and emoticons were removed from the tweets in the\nfinal corpus, so that their presence as a sentiment indicator is\nnot learned by the models.\nIn addition to the filtering process, some preprocessing\nsteps were performed to improve the corpus quality. Details\nabout the preprocessing steps are given in the Supplementary\nMaterial, Section A. After these steps, tweets containing less\nthan 4 tokens were discarded from the corpus. The com-\nplete framework (tweets collection, filtering and preprocessing\nmethods) along with all experimental evaluation will be made\navailable 2.\n2https://github.com/edilsonacjr/pelesent\n(a) (b) (c) (d) (e) (f) (g)\nFig. 1. All emojis used to represent positive emotion. Their respective\nunicodes are: (a) U+1F60A, (b) U+1F60B, (c) U+1F60D, (d) U+1F603, (e)\nU+1F606, (f) U+1F600, and (g) U+1F61D.\n(a) (b) (c) (d) (e) (f)\n(g) (h) (i) (j) (k) (l)\nFig. 2. All emojis used to represent negative emotion. Their respective\nunicodes are: (a) U+1F620, (b) U+1F627, (c) U+1F61E, (d) U+1F628,\n(e) U+1F626, (f) U+1F623, (g) U+1F614, (h) U+1F629, (i) U+1F612, (j)\nU+1F621, (k) U+2639, and (l) U+1F61F.\nIV. EXPERIMENTAL EVALUATION\nIn order to evaluate the quality of the corpus built using\ndistant supervision, we trained state-of-the-art methods for\npolarity classification and applied the learned models to 5 well\nknown manually annotated sentiment corpora. In the follow-\ning, we present these corpora along with the message polarity\nclassification methods, and finally, the obtained results.\nA. Corpora\nSentiment classifiers are usually trained on manually anno-\ntated corpora. Because sentiments may be expressed differ-\nently in different domains [4], it is common to create domain-\nspecific corpus. Since we intend to create a robust and generic\ncorpus that is not domain-specific, we selected 5 corpora for\nevaluation, 2 being from the same domain (Twitter) and 3 from\na different domain (product reviews). Below, we present the\ncorpora that were used.\na) Brazilian Presidential Election [25]: This dataset is\nformed by tweets about the Brazilian presidential election run\nin 2010. The corpus is divided in two parts, one referencing\nDilma Rousseff (BPE-Dilma) and the other Jose´ Serra (BPE-\nSerra), both being the most popular candidates in the election.\nThe corpora were manually annotated in positive and negative,\nand used to evaluate stream based sentiment analysis systems.\nb) Buscape´ [26]: This dataset is formed by product\nreviews extracted from Buscape´ website3. The documents were\nautomatically labeled based on two informations given by the\nusers. The first (Buscape-1) is based on a recommendation tag\nwhile the second (Buscape-2) is based on a 5-star scale (1-2\nstars for negative and 4-5 stars for positive). Both corpora\nare balanced between the two classes, even though there is a\nnotable difference on their sizes, possibly due to the low use\nof the recommendation tag.\n3http://www.buscape.com.br/\nc) Mercado Livre [9]: Similar to the Buscape´ dataset,\nthis corpus is formed by product reviews from the online\nmarketplace Mercado Livre4. The corpus was also automat-\nically annotated based on a 5-star scale given by the authors\nof the reviews. The dataset is balanced between the positive\nand negative classes.\nTable II presents a summary of the corpora.\nTABLE II\nDATASETS USED IN THE EVALUATION OF THE SYSTEM.\nDataset Total Positive Negative\nBPE-Dilma 66, 640 46, 805 19, 835\nBPE-Serra 9, 718 1, 371 8, 347\nBuscape-1 2, 000 1, 000 1, 000\nBuscape-2 13, 685 6, 873 6, 812\nMercado Livre 43, 318 21, 819 21, 499\nB. Machine Learning Methods\nMachine learning has dominated the area of sentiment\nanalysis, mostly because its high performance when manually\nannotated data is available. However, thanks to the great\nvariety of methods, there is no consensus about which method\nis the best in this scenario. In the last editions of SemEval Sen-\ntiment Analysis Task, most of the best methods/systems used\ndeep learning techniques [2], [3]. In this work, the evaluated\nmethods range from simple linear models for classification\nusing vector space models to hybrid (machine learning and\nlexical-based) and Deep Learning methods. The idea was to\nthoroughly evaluate the quality of the corpus regardless of\nthe technique being used for learning. Below, each method is\nbriefly described.\na) Logistic Regression (LR): Also known as logit re-\ngression, LR can be understood as a generalization of linear\nregression models to the binary classification scenario, where\na sigmoid function outputs the class probabilities [27]. In\nthis paper, the logistic regression model predicts the class\nprobabilities of a text, where the classes are ”positive” and\n”negative”. As input for this classifier, three text representa-\ntions were used: (1) a bag-of-words model (LR+tfidf), where\neach document (tweet or review) is represented by its set of\nwords weighted by tf-idf [28]; (2) a word embeddings based\nmodel (LR+w2v), where each document is represented by\nthe weighted average of the embedding vectors (generated by\nWord2Vec [10], [11]) of the words that compose the document,\nthe weights are defined by tf-idf ; (3) the Paragraph Vector\nmodel (LR+d2v), which uses a neural network to generate\nembeddings for words and documents simultaneously in an\nunsupervised manner. Only the vectorial representations of\ndocuments were used by the classifier.\nb) Convolutional Neural Networks (CNNs): With the\npopularity of deep learning, CNNs have been applied to many\ndifferent contexts, including several NLP tasks [29] and, more\nspecifically, sentiment analysis [16]–[19]. Our CNN is similar\n4http://www.mercadolivre.com.br/\nto the architecture proposed by Kim [17], which uses a single\nconvolutional layer. In this architecture, the network receives\nas input a matrix representing the document, and each word\nin the document is represented by a dense continuous vector.\nThe output of the network is the probability of a document\nbeing negative or positive.\nc) Recurrent Convolutional Neural Networks (RCNNs):\nThis deep neural architecture uses both convolutional and re-\ncurrent layers. Recently explored by many works in NLP [30]–\n[32], this architecture has been successfully applied to senti-\nment analysis [3], [32]. Our architecture consists of a slight\nmodification of the one used by Treviso et al. [31], where the\nfinal layer returns the probability for the whole document, in-\ndicating a positive/negative polarity. Using this combination of\nconvolutional and recurrent layers, we explored the principle\nthat nearby words have a greater influence in the classification,\nwhile distant words may also have some impact.\nd) Hybrid: This method is a combination of two clas-\nsifiers previously used for sentiment classification in cross-\ndomain corpora [9] and follows the same setting introduced\nby Avanc¸o [33]. The method consists of a SVM classifier\ncombined with a lexical-based approach. The documents are\nrepresented by arrays of features including a binary bag-\nof-words (presence/absence of terms), emoticons, sentiment\nwords and POS tags. Documents located near the separation\nhyperplane (in a threshold assumed as 0.5) learned by the\nSVM are considered to be uncertain. Those documents are\nthen classified with a lexical-based approach, that uses lin-\nguistic rules for polarity classification in Portuguese.\nFor all methods, well-known machine learning libraries\nwere used, such as Scikit-learn [34] and Keras [35]. Partic-\nularities such as parameters, details about the architecture,\ninitializations and others can be found in the Supplementary\nMaterial, Section B.\nC. Results and Discussion\nTo evaluate and compare the methods in each corpus, F1\nscore (macro-averaged), recall (macro-averaged) and accuracy\nwere chosen, mostly because of their traditional use in senti-\nment analysis [2], [3].\nThe main results are shown in Table III. Along with the\nresults of each polarity classification method, we present the\nstate-of-the-art (SotA) result reported for each corpus. Because\nthe BPE corpora were conceived for a different context, there\nare no SotA reported results for those corpora. We also ranked\neach evaluated method by its F1 score.\nThe differences between the best method (in bold) and\nthe SotA vary between 9.69% and 12.24%, very competitive\nresults given the fact that all SotA reported results were\nobtained by a 10-fold cross validation scheme and our methods\nused a corpus from a different domain for training. Of all the\nmethods, the Hybrid was the one that had the best performance\nin the corpora of product reviews. Such a result was due to the\nregularity of the language in this type of corpus, which makes\nlexical approaches highly effective. However, in domains such\nas Twitter, errors, abbreviations and slangs are very common,\nwhich decreases the effectiveness of lexical-based approaches.\nThis effect can be seen in the BPE-Dilma corpus.\nAn important aspect of Sentiment Analysis is the sensitivity\nof its methods to elements such as domain and temporality.\nIn our evaluation, both were present in the selected corpora,\nwhich demonstrates the robustness of the constructed corpus\nand its resilience to temporality and the non-regularity of the\nlanguage.\nRegarding the deep learning methods (CNN and RCNN),\nboth presented high rankings in almost all corpora. However,\nthere was no huge difference between deep and shallow\nmethods (logistic regression + document representation), indi-\ncating that large datasets decrease the performance difference\nbetween methods from different natures (even between simple\nand complex methods), a result commonly found in the big\ndata era [36].\nTABLE III\nRESULTS OBTAINED BY EACH METHOD TRAINED ON THE DISTANT\nSUPERVISION CORPUS.\nDataset Method F1 score Recall Accuracy\nBPE-Dilma\nLR + w2v 0.57395 0.6037 0.5952\nLR + tfidf 0.64771 0.6443 0.7128\nLR + d2v 0.61354 0.6071 0.7256\nCNN 0.63373 0.6295 0.7051\nRCNN 0.64442 0.6586 0.6816\nHybrid 0.52496 0.5855 0.5295\nSotA − − −\nBPE-Serra\nLR + w2v 0.35156 0.4398 0.3915\nLR + tfidf 0.41105 0.5546 0.4475\nLR + d2v 0.50553 0.6028 0.5915\nCNN 0.42404 0.5929 0.4558\nRCNN 0.52862 0.5975 0.6426\nHybrid 0.57451 0.6073 0.7344\nSotA − − −\nBuscape-1\nLR + w2v 0.72324 0.7250 0.7250\nLR + tfidf 0.74693 0.7480 0.7480\nLR + d2v 0.64276 0.6465 0.6465\nCNN 0.67135 0.6870 0.6870\nRCNN 0.76542 0.7654 0.7654\nHybrid 0.76681 0.7695 0.7695\nSotA 0.8892 − 0.8894\nBuscape-2\nLR + w2v 0.68146 0.6903 0.6910\nLR + tfidf 0.77252 0.7738 0.7742\nLR + d2v 0.70175 0.7027 0.7030\nCNN 0.70484 0.7115 0.7122\nRCNN 0.76563 0.7658 0.7657\nHybrid 0.79171 0.7930 0.7934\nSotA 0.8935 − 0.8935\nMercado Livre\nLR + w2v 0.68616 0.7048 0.7066\nLR + tfidf 0.83283 0.8329 0.8328\nLR + d2v 0.80894 0.8093 0.8097\nCNN 0.77455 0.7800 0.7813\nRCNN 0.85612 0.8561 0.8563\nHybrid 0.86141 0.8614 0.8614\nSotA 0.9583 − 0.9583\nV. CONCLUSION AND FUTURE WORK\nIn recent years, the polarity classification task has drawn\nthe attention of the scientific community, mainly due to its\ndirect application in scenarios such as social media content\nand product reviews. Even though machine learning methods\npresent themselves as high performance alternatives, they\nsuffer from the need of a large amount of data during their\ntraining phases. In this paper, we adapted a distant supervision\napproach to build a large sentiment corpus for Portuguese.\nState-of-the-art methods were trained on this corpus and\napplied to 5 selected corpora, from same domain and different\ndomain (cross-domain). Competitive results were obtained for\nall methods, although our best results did not outperform the\nbest ones reported for the same corpora.\nAs future works, we intend to explore ways to improve\nthe quality of the distant supervision corpus by applying\ntechniques to remove outliers and tweets that do not convey\nany sentiment or convey the wrong sentiment. We also intend\nto modify this framework to make it able to represent the\nneutral class.\nACKNOWLEDGMENT\nE.A.C.J. acknowledges financial support from Google\n(Google Research Awards in Latin America grant) and CAPES\n(Coordination for the Improvement of Higher Education Per-\nsonnel). V.Q.M. acknowledges financial support from FAPESP\n(grant no. 15/05676-8). L.B.S. acknowledges financial support\nfrom Google (Google Research Awards in Latin America\ngrant) and CNPq (National Council for Scientific and Tech-\nnological Development, Brazil). T.F.C.B., M.V.T., and H.B.B.\nacknowledge financial support from CNPq. In part of this work\na GPU donated by NVIDIA Corporation was used.\nREFERENCES\n[1] B. Pang, L. Lee et al., “Opinion mining and sentiment analysis,”\nFoundations and Trends in Information Retrieval, vol. 2, no. 1–2, pp.\n1–135, 2008.\n[2] S. Rosenthal, P. Nakov, S. Kiritchenko, S. M. Mohammad, A. Ritter, and\nV. Stoyanov, “Semeval-2015 task 10: Sentiment analysis in twitter,” in\nProceedings of the 9th international workshop on semantic evaluation\n(SemEval 2015), 2015, pp. 451–463.\n[3] P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, and V. Stoyanov,\n“Semeval-2016 task 4: Sentiment analysis in twitter,” Proceedings of\nthe 10th international workshop on semantic evaluation (SemEval 2016),\npp. 1–18, 2016.\n[4] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen, “Cross-domain\nsentiment classification via spectral feature alignment,” in Proceedings\nof the 19th international conference on World wide web. ACM, 2010,\npp. 751–760.\n[5] A. Go, R. Bhayani, and L. Huang, “Twitter sentiment classification using\ndistant supervision,” CS224N Project Report, Stanford, vol. 1, no. 12,\n2009.\n[6] M. Souza, R. Vieira, D. Busetti, R. Chishman, I. M. Alves et al.,\n“Construction of a portuguese opinion lexicon from multiple resources,”\nin 8th Brazilian Symposium in Information and Human Language\nTechnology, 2011, pp. 59–66.\n[7] P. P. Balage Filho, T. Pardo, and S. Aluı´sio, “An evaluation of the Brazil-\nian Portuguese LIWC dictionary for sentiment analysis,” in Proceedings\nof the 9th Brazilian Symposium in Information and Human Language\nTechnology (STIL), S. M. Aluı´sio and V. D. Feltrim, Eds. Fortaleza-\nCE, Brazil: Sociedade Brasileira de Computac¸a˜o, 21–23 Oct. 2013, pp.\n215–219.\n[8] L. V. Avanc¸o and M. d. G. V. Nunes, “Lexicon-based sentiment analysis\nfor reviews of products in brazilian portuguese,” in Intelligent Systems\n(BRACIS), 2014 Brazilian Conference on. IEEE, 2014, pp. 277–281.\n[9] L. V. Avanc¸o, H. B. Brum, and M. G. Nunes, “Improving opinion\nclassifiers by combining different methods and resources,” in XIII\nEncontro Nacional de Inteligeˆncia Artificial e Computacional (ENIAC),\n2016, pp. 25–36.\n[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\n2013.\n[11] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their composi-\ntionality,” in Advances in neural information processing systems, 2013,\npp. 3111–3119.\n[12] Y. Zhou, Z. Zhang, and M. Lan, “Ecnu at semeval-2016 task 4: An\nempirical investigation of traditional nlp features and word embedding\nfeatures for sentence-level and topic-level sentiment analysis in twitter,”\nProceedings of SemEval, pp. 256–261, 2016.\n[13] E. A. Correˆa Jr, V. Q. Marinho, and L. B. d. Santos, “Nilc-usp at\nsemeval-2017 task 4: A multi-view ensemble for twitter sentiment\nanalysis,” Proceedings of the 11th International Workshop on Semantic\nEvaluation (SemEval’17), 2017.\n[14] Q. Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” in Proceedings of the 31st International Conference on\nMachine Learning (ICML-14), T. Jebara and E. P. Xing, Eds., 2014,\npp. 1188–1196.\n[15] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y.\nNg, C. Potts et al., “Recursive deep models for semantic composition-\nality over a sentiment treebank,” in Proceedings of the conference on\nempirical methods in natural language processing (EMNLP), vol. 1631.\nCiteseer, 2013, p. 1642.\n[16] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional\nneural network for modelling sentences,” in Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers). Baltimore, Maryland: Association for\nComputational Linguistics, June 2014, pp. 655–665. [Online]. Available:\nhttp://www.aclweb.org/anthology/P14-1062\n[17] Y. Kim, “Convolutional neural networks for sentence classification,”\nin Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP). Doha, Qatar: Association\nfor Computational Linguistics, October 2014, pp. 1746–1751.\n[18] A. Severyn and A. Moschitti, “Unitn: Training deep convolutional neural\nnetwork for twitter sentiment classification,” in Proceedings of the\n9th International Workshop on Semantic Evaluation (SemEval 2015),\nAssociation for Computational Linguistics, Denver, Colorado, 2015, pp.\n464–469.\n[19] J. Deriu, M. Gonzenbach, F. Uzdilli, A. Lucchi, V. De Luca, and\nM. Jaggi, “Swisscheese at semeval-2016 task 4: Sentiment classifica-\ntion using an ensemble of convolutional neural networks with distant\nsupervision,” Proceedings of SemEval, pp. 1124–1128, 2016.\n[20] J. Read, “Using emoticons to reduce dependency in machine learning\ntechniques for sentiment classification,” in Proceedings of the ACL\nstudent research workshop. Association for Computational Linguistics,\n2005, pp. 43–48.\n[21] A. Pak and P. Paroubek, “Twitter as a corpus for sentiment analysis and\nopinion mining.” in LREc, vol. 10, no. 2010, 2010.\n[22] D. Davidov, O. Tsur, and A. Rappoport, “Enhanced sentiment learning\nusing twitter hashtags and smileys,” in Proceedings of the 23rd inter-\nnational conference on computational linguistics: posters. Association\nfor Computational Linguistics, 2010, pp. 241–249.\n[23] E. Kouloumpis, T. Wilson, and J. Moore, “Twitter sentiment analysis:\nThe good the bad and the omg,” in In The International Association\nfor the Advancement of Artificial Intelligence Conference on Weblogs\nand Social. Association for the Advancement of Artificial Intelligence,\n2011.\n[24] P. K. Novak, J. Smailovic´, B. Sluban, and I. Mozeticˇ, “Sentiment of\nemojis,” PloS one, vol. 10, no. 12, p. e0144296, 2015.\n[25] I. S. Silva, J. Gomide, A. Veloso, W. Meira Jr, and R. Ferreira, “Effective\nsentiment stream analysis with self-augmenting training and demand-\ndriven projection,” in Proceedings of the 34th international ACM SIGIR\nconference on Research and development in Information Retrieval.\nACM, 2011, pp. 475–484.\n[26] N. S. Hartmann, L. V. Avanc¸o, P. P. Balage Filho, M. S. Duran, M. D.\nG. V. Nunes, T. A. S. Pardo, S. M. Aluisio et al., “A large corpus of\nproduct reviews in portuguese: Tackling out-of-vocabulary words,” in\nInternational Conference on Language Resources and Evaluation, 9th.\nEuropean Language Resources Association-ELRA, 2014.\n[27] K. P. Murphy, Machine Learning: A Probabilistic Perspective. The\nMIT Press, 2012.\n[28] G. Salton, “Automatic text processing: The transformation, analysis, and\nretrieval of,” Reading: Addison-Wesley, 1989.\n[29] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\nP. Kuksa, “Natural language processing (almost) from scratch,” Journal\nof Machine Learning Research, vol. 12, no. Aug, pp. 2493–2537, 2011.\n[30] N. Kalchbrenner and P. Blunsom, “Recurrent convolutional neural net-\nworks for discourse compositionality,” arXiv preprint arXiv:1306.3584,\n2013.\n[31] M. V. Treviso, C. Shulby, and S. M. Aluı´sio, “Sentence segmentation\nin narrative transcripts from neuropsychological tests using recurrent\nconvolutional neural networks,” in Proceedings of the 15th Conference of\nthe European Chapter of the Association for Computational Linguistics:\nVolume 1, Long Papers, 2017, pp. 315–325.\n[32] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural\nnetworks for text classification.” in AAAI, vol. 333, 2015, pp. 2267–\n2273.\n[33] L. V. Avanc¸o, “Sobre normalizac¸a˜o e classificac¸a˜o de polaridade de\ntextos opinativos na web,” Master’s thesis, Universidade de Sa˜o Paulo,\n2015.\n[34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,\n“Scikit-learn: Machine learning in python,” Journal of Machine Learning\nResearch, vol. 12, no. Oct, pp. 2825–2830, 2011.\n[35] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.\n[36] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable effectiveness\nof data,” IEEE Intelligent Systems, vol. 24, no. 2, pp. 8–12, 2009.\n[37] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural networks\nfrom overfitting.” Journal of Machine Learning Research, vol. 15, no. 1,\npp. 1929–1958, 2014.\n[38] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\nSUPPLEMENTARY MATERIAL\nA. Preprocessing\nIn order to properly tokenize and preprocess the tweets, the\nfollowing steps were performed:\n• Punctuation marks forming an emoticon were considered\nas a single token (e.g. :-( and ;) )\n• Sequences of consecutive emojis were split so that each\nemoji formed a single token\n• Additional punctuation marks (not forming any emoticon)\nwere removed\n• Usernames (an @ symbol followed by up to 15 charac-\nters) were replaced by the tag USERNAME\n• Hashtags (a # symbol followed by any sequence of\ncharacters) were replaced by the tag HASHTAG\n• URLs were completely replaced by the tag URL\n• Numbers, including dates, telephone numbers and cur-\nrency values were replaced by the tag NUMBER\n• Subsequent character repetitions were limited to 3 – i.e.,\nall sequences of the same character were trimmed to fit\nthe limit of 3\nThe following tweet is used to illustrate the preprocessing\nsteps:\nOriginal: hj tenho aula de manha˜, tarde e noite. das 8h ate\n19h :(( #cansado\nPreprocessed: hj tenho aula de manha˜ tarde e noite das\nNUMBER ate NUMBER :(( HASHTAG\nB. Details about the machine learning methods\na) Logistic Regression: There is no additional informa-\ntion about this classifier.\nb) Convolutional Neural Networks: The complete archi-\ntecture is presented in Figure 3, where the input layer is a\nmatrix composed by n input words, and each word is a d\ndimensionality real vector. The convolutional layer receives\nthese vectors as input and it is responsible for the automatic\nextraction of nf features depending on a sliding window of\nlength h = {3, 4, 5}. The output from the convolutional layer\nis then passed to the max-overtime pooling layer, and the\nnew extracted features are concatenated. This results in a\nlarge dimensional vector that is passed to a fully connected\nlayer, where the softmax operation [27] is applied, returning\nthe probability of a document being negative or positive. In\nthe penultimate layer, we employed dropout with a constraint\non l2-norms of the weight vectors to reduce the chance of\noverfitting [37].\nN x d\ninput max-poolingCNN with multiple filter\nmaps\nwidths and features\nFig. 3. CNN architecture adapted from [17].\nc) Recurrent Convolutional Neural Networks: The com-\nplete architecture is illustrated in Figure 4. The architecture is\ncomposed by an input layer that has ϕ input features, and\neach feature has a dimensionality of d. The convolutional\nlayer is responsible for the automatic extraction of nf new\nfeatures depending on 3 neighboring words. Then, a max-\npooling operation is applied over time, looking at a region\nof hm = 3 elements to find the most significant features. The\nnew extracted features are fed into a recurrent bidirectional\nlayer which has nf units known as Long Short-Term Memory\n[38], which are able to learn over long dependencies between\nwords. Finally, the last recurrent state output is passed to a\ntotally connected layer, where the softmax operation [27] is\ncalculated, giving the probability of a document being negative\nor positive. Between these two layers, dropout is used to\nreduce the chance of overfitting [37].\nFor both neural network models (CNN and RCNN), we em-\nployed the early stopping strategy (p = 3) to avoid overfitting,\ni.e. the training phase finishes when the validation loss has\nstopped improving. Other experimental settings for CNN and\nRCNN (number of epochs, batch size, learning rate, etc.) can\nbe seen in their original papers [17], [31], respectively.\nd) Hybrid: The two methods combined to classify a\ndocument in polarity classes are described below:\nFig. 4. RCNN architecture adapted from [31].\n• SVM classifier: The SVM employed uses a RBF Ker-\nnel (gamma = 1/n features), C defined as 0.25, and L1\npenalty for regularization.\n• Lexical-based classifier: Each word present in a sentiment\nlexicon receives a value according to its polarity. Positive\nwords are valued as 1 and negative ones as −1. The\npresence of an intensification word (e.g. muito, demais) in\na window around the word multiplies its value by 3. The\npresence of a downtoner divides the current value by 3. A\nnegation multiplies the value of a word by −1, inverting\nits polarity. Whenever a negation is in the same window\nas an intensification, it becomes a downtoner (e.g. na˜o\nmuito), the same occurs with a downtoner (na˜o pouco).\nThe polarity values are then summed up to determine the\ndocument polarity.\n",
      "id": 43120027,
      "identifiers": [
        {
          "identifier": "1707.02657",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "info:doi/10.1109%2fbracis.2017.45",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1109/bracis.2017.45",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1707.02657",
          "type": "OAI_ID"
        },
        {
          "identifier": "2735427346",
          "type": "MAG_ID"
        },
        {
          "identifier": "84329379",
          "type": "CORE_ID"
        },
        {
          "identifier": "466414045",
          "type": "CORE_ID"
        }
      ],
      "title": "PELESent: Cross-domain polarity classification using distant supervision",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2735427346",
      "oaiIds": [
        "info:doi/10.1109%2fbracis.2017.45",
        "oai:arxiv.org:1707.02657"
      ],
      "publishedDate": "2017-07-09T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1707.02657"
      ],
      "updatedDate": "2024-02-26T18:26:18",
      "yearPublished": 2017,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1707.02657"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/43120027"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1810.12897",
      "authors": [
        {
          "name": "Bhatia, Sumit"
        },
        {
          "name": "P, Deepak"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Alexandra",
        "Sumit"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/161509270",
        "https://api.core.ac.uk/v3/outputs/470618836"
      ],
      "createdDate": "2018-10-31T16:11:25",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 289,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/289",
          "logo": "https://api.core.ac.uk/data-providers/289/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Ideological leanings of an individual can often be gauged by the sentiment\none expresses about different issues. We propose a simple framework that\nrepresents a political ideology as a distribution of sentiment polarities\ntowards a set of topics. This representation can then be used to detect\nideological leanings of documents (speeches, news articles, etc.) based on the\nsentiments expressed towards different topics. Experiments performed using a\nwidely used dataset show the promise of our proposed approach that achieves\ncomparable performance to other methods despite being much simpler and more\ninterpretable.Comment: Presented at EMNLP Workshop on Computational Approaches to\n  Subjectivity, Sentiment & Social Media Analysis, 201",
      "documentType": "research",
      "doi": "10.18653/v1/w18-6212",
      "downloadUrl": "https://core.ac.uk/download/161509270.pdf",
      "fieldOfStudy": null,
      "fullText": "ar\nX\niv\n:1\n81\n0.\n12\n89\n7v\n1 \n [c\ns.C\nL]\n  3\n0 O\nct \n20\n18\nTopic-Specific Sentiment Analysis Can Help Identify Political\nIdeology\nSumit Bhatia\nIBM Research AI\nNew Delhi, India\nsumitbhatia@in.ibm.com\nDeepak P\nQueen’s University Belfast\nBelfast, UK\ndeepaksp@acm.org\nAbstract\nIdeological leanings of an individual can of-\nten be gauged by the sentiment one ex-\npresses about different issues. We pro-\npose a simple framework that represents a\npolitical ideology as a distribution of sen-\ntiment polarities towards a set of topics.\nThis representation can then be used to\ndetect ideological leanings of documents\n(speeches, news articles, etc.) based on the\nsentiments expressed towards different top-\nics. Experiments performed using a widely\nused dataset show the promise of our pro-\nposed approach that achieves comparable\nperformance to other methods despite be-\ning much simpler and more interpretable.\n1 Introduction\nThe ideological leanings of a person within\nthe left-right political spectrum are often re-\nflected by how one feels about different top-\nics and by means of preferences among vari-\nous choices on particular issues. For example,\na left-leaning person would prefer nationaliza-\ntion and state control of public services (such\nas healthcare) where privatization would be of-\nten preferred by people that lean towards the\nright. Likewise, a left-leaning person would of-\nten be supportive of immigration and will of-\nten talk about immigration in a positive man-\nner citing examples of benefits of immigration\non a country’s economy. A right-leaning per-\nson, on the other hand, will often have a neg-\native opinion about immigration.\nMost of the existing works on political\nideology detection from text have focused\non utilizing bag-of-words and other syntac-\ntic features to capture variations in lan-\nguage use (Sim et al., 2013; Biessmann, 2016;\nIyyer et al., 2014). We propose an alterna-\ntive mechanism for political ideology detection\nbased on sentiment analysis. We posit that ad-\nherents of a political ideology generally have\nsimilar sentiment toward specific topics (for\nexample, right wing followers are often posi-\ntive towards free markets, lower tax rates, etc.)\nand thus, a political ideology can be represented\nby a characteristic sentiment distribution over\ndifferent topics (Section 3). This topic-specific\nsentiment representation of a political ideol-\nogy can then be used for automatic ideology\ndetection by comparing the topic-specific sen-\ntiments as expressed by the content in a doc-\nument (news article, magazine article, collec-\ntion of social media posts by a user, utterances\nin a conversation, etc.).\nIn order to validate our hypothesis, we con-\nsider exploiting the sentiment information to-\nwards topics from archives of political debates\nto build a model for identifying political ori-\nentation of speakers as one of right or left\nleaning, which corresponds to republicans and\ndemocrats respectively, within the context of\nUS politics. This is inspired by our observa-\ntion that the political leanings of debators are\noften expressed in debates by way of speakers’\nsentiments towards particular topics. Parlia-\nmentary or Senate debates often bring the ide-\nological differences to the centre stage, though\nsomewhat indirectly. Heated debates in such\nforums tend to focus on the choices proposed\nby the executive that are in sharp conflict\nwith the preference structure of the opposi-\ntion members. Due to this inherent tendency\nof parliamentary debates to focus on topics of\ndisagreement, the sentiments exposited in de-\nbates hold valuable cues to identify the polit-\nical orientation of the participants.\nWe develop a simple classification model\nthat uses a topic-specific sentiment summa-\nrization for republican and democrat speeches\nseparately. Initial results of experiments con-\nducted using a widely used dataset of US\nCongress debates (Thomas et al., 2006) are\nencouraging and show that this simple model\ncompares well with classification models that\nemploy state-of-the-art distributional text rep-\nresentations (Section 4).\n2 Related Work\n2.1 Political Ideology Detection\nPolitical ideology detection has been a rela-\ntively new field of research within the NLP\ncommunity. Most of the previous efforts have\nfocused on capturing the variations in lan-\nguage use in text representing content of differ-\nent ideologies. Beissmann et al. (2016) employ\nbag-of-word features for ideology detection in\ndifferent domains such as speeches in German\nparliament, party manifestos, and facebook\nposts. Sim et al. (2013) use a labeled corpus\nof political writings to infer lexicons of cues\nstrongly associated with different ideologies.\nThese “ideology lexicons” are then used to an-\nalyze political speeches and identify their ide-\nological leanings. Iyyer at al. (2014) recently\nadopted a recursive neural network architec-\nture to detect ideological bias of single sen-\ntences. In addition, topic models have also\nbeen used for ideology detection by identify-\ning latent topic distributions across different\nideologies (Lin et al., 2008; Ahmed and Xing,\n2010). Gerrish and Blei (2011) connected text\nof the legislations to voting patterns of legis-\nlators from different parties.\n2.2 Sentiment Analysis for\nControversy Detection\nSentiment analysis has proved to be a use-\nful tool in detecting controversial topics as\nit can help identify topics that evoke differ-\nent feelings among people on opposite side\nof the arguments. Mejova et al. (2014) ana-\nlyzed language use in controversial news ar-\nticles and found that a writer may choose to\nhighlight the negative aspects of the opposing\nview rather than emphasizing the positive as-\npects of ones view. Lourentzou et al. (2015)\nutilize the sentiments expressed in social me-\ndia comments to identify controversial por-\ntions of news articles. Given a news article\nand its associated comments on social media,\nthe paper links comments with each sentence\nof the article (by using a sentence as a query\nand retrieving comments using BM25 score).\nFor all the comments associated with a sen-\ntence, a sentiment score is then computed, and\nsentences with large variations in positive and\nnegative comments are identified as controver-\nsial sentences. Choi et al. (2010) go one step\nfurther and identify controversial topics and\ntheir sub-topics in news articles.\n3 Using Topic Sentiments for\nIdeology Detection\nLet D = {. . . , d, . . .} be a corpus of political\ndocuments such as speeches or social media\npostings. Let L = {. . . , l, . . .} be the set of\nideology class labels. Typical scenarios would\njust have two class labels (i.e., |L| = 2), but we\nwill outline our formulation for a general case.\nFor document d ∈ D, ld ∈ L denotes the class\nlabel for that document. Our method relies\non the usage of topics, each of which are most\ncommonly represented by a probability distri-\nbution over the vocabulary. The set of topics\nover D, which we will denote using T , may be\nidentified using a topic modeling method such\nas LDA (Blei et al., 2003) unless a pre-defined\nset of handcrafted topics is available.\nGiven a document d and a topic t, our\nmethod relies on identifying the sentiment as\nexpressed by content in d towards the topic t.\nThe sentiment could be estimated in the form\nof a categorical label such as one of positive,\nnegative and neutral (Haney, 2013). Within\nour modelling, however, we adopt a more fine-\ngrained sentiment labelling, whereby the sen-\ntiment for a topic-document pair is a prob-\nability distribution over a plurality of ordi-\nnal polarity classes ranging from strongly pos-\nitive to strongly negative. Let sdt repre-\nsent the topic-sentiment polarity vector of\nd towards t such that sdt(x) represents the\nprobability of the polarity class x. Combin-\ning the topic-sentiment vectors for all top-\nics yields a document-specific topic-sentiment\nmatrix (TSM) as follows:\nSd,T =\n\n\n. . . sdt1(x) . . .\n. . . sdt2(x) . . .\n...\n...\n...\n\n (1)\nEach row in the matrix corresponds to a\ntopic within T , with each element quantifying\nthe probability associated with the sentiment\npolarity class x for the topic t within document\nd. The topic-sentiment matrix above may be\nregarded as a sentiment signature for the doc-\nument over the topic set T .\n3.1 Determining Topic-specific\nSentiments\nIn constructing TSMs, we make use of topic-\nspecific sentiment estimations as outlined\nabove. Typical sentiment analysis methods\n(e.g., NLTK Sentiment Analysis1) are de-\nsigned to determine the overall sentiment for\na text segment. Using such sentiment analysis\nmethods in order to determine topic-specific\nsentiments is not necessarily straightforward.\nWe adopt a simple keyword based approach for\nthe task. For every document-topic pair (t, d),\nwe extract the sentences from d that contain\nat least one of the top-k keywords associated\nwith the topic t. We then collate the sentences\nin the order in which they appear in d and\nform a mini-document dt. This document dt is\nthen passed on to a conventional sentiment an-\nalyzer that would then estimate the sentiment\npolarity as a probability distribution over sen-\ntiment polarity classes, which then forms the\nsdt(.) vector. We use k = 5 and the RNN\nbased sentiment analyzer (Socher et al., 2013)\nin our method.\n3.2 Nearest TSM Classification\nWe now outline a simple classification model\nthat uses summaries of TSMs. Given a la-\nbeled training set of documents, we would like\nto find the prototypical TSM corresponding\nto each label. This can be done by identify-\ning the matrix that minimizes the cumulative\ndeviation from those corresponding to the doc-\numents with the label.\nSl,T = argmin\nX\n∑\nd∈D∧ld=l\n||X − Sd,T ||\n2\nF (2)\nwhere ||M ||F denotes the Frobenius norm.\nIt turns out that such a label-specific signa-\nture matrix is simply the mean of the topic-\nsentiment matrices corresponding to docu-\n1http://text-processing.com/demo/sentiment/\nments that bear the respective label, which\nmay be computed using the below equation.\nSl,T =\n1\n|{d|d ∈ D ∧ ld = l}|\n∑\nd∈D∧ld=l\nSd,T\n(3)\nFor an unseen (test) document d′, we first\ncompute the TSM Sd′,T , and assign it the la-\nbel corresponding to the label whose TSM is\nmost proximal to Sd′,T .\nld′ = argmin\nl\n||Sd′,T − Sl,T ||\n2\nF (4)\n3.3 Logistic Regression Classification\nIn two class scenarios with label such as\n{left, right} or {democrat, republican} as we\nhave in our dataset, TSMs can be flattened\ninto a vector and fed into a logistic regression\nclassifier that learns weights - i.e., co-efficients\nfor each topic + sentiment polarity class com-\nbination. These weights can then be used to\nestimate the label by applying it to the new\ndocument’s TSM.\n4 Experiments\n4.1 Dataset\nWe used the publicly available Convote\ndataset2 (Thomas et al., 2006) for our exper-\niments. The dataset provides transcripts of\ndebates in the House of Representatives of the\nU.S Congress for the year 2005. Each file\nin the dataset corresponds to a single, unin-\nterrupted utterance by a speaker in a given\ndebate. We combine all the utterances of a\nspeaker in a given debate in a single file to\ncapture different opinions/view points of the\nspeaker about the debate topic. We call this\ndocument the view point document (VPD)\nrepresenting the speaker’s opinion about dif-\nferent aspects of the issue being debated. The\ndataset also provides political affiliations of all\nthe speakers – Republican (R), Democrat (D),\nand Independent (I). With there being only\nsix documents for the independent class (four\nin training, two in test), we excluded them\nfrom our evaluation. Table 1 summarizes the\nstatistics about the dataset and distribution\nof different classes. We obtained 50 topics\n2http://www.cs.cornell.edu/home/llee/data/convote.html\nTraining Set Test Set\nRepublican (R) 530 194\nDemocrat (D) 641 215\nTotal 1175 411\nTable 1: Distribution of different classes in the\nConVote dataset.\nMethod R D Total\nGloVe d2v 0.6391 0.6465 0.6430\nTSM-NC 0.6907 0.4558 0.5672\nTSM-LR 0.5258 0.7628 0.6504\nGloVe-d2v + TSM 0.5051 0.7023 0.6088\nTable 2: Results achieved by different methods on\nthe ideology classification task.\nusing LDA from Mallet3 run over the train-\ning dataset. The topic-sentiment matrix was\nobtained using the Stanford CoreNLP senti-\nment API4 (Manning et al., 2014) which pro-\nvides probability distributions over a set of five\nsentiment polarity classes.\n4.2 Methods\nIn order to evaluate our proposed TSM-based\nmethods - viz., nearest class (NC) and logistic\nregression (LR) - we use the following methods\nin our empirical evaluation.\n1. GloVe-d2v: We use pre-trained\nGloVe (Pennington et al., 2014) word\nembeddings to compute vector represen-\ntation of each VPD by averaging the\nGloVe vectors for all words in the doc-\nument. A logistic regression classifier is\nthen trained on the vector representations\nthus obtained.\n2. GloVe-d2v+TSM: A logistic regression\nclassifier trained on the GloVe features as\nwell as TSM features.\n4.3 Results\nTable 2 reports the classification results for\ndifferent methods described above. TSM-\nNC, the method that uses the TSMvectors\nand performs simple nearest class classifica-\ntion achieves an overall accuracy of 57%.\nNext, training a logistic regression classifier\n3http://mallet.cs.umass.edu/\n4https://nlp.stanford.edu/sentiment/code.html\ntrained on TSMvectors as features, TSM-LR,\nachieves significant improvement with an over-\nall accuracy of 65.04%. The word embed-\nding based baseline, the GloVe-d2v method,\nachieves slightly lower performance with an\noverall accuracy of 64.30%. However, we do\nnote that the per-class performance of GloVe-\nd2v method is more balanced with about\n64% accuracy for both classes. The TSM-LR\nmethod on the other hand achieves about 76%\nfor R class and only 52% for the D class. The\nresults obtained are promising and lend weight\nto out hypothesis that ideological leanings of\na person can be identified by using the fine-\ngrained sentiment analysis of the viewpoint a\nperson has towards different underlying topics.\n4.4 Discussion\nTowards analyzing the significance of the re-\nsults, we would like to start with drawing at-\ntention to the format of the data used in the\nTSM methods. The document-specific TSM\nmatrices do not contain any information about\nthe topics themselves, but only about the sen-\ntiment in the document towards each topic;\none may recollect that sdt(.) is a quantification\nof the strength of the sentiment in d towards\ntopic t. Thus, in contrast to distributional em-\nbeddings such as doc2vec, TSMs contain only\nthe information that directly relates to sen-\ntiment towards specific topics that are learnt\nfrom across the corpus. The results indicate\nthat TSM methods are able to achieve compa-\nrable performance to doc2vec-based methods\ndespite usage of only a small slice of informa-\ntiom. This points to the importance of senti-\nment information in determining the political\nleanings from text. We believe that leveraging\nTSMs along with distributional embeddings in\na manner that can combine the best of both\nviews would improve the state-of-the-art of po-\nlitical ideology detection.\nNext, we also studied if there are topics that\nare more polarizing than others and how differ-\nent topics impact classification performance.\nWe identified polarizing topics, i.e, topics that\ninvoke opposite sentiments across two classes\n(ideologies) by using the following equation.\ndist(t, R,D) = ||sR,t − sR,t||F (5)\nHere, sR,t and sD,t represent the sentiment\nMost polarizing topics\nH1: republican congress majority administration leadership n’t\nvote party republicans special\nH2: administration process vote work included find n’t true fix\ncarriers\nH3: health programs education funding million program cuts\ncare billion year\nH4: health insurance small care coverage businesses plans ahps\nemployees state\nH5: military center n’t students recruiters policy houston men\nuniversities colleges\nLeast polarizing topics\nL1: enter director march years response found letter criminal\npaid general\nL2: corps nuclear year energy projects committee project million\nfunding funds\nL3: osha safety workers commission health h.r employers\noccupational bills workplace\nL4: gun police industry lawsuits firearms dept chief\nmanufacturers dealers guns\nL5: medal gold medals individuals reagan history legislation\nronald king limiting\nTable 3: List of most polarizing (top) and least polarizing (bottom) topics as computed using equation 5.\nvectors for topic t for republican and democrat\nclasses. Note that these sentiment vectors are\nthe rows corresponding to topic t in TSMs for\nthe two classes, respectively.\nTable 3 lists the top five topics with most\ndistance, i.e., most polarizing topics (top) and\nfive topics with least distance, i.e.,least polar-\nizing topics (bottom) as computed by equa-\ntion 5. Note that the topics are represented us-\ning the top keywords that they contain accord-\ning to the probability distribution of the topic.\nWe observe that the most polarizing topics in-\nclude topics related to healthcare (H3, H4),\nmilitary programs (H5), and topics related to\nadministration processes (H1 and H2). The\nleast polarizing topics include topics related to\nworker safety (L3) and energy projects (L2).\nOne counter-intuitive observation is topic re-\nlated to gun control (L4) that is amongst the\nleast polarizing topics. This anomaly could\nbe attributed to only a few speeches related\nto this issue in the training set (only 23 out\nof 1175 speeches mention gun) that prevents\na reliable estimate of the probability distribu-\ntions. We observed similar low occurrences of\nother lower distance topics too indicating the\npotential for improvements in computation of\ntopic-specific sentiment representations with\nmore data. In fact, performing the nearest\nneighbor classification (TSM−NC) with only\ntop-10 most polarizing topics led to improve-\nments in classification accuracy from 57% to\n61% suggesting that with more data, better\nTSMrepresentations could be learned that are\nbetter at discriminating between different ide-\nologies.\n5 Conclusions\nWe proposed to exploit topic-specific senti-\nment analysis for the task of automatic ideol-\nogy detection from text. We described a sim-\nple framework for representing political ideolo-\ngies and documents as a matrix capturing sen-\ntiment distributions over topics and used this\nrepresentation for classifying documents based\non their topic-sentiment signatures. Empirical\nevaluation over a widely used dataset of US\nCongressional speeches showed that the pro-\nposed approach performs on a par with classi-\nfiers using distributional text representations.\nIn addition, the proposed approach offers sim-\nplicity and easy interpretability of results mak-\ning it a promising technique for ideology detec-\ntion. Our immediate future work will focus on\nfurther solidifying our observations by using a\nlarger dataset to learn better TSMs for differ-\nent ideologies. Further, the framework easily\nlends itself to be used for detecting ideological\nleanings of authors, social media users, news\nwebsites, magazines, etc. by computing their\nTSMs and comparing against the TSMs of dif-\nferent ideologies.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their valuable comments and sugges-\ntions that helped us improve the quality of this\nwork.\nReferences\nAmr Ahmed and Eric P. Xing. 2010. Staying in-\nformed: Supervised and semi-supervised multi-\nview topical analysis of ideological perspective.\nIn EMNLP, pages 1140–1150. ACL.\nFelix Biessmann. 2016. Automating political bias\nprediction. arXiv preprint arXiv:1608.02195.\nDavid M Blei, Andrew Y Ng, and Michael I Jor-\ndan. 2003. Latent dirichlet allocation. Journal\nof machine Learning research, 3(Jan):993–1022.\nYoonjung Choi, Yuchul Jung, and Sung-Hyon\nMyaeng. 2010. Identifying controversial issues\nand their sub-topics in news articles. In Intel-\nligence and Security Informatics, Pacific Asia\nWorkshop, PAISI 2010, Hyderabad, India, June\n21, 2010. Proceedings, volume 6122 of Lec-\nture Notes in Computer Science, pages 140–153.\nSpringer.\nSean Gerrish and David M. Blei. 2011. Predicting\nlegislative roll calls from text. In Proceedings of\nthe 28th International Conference on Machine\nLearning, ICML 2011, Bellevue, Washington,\nUSA, June 28 - July 2, 2011, pages 489–496.\nOmnipress.\nCarol Haney. 2013. Sentiment analysis: Provid-\ning categorical insight into unstructured textual\ndata. Social Media, Sociality, and Survey Re-\nsearch, pages 35–59.\nMohit Iyyer, Peter Enns, Jordan L. Boyd-Graber,\nand Philip Resnik. 2014. Political ideology de-\ntection using recursive neural networks. In ACL\n(1), pages 1113–1122. The Association for Com-\nputer Linguistics.\nWei-Hao Lin, Eric P. Xing, and Alexan-\nder G. Hauptmann. 2008. A joint topic\nand perspective model for ideological dis-\ncourse. In Machine Learning and Knowl-\nedge Discovery in Databases, European Confer-\nence, ECML/PKDD 2008, Antwerp, Belgium,\nSeptember 15-19, 2008, Proceedings, Part II,\nvolume 5212 of Lecture Notes in Computer Sci-\nence, pages 17–32. Springer.\nIsmini Lourentzou, Graham Dyer, Abhishek\nSharma, and ChengXiang Zhai. 2015. Hotspots\nof news articles: Joint mining of news text &\nsocial media to discover controversial points in\nnews. In Big Data, pages 2948–2950. IEEE.\nChristopher D. Manning, Mihai Surdeanu, John\nBauer, Jenny Rose Finkel, Steven Bethard, and\nDavid McClosky. 2014. The stanford corenlp\nnatural language processing toolkit. In ACL\n(System Demonstrations), pages 55–60. The As-\nsociation for Computer Linguistics.\nYelena Mejova, Amy X. Zhang, Nicholas Di-\nakopoulos, and Carlos Castillo. 2014. Contro-\nversy and sentiment in online news. CoRR,\nabs/1409.8152.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors\nfor word representation. In Proceedings of the\n2014 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2014, Oc-\ntober 25-29, 2014, Doha, Qatar, A meeting of\nSIGDAT, a Special Interest Group of the ACL,\npages 1532–1543. ACL.\nYanchuan Sim, Brice D. L. Acree, Justin H. Gross,\nand Noah A. Smith. 2013. Measuring ideological\nproportions in political speeches. In Proceedings\nof the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 91–101.\nAssociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Ja-\nson Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. 2013. Recursive\ndeep models for semantic compositionality over\na sentiment treebank. In Proceedings of the\n2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1631–1642, Seat-\ntle, Washington, USA. Association for Compu-\ntational Linguistics.\nMatt Thomas, Bo Pang, and Lillian Lee. 2006. Get\nout the vote: Determining support or opposition\nfrom congressional floor-debate transcripts. In\nEMNLP 2007, Proceedings of the 2006 Confer-\nence on Empirical Methods in Natural Language\nProcessing, 22-23 July 2006, Sydney, Australia,\npages 327–335. ACL.\n",
      "id": 54165030,
      "identifiers": [
        {
          "identifier": "186284280",
          "type": "CORE_ID"
        },
        {
          "identifier": "470618836",
          "type": "CORE_ID"
        },
        {
          "identifier": "161509270",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1810.12897",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:pure.qub.ac.uk/portal:publications/6539c915-cb4f-4072-ba7a-af542435daa5",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.18653/v1/w18-6212",
          "type": "DOI"
        },
        {
          "identifier": "1810.12897",
          "type": "ARXIV_ID"
        }
      ],
      "title": "Topic-Specific Sentiment Analysis Can Help Identify Political Ideology",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1810.12897",
        "oai:pure.qub.ac.uk/portal:publications/6539c915-cb4f-4072-ba7a-af542435daa5"
      ],
      "publishedDate": "2018-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://pure.qub.ac.uk/ws/files/158831815/wassa2018.pdf",
        "http://arxiv.org/abs/1810.12897"
      ],
      "updatedDate": "2022-11-12T04:17:27",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/161509270.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/161509270"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/161509270/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/161509270/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/54165030"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Klinger, Roman"
        },
        {
          "name": "Ruppenhofer, Josef"
        },
        {
          "name": "Sonntag, Jonathan"
        },
        {
          "name": "Struß, Julia Maria"
        },
        {
          "name": "Wiegand, Michael"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/619681847",
        "https://api.core.ac.uk/v3/outputs/621740672"
      ],
      "createdDate": "2018-02-10T05:13:03",
      "dataProviders": [
        {
          "id": 22756,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/22756",
          "logo": "https://api.core.ac.uk/data-providers/22756/logo"
        },
        {
          "id": 3352,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3352",
          "logo": "https://api.core.ac.uk/data-providers/3352/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "We present the German Sentiment Analysis Shared Task (GESTALT) which consists of two main tasks: Source, Subjective Expression and Target Extraction from Political Speeches (STEPS) and Subjective Phrase and Aspect Extraction from Product Reviews (StAR). Both tasks focused on fine-grained sentiment analysis, extracting aspects and targets with their associated subjective expressions in the German language. STEPS focused on political discussions from a corpus of speeches in the Swiss parliament. StAR fostered the analysis of product reviews as they are available from the website Amazon.de. Each shared task led to one participating submission, providing baselines for future editions of this task and highlighting specific challenges. The shared task homepage can be found at https://sites.google.com/site/iggsasharedtask/",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/148067875.pdf",
      "fieldOfStudy": null,
      "fullText": "IGGSA Shared Tasks on German Sentiment Analysis (GESTALT)Josef Ruppenhofer‡, Roman Klinger∗†, Julia Maria Struß‡,Jonathan Sonntag§, Michael Wiegand◦‡ Dept. of Information Science and Language Technology, Hildesheim University† Institute for Natural Language Processing, University of Stuttgart∗ Semantic Computing Group, CIT-EC, Bielefeld University§ Computational Linguistics, Potsdam University◦ Spoken Language Systems, Saarland University{ruppenho,julia.struss}@uni-hildesheim.deroman.klinger@ims.uni-stuttgart.dejonathan.sonntag@yahoo.demichael.wiegand@lsv.uni-saarland.deAbstractWe present the German Sentiment Anal-ysis Shared Task (GESTALT) which con-sists of two main tasks: Source, Subjec-tive Expression and Target Extraction fromPolitical Speeches (STEPS) and SubjectivePhrase and Aspect Extraction from Prod-uct Reviews (StAR). Both tasks focused onfine-grained sentiment analysis, extractingaspects and targets with their associatedsubjective expressions in the German lan-guage. STEPS focused on political dis-cussions from a corpus of speeches in theSwiss parliament. StAR fostered the anal-ysis of product reviews as they are avail-able from the website Amazon.de. Eachshared task led to one participating sub-mission, providing baselines for future edi-tions of this task and highlighting specificchallenges. The shared task homepage canbe found at https://sites.google.com/site/iggsasharedtask/.1 IntroductionIn opinion mining, we are not only interestedin detecting the presence of opinions (or morebroadly, subjectivity) but determining particularattributes. We want to determine which valence orpolarity an opinion has (positive, negative or neu-tral), how strong it is (intensity), and also knowwhose opinion it is and what it is about. The lasttwo questions are what the task of opinion sourceThis work is licensed under a Creative Commons Attri-bution 4.0 International License (CC BY 4.0). Page numbersand proceedings footer are added by the organizers. Licensedetails: http://creativecommons.org/licenses/by/4.0/and target extraction is concerned with. Sourceand target extraction are capabilities needed forthe analysis of unrestricted language texts, wherethis kind of information cannot be derived frommeta-data and where opinions by multiple sourcesand about multiple, potentially related, targets ap-pear side by side.We present two shared tasks that ran under theauspices of the Interest Group of German Senti-ment Analysis1 (IGGSA). Maintask 1 on Source,Subjective Expression and Target Extraction fromPolitical Speeches (STEPS) constitutes the firstevaluation campaign for source and target ex-traction on German language data. Maintask 2on Subjective Phrase and Aspect Extraction fromProduct Reviews (StAR) focuses on the aspect ex-traction, which is understood as the target of asubjective phrase. For both tasks, publicly avail-able resources have been created, which serve asa reference corpus for the evaluation of opinionsource and target extraction in German.2 Task DescriptionsIn this section, we present the task setting, de-scribe the dataset, the annotation, the subtasks,the evaluation and results for each of the two maintasks (Section 2.1 and Section 2.2), respectively.2.1 Maintask 1Maintask 1 calls for the identification of subjec-tive expressions, sources and targets in parliamen-tary speeches. While these texts can be expectedto be opinionated, they pose the challenges that1https://sites.google.com/site/iggsahome/164sources other than the speaker may be relevantand that the targets, though constrained by topic,can vary widely. As in the case of Maintask 2,the dataset provided is the first one that providespublicly available expression-level annotations onrunning texts of this type for German.2.1.1 DatasetThe STEPS data set stems from the debatesof the Swiss parliament (Schweizer Bundesver-sammlung).2 This particular data set was selectedfor two reasons. First, the source data is opento the public and we can re-distribute it with ourannotations. We were not able to fully ascertainthe copyright situation for German parliamentaryspeeches, which we had also considered. Second,the text calls for annotation of multiple sourcesand targets.As the Swiss parliament is a multi-lingual in-stitution, we were careful to exclude not onlynon-German speeches but also German speechesthat constitute responses to, or comments on,speeches, heckling, and side questions in otherlanguages. This way, our annotators did not haveto label any German data whose correct under-standing might rely on material in a language thatthey might not be able to interpret correctly.Some potential linguistic difficulties consistedin peculiarities of Swiss German found in thedata. For instance, the vocabulary of Swiss Ger-man is different from standard German, often insubtle ways. For instance, the verb vorprellenis used in the following example instead of vor-preschen, which would be expected for Germanspoken in Germany:Es ist unglaublich: Weil die Aussen-ministerin vorgeprellt ist, kann man dasnicht mehr zuru¨cknehmen. (Hans Fehr,Fru¨hjahrsession 2008, Zweite Sitzung –04.03.2008)32The full task test data is available at https://sites.google.com/site/iggsasharedtask/home/testdata-maintask1-salto_tiger-xml.zip . The subtask test data for is at https://sites.google.com/site/iggsasharedtask/home/testdata-maintask1-subtasks-salto_tiger.xml.zip.3http://www.parlament.ch/ab/frameset/d/n/4802/263473/d_n_4802_263473_263632.htm‘It is incredible: because the foreignsecretary acted rashly, we cannot takethat back again.’In order to reduce any negative impact thatmight come from misreadings of the Swiss Ger-man by our annotators, who were German andAustrian rather than Swiss, we selected speechesabout what we deemed to be non-parochial issues.For instance, we picked texts on international af-fairs rather than ones about Swiss municipal gov-ernance.Technically, the STEPS data underwent thefollowing pre-processing pipeline. Sentencesegmentation and tokenization was done usingOpenNLP4, followed by lemmatization with theTreeTagger (Schmid, 1994), constituency pars-ing by the Berkeley parser (Petrov and Klein,2007), and final conversion of the parse treesinto TigerXML-Format using TIGER-tools (Lez-ius, 2002). To perform the annotation we used theSalto-Tool (Burchardt et al., 2006).2.1.2 AnnotationThrough our annotation scheme5, we provide an-notations at the expression level. No sentenceor document-level annotations are manually per-formed or automatically derived.There were no restrictions imposed on annota-tions. The subjective expressions could be verbs,nouns, adjectives or multi-words. The sourcesand targets could refer to any actor or issue as wedid not focus on anything in particular.The definition of subjective expressions (SE)that we used is broad and based on well-knownprototypes. It largely follows the model of whatWilson and Wiebe (2005) subsume under the um-brella term private state, as defined by Quirk etal. (1985): “As a result, the annotation schemeis centered on the notion of private state, a gen-eral term that covers opinions, beliefs, thoughts,feelings, emotions, goals, evaluations, and judg-ments.”:• evaluation (positive or negative):toll ‘great’, doof ‘stupid’4http://opennlp.apache.org/5See https://sites.google.com/site/iggsasharedtask/task-1/STEPS_guide.pdffor the the guidelines we used.165Name Source Target FrameSwissGerman not applicable 14RhetoricalDevices not applicable 64Inferred 344 (7.8%) 177 (3.9%) 97 (2.0%)Uncertain 61 (1.4%) 29 (0.6%) 58 (1.2%)Table 1: Flags annotated across all annotators and files of Maintask 1F1 Dice for true positivesSubjective Expression 63.32 0.92Sources∗ 68.70 0.99Targets∗ 80.63 0.85Table 2: Average inter-annotator agreement across allpairs of annotators on test data of Maintask 1 (F1 is basedon partial overlap; Dice quantifies the amount of overlapfor matches)• (un)certainty:zweifeln ‘doubt’, gewiss ‘certain’• emphasis:sicherlich/bestimmt ‘certainly’• speech acts:sagen ‘say’, anku¨ndigen ‘announce’• mental processes:denken ‘think’, glauben ‘believe’Beyond giving the prototypes, we did not seekto impose on our annotators any particular defini-tion of subjective or opinion expressions from thelinguistic, natural language processing or psycho-logical literature related to subjectivity, appraisal,emotion or related notions.In marking subjective expressions, the anno-tators were told to select minimal spans. Thisguidance was given because we had decided thatwithin the scope of this shared task we wouldforgo any treatment of polarity and intensity. Ac-cordingly, negation, intensifiers and attenuatorsand any other expressions that might affect a min-imal expression’s polarity or intensity could be ig-nored.When labeling sources and targets, annotatorswere asked to first consider syntactic and seman-tic dependents of the subjective expressions. Ifsources and targets were locally unrealized, theannotators could annotate other phrases in thecontext. Where a subjective expression repre-sented the view of the implicit speaker or textauthor, annotators could indicate this by settinga flag Sprecher ‘Speaker’ on the the source ele-ment.For all three types of labels, subjective expres-sions, sources, and targets, annotators had the op-tion of using two additional flags. The first flagwas intended to mark a label instance as Inferiert‘Inferred’. In the case of subjective expressions,this covers, for instance, cases where annotatorswere not sure if an expression constituted a po-lar fact or an inherently subjective expression. Inthe case of sources and targets, the ‘inferred’ labelapplies to cases where the referents cannot be an-notated as local dependents but have to be foundin the context. The second flag afforded annota-tors the ability to mark an annotation as Unsicher‘Uncertain’, if they were unsure whether the spanshould really be labeled with the relevant cate-gory.The annotators were asked to use a flagRhetorisches Stilmittel ‘Rhetorical device’ forsubjective expression instances where subjectiv-ity was conveyed through some kind of rhetoricaldevice such as repetition. Across all three annno-tators, 64 instances were labeled as ‘rhetorical de-166Run MeasureSubjectiveExpression Source Source SE Target Target SERun 3 Prec 63.42 48.55 74.89 56.25 79.71Rec 26.10 11.32 42.46 15.60 58.00F1 36.98 18.36 54.19 24.43 67.14Run 5 Prec 80.56 47.98 58.55 not applicableRec 29.97 10.44 32.65 not applicableF1 43.69 17.14 41.92 not applicableTable 3: Best participant runs for Maintask 1 (3 = rule-based system; 5 = translation-based system, which did not include Targer identification. Results suffixed with sub-jective expressions consider only cases where the system already matched the goldstandard on the subjective expression)vice’ in the data.Finally, the annotation guidelines gave annota-tors the option to mark particular subjective ex-pressions as Schweizerdeutsch ‘Swiss German’when they involved language usage that they werenot fully familiar with. Such cases could then beexcluded or weighted differently for the purposesof system evaluation. In our annotation, thesemarkings were in fact rare with only 14 of suchflag instances across all three annotators.Summing over all three annotators, our datasetcovers 1815 sentences. In total, 4935 subjectiveexpression frame instances were labeled by theannotators combined (2.7 frames/sentence). Re-lated to the frames, 8959 frame element (sourceor target) instances were annotated (1.8 frame el-ements/frame). Although the theory embodied byour guidelines calls for at least one source andtarget label per annotated subjective expressionframe, we find slightly less than one instance ofeach (4427 sources, 4532 targets). In Table 1, wesee that not many flags were annotated by our an-notators. The careful selection of our data withrespect to the topics treated seems to have workedwell. We have few instances of subjective expres-sions that were flagged as Swiss German formu-lations by our annotators. The most common typeof flag was the one for ‘inferred’ labels. Here, in-ference of sources was by far the most commoncase. Note, that fewer labels were marked ‘uncer-tain’ than were marked ‘inferred’. Inference didnot necessarily result in uncertainty.In Table 2, we present results on the inter-annotator agreement on the test data. Oneway of measuring the agreement uses theprecision/recall-framework of evaluation. We cal-culate the relevant numbers based on treating oneannotator as gold and another as system, and aver-aging the results for the three pairs of annotators.For F1, we counted a true positive when therewas partial span overlap. In addition, we presenta token-based multi-κ value (Davies and Fleiss,1982). Given that in our annotation scheme, asingle token can be e.g. a target of one subjectiveexpression while itself being a subjective expres-sion as well, we need to calculate three kappa val-ues covering the binary distinctions between pres-ence of each label and its absence. For subjectiveexpressions κ is 0.39, for sources 0.57, and fortargets 0.46.As exact matches on spans are relatively rare,the Dice coefficient is used to measure the over-lap between a system annotation and a gold stan-dard annotation (Dice, 1945). The Dice coef-ficient dc(S,G) is a similarity measure rangingfrom 0 to 1, wheredc(S,G) =2|S ∩G||S|+ |G| ,and G is the set of tokens in the gold annotationsand S the set of tokens the prediction (the systemlabel), respectively.2.1.3 SubtasksThe STEPS shared task offered a full task as wellas two subtasks:167Full task Identification of subjective expressionswith their respective sources and targets.Subtask 1 Participants are given the subjectiveexpressions and are only asked to identifyopinion sources.Subtask 2 Participants are given the subjectiveexpressions and are only asked to identifyopinion targets.Participants could choose any combination ofthe tasks. However, so as to not give an unfair ad-vantage, the full task was run and evaluated be-fore the gold information on subjective expres-sions was given out for the two subtasks, whichwere run concurrently.2.1.4 Evaluation MetricsThe runs that were submitted by the participantsof the shared task were evaluated on different lev-els, according to the task they chose to participatein. For the full task, there was an evaluation ofthe subjective expressions as well as the targetsand sources for subjective expressions, matchingthe system’s annotations against those in the goldstandard. For subtasks 1 and 2, only the sourcesand targets were evaluated, as the subjective ex-pressions were already given.In this first iteration of the STEPS task, weevaluated against each of our three annotatorsindividually rather than against a single gold-standard. Our intent behind this choice was toretain the variation between the annotators.We used recall to measure the proportion ofcorrect system annotations with respect to thegold standard annotations. Additionally, preci-sion was calculated so as to give the fraction ofcorrect system annotations relative to all the sys-tem annotations. As we did for inter-annotator-agreement, for recall and precision we counted amatch when there was partial span overlap. Sim-ilarly, we again used the Dice coefficient to as-sess the overlap between a system annotation anda gold standard annotation.The group that participated in our main tasksubmitted five different runs, based on two differ-ent system architectures. Table 3 shows the bestresult for each architecture. The scores representaverages across the comparisons relative to eachof the three annotators. The rule-based systemgenerally performed better than the translation-based one. However, the latter was much betterin its precision on recognizing subjective expres-sions in the full task. As is to be expected, whenthe system had already matched the gold standardon the subjective expressions, its performance onsource and target recognition, shown in columnsSource SE , Target SE, is much superior to per-formance in the general case.2.2 Maintask 2: Subjective Phrase andAspect Extraction from Product ReviewsMaintask 2 was designed to foster the develop-ment of systems to automatically extract sub-jective, evaluative phrases from German Ama-zon reviews, aspects described in the review andtheir relation, i.e., which evaluative phrase targetswhich aspect. In addition, another focus is cross-domain learning: The development corpus con-sists of reviews for various products while the testcorpus is from yet another product not known tothe participants before.2.2.1 DatasetFor this task, a data set was provided for train-ing parameters and developing the system. TheUSAGE Review Corpus for Fine Grained MultiLingual Opinion Analysis (Klinger and Cimiano,2014) was previously published and was fullyavailable to the participants from the start of thetask on. It consists of 611 German and 622 En-glish reviews for coffee machines, cutlery sets,microwaves, toasters, trashcans, vacuum clean-ers, and washers from which only the Germanpart has been used in this shared task. To con-struct the test corpus, 1646 reviews for the searchterm Wasserkocher ‘water boiler’ were retrieved.From these, 100 sampled reviews were annotatedand included in the test corpus. The training6 andtest7 data is freely available.2.2.2 AnnotationThe entity classes aspect and evaluative (subjec-tive) expression are annotated in the corpus. Eval-uative expressions are assigned a polarity (posi-6Maintask 2 training data: http://dx.doi.org/10.4119/unibi/citec.2014.147Maintask 2 test data: http://dx.doi.org/10.4119/unibi/2695161168tive, negative, neutral), which is not used in thisshared task, and a set of aspects they refer to. Theannotators were instructed to regard everything asan aspect that is part of a product or related to itand can influence the opinion about it, includingthe whole product itself. Evaluative phrases ex-press an opinion. Negations are not separately an-notated but are part of a phrase. Annotators wereasked to avoid overlapping annotations if possi-ble. The annotations should be as short as possi-ble, as long as the meaning is understandable ifonly the annotations were given (without the sen-tence itself).Every review in the training data is annotatedby two linguists, the test data is annotated by one(the information which of the training data anno-tation corresponds to the annotator of the test datais available).In the following examples, aspects are markedin blue and subjective phrases are marked in red:Ich hatte keine Probleme mit derRu¨ckgabe .I had no problems with the return .return is a target of no problems.no problems is positive.Die Waschmaschine selbst ist toll , derbeiliegende Schlauch ist Schrott.The washer itself is great , the includedhose is junk .washer is a target of great.hose is a target of junk.great is positive.junk is negative.Es sieht sehr hu¨bsch aus, wie einAufbewahrungsbeha¨lter , er ist leicht undeinfach zu benutzen .It looks very neat , like astorage container , and using it is verysimple and easy .– looks is a target of very neat.using is a target of simple and of easy.The inter-annotator agreement of the full train-ing corpus is κ = 0.65 (Cohen’s κ). The inter-annotator F1 measure is 0.71 for aspects, 0.55for subjective phrases and 0.42 for the relationsbetween both (including an error propagation ofhaving the exact same phrases annotated). Thesemeasures can be regarded as upper bounds formeaningful results of an automated approach.Table 4 presents the main statistics of the train-ing and testing corpora. Here, annotator 1 of thetraining corpus performed the annotation of thetest data. Obviously, the number of annotatedphrases is higher in the test data.The most frequent subjective phrases for thedifferent products are very similar. For instance,the phrases gut ‘good’ and sehr zufrieden ‘verysatisfied’ occurs in all top 10 lists of subjec-tive phrases. However, the most frequent aspectphrases are very different, as the product cate-gory itself is frequently used as an aspect (e.g.Kaffeemaschine ‘coffee maker’ or Besteck ‘cut-lery’). In addition, very product class-specificaspects are mentioned frequently, like Wasser‘water’, schneiden ‘cut’, or Edelstahl ‘stainlesssteel’. Some aspects are shared between productcategories, for instance Preis ‘price’ or Qualita¨t‘quality’.Clearly, the cross-domain inference task ismore challenging, as the mentioned aspects arenot as similar as the annotated subjective phrases.2.2.3 SubtasksThe three substasks to be addressed by the parti-cants were:Subtask 2a Identication of subjective phrases.Subtask 2b Identification of aspect phrases.Subtask 2c Identification of subjective phrasesand aspect phrases and indication for eachaspect phrase of which subjective phrase itis the target (if any).2.2.4 Evaluation metrics and BaselineapproachFor evaluation, the F1 measure of the exact matchof the predicted phrases in comparison to the an-notated phrases is taken into account. This isstraight-forward for Subtasks 2a and 2b. In 2c,a pair of aspect and subjective phrase was con-sidered to be correctly identified, if both phrases169Train Ann. 1 Train Ann. 2 TestNumber of reviews 611 100Number of products 127 100Number of Aspects 6340 5055 1662Number of Aspects/Review 10.4 8.3 16.6Number of positive Subj. 3840 3717 823Number of positive Subj./Review 6.3 6.1 8.2Number of negative Subj. 1094 1052 264Number of negative Subj./Review 1.8 1.7 2.6Target Rel. 4085 4643 1013Target Rel./Review 6.7 7.6 10.1Table 4: Statistics of the corpora used in Maintask 2predicted to be participating were identified cor-rectly (on the phrase level) as well as annotated asa pair.For comparison, as a baseline, a machinelearning-based system optimized for in-domaininference was applied8 (Klinger and Cimiano,2013a; Klinger and Cimiano, 2013b). A com-parison of the participant’s result and the baselineis shown in Table 5. It can be observed that thebaseline outperforms the subjective phrase detec-tion, but the result submitted by the participant issuperior in the more difficult cross-domain tasksof aspect extraction. The extraction of relationsclearly remains a challenge.3 Related WorkWhile quite a few shared tasks have addressed therecognition of subjective units of language and,possibly, the classification of their polarity (Se-mEval 2013 Task 2, Twitter Sentiment Analysis(Nakov et al., 2013); SemEval-2010 task 18: Dis-ambiguating sentiment ambiguous adjectives (Wuand Jin, 2010); SemEval-2007 Task 14: AffectiveText (Strapparava and Mihalcea, 2007) inter alia),few tasks have included the extraction of sourcesand targets.The prior work most relevant to the taskspresented here was done in the context of theJapanese NTCIR9 Project. In the NTCIR-6 Opin-8A high-recall combination of the joint configurationand the pipeline setting has been applied.9NII [National Institute of Informatics] Test Collectionion Analysis Pilot Task (Seki et al., 2007), whichwas offered for Chinese, Japanese and English,sources and targets had to be found relative towhole opinionated sentences rather than individ-ual subjective expressions. However, the task al-lowed for multiple opinion sources to be recordedfor a given sentence if there were multiple ex-pressions of opinion. The opinion source for asentence could occur anywhere in the document.In the evaluation, as necessary, co-reference in-formation was used to (manually) check whethera system response was part of the correct chainof co-referring mentions. The sentences in thedocument were judged as either relevant or non-relevant to the topic (=target). Polarity was deter-mined at the sentence level. For sentences withmore than one opinion expressed, the polarity ofthe main opinion was carried over to the sentenceas a whole. All sentences were annotated by threeraters, allowing for strict and lenient (by major-ity vote) evaluation. The subsequent Multilin-gual Opinion Analysis tasks NTCIR-7 (Seki et al.,2008) and NTCIR-8 (Seki et al., 2010) were basi-cally similar in their setup to NTCIR-6.While GESTALT shared tasks focussed onGerman, the most important difference to theshared tasks organized by NTCIR is that it definedthe source and target extraction task at the level ofindividual subjective expressions. There was nocomparable shared task annotating at the expres-sion level, rendering existing guidelines imprac-for IR Systems170Baseline ParticipantSubtask Prec Rec F1 Prec Rec F1Aspect Phrase 65.5 46.4 54.3 55.5 62.2 58.7Subjective Phrase 51.5 41.4 45.9 51.6 32.0 39.5Relation 15.9 8.3 10.9 12.6 13.8 13.2Table 5: Results of the baseline system and the participant’s best submission in Maintask 2.tical and necessitating the development of com-pletely new guidelines.Another more recent shared task related toGESTALT is the Sentiment Slot Filling track(SSF) that was part of the Shared Task for Knowl-edge Base Population of the Text Analysis Con-ference (TAC) organised by the National Instituteof Standards and Technology (NIST) (Mitchell,2013). The major distinguishing characteristicof that shared task, which is offered exclusivelyfor English language data, lies in its retrieval-like setup. Here, the task is to extract all possi-ble opinion sources and targets from a given text.By contrast, in SSF the task is to retrieve sourcesthat have some opinion towards a given target en-tity or targets of some given opinion sources. Inboth cases, the polarity of the underlying opin-ion is also specified within SSF. The given tar-gets or sources are considered a type of query.The opinion sources and targets are to be retrievedfrom a document collection.10 Unlike GESTALT,SSF uses heterogeneous text documents includingboth newswire and discussion forum data fromthe Web.This year’s SemEval-2014 Task 4 on AspectBased Sentiment Analysis (ABSA) on English re-view data for restaurant and laptop reviews (Pon-tiki et al., 2014) constitutes another related sharedtask. It focused on aspect-based polarity detec-tion. The main differences are that the aspect cat-egories were predefined and that the polarity as-signment did not include the detection of the eval-uative phrases. Therefore, the polarity assignmentwas on the aspect level and the relation betweena subjectivity-bearing word was implicit. An-other difference between ABSA and GESTALT(StAR, specifically) is that the number of products10In 2014, the text from which entities are to be retrievedis restricted to one document per query.taken into account is higher in StAR, motivatinga cross-domain inference challenge.4 Conclusion and OutlookWe reported on the first iteration of two sharedtasks for German sentiment analysis. Both tasksfocused on the discovery of subjective expres-sions and their related entities. In the case ofSTEPS, sources and targets had to be foundand linked to subjective expressions in politicalspeeches, in the case of StAR, aspects had tobe identified and tied to subjective expressions inAmazon reviews.Although a preliminary call for interest had in-dicated interest by 3–4 groups for each of thetasks, in the end each task had only one partic-ipant. We therefore solicited feedback from ac-tual and potential participants at the end of theIGGSA-GESTALT workshop in order to be ableto tailor the tasks better in a future iteration.Based on the discussion, both shared tasks planon including polarity in the evaluation for theirnext iteration. For both tasks, there was discus-sion what a suitable evaluation procedure wouldbe, in particular whether partial matches shouldbe the basis of the main measures or if exactmatches would be more desirable.Specific to STEPS, we are considering con-ducting the evaluation in alternative ways on a fu-ture iteration of the task. One direction to pur-sue is to derive new versions of the gold stan-dard based on the level of inter-annotator agree-ment on the labels. In a full-agreement mode, wewould only retain annotations of the gold stan-dard that had majority or even full agreement onthe subjective expression level for all three an-notators. Another alternative would consist inestablishing an expert-adjudicated gold-standard,after all. The benefit of any of these alterna-171tive evaluation modes would be that a clear ob-jective function can be learnt and that the up-per bound for system performance would againbe 100% precision/recall/F1-score, whereas it waslower for this iteration given that existing differ-ences between the annotators necessarily led tofalse positives and negatives.For the next iteration of GESTALT, we plan tomake a baseline system available, such that thebarrier to participation in the shared task is lowerand participants’ efforts can be focused on the ac-tual methods.AcknowledgmentsWe would like to thank Simon Clematide forhelping us get access to the Swiss data for theSTEPS task. For their support in preparingand carrying out the annotations of this data,we would like to thank Jasper Brandes, MelanieDick, Inga Hannemann, and Daniela Schneevogt.We thank the German Society for Computa-tional Linguistics for its financial support of theSTEPS annotation effort. For the annotationsused in the StAR task, we thank Luci Fillingerand Frederike Strunz. Roman Klinger was par-tially funded by the It’s OWL project (‘IntelligentTechnical Systems Ostwestfalen-Lippe’, http://www.its-owl.de/), a leading-edge tech-nology and research cluster funded by GermanMinistry of Education and Research (BMBF).This first and last author were partially supportedby the German Research Foundation (DFG) un-der grants RU 1873/2-1 and WI 4204/2-1, respec-tively.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, and Sebastian Pado. 2006. SALTO - AVersatile Multi-Level Annotation Tool. In Proceed-ings of the 5th Conference on Language Resourcesand Evaluation, pages 517–520.Mark Davies and Joseph L. Fleiss. 1982. Measur-ing agreement for multinomial data. Biometrics,38(4):1047–1051.Lee R. Dice. 1945. Measures of the Amount ofEcologic Association Between Species. Ecology,26(3):297–302.Roman Klinger and Philipp Cimiano. 2013a. Bi-directional inter-dependencies of subjective expres-sions and targets and their value for a joint model.In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 848–854, Sofia, Bulgaria,August. Association for Computational Linguistics.Roman Klinger and Philipp Cimiano. 2013b. Jointand pipeline probabilistic models for fine-grainedsentiment analysis: Extracting aspects, subjectivephrases and their relations. In Data Mining Work-shops (ICDMW), 2013 IEEE 13th InternationalConference on, pages 937–944, Dec.Roman Klinger and Philipp Cimiano. 2014. Theusage review corpus for fine grained multi lingualopinion analysis. In Nicoletta Calzolari (Confer-ence Chair), Khalid Choukri, Thierry Declerck,Hrafn Loftsson, Bente Maegaard, Joseph Mariani,Asuncion Moreno, Jan Odijk, and Stelios Piperidis,editors, Proceedings of the Ninth InternationalConference on Language Resources and Evalua-tion (LREC’14), Reykjavik, Iceland, may. EuropeanLanguage Resources Association (ELRA).Wolfgang Lezius. 2002. TIGERsearch - Ein Such-werkzeug fu¨r Baumbanken. In Stephan Buse-mann, editor, Proceedings of KONVENS 2002,Saarbru¨cken, Germany.Margaret Mitchell. 2013. Overview of the TAC2013Knowledge Base Population Evaluation: EnglishSentiment Slot Filling. In Proceedings of the TextAnalysis Conference (TAC), Gaithersburg, MD,USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wil-son. 2013. SemEval-2013 Task 2: SentimentAnalysis in Twitter. In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 312–320, Atlanta and Georgia and USA. As-sociation for Computational Linguistics.Slav Petrov and Dan Klein. 2007. Improved inferencefor unlexicalized parsing. In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404–411, Rochester, New York, April.Association for Computational Linguistics.Maria Pontiki, Dimitris Galanis, John Pavlopoulos,Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar. 2014. Semeval-2014 task 4:Aspect based sentiment analysis. In Proceedings ofthe 8th International Workshop on Semantic Eval-uation (SemEval 2014), pages 27–35, Dublin, Ire-land, August. Association for Computational Lin-guistics and Dublin City University.172Randolph Quirk, Sidney Greenbaum, Geoffry Leech,and Jan Svartvik. 1985. A comprehensive grammarof the English language. Longman.Helmut Schmid. 1994. Probabilistic part-of-speechtagging using decision trees. In Proceedings of In-ternational Conference on New Methods in Lan-guage Processing, Manchester, UK.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi.Chen, Noriko Kando, and Chin-Yew Lin. 2007.Overview of opinion analysis pilot task at ntcir-6. In Proceedings of NTCIR-6 Workshop Meeting,pages 265–278.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,Hsin-Hsi Chen, and Noriko Kando. 2008.Overview of multilingual opinion analysis task atNTCIR-7. In Proceedings of the 7th NTCIR Work-shop Meeting on Evaluation of Information Ac-cess Technologies: Information Retrieval, QuestionAnswering, and Cross-Lingual Information Access,pages 185–203.Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, andNoriko Kando. 2010. Overview of MultilingualOpinion Analysis Task at NTCIR-8: A Step TowardCross Lingual Opinion Analysis. In Proceedings ofthe 8th NTCIR Workshop Meeting on Evaluation ofInformation Access Technologies: Information Re-trieval, Question Answering and Cross-Lingual In-formation Access, pages 209–220.Carlo Strapparava and Rada Mihalcea. 2007.SemEval-2007 Task 14: Affective Text. In EnekoAgirre, Lluı´s Ma`rquez, and Richard Wicentowski,editors, Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007), pages 70–74. Association for ComputationalLinguistics.Theresa Wilson and Janyce Wiebe. 2005. Annotatingattributions and private states. In Proceedings of theWorkshop on Frontiers in Corpus Annotations II:Pie in the Sky, pages 53–60, Ann Arbor, Michigan,June. Association for Computational Linguistics.Yunfang Wu and Peng Jin. 2010. SemEval-2010 Task18: Disambiguating Sentiment Ambiguous Adjec-tives. In Katrin Erk and Carlo Strapparava, edi-tors, Proceedings of the 5th International Workshopon Semantic Evaluation, pages 81–85, Stroudsburgand PA and USA. Association for ComputationalLinguistics.173",
      "id": 80476409,
      "identifiers": [
        {
          "identifier": "oai:hilpub.uni-hildesheim.de:ubhi/16007",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:hildok.bsz-bw.de:295",
          "type": "OAI_ID"
        },
        {
          "identifier": "621740672",
          "type": "CORE_ID"
        },
        {
          "identifier": "148067875",
          "type": "CORE_ID"
        },
        {
          "identifier": "619681847",
          "type": "CORE_ID"
        }
      ],
      "title": "IGGSA Shared Tasks on German Sentiment Analysis (GESTALT)",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:hildok.bsz-bw.de:295",
        "oai:hilpub.uni-hildesheim.de:ubhi/16007"
      ],
      "publishedDate": "2014-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://hildok.bsz-bw.de/files/295/04_01.pdf"
      ],
      "updatedDate": "2024-11-20T20:02:14",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/148067875.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/148067875"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/148067875/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/148067875/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/80476409"
        }
      ]
    },
    {
      "acceptedDate": "2016-12-22T00:00:00",
      "arxivId": "1609.06772",
      "authors": [
        {
          "name": "Ekman P."
        },
        {
          "name": "Jiang Y.-G."
        },
        {
          "name": "Krizhevsky A."
        },
        {
          "name": "Resch B."
        },
        {
          "name": "You Q."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Mohamed",
        "Yi"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/188789615"
      ],
      "createdDate": "2016-12-01T20:56:48",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2016-01-01T00:00:00",
      "abstract": "We perform spatio-temporal analysis of public sentiment using geotagged photo\ncollections. We develop a deep learning-based classifier that predicts the\nemotion conveyed by an image. This allows us to associate sentiment with place.\nWe perform spatial hotspot detection and show that different emotions have\ndistinct spatial distributions that match expectations. We also perform\ntemporal analysis using the capture time of the photos. Our spatio-temporal\nhotspot detection correctly identifies emerging concentrations of specific\nemotions and year-by-year analyses of select locations show there are strong\ntemporal correlations between the predicted emotions and known events.Comment: To appear in ACM SIGSPATIAL 201",
      "documentType": "research",
      "doi": "10.1145/2996913.2996978",
      "downloadUrl": "http://arxiv.org/abs/1609.06772",
      "fieldOfStudy": null,
      "fullText": "Spatio-Temporal Sentiment Hotspot Detection Using\nGeotagged Photos\nYi Zhu and Shawn Newsam\nElectrical Engineering & Computer Science\nUniversity of California at Merced\nyzhu25,snewsam@ucmerced.edu\nABSTRACT\nWe perform spatio-temporal analysis of public sentiment us-\ning geotagged photo collections. We develop a deep learning-\nbased classifier that predicts the emotion conveyed by an\nimage. This allows us to associate sentiment with place.\nWe perform spatial hotspot detection and show that differ-\nent emotions have distinct spatial distributions that match\nexpectations. We also perform temporal analysis using the\ncapture time of the photos. Our spatio-temporal hotspot\ndetection correctly identifies emerging concentrations of spe-\ncific emotions and year-by-year analyses of select locations\nshow there are strong temporal correlations between the pre-\ndicted emotions and known events.\nCCS Concepts\n•Computing methodologies → Scene understanding;\nNeural networks; •Human-centered computing → Geo-\ngraphic visualization;\nKeywords\nHotspot detection, emotion recognition, geotagged photos,\nspatio-temporal geographic analysis, deep learning\n1. INTRODUCTION\nSpatio-temporal hotspot detection is an important com-\nponent of making cities smart, especially for tasks such as\nmonitoring, early warning, resource allocation, and sustain-\nable management. Hotspot analysis is typically conducted\nby mapping crime rates, monitoring disease outbreaks, lo-\ncating traffic accidents, etc. In this paper, we focus instead\non determining the emotional states of a city’s inhabitants\nas conveyed through their photos as a step towards creating\nan affect-aware city.\nEmotions play important roles in everyone’s daily life. No\nmatter what you do, you will have feelings associated with\nyour activities. Services like Twitter, Facebook, Flickr, or\nSnapchat are great platforms for people to share their emo-\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full cita-\ntion on the first page. Copyrights for components of this work owned by others than\nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-\npublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGSPATIAL’16, October 31-November 03, 2016, Burlingame, CA, USA\nc© 2016 ACM. ISBN 978-1-4503-4589-7/16/10. . . $15.00\nDOI: http://dx.doi.org/10.1145/2996913.2996978\ntional states by posting words/pictures/videos. With geo-\ntagged and timestamped social multimedia, we can associate\nsentiment with geographical locations over time.\nThere exists work on detecting emotions from text such\nas in Twitter posts [12], but much less work on using im-\nage/video data. The reason is simple, words can determin-\nistically express people’s emotions, like “This is awesome!”,\n“I feel blue”, “So upset”. However, predicting emotions in vi-\nsual data is much more subtle and difficult. Luckily, the field\nof computer vision has made great advances recently in high-\nlevel image understanding thanks in large part to deep learn-\ning. With respect to our problem, large-scale visual datasets\nfor emotion recognition [11, 14, 5] have been created allowing\ndeep neural networks to be trained and achieve respectable\nperformance on emotion recognition over the last five years.\nThis has opened the opportunity for work such as ours to ex-\nploit these advances for performing spatio-temporal hotspot\ndetection of public emotion using geo-referenced photos.\nThe major contributions of our work include: (i) We con-\nduct the first investigation into sentiment hotspot detection\nin space and time via geotagged photos. (ii) The spatial\nhotspots for the different emotions have distinct spatial dis-\ntributions and agree with expectations. (iii) Our temporal\nhotspot analysis is able to detect emerging concentrations\nof emotions. And, a year-by-year analysis of specific re-\ngions finds strong correlations between emotions and tem-\nporal events, such as between the level of joy and the success\nof the San Francisco Giants at AT&T park, and between the\nlevel of disgust and the increase in gentrification in the Mis-\nsion residential neighborhood.\n2. RELATEDWORK\nOur work is related to several lines of research.\nGeo-Referenced Multimedia The exponential growth of\npublicly available geo-referenced multimedia has created a\nrange of interesting opportunities to learn about our world.\nAt the intersection of geographic information science and\ncomputer vision, large collections of geotagged photos have\nbeen used to map world phenomena [1], classify land use [15],\ngeolocate photos [4], recognize and model landmarks [13],\nperform smart city and urban planning [10], etc. Although\nonline photo collections represent a wealth of information,\nthey present challenges due to how noisy and diverse they\nare. The challenges in using them for geographic discovery\ninclude inaccurate location information, uneven spatial dis-\ntribution, and varying photographer intent. We are mindful\nof these and recognize they likely temper our results.\nDeep Learning Deep learning has advanced a number of\npattern recognition and machine learning areas including\nar\nX\niv\n:1\n60\n9.\n06\n77\n2v\n1 \n [c\ns.C\nV]\n  2\n1 S\nep\n 20\n16\ncomputer vision in which it debuted as deep convolutional\nneural networks (ConvNets) in 2012 [6]. Since then, re-\nsearchers have applied deep ConvNets to a range of vision\nproblems, obtaining state-of-the-art results. Key to Con-\nvNets’ performance is their ability to learn high-level or\nsemantic features from the data as opposed to the hand-\ncrafted low- to mid-level features traditionally used in image\nanalysis. This level of image analysis, or understanding, is\nimportant for our task since detecting the emotion conveyed\nin an image is a high-level and abstract task.\nEmotion Recognition Emotions represent higher intelli-\ngence and so being able to recognize them is key to artifi-\ncial intelligence. For example, real-time emotion recognition\nduring a customer service phone call can lead to a more sat-\nisfactory experience; analyzing a Twitter user’s emotional\nstate can help detect an emotional crisis; and, a chatting\nrobot who is able to recognize emotions can have better in-\nteraction with users.\nEmotions can be conveyed and therefore detected, at least\nin principle, in various multimedia sources such as text, im-\nages, and videos. We develop our own deep learning based\nsystem to detect the emotions conveyed in geotagged images.\nThis then allows us to associate sentiment with place.\n3. METHODOLOGY\nWe first describe our approach to detecting the emotion\nconveyed by an image. We then describe our spatial and\nspatio-temporal hotspot detection using the Gi* statistic [9]\nand Mann-Kendall test [8].\n3.1 Emotion Recognition\nAs mentioned above, emotion can be conveyed by a num-\nber of multimedia sources such as text, audio, image, videos,\netc. Visual emotion analysis is appealing since vision, as the\nrichest sense, is arguably the most effective at conveying\nemotion. Existing work on visual emotion analysis can be\nclassified into two approaches, dimensional models [7] and\ncategorical models [11, 14]. We focus on categorical analysis\nusing Ekman’s six basic emotions [3]: anger, disgust, fear,\njoy, sadness, and surprise.\nOur goal is using geotagged photos for sentiment hotspot\ndetection. The foundation of our approach is assigning each\nphoto one of the six emotions. We therefore design a per-\nimage emotion classifier using ConvNets. This is motivated\nby the finding of You et al. [14] that ConvNets outperform\ntraditional hand-crafted low-level features on most classes\nin a visual emotion analysis task. Specifically, we start with\na VGG-16 network that has been pre-trained on ImageNet\n[2], and then fine-tune it using the Emotion6 dataset [11].\nOnce trained, our classifier achieves an average accuracy of\n61.95%, which is reasonable and performs much better than\nrandom guess (which is 16.67%).\n3.2 Spatial Hotspot Detection\nOnce we have labeled the geotagged images, we can map\nand start to investigate the spatial distribution of public sen-\ntiment. To simplify the analysis, we divide our study area,\nthe city of San Francisco, into a 1000×1000 grid and assign\neach image to the closest bin center. The resulting quantiza-\ntion of the image locations does not affect our results since\neach bin measures less than approximately 12 × 14 meters\nwhich is finer than the scale of our analysis. All the spa-\ntial analysis below is based on the grid instead of the point\nlocations of the photos.\nOur data can now be considered a 1000×1000×6 datacube\nin which the third dimension is the number of images labeled\nwith a particular emotion. We normalize for the uneven\nspatial distribution of the images by computing the ratio of\neach emotion in each bin. That is, for each emotion, we\ncompute a 1000× 1000 grid where each bin is assigned\nratioek =\nnumber of photos in bin k of emotion e\nnumber of photos in bin k\n, (1)\nwhere k is the spatial index of the bin, and e is the emotion\nclass. Each value in a bin indicates the percentage of a\nparticular emotion evoked at the bin’s location. Hence, for\neach location, the third dimension should sum to 1.\nWe use the Getis-Ord Gi* statistic [9] to find where high\nand low emotion ratios cluster spatially. Note that, for each\nemotion, only bins that contain photos are considered and\nnothing is computed for bins that do not have any photos.\n3.3 Spatio-Temporal Hotspot Detection\nOur geotagged photos have timestamps which enables us\nto perform temporal analysis. These timestamps indicate\nwhen the photo was taken. We temporally bin the photos\nat yearly intervals. Our photos span ten years so we now\nhave a 1000 × 1000 × 10 × 6 datacube in which each bin is\nthe ratio of images with a particular emotion to the total\nimages for a particular location for a particular year. This\nnow allows us to perform spatio-temporal hotspot detection.\nGlobal Detection We perform spatio-temporal hotspot de-\ntection using the emerging hotspot analysis tool1 in ArcGIS.\nWe perform this analysis for each emotion separately. First,\nthe Gi* statistic is computed spatially for each year. This is\nthen followed by a Mann-Kendall test [8] to detect temporal\ntrends at each spatial location. This test essentially looks for\ncorrelations between a spatial location’s Gi* value and time.\nThe emerging hotspot analysis tool classifies each spatial lo-\ncation into one of 17 categories: new hot (cold) spot, consec-\nutive hot (cold) spot, intensifying hot (cold) spot, persistent\nhot (cold) spot, diminishing hot (cold) spot, sporadic hot\n(cold) spot, oscillating hot (cold) spot, historical hot (cold)\nspot, and no trend detected.\nLocal Temporal Analysis The emerging hotspot analysis\ntool identifies spatio-temporal hotspots but does not pro-\nvide detailed information on the year-to-year changes. We\ntherefore perform local analysis at a few locations with the\ngoal of relating the changes to known temporal events. We\nexplore a region’s emotional trend over time by computing\nthe emotion ratio for a bounding-box at yearly intervals:\nratioey =\nNumber of photos locally of emotion e in year y\nNumber of photos locally in year y\n.\n(2)\n4. EXPERIMENTS AND RESULTS\nWe first describe the training and performance of our emo-\ntion classifier. We then present the results of the hotspot\ndetection both in time and space.\n4.1 Datasets\nEmotion6 We use the Emotion6 [11] image dataset to fine-\ntune our emotion classifier. This dataset contains 1,980 im-\nages evenly divided into six emotion classes: anger, disgust,\nfear, joy, sadness, and surprise. The images were collected\n1https://desktop.arcgis.com/en/arcmap/latest/tools/space-\ntime-pattern-mining-toolbox/emerginghotspots.htm\nMission\nDistrict\nMessy Streets &\nRestaurants\n(a)\nAT&T\nPark\nBotanical Garden\nCandlestick\nPark\nAlamoSquare\nPark\nLake\nMerced\n(b)\nAT&T\nPark\nFishermans\nWharf\nPier 70\nAlamoSquare\nPark\nConservatory\t\r  \nof\t\r  Flowers\nCity Hall\nCivic Center Plaza\nUnion Square\nWestfieldCenter\nFerry\n(c)\nFigure 1: Spatial hotspot detection: (a) disgust; (b) joy. Red, yellow, blue represent hot, not significant and\ncold spots, respectively. Spatio-temporal hotspot detection: (c) joy. See the text for more details.\nfrom Flickr by using the class labels and synonyms as search\nterms. We randomly split the dataset into training and val-\nidation subsets in the ratio 8 : 2. All images are resized to\n256× 256 pixels for input to the classifier.\nGeotagged Photos We download geotagged photos from\nFlickr for San Francisco city for the ten year period from\n2006 to 2015. These are the images we label with our emo-\ntion classifier. The total number of images is around 1.9\nmillion. However, some of the images are too dark/light,\ntoo small, or just a placeholder in Flickr, and so we perform\na simple filtering step to remove these images. The dataset\nafter filtering contains 1,753,903 images. The distribution\nby year and predicted emotion can be seen in figure 2.\nFigure 2(a) conveys a sense of popularity of the Flickr\nplatform over the ten year period. The number of uploaded\nphotos reaches a peak of 259,741 in 2011 and then falls each\nyear after that. This decline is interesting although we leave\nit to the reader to stipulate on its cause. Figure 2(b) shows\nthe distribution of emotions as predicted by our classifier.\nThere are more joy and sadness images than other emotions.\nYear\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nN\num\nbe\nr o\nf P\nho\nto\ns\n×10 5\n0\n0.5\n1\n1.5\n2\n2.5\nEmotion Classes\nanger disgust fear joy sadness surprise\nN\num\nbe\nr o\nf P\nho\nto\ns\n×10 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure 2: Number of geo-referenced photos in San\nFrancisco area. Left: per year; right: per emotion.\n4.2 Spatial Hotspot Analysis\nWe now present the results of our spatial hotspot detec-\ntion. We use the optimized hotspot analysis tool2 in ArcGIS\nto compute and visualize our results.\nOne of the challenges of our work is that there is no\nground-truth for evaluation. Nonetheless, we make the fol-\nlowing qualitative observations from the results in Fig. 1:\n(i) Different emotions have distinct spatial patterns (we only\nvisualize the results of emotions joy and disgust for illustra-\ntion). This indicates that our emotion classifier is detecting\nconsistent signals in the geotagged photos. (ii) The detected\nhotspots make sense. For example, Fig. 1(b) shows that joy\nhotspots are detected at the San Francisco botanical garden,\n2http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-\nstatistics-toolbox/optimized-hot-spot-analysis.htm\nAlamo square park, AT&T park, Candlestick park, and the\nFort Mason chapel. Further, these locations are detected\nas coldspots or not being significant for the other emotions.\n(iii) Some places are a mix of all emotions. These places,\nsuch as downtown San Francisco, have a wide variety of\nscenes which results in a relatively balanced distribution.\n4.3 Spatio-Temporal Hotspot Detection\nThe goal here is to identify locations that are significiant in\nboth space and time. We first conduct global detection and\nthen perform temporal analysis for select locations.\nGlobal Detection Fig. 1(c) shows the spatio-temporal\nhotspots detected for the north-east part of San Francisco.\nWe make the following observations: (i) Pier 70 (blue box)\nis detected as a new hotspot which means that is a sta-\ntistically significant hot spot for the final time step (2015)\nbut has never been a statistically significant hot spot before.\nThis makes sense since the Pier 70 buildings were recently\nrenovated in 2014 to host large corporate parties, concert\nevents, expositions, etc. (ii) Tourist destinations and pub-\nlic spaces are detected as intensifying hotspots which means\nthey have been a statistically significant hot spot for ninety\npercent of the time-step intervals, including the final time\nstep, and the intensity of clustering of high counts in each\ntime step is increasing overall and that increase is statisti-\ncally significant. The fact that these are intensifying and not\njust persistent hotspots is interesting. We postulate that it is\ndue to the economic recovery that has occurred during the\nlatter part of our time period which especially affects the\ntourist and leisure industry. (iii) Many locations are de-\ntected as consecutive hotspots which means there is a single\nuninterrupted run of statistically significant hot spot bins\nin the final time-step intervals but the location has never\nbeen a statistically significant hot spot prior to the final hot\nspot run and less than ninety percent of all time-steps are\nstatistically significant hot spots. These locations also tend\nto be detected as cold spots or as having no significance in\nthe spatial hopspot results in Fig. 1(b). Taken together,\nthese results indicate that these locations have recently be-\ncome hotspots. This again could be the result of the im-\nproved economy. It could also be the result of more photos\nbeing captured with GPS-enabled smartphones recently and\nthus having more accurate location information. This would\nmake the photos more concentrated.\nLocal Analysis AT&T park shows up as a spatio-temporal\nhotspot with respect to joy. It is also a location whose sen-\ntiment one might expect to be correlated with the perfor-\nmance of the professional baseball team that plays there, the\nYear\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nJo\ny \nRa\ntio\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n3\n5\n4\n3\n1\n2 1\n4\n2 2\nYear\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nD\nis\ngu\nst\n R\nat\nio\n0\n0.005\n0.01\n0.015\n0.02\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nYear\n0\n2\n4\n6\n8\n10\n12\nM\ned\nia\nn \nSa\nle\ns \nPr\nic\ne \nin\n M\nis\nsi\non\n ($\n)\n×105\nFigure 3: Observed temporal trends for select regions. Left: Joy ratio computed yearly for AT&T park.\nShown above the bars are the end of season rankings of the SF Giants who play at the park. Notice the\nstrong correlation. Middle: Disgust ratio computed yearly for the Mission neighborhood. Notice the steady\nincrease since 2008 which is about when it started to become very popular with young professionals in the\ntech industry. Right: The average house price in the Mission neighborhood from a real estate website3.\nNotice the correlation with the disgust ratio.\nSF Giants. To investigate this, we calculate the joy ratio per\nyear for a window centered on the park. These values are\nplotted in Fig. 3(a). Shown above each bar is the end-of-\nseason ranking of the Giants for each year. The joy ratio\nand ranking are clearly correlated demonstrating that we\nare able to detect public sentiment from geotagged photos.\nWe also perform this local temporal analysis for another\nlocation, the Mission, for the emotion disgust. The Mis-\nsion is one of the less expensive residential neighborhoods\nin San Francisco and is shown to exhibit a relatively large\nnumber of disgust spatial hotspots as shown Fig. 1(a) (this\nfigure also delineates the neighborhood). We compute the\nper year disgust ratio in a window centered on the Mission\nand plot the results in Fig. 3(b). There is a clear increasing\ntrend since 2008 which is about when the Mission started\nto become very popular with young professionals in the tech\nindustry. These were not the traditional Mission residents\nand the detected increase in disgust could be a result of\ntheir reaction to the dirtiness, etc. of the streets. In fact,\nthe yearly disgust ratio is strongly correlated with the aver-\nage home price for the Mission, shown in Fig. 3(c)3. This\nincrease in housing prices is likely also a result of the new\ndemographic.\n5. CONCLUSIONS\nWe conduct the first investigation into using geotagged\nsocial multimedia for spatio-temporal sentiment hotspot de-\ntection. We leverage deep ConvNets to develop an emotion\nclassifier to predict the emotions conveyed in geotagged pho-\ntos. This allows us to associate sentiment with place. We\napply the Getis-Ord Gi* statistic to detect spatial hotspots,\nand show that different emotions have distinct spatial dis-\ntributions that match expectations. We detect emerging\nconcentrations of emotions through spatio-temporal hotspot\ndetection and show that year-by-year analyses of select lo-\ncations are correlated with known events.\n6. ACKNOWLEDGMENTS\nWe gratefully acknowledge the support of NVIDIA Cor-\nporation through the donation of the Titan X GPU used in\nthis work. This work was funded in part by a National Sci-\nence Foundation CAREER grant, #IIS-1150115, and a seed\ngrant from the Center for Information Technology in the In-\n3http://www.trulia.com/real estate/Mission-\nSan Francisco/1436/market-trends/\nterest of Society (CITRIS). We would like to thank the UC\nMerced Spatial Analysis and Research Center (SpARC) for\nhelp with the hotspot analysis.\n7. REFERENCES\n[1] D. Crandall et al. Mapping the World’s Photos. In\nWWW, 2009.\n[2] J. Deng et al. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR, 2009.\n[3] P. Ekman et al. What Emotion Categories or\nDimensions can Observers Judge from Facial\nBehavior. Emotion in the Human Face, 1982.\n[4] J. Hays and A. A. Efros. IM2GPS: Estimating\nGeographic Information from a Single Image. In\nCVPR, 2008.\n[5] Y.-G. Jiang et al. Speech Emotion Recognition Using\nDeep Neural Network and Extreme Learning Machine.\nIn AAAI, 2014.\n[6] A. Krizhevsky et al. ImageNet Classification with\nDeep Convolutional Neural Networks. In NIPS, 2012.\n[7] X. Lu et al. On Shape and the Computability of\nEmotions. In ACM MM, 2012.\n[8] H. B. Mann. Nonparametric Tests Against Trend.\nEconometrica, 1945.\n[9] J. Ord and A. Getis. Local Spatial Autocorrelation\nStatistics: Distributional Issues and an Application.\nGeographical Analysis, 1995.\n[10] S. Paldino et al. Urban Magnetism Through The Lens\nof Geo-tagged Photography. arXiv preprint\narXiv:1503.05502, 2015.\n[11] K.-C. Peng et al. A Mixed Bag of Emotions: Model,\nPredict, and Transfer Emotion Distributions. In\nCVPR, 2015.\n[12] B. Resch et al. Citizen-Centric Urban Planning\nthrough Extracting Emotion Information from\nTwitter in an Interdisciplinary Space-Time-Linguistics\nAlgorithm. Urban Planning, 2016.\n[13] N. Snavely et al. Modeling the World from Internet\nPhoto Collections. IJCV, 2008.\n[14] Q. You et al. Building a Large Scale Dataset for Image\nEmotion Recognition: The Fine Print and The\nBenchmark. In ACM MM, 2016.\n[15] Y. Zhu and S. Newsam. Land Use Classification Using\nConvolutional Neural Networks Applied to\nGround-Level Images. In ACM SIGSPATIAL, 2015.\n",
      "id": 37623147,
      "identifiers": [
        {
          "identifier": "10.1145/2996913.2996978",
          "type": "DOI"
        },
        {
          "identifier": "73378914",
          "type": "CORE_ID"
        },
        {
          "identifier": "2554034627",
          "type": "MAG_ID"
        },
        {
          "identifier": "188789615",
          "type": "CORE_ID"
        },
        {
          "identifier": "1609.06772",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1609.06772",
          "type": "OAI_ID"
        }
      ],
      "title": "Spatio-Temporal Sentiment Hotspot Detection Using Geotagged Photos",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2554034627",
      "oaiIds": [
        "oai:arxiv.org:1609.06772"
      ],
      "publishedDate": "2016-09-21T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1609.06772"
      ],
      "updatedDate": "2021-08-04T20:06:47",
      "yearPublished": 2016,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1609.06772"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/37623147"
        }
      ]
    },
    {
      "acceptedDate": "2018-11-15T00:00:00",
      "arxivId": "1804.05276",
      "authors": [
        {
          "name": "Deb, Ashok"
        },
        {
          "name": "Ferrara, Emilio"
        },
        {
          "name": "Lerman, Kristina"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/540163085",
        "https://api.core.ac.uk/v3/outputs/201626531"
      ],
      "createdDate": "2018-04-18T20:35:28",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 22080,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/22080",
          "logo": "https://api.core.ac.uk/data-providers/22080/logo"
        },
        {
          "id": 645,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/645",
          "logo": "https://api.core.ac.uk/data-providers/645/logo"
        }
      ],
      "depositedDate": "2018-11-15T00:00:00",
      "abstract": "Recent high-profile cyber attacks exemplify why organizations need better\ncyber defenses. Cyber threats are hard to accurately predict because attackers\nusually try to mask their traces. However, they often discuss exploits and\ntechniques on hacking forums. The community behavior of the hackers may provide\ninsights into groups' collective malicious activity. We propose a novel\napproach to predict cyber events using sentiment analysis. We test our approach\nusing cyber attack data from 2 major business organizations. We consider 3\ntypes of events: malicious software installation, malicious destination visits,\nand malicious emails that surpassed the target organizations' defenses. We\nconstruct predictive signals by applying sentiment analysis on hacker forum\nposts to better understand hacker behavior. We analyze over 400K posts\ngenerated between January 2016 and January 2018 on over 100 hacking forums both\non surface and Dark Web. We find that some forums have significantly more\npredictive power than others. Sentiment-based models that leverage specific\nforums can outperform state-of-the-art deep learning and time-series models on\nforecasting cyber attacks weeks ahead of the events",
      "documentType": "research",
      "doi": "10.3390/info9110280",
      "downloadUrl": "http://arxiv.org/abs/1804.05276",
      "fieldOfStudy": "computer science",
      "fullText": "Predicting Cyber Events by Leveraging Hacker Sentiment\nAshok Deb, Kristina Lerman, Emilio Ferrara\nUSC Information Sciences Institute, Marina del Rey, CA\nEmail: {ashok, lerman, ferrarae}@isi.edu\nAbstract— Recent high-profile cyber attacks exemplify why\norganizations need better cyber defenses. Cyber threats are\nhard to accurately predict because attackers usually try to\nmask their traces. However, they often discuss exploits and\ntechniques on hacking forums. The community behavior of the\nhackers may provide insights into groups’ collective malicious\nactivity. We propose a novel approach to predict cyber events\nusing sentiment analysis. We test our approach using cyber\nattack data from 2 major business organizations. We consider\n3 types of events: malicious software installation, malicious\ndestination visits, and malicious emails that surpassed the target\norganizations’ defenses. We construct predictive signals by\napplying sentiment analysis on hacker forum posts to better\nunderstand hacker behavior. We analyze over 400K posts\ngenerated between January 2016 and January 2018 on over\n100 hacking forums both on surface and Dark Web. We find\nthat some forums have significantly more predictive power than\nothers. Sentiment-based models that leverage specific forums\ncan outperform state-of-the-art deep learning and time-series\nmodels on forecasting cyber attacks weeks ahead of the events.\nI. INTRODUCTION\nRecent high-profile cyber attacks—the massive denial of\nservice attack using Mirai botnet, infections of comput-\ners word-wide with WannaCry and Petya ransomware, the\nEquifax data breach—highlight the need for organizations\nto develop cyber crime defenses. Cyber threats are hard\nto identify and predict because the hackers that conduct\nthese attacks often obfuscate their activity and intentions.\nHowever, they may still use publicly accessible forums dis-\ncuss vulnerabilities and share tradecraft about how to exploit\nthem. The behavior of the hacker community, as expressed\nin such venues, may provide insights into group’s malicious\nintent. It has been shown that computational models based on\nvarious behavior learning theories can help in cyber security\nsituational awareness [1]. While cyber situation awareness\n[2], [3] is critical for defending networks, it is focused\non detecting cyber events. In this paper, we describe a\ncomputational method that analyzes discussions on hacker\nforums to predict cyber attacks.\nOpinion mining or sentiment analysis can be linked all\nthe way back to Freud’s 1901 paper on how slips of the\ntongue can reveal a person’s hidden intentions [4]. While\nsentiment analysis was originally developed in the field of\nlinguistics and psychology, it has recently been applied to\na number of other fields with the first seminal work in the\ncomputational sciences being Pang 2002 [5]. Historically, the\ncontext it has been applied to are social networks, comments\n(such as on news sites) and reviews (either for products or\nmovies). In this work, we apply sentiment analysis to posts\non Dark Web forums with the purpose of forecasting cyber\nattacks. The Dark Web consists of websites that are not\nindexed nor searchable by standard search engine and can\nonly be accessed using a special browser service.\nWe further explore the link between community behavior\nand malicious activity. The connection between security\nand human behavior has been studied in designing new\ntechnology [6], here we look to reverse engineer by mapping\nthe malicious events to hacker behavior. Social media has\nbeen shown to be a source of useful data on human behavior\nand used to predict real world events [7], [8], [9]. Here, we\ninspect the ability of hacker forums to predict cyber events.\nWe consider each individual forum, applying sentiment anal-\nysis to each post in the forum. After computing a daily\naverage per forum and a 7 day running average sentiment\nsignal per forum, we test these signals against ground truth\ndata. We determine some forums have significantly more\npredictive power and these isolated forums can beat the\nevaluation models in 36% of the months under study using\nprecision and recall of predictions within a 39-hour window\nof the event.\nII. DATA\nA. Hacker Forum Texts\nWe look at hacking forums from both the Surface Web\nand the Dark Web from 1 January 2016 to 31 January 2018.\nThe Dark Web refers to sites accessible through The Onion\nRouter private network platform [10]. The Surface Web\nrefers to the World Wide Web accessible through standard\nbrowsers. In this paper, we focus only on English posts from\n113 forums which were identified based on cyber security\nkeywords consisting of 432,060 posts. The text from these\nforums were accessed using the methods proposed in [11],\n[12].\nB. Ground truth data\nWe use ground truth data of cyber attacks from 2 major\norganizations in the Defense Industrial Base (DIB) industry.\nHenceforth, we will refer to them as Organization A and\nOrganization B for anonymity. The ground truth comprises\n3 event types:\n• endpoint-malware: a malicious software installation\nsuch as ransomware, spyware and adware is discovered\non a company endpoint device.\nar\nX\niv\n:1\n80\n4.\n05\n27\n6v\n1 \n [c\ns.C\nL]\n  1\n4 A\npr\n 20\n18\n• malicious-destination a visit by a user to a URL or IP\naddress that is malicious in nature or a compromised\nwebsite.\n• malicious-email receipt of an email that contains a\nmalicious email attachment and/or a link to a known\nmalicious destination.\nIII. SENTIMENT ANALYSIS\nThe first effective use of sentiment analysis in a predictive\nsense was by Pang et. al. [5] in assessing movie reviews.\nSince then, sentiment analysis has expanded to other fields.\nSentiment analysis can be done with or without supervision\n(label training data). Supervised methods can be adapted to\ncreate trained models for specific purposes and contexts. The\ndrawback is that labeled data may be highly costly and often\nresearchers end up using AMT - Amazon Mechanical Turk.\nThe alternative is to use lexical-based methods that do not\nrely on labeled data; however, it is hard to create a unique\nlexical-based dictionary to be used for all different contexts.\nDeep learning methods allow for additional functions like\ntaking into account order of words in a sentence like the\nStanford Recursive Deep Model. Methods can either be 2\nway (positive or negative) or 3 way (positive, neutral, neg-\native). Furthermore, dictionary based sentiment algorithms\nare either polarity-based where sentiment is based only of\nthe frequency of positive or negative words whereas valence-\nbased methods factor the intensity of the words into polarity.\nThere are a number of issues with sentiment analysis which\ninclude: word pairs, word tuples, emoticons, slang, sarcasm,\nirony, questions, URLs, code, domain specific use of words\n(shoot an email, dead link), and inversions (small is good\nfor portable electronics) which are difficult for computerized\ntext analysis to handle.\nStudies have found that a methods prediction performance\nvaries considerably from one dataset to another. VADER\nworks well for some tweets, but not for others, depending\non the context. SentiStrength has good Macro F1 values, but\nhas low coverage because it tends to classify a high number\nof instances as neutral.\nThe choice of a sentiment analysis is highly dependent\non the data and application, therefore you need to take\ninto account prediction performance and coverage. There\nis no single method that always achieves a consistent rank\nposition for different datasets. Therefore, in this paper we\ntest multiple methods for sentiment analysis. Most languages\nthemselves are biased positive and if a lexicon is built on\ndata, the positive bias that data can lead to a bias in the\nlexicon. This is why most methods are better at classifying\npositive than neutral or negative methods meaning that they\nare biased, neutral are the hardest to detect [13].\nA. Vader\nVADER: Valence Aware Dictionary for sEntiment Rea-\nsoning [14] is a rule-based sentiment model that has both a\ndictionary and associated intensity measures. It’s dictionary\nhas been tuned for microblog-like contexts and they incor-\nporate 5 generalizable rules that goes beyond pure dictionary\nlookups:\n1) Increase intensity due to exclamation point\n2) Increase intensity due to all caps in the presence of\nother non-all cap words\n3) Increase intensity with degree modifiers i.e. extremely\n4) Negate sentiment with contrastive conjunction i.e. but\n5) Examine the preceding tri-gram to identify cases where\nnegation flips the polarity of the text.\nTherefore, VADER not only captures positive or negative,\nbut also how positive and how negative beyond simple words\ncounts. It is made further robust by the additional rules. It’s\n”gold standard” lexicon was developed manually and with\nAmazon Mechanical Turk. Vader scores range from 0.0 to\n1.0.\nB. LIWC\nLinguistic Inquiry and Word Count (LIWC) [15] was a\npioneer in the computerized text analysis field with the first\nmajor iteration in 2007, we used the updated version LIWC\n2015. It has two components: the processing component and\nthe dictionaries. The heart of LIWC are the dictionaries that\ncontain the lookup words in psychometric categories which is\nable to resolve content words from style words. LIWC counts\nthe inputted words in psychologically meaningful categories\nwhich produces close to 100 dimensions for any given text\nbeing analyzed. For the purposes of this research, we are\nonly focused on Tone which bests maps to sentiment as we\nhave defined it. The Tone scores range from 0 to 100. LIWC\nalso ignores context, irony, sarcasm, and idioms.\nC. SentiStrength\nSentiStrength [16] is another lexicon-based sentiment clas-\nsifier which leverages dictionaries and non-lexical linguistics\ninformation to detect sentiment. SentiStrength focuses on the\nstrength of the sentiment and uses weights for the words\nin its dictionaries. Additionally, positive sentiment strength\nand negative sentiment strength is scored separately. Each\nis scored from 1 to 5, with 5 being the greatest strength.\nFor our purposes, we seek overall sentiment so we subtract\nthe negative sentiment from the positive sentiment so that\nstrongly positive (5,1) becomes 4, neutral (1,1) becomes\n0 and strongly negative (1,5) becomes -4. Therefore, Sen-\ntiStrength scorese range from -4 to 4. SentiStrength is\ndesigned to do better with social media; however, it can’t\nexploit indirect indicators of sentiment. It is also weaker for\npositive sentiment in news-related discussions.\nIV. METHODOLOGY\nIn this section we document the methodology used and\nprocess workflow from the data processing to signal gener-\nation through warning generation and signal testing. Three\ncases studies are used to illustrate the process via example\nand Figure X provides a visual reference.\nA. Processing the Data\nWorking with researchers at Arizona State University, we\nwere able to develop a database of posts from forums on\nboth the Dark Web and Surface Web which discuss computer\nsecurity and network vulnerability topics. To protect the\nfuture utility of these sources, each forum has been coded\nwith a number (forumid) from 1 to 350. The data consist\nof the forumid, date the post was made, and the text of the\npost. The data in this study was from 1 January 2016 to 31\nJanuary 2018. The data was collected by ASU and we used\nan API to pull and store the data in a local server and access\nit via Apache Lucene’s Elastic Search engine.\nB. Evaluating Sentiment Analysis\nAfter a review of the sentiment analysis methods in Sen-\ntiBench [13], we decided to use Vader[14], SentiStrenght[16]\nand LIWC15[15]. For social networks, VADER and LIWC15\nwere found to be the best method for 3-class classification\nand SentiStrength was the winner for 2-class classification.\n[13] These three methods were used because they Vader has\na Python module, SentiStrenght has a Java implementation\nand LIWC15 is a stand-alone program.\nC. Computing Daily Averages\nA sentiment score for each forum post was computed using\nthe three sentiment methods outlined above. Since there can\nbe multiple posts on a forum for a day, we characterization\nthe overall sentiment of the day with a daily average. There\ncan be a wide range of sentiment scores for any given day,\nespecially if there are a lot of posts from on a popular forum.\nIn order to understand the trend of sentiment over time, we\ncompute running averages.\nD. Computing Running Averages\nA running daily average was computed in order to assess\nthe trend of sentiment over time. The more days in the run-\nning average, the smoother the curve and the harder to detect\na change. Whereas no using a running average or making it\nonly 1 or 2 days would have many jump discontinuities and\nswings. We looked at adjusting the running average from 1\nto 30 days and settled on 7 days primarily because that was\nour original prediction window. Figure 1 shows the average\nF1 score various signals computed with running averages of\n3, 7, 10 and 14 days.\nE. Standardizing the Score\nTo make the 3 sentiment scores more comparable, their\nscores were standardized. As previously mentioned, VADER\ngenerates sentiment scores on a scale of 0 to 1, SentiStrength\ngoes from -4 to 4, and LIWC goes from 0 to 100 for Tone.\nWhile standardizing the scores do not affect the correlation\nany forum would have with the ground truth from our\ntarget organizations, it will be necessary when we potentially\ncombine signals from various forums and sentiment methods\nto find more powerful predictors.\nFig. 1: Average F1 Scores by Signal using Different Running\nAverages\nF. Compute Correlations to Find Potential Signals\nAs previously mention, we have ground truth events\nfrom 2 defense industrial base organizations of 3 different\ncyber event types. The event types are endpoint-malware,\nmalicious-destination and malicious-email. Correlations were\ncomputed between all forum-sentiments against all event\ntypes from both organizations. Additionally, since we are\nlooking for predictive signals, we computed correlations with\na negative lag from 0 to 30 days with a lag of -30 meaning\noffset the sentiment signal 30 days before the organization’s\nevent occurrence. A number of signals stood out as being\nmore correlated than others against certain event types as\nseen in Figure I. This shows the LIWC sentiment on Forum\n84 against Organization B’s endpoint-malware events. The\nfact that multiple, consecutive lags have low p-values gives\nsome indication that this might be a useful signal.\nG. Forecasting Models\nWe also apply widely-used ARIMA model for forecasting\nevents. ARIMA stands for autoregressive integrated mov-\ning average. The key idea is that the number of current\nevents (yt) depends on the past counts and forecast errors.\nFormally, ARIMA(p,d,q) defines an autoregressive model\nwith p autoregressive lags, d difference operations, and q\nmoving average lags (see [17]). Given the observed series of\nevents Y = (y1, y2, . . . , yT ), ARIMA(p,d,q) applies d (≥ 0)\ndifference operations to transform Y to a stationary series Y ′.\nThen the predicted value y′t at time point t can be expressed\nin terms of past observed values and forecasting errors which\nis as follows:\ny′t = µy ++\np∑\ni=1\nαiy\n′\nt−i +\nq∑\nj=1\nβjet−j + et (1)\nHere µy is a constant, αi is the autoregressive (AR)\ncoefficient at lag i, βj is the moving average (MA) coefficient\nat lag j, et−j = y′t−j− yˆ′t−j is the forecast error at lag j, and\net is assumed to be the white noise (et ∼ N (0, σ2)). The\nAR model is essentially an ARIMA model without moving\naverage terms.\nTABLE I: Best Signals for Organization B’s Events\nForum# Sent Lag Correlation p Value Events\n84 LIWC -11 0.2170 0.000055 EP-Mal\n84 LIWC -12 0.2221 0.000037 EP-Mal\n84 LIWC -14 0.2185 0.000052 EP-Mal\n219 Vader -18 -0.2329 0.000079 EP-Mal\n264 LIWC -10 0.2472 0.000040 EP-Mal\n264 LIWC -12 0.2362 0.000095 EP-Mal\n264 LIWC -15 0.2380 0.000091 EP-Mal\n159 Senti -14 0.8498 0.000008 Mal-Email\n266 Senti -14 -0.5517 0.000058 Mal-Email\n261 LIWC -3 0.2173 0.000043 Mal-Dest\n266 Senti -27 -0.6243 0.000080 Mal-Dest\nWe use maximum likelihood estimation for learning the\nparameters; more specifically, parameters are optimized with\nLBFGS method [18]. These models assume that (p, d, q) are\nknown and the series is weakly stationary. To select the\nvalues for (p, d, q) we employ grid search over the values\nof (p, d, q) and select the one with minimum AIC score.\nH. Testing Signals with ARIMAX\nAgain, Table I shows the signals that are better correlated\nwith Organization B’s ground truth events. The next step\nis to test these signals to see if they have any predictive\npower. To do this, the ARIMA model is used with the ground\ntruth events to develop a baseline model from which to\ncompare potential signals for the potential to have predictive\npower. Additionally, 4 other methods were used for com-\nparison: Dark-Mentions, Deep-Exploit [19], ARIMAX with\nabuse.ch and a daywise-hourly-baserate model. Using ground\ntruth events from both Organization A and Organization\nB, sentiment signals from the various forums, computed\nwith the different methodologies were tested. Testing was\ndone across the 3 event types for both Organizations with\nPrecision, Recall and F1 computed to evaluate the signal. The\ntimeseries of the sentiment for a given forum and sentiment\nmethod was used as the input to the timeseries forecasting\nmodel to predict future events. The model was trained on\ndata from April 1, 2016 to May 31, 2017, in order to\nstart generating warnings for the month of June 2017. After\npredictions were made for the month of June, they were\nscored against the actual ground truth and then the model\nwas ran again to predict warnings for August 2017. This\nwas done for all the way through January 2018.\nI. Scoring\nTo determine how well the signals under study performed,\na matching algorithm was used to compare the date occur-\nrence of the predicted events with the actual events that\noccurred. Using the matching algorithm, we could consis-\ntently score which predicted events should be mapped to\nactual events and which predicted events did not occur as\nwell as which actual events were not predicted. There is a\nwindow around the actual events which varies based on the\nevent type. Endpoint-maleware has to be within 0.875 days,\nmalicious-destination within 1.625 days and malicious-email\nwithin 1.375 days.\nJ. External Signals\nCurrently, there are other external signals that the data\nprovider Organizations are currently evaluating for predictive\npotential. Again, external signals are timeseries information\nderived from open sources that are not based on informa-\ntion system network data. The other external signals under\nevaluation are:\n• ARIMAX: is the same model outlined in §4.7; however,\ntime series counts of malicious activity are acquired\nfrom https://abuse.ch and used in conjunction\nwith historical data.\n• Baseline: is the exact same model in §4.7 with no\nexternal signal and using only historical ground truth\ndata to predict the future rate of attack.\n• Daywise-Baserate: is the same as the ARIMAX model\nmentioned above; however, the model takes day of the\nweek into consideration assuming that the event rate for\neach day of the week is not the same.\n• Deep-Exploit: is an ARIMA model that is based on the\nvulnerability analysis determined by [20]. This method\nreferred to as DarkEmbed learns the embeddings of\nDark Web posts and then uses a trained exploit classifier\nto predicted which vulnerabilities in Dark Web posts\nmight be exploited.\n• Dark-Mentions: Is an extension of [21] which predicts\nif a disclosed vulnerability will be exploited based on\na variety of data sources in addition to the Dark Web\nusing methods still being developed. These predictions\nare used to construct a rule based forecasting method\nbased on keyword mentions in Dark Web forums and\nmarketplaces.\nV. RESULTS\nAfter generating ARIMAX models with each potential\nsignal, they were scored as mentioned above for each month\nfrom July 2017 to January 2018. The following tables show\nthe results for the months under study. By month, you can see\nthe number of actual ground truth events (Evt), the number of\nwarnings generated by each signal (Warn), and the precision\n(P), recall (R) and F1 score for each. The table is sorted by\nlargest F1 score for each month with only the top five signals\nlisted. Signals generated by sentiment analysis that were part\nof the top five for each month are highlighted in light blue.\nA. Organization A\nTable II shows Organization A’s endpoint-malware where\nsentiment signals dominated July, September and November\nand did reasonable well in the remaining months. Every\nmonth a sentiment signal beat at least on evaluation model.\nMalicious-Destination (Table III) had periodic performance\nJuly, September, November and January but the case is\nnot as strong as Endpoint-Malware. Lastly, Table IV shows\nMalicious-Email results which illustrate that sentiment sig-\nnals did well in July to September with waning results for\nthe later months. Upon further inspection this is believed to\nbe due to some key forums going offline toward the end of\nthe year.\nTABLE II: Results from Organization A’s Endpoint-Malware\nMonth Evt Warn Signal P R F1\nJuly 15 14 forum211-Senti 0.57 0.53 0.55\nJuly 15 29 forum196-LIWC 0.41 0.80 0.55\nJuly 15 27 forum89-Senti 0.41 0.73 0.52\nJuly 15 12 forum111-LIWC 0.58 0.47 0.52\nJuly 15 9 baseline 0.67 0.40 0.50\nAugust 19 14 baseline 0.71 0.53 0.61\nAugust 19 11 forum111-LIWC 0.82 0.47 0.60\nAugust 19 35 forum8-Vader 0.46 0.84 0.59\nAugust 19 8 daywise-baserate 1.00 0.42 0.59\nAugust 19 23 forum230-Senti 0.52 0.63 0.57\nSeptember 18 16 forum111LIWC 0.69 0.61 0.65\nSeptember 18 32 forum250LIWC 0.50 0.89 0.64\nSeptember 18 35 forum211vader 0.46 0.89 0.60\nSeptember 18 41 forum147LIWC 0.41 0.94 0.58\nSeptember 18 41 forum194LIWC 0.41 0.94 0.58\nOctober 6 14 daywise-baserate 0.29 0.67 0.40\nOctober 6 35 baseline 0.17 1.00 0.29\nOctober 6 29 forum8vader 0.17 0.83 0.29\nOctober 6 37 forum111LIWC 0.16 1.00 0.28\nOctober 6 43 forum211vader 0.14 1.00 0.24\nNovember 27 38 forum6senti 0.63 0.89 0.74\nNovember 27 42 forum147LIWC 0.60 0.93 0.72\nNovember 27 40 forum111LIWC 0.60 0.89 0.72\nNovember 27 41 forum211senti 0.59 0.89 0.71\nNovember 27 43 forum121LIWC 0.56 0.89 0.69\nDecember 13 18 arimax 0.33 0.46 0.39\nDecember 13 16 dark-mentions 0.31 0.38 0.34\nDecember 13 80 forum121LIWC 0.16 1.00 0.28\nDecember 13 73 forum194LIWC 0.16 0.92 0.28\nDecember 13 10 deep-exploit 0.30 0.23 0.26\nJanuary 1 15 dark-mentions 0.07 1.00 0.13\nJanuary 1 37 forum6senti 0.03 1.00 0.05\nJanuary 1 61 forum147LIWC 0.02 1.00 0.03\nJanuary 1 64 baseline 0.02 1.00 0.03\nJanuary 1 19 arimax 0.00 0.00 0.00\nB. Organization B\nTable V shows that sentiment signals do best for July\nand October for Malicious-Destination. While baseline and\ndaywise-baserate dominate the other months, sentiment sig-\nnals perform better than the other evaluation models. Similar\nto Organization A, the Malicious-Destination for Organiza-\ntion B (Table VI) does the best early in July in August\nand moderately well in September to November until de-\ngrading to below all evaluation models in December and\nJanuary. This may be due the few number of events and\nperhaps sentiment signals do not perform the best under low\nfrequency conditions. The performance for Malicious-Email\n(Table VII) is oddly cyclical; however, sentiment signals\ndominated December and beat at least one evaluation model\nfor every month.\nVI. RELATED WORK\nGiven the serious nature of cyber attacks, naturally there\nare a number of other research efforts to predict such attacks.\nAs it relates to our efforts, the three main areas of research\nare sentiment analysis in cyber security, predictive methods\nfor cyber attacks and leveraging dark web data in cyber\nsecurity.\nTABLE III: Results from Organization A’s Malicious-\nDestination\nMonth Evt Warn Signal P R F1\nJuly 4 5 baseline 0.40 0.50 0.44\nJuly 4 3 daywise-baserate 0.33 0.25 0.29\nJuly 4 17 dark-mentions 0.12 0.50 0.19\nJuly 4 42 forum266-LIWC 0.05 0.50 0.09\nJuly 4 0 arimax 0.00 0.00 0.00\nAugust 10 6 baseline 1.00 0.60 0.75\nAugust 10 10 daywise-baserate 0.60 0.60 0.60\nAugust 10 8 dark-mentions 0.50 0.40 0.44\nAugust 10 0 arimax 0.00 0.00 0.00\nAugust 10 0 deep-exploit 0.00 0.00 0.00\nSeptember 4 15 forum194LIWC 0.20 0.75 0.32\nSeptember 4 15 forum210LIWC 0.20 0.75 0.32\nSeptember 4 15 forum264LIWC 0.20 0.75 0.32\nSeptember 4 15 forum6senti 0.20 0.75 0.32\nSeptember 4 15 forum194LIWC 0.20 0.75 0.32\nOctober 2 0 arimax 0.00 0.00 0.00\nOctober 2 0 dark-mentions 0.00 0.00 0.00\nOctober 2 5 daywise-baserate 0.00 0.00 0.00\nOctober 2 0 deep-exploit 0.00 0.00 0.00\nNovember 1 5 daywise-baserate 0.20 1.00 0.33\nNovember 1 6 forum111LIWC 0.17 1.00 0.29\nNovember 1 6 forum147LIWC 0.17 1.00 0.29\nNovember 1 30 forum210senti 0.03 1.00 0.06\nNovember 1 0 arimax 0.00 0.00 0.00\nDecember 1 10 daywise-baserate 0.10 1.00 0.18\nDecember 1 11 dark-mentions 0.09 1.00 0.17\nDecember 1 0 arimax 0.00 0.00 0.00\nDecember 1 0 deep-exploit 0.00 0.00 0.00\nJanuary 2 24 forum111LIWC 0.08 1.00 0.15\nJanuary 2 0 arimax 0.00 0.00 0.00\nJanuary 2 10 dark-mentions 0.00 0.00 0.00\nJanuary 2 9 daywise-baserate 0.00 0.00 0.00\nJanuary 2 0 deep-exploit 0.00 0.00 0.00\nA. Sentiment Analysis in Cyber Security\nThe closest work which has applied sentiment analysis\nof hacker forums to cyber security is [22]. While much\nresearch has investigated the specifics of cyber attacks, [22]\ninvestigates the actual cyber actors via their communication\nactivities. Their focus was the cyber-physical systems related\nto critical infrastructure and they developed an automated\nanalysis tool to identify potential threats against such in-\nfrastructure. Despite recognizing that there are over 140\nhacker forums on the public web, the authors chose only one\nforum to scrape the complete forum once. They leveraged\nthe Open Discussion Forum Crawler to do the scrapping\nand then used OpenNLP to tag parts of speech, filtering on\nnouns. Those nouns were cross referenced with three list\nof malicious keywords to identify posts whose sentiment\nwould be determined with SentiStrength. Contextual analysis\nof keyword pairings with sentiment scores allowed them to\nconfirm current statistics about critical infrastructure cyber\nattacks. The main differences illustrated in our work is\nthat we use looked at over 100 forums, not just one from\nboth the Dark Web and Surface Web. In collecting posts\nfor over a two year period, we found the sentiment of all\nposts applying Vader and LIWC for sentiment in addition to\njust SentiStrength. Furthermore, we were able to model our\ndata against ground truth events from companies making our\napproach predictive in nature.\nTABLE IV: Results from Organization A’s Malicious-Email\nMonth Evt Warn Signal P R F1\nJuly 26 21 forum210-LIWC 0.76 0.62 0.68\nJuly 26 27 forum250-LIWC 0.67 0.69 0.68\nJuly 26 19 forum147-LIWC 0.74 0.54 0.62\nJuly 26 36 forum159-Senti 0.53 0.73 0.61\nJuly 26 17 forum28-LIWC 0.76 0.50 0.60\nAugust 11 17 forum179-Vader 0.59 0.91 0.71\nAugust 11 15 forum250-LIWC 0.60 0.82 0.69\nAugust 11 7 daywise-baserate 0.86 0.55 0.67\nAugust 11 18 forum210-Senti 0.50 0.82 0.62\nAugust 11 25 forum159-Senti 0.44 1.00 0.61\nSeptember 15 36 forum264LIWC 0.36 0.87 0.51\nSeptember 15 17 daywise-baserate 0.47 0.53 0.50\nSeptember 15 18 forum210senti 0.44 0.53 0.48\nSeptember 15 45 forum147LIWC 0.31 0.93 0.47\nSeptember 15 46 forum6senti 0.28 0.87 0.43\nOctober 11 14 daywise-baserate 0.50 0.64 0.56\nOctober 11 8 deep-exploit 0.50 0.36 0.42\nOctober 11 42 forum264LIWC 0.17 0.64 0.26\nOctober 11 51 forum194LIWC 0.16 0.73 0.26\nOctober 11 102 forum8vader 0.11 1.00 0.19\nNovember 50 16 daywise-baserate 0.69 0.22 0.33\nNovember 50 4 deep-exploit 0.75 0.06 0.11\nNovember 50 0 arimax 0.00 0.00 0.00\nNovember 50 0 dark-mentions 0.00 0.00 0.00\nDecember 17 22 daywise-baserate 0.55 0.71 0.62\nDecember 17 10 deep-exploit 0.80 0.47 0.59\nDecember 17 5 dark-mentions 0.80 0.24 0.36\nDecember 17 0 arimax 0.00 0.00 0.00\nJanuary 40 18 daywise-baserate 0.94 0.43 0.59\nJanuary 40 8 deep-exploit 0.75 0.15 0.25\nJanuary 40 6 dark-mentions 0.83 0.13 0.22\nJanuary 40 0 arimax 0.00 0.00 0.00\nBiSAL [23] did sentiment analysis in English and Arabic\non Dark Web forums with slight modification to cyber\nsecurity terms. Other work such as [24] use sentiment in\nmeasuring radicalization. Remaining research in sentiment\nanalysis, not specific to cyber security was presented earlier.\nB. Predicting Cyber Attack\nThe issue of predicting cyber attacks is not new and\ntheir has been a considerable research effort in this field.\nThe efforts split along two categories, using network traffic\nor non-network traffic. Forecasting methods such as [25],\n[26], [27] analyze network traffic. Where [25] is specific\nto predicting attacks using IPV4 packet traffic, and [26]\nlooks at various network sensors at different layers to prevent\nunwanted Internet traffic, whereas [27] combines DNS traffic\nwith security metadata such as number of policy violations\nand the number of clients in the network. Many researchers\nsuch as [28] based cyber prediction on open source infor-\nmation. They use the National Vulnerability Database. They\nhighlight the difficulty in using public sources for building\neffective models. Other work has focused on detecting cyber\nbullying using graph detection models [29] with success, but\nis limited in malicious activity and not a predictive model.\nThe closest to our research is Gandotra et al [30] who\noutlined a number cyber prediction efforts using statistical\nmodeling and algorithmic modeling. They highlight several\nsignificant challenges that we tried to address. The first\nchallenge is that open source ground truth is incomplete\nTABLE V: Results from Organization B’s Endpoint-Malware\nMonth Evt Warn Signal P R F1\nJuly 18 47 forum264LIWC 0.38 1.00 0.55\nJuly 18 50 forum250LIWC 0.36 1.00 0.53\nJuly 18 43 baseline 0.37 0.89 0.52\nJuly 18 35 forum8senti 0.37 0.72 0.49\nJuly 18 50 forum111LIWC 0.32 0.89 0.47\nAugust 28 39 baseline 0.67 0.93 0.78\nAugust 28 31 forum264LIWC 0.65 0.71 0.68\nAugust 28 32 forum121LIWC 0.63 0.71 0.67\nAugust 28 35 forum211vader 0.60 0.75 0.67\nAugust 28 33 forum194LIWC 0.61 0.71 0.66\nSeptember 31 40 baseline 0.60 0.77 0.68\nSeptember 31 38 forum210senti 0.61 0.74 0.67\nSeptember 31 37 forum121LIWC 0.57 0.68 0.62\nSeptember 31 46 forum219vader 0.50 0.74 0.60\nSeptember 31 30 forum194LIWC 0.60 0.58 0.59\nOctober 53 44 forum210LIWC 0.77 0.64 0.70\nOctober 53 47 baseline 0.74 0.66 0.70\nOctober 53 41 forum264LIWC 0.78 0.60 0.68\nOctober 53 39 forum250LIWC 0.74 0.55 0.63\nOctober 53 40 forum8vader 0.73 0.55 0.62\nNovember 37 52 daywise-baserate 0.62 0.86 0.72\nNovember 37 49 forum121LIWC 0.57 0.76 0.65\nNovember 37 53 forum147LIWC 0.55 0.78 0.64\nNovember 37 50 forum111LIWC 0.56 0.76 0.64\nNovember 37 50 forum194LIWC 0.56 0.76 0.64\nDecember 35 30 daywise-baserate 0.67 0.57 0.62\nDecember 35 27 baseline 0.63 0.49 0.55\nDecember 35 23 forum250LIWC 0.65 0.43 0.52\nDecember 35 28 forum194LIWC 0.57 0.46 0.51\nDecember 35 29 forum147LIWC 0.55 0.46 0.50\nJanuary 43 42 baseline 0.60 0.58 0.59\nJanuary 43 37 daywise-baserate 0.59 0.51 0.55\nJanuary 43 35 forum219vader 0.60 0.49 0.54\nJanuary 43 37 forum111LIWC 0.57 0.49 0.53\nJanuary 43 37 forum147LIWC 0.57 0.49 0.53\nand should be compiled from multiple sources and analysis\ndoesn’t scale to real world scenarios. We were able to get\nground truth data from 2 companies that operate in the\ndefense industrial base, this ground truth is across three\ndifferent attack vectors and is over a two year time period.\nThe additional challenges in [30] focus on the volume, speed\nand heterogeneity of network data which we avoid since we\nare attempting to prevent cyber events specifically with non-\nnetwork data. They also present two modeling approaches\nof statistical modeling and algorithmic modeling. We used\nstatistical models not unlike what they present as classical\ntime series models with auto-regressive, integrated moving\naverage with historical data and external signals.\nC. Dark Web Research\nThere has been a lot of research recently concerning the\nDark Web or websites not indexed by major search engines.\nTypically the Dark Web refers to the TOR [10] network\nwhich is only accessible via specialized browsers. It has been\nshown by [12] that from an overall cyber security threat\nperspective, the Dark Web provides a valuable source of\ninformation for malicious activity. They developed a system\nthat scrapes hacker forum and marketplace sites on the Dark\nWeb to develop threat warnings for cyber defenders. We\nleverage the same data source but perform sentiment analysis\nTABLE VI: Results from Organization B’s Malicious-\nDestination\nMonth Evt Warn Signal P R F1\nJuly 6 8 forum130vader 0.63 0.83 0.71\nJuly 6 8 forum8senti 0.63 0.83 0.71\nJuly 6 8 forum111LIWC 0.50 0.67 0.57\nJuly 6 12 forum194LIWC 0.42 0.83 0.56\nJuly 6 9 forum210senti 0.44 0.67 0.53\nAugust 8 6 forum210senti 0.67 0.50 0.57\nAugust 8 17 daywise-baserate 0.35 0.75 0.48\nAugust 8 13 forum211senti 0.38 0.63 0.48\nAugust 8 5 forum210LIWC 0.60 0.38 0.46\nAugust 8 21 forum8vader 0.29 0.75 0.41\nSeptember 6 11 daywise-baserate 0.55 1.00 0.71\nSeptember 6 9 forum210LIWC 0.56 0.83 0.67\nSeptember 6 10 forum250LIWC 0.30 0.50 0.37\nSeptember 6 11 forum121LIWC 0.27 0.50 0.35\nSeptember 6 1 forum147LIWC 1.00 0.17 0.29\nOctober 9 8 daywise-baserate 0.25 0.22 0.24\nOctober 9 2 forum121LIWC 0.50 0.11 0.18\nOctober 9 114 forum210senti 0.03 0.33 0.05\nOctober 9 0 arimax 0.00 0.00 0.00\nOctober 9 0 dark-mentions 0.00 0.00 0.00\nNovember 4 14 daywise-baserate 0.29 1.00 0.44\nNovember 4 5 forum210LIWC 0.20 0.25 0.22\nNovember 4 21 forum219vader 0.10 0.50 0.16\nNovember 4 9 forum211vader 0.11 0.25 0.15\nNovember 4 13 forum210senti 0.08 0.25 0.12\nDecember 3 12 daywise-baserate 0.17 0.67 0.27\nDecember 3 0 arimax 0.00 0.00 0.00\nDecember 3 0 dark-mentions 0.00 0.00 0.00\nDecember 3 0 deep-exploit 0.00 0.00 0.00\nJanuary 5 18 daywise-baserate 0.22 0.80 0.35\nJanuary 5 0 arimax 0.00 0.00 0.00\nJanuary 5 0 dark-mentions 0.00 0.00 0.00\nJanuary 5 0 deep-exploit 0.00 0.00 0.00\nto not only predict future threats, but to predict actual attacks.\nThey also leverage the Deep Net which is the portion of the\nSurface Web not indexed by standard search engines.\nWhile not using sentiment analysis, [31] offers insight to\nthe trust establishment between participants in Dark Web\nforums. There may be behavioral patterns of malicious actors\nthat provide insight to future activity. Dark Web conversa-\ntions were shown to provide earlier insights than Surface\nWeb conversations by [19] indicating potential predictive\npower for cyber events. [19] highlight two cases with a major\nDDoS attack and the Mirai attack. There may also be early\ninsights on the Surface Web in many of the social media\nsites as illustrated in [32]. Our work focused only on forums\nwhere it was likely that computer security items would be\ndiscussed, but does contain a mix of Dark Web and Surface\nWeb. There has been work using natural language processing\non Dark Web text for predictive method such as [20]. Other\npredictive approaches such as Cyber Attacker Model Profile\n(CAMP) [33], focus on the macro level of a country and\nfinancial cyber crimes, where we look at a wider range of\nmalicious activity against specific target organizations.\nVII. CONCLUSIONS\nMalicious activity can be very devastating to national\nsecurity, economies, businesses and personal lives. As such,\ncyber security professionals working with major organiza-\ntions and nation states could use all the help they can get\nTABLE VII: Results from Organization B’s Malicious-Email\nMonth Evt Warn Signal P R F1\nJuly 24 49 forum210LIWC 0.33 0.67 0.44\nJuly 24 56 forum210senti 0.30 0.71 0.43\nJuly 24 75 baseline 0.23 0.71 0.34\nJuly 24 81 daywise-baserate 0.21 0.71 0.32\nJuly 24 81 forum130vader 0.21 0.71 0.32\nAugust 57 55 forum111LIWC 0.55 0.53 0.54\nAugust 57 70 baseline 0.49 0.60 0.54\nAugust 57 91 daywise-baserate 0.43 0.68 0.53\nAugust 57 107 forum147LIWC 0.39 0.74 0.51\nAugust 57 153 forum6senti 0.33 0.88 0.48\nSeptember 179 70 daywise-baserate 0.76 0.30 0.43\nSeptember 179 102 forum210senti 0.58 0.33 0.42\nSeptember 179 180 forum210LIWC 0.40 0.40 0.40\nSeptember 179 100 forum147LIWC 0.54 0.30 0.39\nSeptember 179 76 baseline 0.57 0.24 0.34\nOctober 71 125 daywise-baserate 0.50 0.87 0.63\nOctober 71 118 baseline 0.49 0.82 0.61\nOctober 71 90 forum211senti 0.53 0.68 0.60\nOctober 71 142 forum194LIWC 0.44 0.89 0.59\nOctober 71 150 forum210senti 0.42 0.89 0.57\nNovember 426 104 daywise-baserate 0.67 0.16 0.26\nNovember 426 205 forum264LIWC 0.39 0.19 0.25\nNovember 426 118 baseline 0.55 0.15 0.24\nNovember 426 251 forum210LIWC 0.31 0.18 0.23\nNovember 426 579 forum210senti 0.20 0.27 0.23\nDecember 51 69 forum210LIWC 0.30 0.41 0.35\nDecember 51 329 forum147LIWC 0.09 0.55 0.15\nDecember 51 313 forum111LIWC 0.08 0.51 0.14\nDecember 51 249 forum194LIWC 0.08 0.41 0.14\nDecember 51 284 forum211senti 0.08 0.45 0.14\nJanuary 10 12 deep-exploit 0.25 0.30 0.27\nJanuary 10 103 daywise-baserate 0.10 1.00 0.18\nJanuary 10 186 baseline 0.05 1.00 0.10\nJanuary 10 226 forum111LIWC 0.04 1.00 0.08\nin preventing malicious activity. We present a methodology\nto predict malicious cyber events by exploiting malicious\nactor’s behavior via sentiment analysis of posts on hacker\nforums. These forums on both Surface Web and Dark Web\nhave some predictive power to be used as signals external to\nthe network for forecasting attacks using time series models.\nUsing ground truth data from two major organizations in the\nDefense Industrial Base across three different cyber event\ntypes, we show that sentiment signals can be more predictive\nthan a baseline time series model. Additionally, they will\noften beat other state of the art external signals, in the 7\nmonths under study across the 3 event types from the 2\norganizations, sentiment signals performed the best 15 out\nof 42 times or 36%. The signal parameters need to be tuned\nover significant historical data and the source forum could be\nshut off or taken down at any time; however, an automated\nimplementation of this system would still be value added.\nACKNOWLEDGMENT\nThis work has been partly funded by the Intelligence Advanced Research\nProjects Activity (IARPA). The views and conclusions contained herein are\nthose of the authors and should not be interpreted as necessarily representing\nthe official policies, either expressed or implied, of IARPA, or the U.S.\nGovernment. The U.S. Government had no role in study design, data\ncollection and analysis, decision to publish, or preparation of the manuscript.\nThe U.S. Government is authorized to reproduce and distribute reprints for\ngovernmental purposes notwithstanding any copyright annotation therein.\nREFERENCES\n[1] V. Dutt, Y.-S. Ahn, and C. Gonzalez, “Cyber situation awareness:\nmodeling detection of cyber attacks with instance-based learning\ntheory,” Human Factors, vol. 55, no. 3, pp. 605–618, 2013.\n[2] S. Jajodia, P. Liu, V. Swarup, and C. Wang, Cyber situational aware-\nness. Springer, 2009.\n[3] U. Franke and J. Brynielsson, “Cyber situational awareness–a system-\natic review of the literature,” Computers & Security, vol. 46, pp. 18–31,\n2014.\n[4] S. Freud, “1960. the psychopathology of everyday life,” The standard\nedition of the complete psychological works of Sigmund Freud, vol. 6,\n1901.\n[5] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment\nclassification using machine learning techniques,” in Proceedings of\nthe ACL-02 conference on Empirical methods in natural language\nprocessing-Volume 10, pp. 79–86, Association for Computational\nLinguistics, 2002.\n[6] S. L. Pfleeger and D. D. Caputo, “Leveraging behavioral science to\nmitigate cyber security risk,” Computers & security, vol. 31, no. 4,\npp. 597–611, 2012.\n[7] S. Agarwal and A. Sureka, “Applying social media intelligence for\npredicting and identifying on-line radicalization and civil unrest ori-\nented threats,” arXiv preprint arXiv:1511.06858, 2015.\n[8] S. Asur and B. A. Huberman, “Predicting the future with social\nmedia,” in Proceedings of the 2010 IEEE/WIC/ACM International\nConference on Web Intelligence and Intelligent Agent Technology-\nVolume 01, pp. 492–499, IEEE Computer Society, 2010.\n[9] E. Kalampokis, E. Tambouris, and K. Tarabanis, “Understanding the\npredictive power of social media,” Internet Research, vol. 23, no. 5,\npp. 544–559, 2013.\n[10] R. Dingledine, N. Mathewson, and P. Syverson, “Tor: The second-\ngeneration onion router,” tech. rep., Naval Research Lab Washington\nDC, 2004.\n[11] J. Robertson, A. Diab, E. Marin, E. Nunes, V. Paliath, J. Shakarian, and\nP. Shakarian, Darkweb Cyber Threat Intelligence Mining. Cambridge\nUniversity Press, 2017.\n[12] E. Nunes, A. Diab, A. Gunn, E. Marin, V. Mishra, V. Paliath,\nJ. Robertson, J. Shakarian, A. Thart, and P. Shakarian, “Darknet\nand deepnet mining for proactive cybersecurity threat intelligence,”\nin Intelligence and Security Informatics (ISI), 2016 IEEE Conference\non, pp. 7–12, IEEE, 2016.\n[13] R. et al, “Sentibench - a benchmark comparison of state-of-the-practice\nsentiment analysis methods,” in EPJ Data Science, pp. 5–23, 2016.\n[14] C. Hutto and E. Gilbert, “Vader: a parsimonious rule-based model for\nsentiment analysis of social media text,” in 8th international AAAI\nconference on weblogs and social media (ICWSM), AAAI, 2014.\n[15] J. W. Pennebaker, M. E. Francis, and R. J. Booth, “Linguistic inquiry\nand word count: Liwc 2001,” Mahway: Lawrence Erlbaum Associates,\nvol. 71, no. 2001, p. 2001, 2001.\n[16] “Heart and soul: sentiment strength detection in the social web with\nsentistrength,” in Journal of Language and Social Psychology, pp. 24–\n54, 2010.\n[17] R. H. Shumway and D. S. Stoffer, Time series analysis and its\napplications: with R examples. Springer Science & Business Media,\n2010.\n[18] S. Seabold and J. Perktold, “Statsmodels: Econometric and statistical\nmodeling with python,” in Proceedings of the 9th Python in Science\nConference, vol. 57, p. 61, 2010.\n[19] A. Sapienza, A. Bessi, S. Damodaran, P. Shakarian, K. Lerman, and\nE. Ferrara, “Early warnings of cyber threats in online discussions,”\nin 2017 IEEE International Conference on Data Mining Workshops\n(ICDMW), pp. 667–674, IEEE, 2017.\n[20] N. Tavabi, P. Goyal, M. Almukaynizi, P. Shakarian, and K. Lerman,\n“Darkembed: Exploit prediction with neural language models,” in\nIAAI 2018: Thirtieth Annual Conference on Innovative Applications\nof Artificial Intelligence, 2018.\n[21] M. Almukaynizi, E. Nunes, K. Dharaiya, M. Senguttuvan, J. Shakar-\nian, and P. Shakarian, “Proactive identification of exploits in the wild\nthrough vulnerability mentions online,” in Cyber Conflict (CyCon US),\n2017 International Conference on, pp. 82–88, IEEE, 2017.\n[22] M. Macdonald, R. Frank, J. Mei, and B. Monk, “Identifying digital\nthreats in a hacker web forum,” in Advances in Social Networks Analy-\nsis and Mining (ASONAM), 2015 IEEE/ACM International Conference\non, pp. 926–933, IEEE, 2015.\n[23] K. Al-Rowaily, M. Abulaish, N. A.-H. Haldar, and M. Al-Rubaian,\n“Bisal–a bilingual sentiment analysis lexicon to analyze dark web\nforums for cyber security,” Digital Investigation, vol. 14, pp. 53–62,\n2015.\n[24] H. Chen, “Sentiment and affect analysis of dark web forums: Measur-\ning radicalization on the internet,” in Intelligence and Security Infor-\nmatics, 2008. ISI 2008. IEEE International Conference on, pp. 104–\n109, IEEE, 2008.\n[25] H. Park, S.-O. D. Jung, H. Lee, and H. P. In, “Cyber weather forecast-\ning: Forecasting unknown internet worms using randomness analysis,”\nin IFIP International Information Security Conference, pp. 376–387,\nSpringer, 2012.\n[26] E. Pontes, A. E. Guelfi, S. T. Kofuji, and A. A. Silva, “Applying\nmulti-correlation for improving forecasting in cyber security,” in\nDigital Information Management (ICDIM), 2011 Sixth International\nConference on, pp. 179–186, IEEE, 2011.\n[27] N. O. Leslie, R. E. Harang, L. P. Knachel, and A. Kott, “Statistical\nmodels for the number of successful cyber intrusions,” The Journal of\nDefense Modeling and Simulation, p. 1548512917715342, 2017.\n[28] S. Zhang, X. Ou, and D. Caragea, “Predicting cyber risks through na-\ntional vulnerability database,” Information Security Journal: A Global\nPerspective, vol. 24, no. 4-6, pp. 194–206, 2015.\n[29] V. Nahar, S. Unankard, X. Li, and C. Pang, “Sentiment analysis for\neffective detection of cyber bullying,” in Asia-Pacific Web Conference,\npp. 767–774, Springer, 2012.\n[30] E. Gandotra, D. Bansal, and S. Sofat, “Computational techniques for\npredicting cyber threats,” in Intelligent Computing, Communication\nand Devices, Advance in Intelligent Systems and Computing, pp. 247–\n253, 2015.\n[31] D. Lacey and P. M. Salmon, “Its dark in there: Using systems\nanalysis to investigate trust and engagement in dark web forums,” in\nInternational Conference on Engineering Psychology and Cognitive\nErgonomics, pp. 117–128, Springer, 2015.\n[32] C. Sabottke, O. Suciu, and T. Dumitras, “Vulnerability disclosure in\nthe age of social media: Exploiting twitter for predicting real-world\nexploits.,” in USENIX Security Symposium, pp. 1041–1056, 2015.\n[33] P. A. Watters, S. McCombie, R. Layton, and J. Pieprzyk, “Charac-\nterising and predicting cyber attacks using the cyber attacker model\nprofile (camp),” Journal of Money Laundering Control, vol. 15, no. 4,\npp. 430–441, 2012.\n",
      "id": 50753704,
      "identifiers": [
        {
          "identifier": "oai:mdpi.com:/2078-2489/9/11/280/",
          "type": "OAI_ID"
        },
        {
          "identifier": "2796638516",
          "type": "MAG_ID"
        },
        {
          "identifier": "oai:arxiv.org:1804.05276",
          "type": "OAI_ID"
        },
        {
          "identifier": "1804.05276",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "154990569",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:doaj.org/article:79ddf15cbe4a494f961731da71c82339",
          "type": "OAI_ID"
        },
        {
          "identifier": "201626531",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.3390/info9110280",
          "type": "DOI"
        },
        {
          "identifier": "540163085",
          "type": "CORE_ID"
        }
      ],
      "title": "Predicting Cyber Events by Leveraging Hacker Sentiment",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2796638516",
      "oaiIds": [
        "oai:arxiv.org:1804.05276",
        "oai:mdpi.com:/2078-2489/9/11/280/",
        "oai:doaj.org/article:79ddf15cbe4a494f961731da71c82339"
      ],
      "publishedDate": "2018-04-14T00:00:00",
      "publisher": "'MDPI AG'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1804.05276"
      ],
      "updatedDate": "2023-02-11T01:28:27",
      "yearPublished": 2018,
      "journals": [
        {
          "title": "Information",
          "identifiers": [
            "2078-2489"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1804.05276"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/50753704"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Basili, Roberto"
        },
        {
          "name": "Castellucci, Giuseppe"
        },
        {
          "name": "Croce, Danilo"
        },
        {
          "name": "VANZO, ANDREA"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Basili, Roberto",
        "Croce, Danilo",
        "Lenci, Alessandro",
        "Vanzo, Andrea",
        "Magnini, Bernardo",
        "Castellucci, Giuseppe"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/54528697"
      ],
      "createdDate": "2016-11-12T07:32:51",
      "dataProviders": [
        {
          "id": 1084,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1084",
          "logo": "https://api.core.ac.uk/data-providers/1084/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Studi recenti per la Sentiment\r\nAnalysis in Twitter hanno tentato di creare\r\nmodelli per caratterizzare la polarit´a di\r\nun tweet osservando ciascun messaggio\r\nin isolamento. In realt`a, i tweet fanno\r\nparte di conversazioni, la cui natura pu`o\r\nessere sfruttata per migliorare la qualit`a\r\ndell’analisi da parte di sistemi automatici.\r\nIn (Vanzo et al., 2014) `e stato proposto un\r\nmodello basato sulla classificazione di sequenze\r\nper la caratterizzazione della polarit`\r\na dei tweet, che sfrutta il contesto in\r\ncui il messaggio `e immerso. In questo lavoro,\r\nsi vuole verificare l’applicabilit`a di\r\ntale metodologia anche per la lingua Italiana.Recent works on Sentiment\r\nAnalysis over Twitter leverage the idea\r\nthat the sentiment depends on a single\r\nincoming tweet. However, tweets are\r\nplunged into streams of posts, thus making\r\navailable a wider context. The contribution\r\nof this information has been recently\r\ninvestigated for the English language by\r\nmodeling the polarity detection as a sequential\r\nclassification task over streams of\r\ntweets (Vanzo et al., 2014). Here, we want\r\nto verify the applicability of this method\r\neven for a morphological richer language,\r\ni.e. Italian",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/54528697.pdf",
      "fieldOfStudy": null,
      "fullText": "A context based model for Sentiment Analysis in Twitterfor the Italian LanguageAndrea Vanzo(†), Giuseppe Castellucci(‡), Danilo Croce(†) and Roberto Basili(†)(†)Department of Enterprise Engineering(‡)Department of Electronic EngineeringUniversity of Roma, Tor VergataVia del Politecnico 1, 00133 Roma, Italy{vanzo,croce,basili}@info.uniroma2.it, castellucci@ing.uniroma2.itAbstractEnglish. Recent works on SentimentAnalysis over Twitter leverage the ideathat the sentiment depends on a singleincoming tweet. However, tweets areplunged into streams of posts, thus makingavailable a wider context. The contribu-tion of this information has been recentlyinvestigated for the English language bymodeling the polarity detection as a se-quential classification task over streams oftweets (Vanzo et al., 2014). Here, we wantto verify the applicability of this methodeven for a morphological richer language,i.e. Italian.Italiano. Studi recenti per la SentimentAnalysis in Twitter hanno tentato di crearemodelli per caratterizzare la polarita´ diun tweet osservando ciascun messaggioin isolamento. In realta`, i tweet fannoparte di conversazioni, la cui natura puo`essere sfruttata per migliorare la qualita`dell’analisi da parte di sistemi automatici.In (Vanzo et al., 2014) e` stato proposto unmodello basato sulla classificazione di se-quenze per la caratterizzazione della po-larita` dei tweet, che sfrutta il contesto incui il messaggio e` immerso. In questo la-voro, si vuole verificare l’applicabilita` ditale metodologia anche per la lingua Ital-iana.1 IntroductionWeb 2.0 and Social Networks allow users to writeabout their life and personal experiences. Thishuge amount of data is crucial in the study of theinteractions and dynamics of subjectivity on theWeb. Sentiment Analysis (SA) is the computa-tional study and automatic recognition of opinionsand sentiments. Twitter is a microblogging ser-vice that counts about a billion of active users. InTwitter, SA is traditionally treated as any other textclassification task, as proved by most systems par-ticipating to the Sentiment Analysis in Twitter taskin SemEval-2013 (Nakov et al., 2013). A MachineLearning (ML) setting allows to induce detectionfunctions from real world labeled examples. How-ever, the shortness of the message and the resultingsemantic ambiguity represent a critical limitation,thus making the task very challenging. Let us con-sider the following message between two users:Benji: @Holly sono completamente d’accordo con teThe tweet sounds like to be a reply to the previ-ous one. Notice how no lexical or syntactic prop-erty allows to determine the polarity. Let’s looknow at the entire conversation:Benji : @Holly con un #RigoreAl90 vinci facile!!Holly : @Benji Lui vince sempre pero` :) accantoa chiunque.. Nessuno regge il confronto!Benji : @Holly sono completamente d’accordo con teThe first is clearly a positive tweet, followed bya positive one that makes the third positive aswell. Thus, through the conversation we can dis-ambiguate even a very short message. We wantto leverage on this to define a context-sensitiveSA model for the Italian language, in line with(Vanzo et al., 2014). The polarity detection of atweet is modeled as a sequential classification taskthrough the SVMhmm learning algorithm (Altun etal., 2003), as it allows to classify an instance (i.e.a tweet) within an entire sequence. First experi-mental evaluations confirm the effectiveness of theproposed sequential tagging approach combinedwith the adopted contextual information even inthe Italian language.A survey of the existing approaches is presentedin Section 2. Then, Section 3 provides an ac-count of the context-based model. The experimen-tal evaluation is presented in Section 4.2 Related WorkThe spread of microblog services, where userspost real-time opinions about “everything”, posesdifferent challenges in Sentiment Analysis. Clas-sical approaches (Pang et al., 2002; Pang and Lee,2008) are not directly applicable to tweets: theyfocus on relatively large texts, e.g. movie or prod-uct reviews, while tweets are short and informaland a finer analysis is required. Recent works triedto model the sentiment in tweets (Go et al., 2009;Davidov et al., 2010; Bifet and Frank, 2010; Zan-zotto et al., 2011; Croce and Basili, 2012; Si etal., 2013). Specific approaches, e.g. probabilis-tic paradigms (Pak and Paroubek, 2010) or Kernelbased (Barbosa and Feng, 2010; Agarwal et al.,2011; Castellucci et al., 2013), and features, e.g.n-grams, POS tags, polarity lexicons, have beenadopted in the tweet polarity recognition task.In (Mukherjee and Bhattacharyya, 2012) con-textual information, in terms of discourse rela-tions is adopted, e.g. the presence of conditionalsand semantic operators like modals and negations.However, these features are derived by consider-ing a tweet in isolation. The approach in (Vanzoet al., 2014) considers a tweet within its context,i.e. the stream of related posts. In order to ex-ploit this information, a Markovian extension of aKernel-based categorization approach is there pro-posed and it is briefly described in the next section.3 A Context Based Model for SAAs discussed in (Vanzo et al., 2014), contextualinformation about one tweet stems from variousaspects: an explicit conversation, the user attitudeor the overall set of recent tweets about a topic(for example a hashtag like #RigoreAl90). Inthis work, we concentrate our analysis only on theexplicit conversation a tweet belongs to. In linewith (Vanzo et al., 2014), a conversation is a se-quence of tweets, each represented as vectors offeatures characterizing different semantic proper-ties. The Sentiment Analysis task is thus modeledas a sequential classification function that asso-ciates tweets, i.e. vectors, to polarity classes.3.1 Representing TweetsThe proposed representation makes use of differ-ent representations that allow to model differentaspects within a Kernel-based paradigm.Bag of Word (BoWK). The simplest Kernel func-tion describes the lexical overlap between tweets,thus represented as a vector, whose dimensionscorrespond to the presence or not of a word. Evenif very simple, the BoW model is one of the mostinformative representation in Sentiment Analysis,as emphasized since (Pang et al., 2002).Lexical Semantic Kernel (LSK). In order to gen-eralize the BoW model, we provide a furtherrepresentation. A vector for each word is ob-tained from a co-occurrence Word Space built ac-cording to the Distributional Analysis technique(Sahlgren, 2006). A word-by-context matrix M isbuilt through large scale corpus analysis and thenprocessed through Latent Semantic Analysis (Lan-dauer and Dumais, 1997). Dimensionality reduc-tion is applied to M through Singular Value De-composition (Golub and Kahan, 1965): the origi-nal statistical information about M is captured bythe new k-dimensional space, which preserves theglobal structure while removing low-variance di-mensions, i.e. distribution noise. A word can beprojected in the reduced Word Space: the distancebetween vectors surrogates the notion of paradig-matic similarity between represented words, e.g.the most similar words of vincere are perdere andpartecipare. A vector for each tweet is representedthrough the linear combination of its word vectors.Whenever the different representations areavailable, we can combine the contribution of bothvector simply through a juxtaposition, in order toexploit both lexical and semantic properties.3.2 SA as a Sequential Tagging ProblemContextual information is embodied by the streamof tweets in which a message ti is immersed. Astream gives rise to a sequence on which sequencelabeling can be applied: the target tweet is here la-beled within the entire sequence, where contextualconstraints are provided by the preceding tweets.Let formally define a conversational context.Conversational context. For every tweet ti ∈ T ,let r(ti) : T → T be a function that returns eitherthe tweet to which ti is a reply to, or null if ti isnot a reply. Then, the conversational context ΛC,liof tweet ti (i.e., the target tweet) is the sequenceof tweet iteratively built by applying r(·), until ltweets have been selected or r(·) = null.A markovian approach. The sentiment predic-tion of a target tweet can be seen as a sequen-tial classification task over a context, and theSVMhmm algorithm can be applied. Given an in-put sequence x = (x1 . . . xl) ⊆ X , where x is atweet context, i.e. the conversational context pre-viously defined, and xi is a feature vector rep-resenting a tweet, the model predicts a tag se-quence y = (y1 . . . yl) ∈ Y+ after learning a lin-ear discriminant function F : P(X ) × Y+ → Rover input/output pairs. The labeling f(x) is de-fined as: f(x) = arg maxy∈Y+ F (x,y;w). Inthese models, F is linear in some combined fea-ture representation of inputs and outputs Φ(x,y),i.e. F (x,y;w) = 〈w,Φ(x,y)〉. As Φ extractsmeaningful properties from an observation/labelsequence pair (x,y), in SVMhmm it is modeledthrough two types of features: interactions be-tween attributes of the observation vectors xi anda specific label yi (i.e. emissions of a tweet w.r.t.a polarity class) as well as interactions betweenneighboring labels yi along the chain (i.e. transi-tions of polarity labels in a conversation context.).Thus, through SVMhmm the label for a target tweetis made dependent on its context history. Themarkovian setting acquires patterns across tweetsequences to recognize sentiment even for trulyambiguous tweets. Further details about the mod-eling and the SVMhmm application to tweet label-ing can be found in (Vanzo et al., 2014).4 Experimental EvaluationThe aim of the experiments is to verify the appli-cability of the model proposed in (Vanzo et al.,2014) in a different language, i.e. Italian. Inorder to evaluate the models discussed above inan Italian setting, an appropriate dataset has beenbuilt by gathering1 tweets from Twitter servers.By means of Twitter APIs2, we retrieved thewhole corpus by querying several Italian hot top-ics, i.e. expo, mose, renzi, prandelli,mondiali, balotelli and commonly usedemoticons, i.e. :) and :( smiles. Each tweet tiand its corresponding conversation ΛC,li have beenincluded into the dataset if and only if the con-versation itself was available (i.e. |ΛC,li | > 1).Then, three annotators labeled each tweet witha sentiment polarity label among positive,negative, neutral and conflicting3,obtaining a inter-annotator agreement of 0.829,measured as the mean accuracy computed betweenannotators pairs.1The process has been run during June-July 20142http://twitter4j.org/3A tweet is said to be conflicting when it expresses both apositive and negative polarityAs about 1,436 tweets, including conversa-tions, were gathered from Twitter, a static split of64%/16%/20% in Training/Held-out/Test respec-tively, has been carried out as reported in Table 1.train dev testPositive 212 61 69Negative 211 42 92Neutral 387 72 87Conflicting 129 26 48939 201 296Table 1: Dataset compositionTweets have been analyzed through the Chaosnatural language parser (Basili et al., 1998). Anormalization step is previously applied to eachmessage: fully capitalized words are converted inlowercase; reply marks, hyperlinks and hashtagsare replaced with the pseudo-tokens, and emoti-cons have been classified with respect to 13 differ-ent classes. LSK vectors are obtained from a WordSpace derived from a corpus of about 3 milliontweets, downloaded during July and September2013. The methodology described in (Sahlgren,2006) with the setting discussed in (Croce and Pre-vitali, 2010) has been applied.Performance scores are reported in terms of Pre-cision, Recall and F-Measure. We also report boththe F pnn1 score as the arithmetic mean between theF1s of positive, negative and neutral classes, andthe F pnnc1 considering even the conflicting class.It is worth noticing that a slightly different set-ting w.r.t. (Vanzo et al., 2014) has been used. Inthis work we manually labeled every tweet in eachconversation and performance measures considersall the tweets. On the contrary in (Vanzo et al.,2014) only the last tweet of the conversation ismanually labeled and considered in the evaluation.4.1 Experimental ResultsExperiments are meant to verify the ability of acontext-based model in the Italian setting. Asa baseline we considered a multi-class classifierwithin the SVMmulticlass framework (Tsochan-taridis et al., 2004). Each tweet in a conversationis classified considering it in isolation, i.e. withoutusing contextual information. In Table 2, perfor-mances of the Italian dataset are reported, whileTable 3 shows the outcomes of experiments overthe English dataset (Vanzo et al., 2014). Here, w/oconv results refer to a baseline computed with theSVMmulticlass algorithm, while w/ conv results re-fer to the application of the model described in thePrecision Recall F1 Fpnn1 Fpnnc1pos neg neu conf pos neg neu conf pos neg neu confBoWKw/o conv .705 .417 .462 .214 .449 .109 .690 .438 .549 .172 .553 .288 .425 .390w conv .603 .580 .379 .375 .507 .435 .701 .063 .551 .497 .492 .107 .513 .412BoWK+LSKw/o conv .507 .638 .416 .000 .493 .402 .793 .000 .500 .493 .545 .000 .513 .385w conv .593 .560 .432 .368 .464 .457 .736 .146 .520 .503 .545 .209 .523 .444Table 2: Evaluation results of the Italian setting.Precision Recall F1 Fpnn1pos neg neu pos neg neu pos neg neuBoWKw/o conv .713 .496 .680 .649 .401 .770 .679 .444 .723 .615w/ conv .723 .511 .722 .695 .472 .762 .709 .491 .741 .647BoWK+LSKw/o conv .754 .595 .704 .674 .486 .804 .712 .535 .751 .666w/ conv .774 .554 .717 .682 .542 .791 .725 .548 .752 .675Table 3: Evaluation results on the English language from (Vanzo et al., 2014)previous sections with the SVMhmm algorithm. Inthe last setting, the whole conversational contextof each tweet is considered.Firstly, all w/o conv models beneficiate by thelexical generalization provided by the Word Spacein the LSA model. In fact, the information derivedfrom the Word Space seems beneficial in its rela-tive improvement with respect to the simple BoWKernel accuracy, up to an improvement of 20.71%of Fpnn1 , from .425 to .513. However, it is not al-ways true, in particular w.r.t. the conflicting classwhere the smoothing provided by the generaliza-tion negatively impact on the classifiers, that arenot able to discriminate the contemporary pres-ence of positive and negative polarity.Most importantly, the contribution of conver-sations is confirmed in all context-driven models,i.e. w/conv improves w.r.t. their w/o conv coun-terpart. Every polarity category benefits from theintroduction of contexts, although many tweets an-notated with the conflicting (conf ) class are notcorrectly recognized: contextual information un-balances the output of a borderline tweet with thepolarity of the conversations. The impact of con-versational information contribute to a statisticallysignificant improvement of 20.71% in the BoWKsetting, and of 1.95% in the BoWK+LSK setting.In (Vanzo et al., 2014) a larger dataset (10,045examples) has been used for the evaluation of con-textual models in an English setting. The datasetis provided by ACL SemEval-2013 (Nakov et al.,2013). Results are thus not directly comparable,as in this latter dataset, where even tweets with-out a conversational contexts are included, onlythe target tweet is manually labeled and the labelsof remaining tweets have been automatically pre-dicted in a semi supervised fashion, as discussedin (Vanzo et al., 2014). Additionally, the conflict-ing class, where a lexical overlap is observed withboth positive and negative classes, is not consid-ered. However, results in Table 3 show that theBoWK setting benefits by the introduction of thelexical generalization, given by the LSK, with aperformance improvement of 8.29%. When thefocus is held within the same Kernel setting, inboth BoWK and BoWK+LSK, the conversationalinformation seems to be beneficial as increases of5.20% and 1.35%, respectively, are observed.5 ConclusionsIn this work, the role of contextual information insupervised Sentiment Analysis over Twitter is in-vestigated for the Italian language. Experimentalresults confirm the empirical findings presented in(Vanzo et al., 2014) for the English language. Al-though the size of the involved dataset is still lim-ited, i.e. about 1,400 tweets, the importance ofcontextual information is emphasized within theconsidered markovian approach: it is able to takeadvantage of the dependencies that exist betweendifferent tweets in a conversation. The approachis also largely applicable as all experiments havebeen carried out without the use of any manualcoded resource, but mainly exploiting unannotatedmaterial within the distributional method. A largerexperiment, eventually on an oversized dataset,such as SentiTUT4, will be carried out.4http://www.di.unito.it/~tutreeb/sentiTUT.htmlReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau. 2011. Sentimentanalysis of twitter data. In Proceedings of theWorkshop on Languages in Social Media, LSM ’11,pages 30–38, Stroudsburg, PA, USA. Associationfor Computational Linguistics.Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003.Hidden Markov support vector machines. In Pro-ceedings of the International Conference on Ma-chine Learning.Luciano Barbosa and Junlan Feng. 2010. Robust sen-timent detection on twitter from biased and noisydata. In Chu-Ren Huang and Dan Jurafsky, editors,COLING (Posters), pages 36–44. Chinese Informa-tion Processing Society of China.Roberto Basili, Maria Teresa Pazienza, and Fabio Mas-simo Zanzotto. 1998. Efficient parsing for informa-tion extraction. In Proc. of the European Conferenceon Artificial Intelligence, pages 135–139.Albert Bifet and Eibe Frank. 2010. Sentiment knowl-edge discovery in twitter streaming data. In Pro-ceedings of the 13th International Conference onDiscovery Science, DS’10, pages 1–15, Berlin, Hei-delberg. Springer-Verlag.Giuseppe Castellucci, Simone Filice, Danilo Croce,and Roberto Basili. 2013. Unitor: Combiningsyntactic and semantic kernels for twitter sentimentanalysis. In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 369–374, Atlanta, Georgia, USA, June. Asso-ciation for Computational Linguistics.Danilo Croce and Roberto Basili. 2012. Grammaticalfeature engineering for fine-grained ir tasks. In Gi-ambattista Amati, Claudio Carpineto, and GiovanniSemeraro, editors, IIR, volume 835 of CEUR Work-shop Proceedings, pages 133–143. CEUR-WS.org.Danilo Croce and Daniele Previtali. 2010. Mani-fold learning for the semi-supervised induction offramenet predicates: An empirical investigation. InProceedings of the 2010 Workshop on GEometricalModels of Natural Language Semantics, GEMS ’10,pages 7–16, Stroudsburg, PA, USA. Association forComputational Linguistics.Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.Enhanced sentiment learning using twitter hashtagsand smileys. In Chu-Ren Huang and Dan Jurafsky,editors, COLING (Posters), pages 241–249. ChineseInformation Processing Society of China.Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-ter sentiment classification using distant supervision.Processing, pages 1–6.G. Golub and W. Kahan. 1965. Calculating the singu-lar values and pseudo-inverse of a matrix. Journal ofthe Society for Industrial and Applied Mathematics,2(2).T. Landauer and S. Dumais. 1997. A solution to plato’sproblem: The latent semantic analysis theory ofacquisition, induction and representation of knowl-edge. Psychological Review, 104(2):211–240.Subhabrata Mukherjee and Pushpak Bhattacharyya.2012. Sentiment analysis in twitter with lightweightdiscourse analysis. In Proceedings of COLING,pages 1847–1864.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013. Semeval-2013 task 2: Sentiment analysisin twitter. In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 312–320, Atlanta, Georgia, USA, June. Asso-ciation for Computational Linguistics.Alexander Pak and Patrick Paroubek. 2010. Twit-ter as a corpus for sentiment analysis and opinionmining. In Proceedings of the 7th Conference onInternational Language Resources and Evaluation(LREC’10), Valletta, Malta, May. European Lan-guage Resources Association (ELRA).Bo Pang and Lillian Lee. 2008. Opinion mining andsentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002. Thumbs up? sentiment classification us-ing machine learning techniques. In Proceedings ofEMNLP, pages 79–86.Magnus Sahlgren. 2006. The Word-Space Model.Ph.D. thesis, Stockholm University.Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,Huayi Li, and Xiaotie Deng. 2013. Exploiting topicbased twitter sentiment for stock prediction. In ACL(2), pages 24–29.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun. 2004. Support vectormachine learning for interdependent and structuredoutput spaces. In Proceedings of the twenty-first in-ternational conference on Machine learning, ICML’04, pages 104–, New York, NY, USA. ACM.Andrea Vanzo, Danilo Croce, and Roberto Basili.2014. A context-based model for sentiment anal-ysis in twitter. In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics, pages 2345–2354, Dublin, Ireland, Au-gust. Dublin City University and Association forComputational Linguistics.Fabio M. Zanzotto, Marco Pennaccchiotti, and KostasTsioutsiouliklis. 2011. Linguistic Redundancy inTwitter. In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 659–669, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.",
      "id": 30335914,
      "identifiers": [
        {
          "identifier": "oai:iris.uniroma1.it:11573/871178",
          "type": "OAI_ID"
        },
        {
          "identifier": "54528697",
          "type": "CORE_ID"
        }
      ],
      "title": "A context based model for sentiment analysis in twitter for the italian language",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:iris.uniroma1.it:11573/871178"
      ],
      "publishedDate": "2014-01-01T00:00:00",
      "publisher": "Pisa University Press srl",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://iris.uniroma1.it/retrieve/handle/11573/871178/230822/Vanzo_A-context-based_Postprint_2014.pdf"
      ],
      "updatedDate": "2022-12-08T12:12:03",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/54528697.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/54528697"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/54528697/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/54528697/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/30335914"
        }
      ]
    },
    {
      "acceptedDate": "2013-10-16T00:00:00",
      "arxivId": "1305.6143",
      "authors": [
        {
          "name": "Arora, Ishan"
        },
        {
          "name": "Bhatia, Arjun"
        },
        {
          "name": "Narayanan, Vivek"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/456302185"
      ],
      "createdDate": "2014-10-24T19:18:29",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2013-01-01T00:00:00",
      "abstract": "We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.Comment: 8 pages, 2 figure",
      "documentType": "research",
      "doi": "10.1007/978-3-642-41278-3_24",
      "downloadUrl": "http://arxiv.org/abs/1305.6143",
      "fieldOfStudy": null,
      "fullText": "Fast and accurate sentiment classification using an \nenhanced Naive Bayes model. \nVivek Narayanan1, Ishan Arora2, Arjun Bhatia3 \nDepartment of Electronics Engineering, \nIndian Institute of Technology (BHU), Varanasi, India \n1\nvivek.narayanan.ece09@iitbhu.ac.in \n2ishan.arora.ece09@iitbhu.ac.in \n3\narjun.bhatia.ece09@iitbhu.ac.in\nAbstract. We have explored different methods of improving the accuracy of a \nNaive Bayes classifier for sentiment analysis. We observed that a combination \nof methods like effective negation handling, word n-grams and feature selection \nby mutual information results in a significant improvement in accuracy. This \nimplies that a highly accurate and fast sentiment classifier can be built using a \nsimple Naive Bayes model that has linear training and testing time complexi-\nties. We achieved an accuracy of 88.80% on the popular IMDB movie reviews \ndataset. The proposed method can be generalized to a number of text categori-\nzation problems for improving speed and accuracy. \n \nKeywords :- Sentiment classification, Negation Handling, Mutual Information, \nFeature Selection, n-grams \n1 Introduction \nAmong the most researched topics of natural language processing is sentiment analy-\nsis. Sentiment analysis involves extraction of subjective information from documents \nlike online reviews to determine the polarity with respect to certain objects. It is use-\nful for identifying trends of public opinion in the social media, for the purpose of \nmarketing and consumer research. It has its uses in getting customer feedback about \nnew product launches, political campaigns and even in financial markets [14]. It aims \nto determine the attitude of a speaker or a writer with respect to some topic or simply \nthe contextual polarity of a document. Early work in this area was done by Turney and \nPang ([2], [7]) who applied different methods for detecting the polarity of product and \nmovie reviews. \n \nSentiment analysis is a complicated problem but experiments have been done using \nNaive Bayes, maximum entropy classifiers and support vector machines. Pang et al. \nfound the SVM to be the most accurate classifier in [2]. In this paper we present a \nsupervised sentiment classification model based on the Naïve Bayes algorithm.  \nNaïve Bayes is a very simple probabilistic model that tends to work well on text \nclassifications and usually takes orders of magnitude less time to train when com-\npared to models like support vector machines. We will show in this paper that a high \ndegree of accuracy can be obtained using Naïve Bayes model, which is comparable to \nthe current state of the art models in sentiment classification. \n2 Data \nWe used a publicly available dataset of movie reviews from the Internet Movie Data-\nbase (IMDb) [1] which was compiled by Andrew Maas et al. It is a set of 25,000 \nhighly polar movie reviews for training, and 25,000 for testing. Both the training and \ntest sets have an equal number of positive and negative reviews. We chose movie \nreviews as our data set because it covers a wide range of human emotions and cap-\ntures most of the adjectives relevant to sentiment classification. Also, most existing \nresearch on sentiment classification uses movie review data for benchmarking.  \nWe used the 25,000 documents in the training set to build our supervised learning \nmodel. The other 25,000 were used for evaluating the accuracy of our classifier. \n3 Naïve Bayes Classifier \nA Naive bayes classifier is a simple probabilistic model based on the Bayes rule along \nwith a strong independence assumption. \n \nThe Naïve Bayes model involves a simplifying conditional independence assumption. \nThat is given a class (positive or negative), the words are conditionally independent of \neach other. This assumption does not affect the accuracy in text classification by \nmuch but makes really fast classification algorithms applicable for the problem. Ren-\nnie et al discuss the performance of Naïve Bayes on text classification tasks in their \n2003 paper. [6] \nIn our case, the maximum likelihood probability of a word belonging to a particu-\nlar class is given by the expression: \n\u0001\u0002\u0003\u0004| \u0007) =\n\n\u000b\f\r\u000e \u000b\u000f \u0003\u0004 \u0010\r \u0011\u000b\u0007\f\u0012\u0013\r\u000e\u0014 \u000b\u000f \u0007\u0015\u0016\u0014\u0014 \u0007 \n\u0017\u000b\u000e\u0016\u0015 \r\u000b \u000b\u000f \u0018\u000b\u0019\u0011\u0014 \u0010\r \u0011\u000b\u0007\f\u0012\u0013\r\u000e\u0014 \u000b\u000f \u0007\u0015\u0016\u0014\u0014 \u0007\n \n                         (1) \nThe frequency counts of the words are stored in hash tables during the training \nphase. \nAccording to the Bayes Rule, the probability of a particular document belonging to \na class ci is given by, \n\u0001\u0002\u0007\u0004|\u0011) =  \n\u0001\u0002\u0011 |\u0007\u0004) ∗ \u0001\u0002\u0007\u0004)\n\u0001\u0002\u0011)\n     \u00022) \n3 \n \nIf we use the simplifying conditional independence assumption, that given a class \n(positive or negative), the words are conditionally independent of each other. Due to \nthis simplifying assumption the model is termed as “naïve”. \n\u0001\u0002\u0007\u0004|\u0011) =  \n\u0002∏ \u0001\u0002\u0003\u0004|\u0007\u001d)) ∗ \u0001\u0002\u0007\u001d)\n\u0001\u0002\u0011)\n    \u00023) \nHere the xi s are the individual words of the document. The classifier outputs the \nclass with the maximum posterior probability. \nWe also remove duplicate words from the document, they don’t add any additional \ninformation; this type of naïve bayes algorithm is called Bernoulli Naïve Bayes. In-\ncluding just the presence of a word instead of the count has been found to improve \nperformance marginally, when there is a large number of training examples. \n4 Laplacian Smoothing \nIf the classifier encounters a word that has not been seen in the training set, the \nprobability of both the classes would become zero and there won’t be anything to \ncompare between. This problem can be solved by Laplacian smoothing \n \n\u0001\u0002\u0003\u0004|\u0007\u001d) =\n\n\u000b\f\r\u000e\u0002\u0003\u0004) +   \n\u0002 + 1) ∗ \u0002\"\u000b \u000b\u000f \u0018\u000b\u0019\u0011\u0014 \u0010\r \u0007\u0015\u0016\u0014\u0014 \u0007\u001d)\n     \u00024) \n \nUsually, k is chosen as 1. This way, there is equal probability for the new word to \nbe in either class. Since Bernoulli Naïve Bayes is used, the total number of words in a \nclass is computed differently. For the purpose of this calculation, each document is \nreduced to a set of unique words with no duplicates. \n5 Negation Handling \nNegation handling was one of the factors that contributed significantly to the accuracy \nof our classifier. A major problem faced during the task of sentiment classification is \nthat of handling negations. Since we are using each word as feature, the word “good” \nin the phrase “not good” will be contributing to positive sentiment rather that negative \nsentiment as the presence of “not” before it is not taken into account.  \nTo solve this problem we devised a simple algorithm for handling negations using \nstate variables and bootstrapping. We built on the idea of using an alternate represen-\ntation of negated forms as shown by Das & Chen in [3]. Our algorithm uses a state \nvariable to store the negation state. It transforms a word followed by a not or n’t into \n“not_” + word.  Whenever the negation state variable is set, the words read are treated \nas “not_” + word. The state variable is reset when a punctuation mark is encountered \nor when there is double negation. The pseudo code of the algorithm is described be-\nlow: \nPSEUDO CODE:.  \nnegated := False \nfor each word in document: \n   if negated = True: \n       Transform word to “not_” + word. \n   if word is “not” or “n’t”: \n       negated := not negated \n   if a punctuation mark is encountered \n       negated := False. \nSince the number of negated forms might not be adequate for correct classifications. \nIt is possible that many words with strong sentiment occur only in their normal forms \nin the training set. But their negated forms would be of strong polarity. \nWe addressed this problem by adding negated forms to the opposite class along \nwith normal forms of all the features during the training phase. That is to say if we \nencounter the word “good” in a positive document during the training phase, we in-\ncrement the count of “good” in the positive class and also increment the count of \n“not_good” for the negative class.  This is to ensure that the number of “not_” forms \nare sufficient for classification. This modification resulted in a significant improve-\nment in classification accuracy (about 1%) due to bootstrapping of negated forms \nduring training. This form of negation handling can be applied to a variety of text \nrelated applications. \n6 n - grams \nGenerally, information about sentiment is conveyed by adjectives or more specifically \nby certain combinations of adjectives with other parts of speech.  \nThis information can be captured by adding features like consecutive pairs of \nwords (bigrams), or even triplets of words (trigrams). Words like \"very\" or \"definite-\nly\" don't provide much sentiment information on their own, but phrases like \"very \nbad\" or \"definitely recommended\" increase the probability of a document being nega-\ntively or positively biased. By including bigrams and trigrams, we were able to cap-\nture this information about adjectives and adverbs. Using bigrams and trigrams re-\nquire a substantial amount of data in the training set, but this is not a problem as our \ntraining set had 25,000 reviews. But the data may not be enough to add 4-grams, as \nthis may over-fit the training set. The counts of the n-grams were stored in a hash \ntable along with the counts of unigrams. \n7 Feature Selection \n \nFeature selection is the process of removing redundant features, while retaining those \nfeatures that have high disambiguation capabilities.  \n  \nThe use of higher dimensional features like bigrams and trigrams pr\nlem, that of the number of feature\nMost of these features are redundant and noisy in nature. Including them would affect \nboth efficiency and accuracy. A basic filtering step of removing the features/terms \nwhich occur only once is performed. Now\n1,500,000 features. The features are then further filtered on the basis of mutual info\nmation [3]. \n7.1    Mutual Information\nMutual information is a quantity that measures the mutual dependence of the two \nrandom variables. Formally, the mutual information of two discrete random v\nriables X and Y can be defined as:\nWhere p(x,y) is the joint probability distribution function of \nand P(Y)  are the marginal probability distribution functions of \nHere X is an individual feature which can take two values, the feature is present or \nabsent and Y is the class, positive or negative. We selected the top \nmaximum mutual information. By plotting a graph between accuracy and number of \nfeatures, the optimal value for \n \nA plot of Accuracy versus Number of features\nFig. 1. Plot of Accuracy v/s No\n5 \nesents a pro\ns increasing from 300,000 to about 11,000,000. \n the number of features is reduced to about \n \n \n(5) \n \nX and Y, and P(X)  \nX and Y respectively. \nk features with \nk was found out to be 32,000.  \n is shown in Fig 1: \n \n of features selected on Validation set \nb-\nr-\na-\n8 Results \nWe implemented the classifier in Python using hash tables to store the counts of \nwords in their respective classes. The code is available at [13].  Training involved \npreprocessing data and applying negation handling before counting the words. Since \nwe were using Bernoulli Naive Bayes, each word is counted only once per document. \nOn a laptop running an Intel Core 2 Duo processor at 2.1 GHz, training took around 1 \nminute 30 seconds and used about 700 megabytes of memory. The memory usage \nstems largely from bigrams and trigrams prior to feature selection. \nThe optimal number of features was chosen by using a validation set of 1000 doc-\numents, the plot of accuracy v/s number of features is shown in Fig 1. Then the accu-\nracy was measured on the entire test set of 25000 documents.  The time for feature \nselection was about 3 minutes.  \n8.1   Performance and Comparison \nWe obtained an overall classification accuracy of 88.80% on the test set of 25000 \nmovie reviews. The running time of our algorithm is O(n + V lg V) for training and \nO(n) for testing, where n is the number of words in the documents (linear) and V the \nsize of the reduced vocabulary. It is much faster than other machine learning algo-\nrithms like Maxent classification or Support Vector Machines which take a long time \nto converge to the optimal set of weights.  The accuracy is comparable to that of the \ncurrent state-of-the-art algorithms used for sentiment classification on movie reviews. \nIt achieved a better or similar accuracy when compared to more complicated models \nlike SVMs, autoencoders, contextual valence shifters, matrix factorisation, appraisal \ngroups etc used in [2], [7], [8], [9], [10], [11] on the dataset of IMDb movie reviews. \n8.2   Results Timeline \nThe table and graph illustrate the evolution of the accuracy of our classifier and how \nthe inclusion of certain features helped. \nTable 1.RESULTS TIMELINE \n \nFeature Added Accuracy on test set \nOriginal Naive Bayes \nalgorithm with Laplacian \nSmoothing \n73.77% \nHandling negations 82.80% \nBernoulli  Naive Bayes 83.66% \nBigrams and trigrams 85.20% \nFeature Selection 88.80% \n \n7 \n \n \nFig. 2. Evolution of classification accuracy. \n9 Conclusion  \nOur results show that a simple Naive Bayes classifier can be enhanced to match the \nclassification accuracy of more complicated models for sentiment analysis by choos-\ning the right type of features and removing noise by appropriate feature selection. \nNaive Bayes classifiers due to their conditional independence assumptions are ex-\ntremely fast to train and can scale over large datasets. They are also robust to noise \nand less prone to overfitting. Ease of implementation is also a major advantage of \nNaive Bayes. They were thought to be less accurate than their more sophisticated \ncounterparts like support vector machines and logistic regression but we have shown \nthrough this paper that a significantly high accuracy can be achieved. The ideas used \nin this paper can also be applied to the more general domain of text classification.  \n \nAcknowledgement \n \nWe would like to express our sincere gratitude to our mentor, Professor R. R. Das \nfor sparing his valuable time and guiding us during the project. We are also thankful \nto Prof. P. K. Mukherjee for his support and encouragement. Lastly, we would like to \nthank all the faculty members and support staff of the Department of Electronics En-\ngineering, IIT (BHU). \n \nReferences \n1. Large Movie Review Dataset. (n.d.). Retrieved from \nhttp://ai.stanford.edu/~amaas/data/sentiment/ \n50\n60\n70\n80\n90\n100\nAccuracy\n1. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. \"Thumbs up?: sentiment classifica-\ntion using machine learning techniques.\" Proceedings of the ACL-02 conference on Empir-\nical methods in natural language processing-Volume 10. Association for Computational \nLinguistics, 2002. \n2.  Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. Introduction to in-\nformation retrieval. Vol. 1. Cambridge: Cambridge University Press, 2008. \n3. Das, Sanjiv, and Mike Chen. \"Yahoo! for Amazon: Sentiment parsing from small talk on \nthe web.\" EFA 2001 Barcelona Meetings. 2001. \n4.  Pauls, Adam, and Dan Klein. \"Faster and smaller n-gram language models.\"Proceedings \nof the 49th annual meeting of the Association for Computational Linguistics: Human Lan-\nguage Technologies. Vol. 1. 2011. \n5.      Rennie, Jason D., et al. \"Tackling the poor assumptions of naive bayes text classifi-\ners.\" MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-. \nVol. 20. No. 2. 2003. \n6. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and \nChristopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th An-\nnual Meeting of the Association for Computational Linguistics (ACL 2011). \n7.  Kennedy, Alistair, and Diana Inkpen. \"Sentiment classification of movie reviews using \ncontextual valence shifters.\" Computational Intelligence 22.2 (2006): 110-125. \n8.  Li, Tao, Yi Zhang, and Vikas Sindhwani. \"A non-negative matrix tri-factorization ap-\nproach to sentiment classification with lexical prior knowledge.\" Proceedings of the Joint \nConference of the 47th Annual Meeting of the ACL and the 4th International Joint Confe-\nrence on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association \nfor Computational Linguistics, 2009. \n9.  Matsumoto, S., Takamura, H., & Okumura, M. (2005). Sentiment classification using \nword sub-sequences and dependency sub-trees. In PAKDD 2005 (pp. 301–311). \n10. Springer-Verlag: Berlin, Heidelberg. \n11.  Whitelaw, Casey, Navendu Garg, and Shlomo Argamon. \"Using appraisal groups for sen-\ntiment analysis.\" Proceedings of the 14th ACM international conference on Information \nand knowledge management. ACM, 2005. \n12. Socher, Richard, et al. \"Semi-supervised recursive autoencoders for predicting sentiment \ndistributions.\" Proceedings of the Conference on Empirical Methods in Natural Language \nProcessing. Association for Computational Linguistics, 2011. \n13.  Source code of classifier developed for this paper –[Online] \nhttp://github.com/vivekn/sentiment \n14.  Devitt, Ann, and Khurshid Ahmad. \"Sentiment polarity identification in financial news: A \ncohesion-based approach.\" ANNUAL MEETING-ASSOCIATION FOR \nCOMPUTATIONAL LINGUISTICS. Vol. 45. No. 1. 2007. \n15. Peng, Fuchun, and Dale Schuurmans. \"Combining naive Bayes and n-gram language mod-\nels for text classification.\" Advances in Information Retrieval. Springer Berlin Heidelberg, \n2003. 335-350. \n",
      "id": 17125007,
      "identifiers": [
        {
          "identifier": "24935500",
          "type": "CORE_ID"
        },
        {
          "identifier": "2106095291",
          "type": "MAG_ID"
        },
        {
          "identifier": "info:doi/10.1007%2f978-3-642-41278-3_24",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1007/978-3-642-41278-3_24",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1305.6143",
          "type": "OAI_ID"
        },
        {
          "identifier": "1305.6143",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "456302185",
          "type": "CORE_ID"
        }
      ],
      "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes\n  model",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2106095291",
      "oaiIds": [
        "info:doi/10.1007%2f978-3-642-41278-3_24",
        "oai:arxiv.org:1305.6143"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "'Springer Science and Business Media LLC'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1305.6143"
      ],
      "updatedDate": "2024-03-03T00:12:13",
      "yearPublished": 2013,
      "journals": [
        {
          "title": null,
          "identifiers": [
            "0302-9743"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1305.6143"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17125007"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "De las Heras-Pedrosa, Carlos"
        },
        {
          "name": "Pelaez-Sanchez, Jose Ignacio"
        },
        {
          "name": "Sánchez-Núñez, Pablo"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/288162334",
        "https://api.core.ac.uk/v3/outputs/618762668",
        "https://api.core.ac.uk/v3/outputs/540307207",
        "https://api.core.ac.uk/v3/outputs/323326686",
        "https://api.core.ac.uk/v3/outputs/597637488"
      ],
      "createdDate": "2020-03-22T00:09:33",
      "dataProviders": [
        {
          "id": 22080,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/22080",
          "logo": "https://api.core.ac.uk/data-providers/22080/logo"
        },
        {
          "id": 1730,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1730",
          "logo": "https://api.core.ac.uk/data-providers/1730/logo"
        },
        {
          "id": 2072,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2072",
          "logo": "https://api.core.ac.uk/data-providers/2072/logo"
        },
        {
          "id": 681,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/681",
          "logo": "https://api.core.ac.uk/data-providers/681/logo"
        },
        {
          "id": 11082,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11082",
          "logo": "https://api.core.ac.uk/data-providers/11082/logo"
        },
        {
          "id": 1114,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1114",
          "logo": "https://api.core.ac.uk/data-providers/1114/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Opinion mining and sentiment analysis has become ubiquitous in our society, with\r\napplications in online searching, computer vision, image understanding, artificial intelligence and\r\nmarketing communications (MarCom). Within this context, opinion mining and sentiment analysis\r\nin marketing communications (OMSAMC) has a strong role in the development of the field by\r\nallowing us to understand whether people are satisfied or dissatisfied with our service or product\r\nin order to subsequently analyze the strengths and weaknesses of those consumer experiences. To\r\nthe best of our knowledge, there is no science mapping analysis covering the research about opinion\r\nmining and sentiment analysis in the MarCom ecosystem. In this study, we perform a science\r\nmapping analysis on the OMSAMC research, in order to provide an overview of the scientific work\r\nduring the last two decades in this interdisciplinary area and to show trends that could be the basis\r\nfor future developments in the field. This study was carried out using VOSviewer, CitNetExplorer\r\nand InCites based on results from Web of Science (WoS). The results of this analysis show the\r\nevolution of the field, by highlighting the most notable authors, institutions, keywords,\r\npublications, countries, categories and journals.The research was funded by Programa Operativo FEDER Andalucía 2014‐2020, grant number “La\r\nreputación de las organizaciones en una sociedad digital. Elaboración de una Plataforma Inteligente para la\r\nLocalización, Identificación y Clasificación de Influenciadores en los Medios Sociales Digitales (UMA18‐\r\nFEDERJA‐148)” and The APC was funded by the same research gran",
      "documentType": "research",
      "doi": "10.3390/socsci9030023.",
      "downloadUrl": "https://core.ac.uk/download/288162334.pdf",
      "fieldOfStudy": null,
      "fullText": " Soc. Sci. 2020, 9, 23; doi:10.3390/socsci9030023  www.mdpi.com/journal/socsci Article Opinion  Mining  and  Sentiment  Analysis  in Marketing  Communications:  A  Science  Mapping Analysis in Web of Science (1998–2018) Pablo Sánchez‐Núñez 1,*, Carlos de las Heras‐Pedrosa 2 and José Ignacio Peláez 3 1  Doctorate Program in Communication by Universidad de Cádiz, Universidad de Huelva, Universidad de Málaga and Universidad de Sevilla, 29071 Malaga, Spain; psancheznunez@uma.es 2  Department of Audiovisual Communication and Advertising, Faculty of Communication Sciences, Universidad de Málaga, 29071 Malaga, Spain; cheras@uma.es 3  Department of Languages and Computer Sciences, Higher Technical School of Computer Engineering, Universidad de Málaga, 29071 Malaga, Spain; jipelaez@uma.es *  Correspondence: psancheznunez@uma.es Received: 20 January 2020; Accepted: 24 February 2020; Published: 28 February 2020 Abstract:  Opinion mining  and  sentiment  analysis  has  become  ubiquitous  in  our  society,  with applications in online searching, computer vision, image understanding, artificial intelligence and marketing communications (MarCom). Within this context, opinion mining and sentiment analysis in marketing  communications  (OMSAMC) has a  strong  role  in  the development of  the  field by allowing us to understand whether people are satisfied or dissatisfied with our service or product in order to subsequently analyze the strengths and weaknesses of those consumer experiences. To the best of our knowledge, there is no science mapping analysis covering the research about opinion mining  and  sentiment  analysis  in  the MarCom  ecosystem.  In  this  study, we perform  a  science mapping analysis on the OMSAMC research, in order to provide an overview of the scientific work during the last two decades in this interdisciplinary area and to show trends that could be the basis for future developments in the field. This study was carried out using VOSviewer, CitNetExplorer and  InCites based on  results  from Web of Science  (WoS). The  results of  this analysis  show  the evolution  of  the  field,  by  highlighting  the  most  notable  authors,  institutions,  keywords, publications, countries, categories and journals. Keywords: sentiment analysis; opinion mining; advertising; marketing; science mapping analysis; Web of Science (WoS); bibliometric indicators; scientific collaboration  1. Introduction Sentiment analysis and opinion mining are an automatic mass classification of textual and visual information,  which  focuses  on  cataloguing  and  classifying  data  according  to  the  polarity—the positive  or negative  connotation—of  the  language used  in  them  (Pang & Lee,  2008; Prabowo & Thelwall, 2009). These positive or negative connotations of the  language are reflected  in opinions, attitudes and  emotions  expressed by  Internet users  (Mostafa 2013)  in online mentions on digital ecosystems (Kennedy 2012; Mäntylä et al. 2018). Knowing what others  think and  feel can be  fundamental  to most of us during  the decision‐making process  (Bericat 2016; Saaty and Vargas 2012). User opinions not only help people make informed decisions, but also help organizations identify customer opinions, attitudes and emotions about the products and services they offer (Peláez et al. 2019). In  this  context,  Opinion  Mining  and  Sentiment  Analysis  in  Marketing  Communications (OMSAMC) are extremely important when it comes to analyzing consumer buying patterns (Peláez Soc. Sci. 2020, 9, 23  2  of  22  et al. 2018; Sebastian 2014), collecting customer feedback from social media, websites or online forms (Liu and Ji 2018), as well as knowing what types of stimuli impact people (Baraybar‐Fernández et al. 2017), understanding the reasons that motivates people to like a product/service (Peláez et al. 2019), conducting market research (Wereda and Woźniak 2019), categorizing customer service requests and predicting consumer behavior, among others (Baron et al. 2017). The way of researching in this field has varied considerably over the last few years. There are many different approaches  in  the  field of opinion mining and  sentiment analysis. Some of  them provide new frameworks for measuring customer‐specific variables (Kang and Park 2014), as well as systematic reviews with bibliometric indicators of productivity, impact and collaboration (Martínez‐López et al. 2018), Altmetrics (Thelwall et al. 2013) or scientific mapping analysis (Piryani et al. 2017). In this last case, the study of scientific mapping allows us to study the impact and visibility of scientific publications. The study of scientific collaboration plays a decisive role  in  the expansion, visibility, specialization, consolidation and emergence of the results of scientific production. Being able to identify their topological structure is a key and disruptor element for the study of the reception and transmission of knowledge (Newman 2000). This  article  aims  to  trace  the  evolution  of OMSAMC  by  asking  the  following  questions  in different academic research scenarios:  How has scientific research at OMSAMC progressed from 1998 to 2018?  In which countries and organizations has most of the research on OMSAMC been carried out?  What are the main sources of publication (journals) that publish research on OMSAMC?    Who are the most productive and cited authors in OMSAMC research during the study period?  What is the degree of international scientific collaboration in OMSAMC research?  What type of authoring patterns are observed in the results of OMSAMC research?  What are the main concepts that appear in OMSAMC research publications?  What are the main themes, approaches and methods of OMSAMC?  What are the main areas of application of OMSAMC research? This work offers a science mapping analysis that studies the development of OMSAMC during the period 1998–2018. We have used both manual and computational analysis for this purpose. The collected data obtained from Web of Science (WoS) database is analyzed computationally with the aim to identify the year‐wise number and rate of growth of proceedings, articles, reviews and book chapters,  as well  as  the  different  types  of  authorships  on OMSAMC,  collaboration  and  citation patterns,  the most productive  authors,  journals, keywords,  institutions  and  countries during  the period. Thereafter a detailed manual analysis of the research publication data is performed to identify popular trends, patterns, approaches and possible application areas of OMSAMC. To  the best of our knowledge,  this  is  the  first work of  its kind, differing  from  the rest of  the science mapping analysis works and scientometric studies in opinion mining and sentiment analysis (Piryani et al. 2017) due to the research focused in marketing communications (MarCom).   This  perspective  change  from  previous  works  makes  this  study  more  targeted  towards marketing communications scholars, therefore allowing a more precise analysis on the topic while taking into consideration  its particularities. This results  in a set of scientific network and research productivity clusters that describe very concisely the current interactions and research areas within this field. For this reason, this study may be useful for experts in information processing and scientific communication, specialists in research policy formulation as well as for students and researchers in the fields of communication, computer science, psychology, marketing or artificial intelligence. The research has been organized as follows: Section 2 is the results section and presents in an analytical way  the data obtained  from  the  analysis of  scientific maps on  sentiment  analysis  and opinion mining in marketing communications. Section 3 is the discussion and debates the results, presenting the different approaches and levels of OMSAMC, the main sources of data, the areas of application as well as the possible future lines of research. Section 4 is the Materials and Methods section, describing the data collection and the methodology.   Soc. Sci. 2020, 9, 23  3  of  22   2. Results In this section, we present the science mapping and scientometric indicators computed through computational  analysis of  the data. The  subsections below present details of different  indicators computed and figures and tables illustrating the resultant values of the analysis.   The  results  are derived  from bibliometric  analysis obtained  from  the Web of  Science  (WoS) database. To date, the 845 studies (1998–2018) that were found and that will form the basis of this analysis  is  presented.  In  addition,  it  limited  the  analyzing  to  the  document  types  “Articles”, “Reviews”, “Book Chapter” and “Proceedings paper” written in English.   2.1. Distribution of Documents by Year (1998–2018) The distribution of publications during the period 1998–2018 is shown in Figure 1. During the first decade (1998–2007) of studies, a sustained growth of publications (51) is observed while in the second decade  (2008–2018)  it was detected  that OMSAMC has  shown exponential growth  in  the number of investigations (794).    Figure 1. Publication years and record count. 2.2. Citation Report The  total publications  retrieved  (Table  1)  combined had  a  sum  of  9557  citations  (Figure  2), making an average of 11.31 citations per paper. The H‐index  is 49, which means that there are 49 studies that have received at least 49 citations. Table 1. Citation report. Citation Report Total publications  845 Sum of times cited  9557 Average citations per item    11.31 H‐index  49  Soc. Sci. 2020, 9, 23  4  of  22   Figure 2. Citations by years and record count. 2.3. Top 25 Co‐Authorship Analysis (Authors and Record Count) The 25 most productive  international collaborations can be seen  in Table 2, which presents a ranking of  the 25 most  influential authors  for OMSAMC, along with  the number of  international collaboration documents produced, the sum of citations and the total link strength.   Table 2. Top 25 Co‐authorship analysis (authors). The relatedness of items is determined based on their  number  of  co‐authored  documents. Minimum  number  of  documents  of  an  author  (3)  and minimum number of citations of an author (1). Of the 2505 authors, 38 met the thresholds. Ranking  Author  Documents  Citations  Total Link Strength 1.   Pieters, R  6  775  6 2.   Wedel, M  6  775  6 3.   Cambria, Erik  8  306  4 4.   Wedel, Michel  8  292  6 5.   Pieters, Rik  6  288  6 6.   Poria, Soujanya  3  205  3 7.   Wojdynski, Bartosz W.  4  123  3 8.   Bang, Hyejin  3  35  3 9.   Bodendorf, Freimut  4  31  3 10.   Kaiser, Carolin  3  31  3 11.   Holmberg, Nils  3  18  3 12.   Sandberg, Helena  3  18  3 13.   Recupero, Diego Reforgiato  3  10  2 14.   Dragoni, Mauro  4  7  1 15.   Farkas, Richard  3  5  3 16.   Hangya, Viktor  3  5  3 17.   Oliveira, Eugenio  3  5  9 18.   Reis, Luis Paulo  3  5  9 19.   Teixeira, Jorge  3  5  9 20.   Vinhas, Vasco  3  5  9 21.   Lu, Hanqing  3  4  1 22.   Xu, Changsheng  3  4  1 23.   Kincl, Tomas  3  2  6 24.   Novak, Michal  4  2  6 25.   Pribil, Jiri  3  2  6  Soc. Sci. 2020, 9, 23  5  of  22  A citation network (1998–2018) is shown in Figure 3, where we find four clusters: a 1st group of 136 publications, 2nd group of 55 publications, 3rd group of 21 publications and 4th group of 15 publications. Additionally, there are 618 publications that do not belong to any cluster. The citation visualization network is based in 60 publications (based on their citation score).   The authors Beijer and Crundall are founded but they are isolated from the rest of the literature. Pieter’s publication is not only included in the main network but also originates it.    Figure 3. Citation network (1998–2018). 2.4. Group Authors and Record Count A Group Author is an organization or institution that is credited with authorship of an article by the source publication. OMSAMC Group Authors and Record Count are shown in Figure 4. We observed that in the Top 5 Group Authors, IEEE has the biggest number of corporate authors with 140, followed by ACM with 14, Association for Computer Machinery with 3 and towards the end ASME and DEStech Publications, Inc., both with 3.  Figure 4. Group Authors and Record Count (669 records (79.172%) do not contain data in the field being analyzed). Soc. Sci. 2020, 9, 23  6  of  22  2.5. Top 25 Co‐Authorship Analysis (Countries/Regions and Record Count) This section presents the results of the most influential Co‐Authorship countries in OMSAMC publications (Figure 5a), as well as the document and citation details of the first Top 10 countries in OMSAMC Co‐Authorship. The  first place  is occupied by  the USA  (211 documents and 4939 citations),  followed by The Netherlands  (36 documents and 1623 citations),  Italy  (50 documents and 824 citations), China  (90 documents and 670 citations), England (45 documents and 590 citation), Germany (44 documents and 460 citations), France (24 documents and 452 citations), Australia (32 documents and 374 citations), Singapore (25 documents and 361 citations) and towards the end by Spain (37 documents and 344 citations). InCites regional analysis (Figure 5b) allows us to view and compare indicators at the country level, seeing the geographical distribution of the top producing regions, authors in publications in each  research area and  identification  research  trends  in countries of  interest  (Fonseca et al. 2016; Luukkonen  et  al.  1992).  InCites Regional  Indicators measure  Productivity  (% Documents  in Q1 Journals,  count  of  documents  in  Q1  Journals),  Collaboration  (%  International  Collaborations, percentage of publications that have international co‐authors) and Impact (Times‐Cited, number of times the set of publications has been cited).  (a)  Soc. Sci. 2020, 9, 23  7  of  22   (b) Figure 5. (a) Top 25 Co‐authorship analysis (countries). The relatedness of items is determined based on their number of co‐authored documents. Minimum number of documents of a country (5) and minimum number of citations of a country (5). (b) InCites Top 25 Locations and Record Count. 2.6. Top 25 Co‐Authorship Analysis (Organizations and Record Count) Table  3  presents  a  ranking  of  the  most  influential  international  collaborations  through universities along with  several documents and  sum  citations  indicators;  two  indicators of global university ranking according to the 2019 Quacquarelli Symonds (QS) World University Rankings and 2019 Academic Ranking of World Universities (ARWU) that allow us to measure the relative position in which we find the most influential institutions in OMSAMC. Table 3. Top 25 International Collaborations/Organizations and Record Count. Minimum number of documents of a country (5) and minimum number of citations of a country (5). Ranking  Organization  Documents Citations Total Link Strength ARWU 2019 QS 2019 1.  Tilburg University  14  1085  14  501–600  319 2.  University of Michigan  7  739  8  20  20 3.  University of Pennsylvania  5  461  1  17  19 4.  The University of Maryland  12  372  9  46  126 5.  Nanyang Technological University  10  302  3  73  12 6.  City University of Hong Kong  7  286  2  201–300  55 7.  Copenhagen Business School  5  193  0  701–800  ‐ 8.  University of Nottingham  7  158  0  101–150  82 Soc. Sci. 2020, 9, 23  8  of  22  9.  University of California San Diego  7  151  5  18  41 10.  The University of Georgia  9  150  6  201–300  431 11.  Michigan State University  6  138  5  101–150  141 12.  Pennsylvania State University  6  129  7  98  95 13.  The University of Arizona  6  100  4  101–150  246 14.  Korea Advanced Institute of Science and Technology  5  85  1  201–300  40 15.  University of Amsterdam  5  74  0  101–150  57 16.  University of Florida  8  58  5  95  180 17.  University of Minnesota  5  48  5  41  156 18.  Microsoft Research Asia  5  42  1  ‐  ‐ 19.  University of Pittsburgh  6  39  1  89  136 20.  The Australian National University  6  34  5  76  24 21.  Zhejiang University  5  34  1  70  68 22.  University of Malaya  5  22  1  301–400  87 23.  National University of Singapore  7  19  3  67  11 24.  Chinese Academy of Sciences  8  18  2  ‐  ‐ 25.  Politecnico di Milano  5  17  1  201–300  156  Within  the  first 10 universities, 50% are  in  the United States,  followed by  institutions  in The Netherlands (1), Singapore (5), Hong Kong (6), Denmark (7) and United Kingdom (8). Further down the rankings are other institutions in Korea, China, Australia, Malaysia and Italy. The first institution in  the  ranking  in  terms  of  international  collaboration  is  Tilburg  University  with  a  total  of  14 documents published in OMSAMC, where 14 of these studies have received 1085 citations. As for the relative position of the University, Tilburg University is located within the first 501–600 ARWU 2019 and  319 QS  2019.  The  following  university  is University  of Michigan, with  a  total  of  7  articles published, of which 7 have been  cited  at  least 739  times. The  following one  is  the University of Pennsylvania, with 5 papers published and a ratio of 461 citation.   Only 12 of the Top 25 rankings of universities are in the Top 100 ranking according to ARWU: University  of  Michigan,  University  of  Pennsylvania,  The  University  of  Maryland,  Nanyang Technological  University,  University  of  California  San  Diego,  Pennsylvania  State  University, University of Florida, University of Minnesota, University of Pittsburgh, The Australian National University, Zhejiang University and National University of Singapore. Of these, 8 are in the United States while only 8 are part of the Top 100 according to QS: University of Michigan, University of Pennsylvania, Nanyang Technological University, University of California San Diego, Pennsylvania State University, The Australian National University, Zhejiang University and National University of Singapore. 2.7. Top 25 OMSAMC Funding Agencies and Top 25 InCites Funding Agencies and Record Count In  this  section we  analyze  the  impact  of OMSAMC  research  that  has  been  funded  and/or published by funding agencies. We assess this by leveraging unified funding acknowledgment data from the Web of Science (Figure 6a), with the aim to understand whether these funds were spent in studies that made a disruptive scientific advance (Álvarez‐Bornstein et al. 2017). InCites Funding  Indicators  (Figure  6b) measure Productivity  (% Documents  in Q1  Journals, count of documents in Q1 Journals) and Impact (Citation Impact, average number of citations per paper).    Soc. Sci. 2020, 9, 23  9  of  22   (a)  Soc. Sci. 2020, 9, 23  10  of  22   (b) Figure 6.  (a) Top 25 OMSAMC Funding Agencies and Record Count.  (b)  InCites Top 25 Funding Agencies and Record Count. 2.8. Research Areas and Record Count There is a total of 5 general categories on the Web of Science (Arts and Humanities, Life Sciences and  Biomedicine,  Physical  Sciences,  Social  Sciences  and  Technology).  Within  these  5  general categories there are 71 different sub‐categories. Figure 7 shows the abovementioned sub‐categories. Among  the Top 10 most representative categories  in OMSAMC we  find  the  following: Computer Science (450 registers and 53.254% of 845), Engineering (235 registers and 27.811% of 845), Business Economics  (158  registers  and  18.698%  of  845),  Psychology  (57  registers  and  6.746%  of  845), Telecommunications  (55 registers and 6509% of 845), Communication  (34 registers and 4.024% of 845), Information Science and Library Science (26 registers and 3.077% of 845), Operations Research and Management  Science  (26  registers  and  3.077%  of  845),  Social  Science  and Other  Topics  (25 registers and 2.959% of 845); towards the end we find Imaging Science and Photographic Technology (22 registers and 2.604% of 845).   Soc. Sci. 2020, 9, 23  11  of  22   Figure 7. Research areas and record count. 2.9. Top 25 Co‐Citation Analysis (Sources) As shown in Table 4, the most cited journals in OMSAMC have a clear focus on marketing since they prominently cited marketing magazines. Journal of Consumer Research (1st ranked with a sum of 521 citations), Journal of Marketing Research (2nd ranked with a sum of 371 citations), Journal of Marketing (3rd ranked with a sum of 316 citations), Lecture Notes in Computer Science LNCS (4th ranked with  a  sum  of  296  citations)  and  Journal  of Advertising  (5th  ranked with  a  sum  of  279 citations) are the most cited journals in OMSAMC. The first, second and third are usually regarded as the three most influential magazines in consumerism and marketing, while the fourth and fifth magazine  shows  its  clear  thematic  connection  (computer  science  and  advertising  journals) with OMSAMC.   Soc. Sci. 2020, 9, 23  12  of  22  Table 4. The Top 25 Co‐Citation analysis (sources). The relatedness of items is determined based on the number of times they are cited together. Minimum number of citations of a source (75). Of the 11,186 sources, 34 met the threshold. Ranking  Source  Citations  Total Link Strength 1.   Journal of Consumer Research  521  10,340 2.   Journal of Marketing Research  371  9258 3.   Journal of Marketing  316  7353 4.  Lecture Notes in Computer Science LNCS 296  1707 5.   Journal of Advertising  279  5370 6.  Journal of Advertising Research 276  6244 7.  IEEE Transactions on Pattern Analysis and Machine Intelligence 249  1748 8.   Expert Systems with Applications  246  6128 9.   Marketing Science  239  7223 10.   Decision Support Systems  213  5198 11.  IEEE Conference on Computer Vision and Pattern Recognition 161  1015 12.   Management Science  155  5277 13.  Journal of Interactive Marketing 138  3716 14.   Psychology & Marketing  138  3651 15.   Psychological Bulletin  135  2116 16.   Vision Research  125  1835 17.  Journal of Personality and Social Psychology 121  2377 18.   Computers in Human Behavior  120  2656 19.   Journal of Business Research  114  4262 20.   Journal of the Association for  112  880 Soc. Sci. 2020, 9, 23  13  of  22  Information Science and Technology 21.  International Journal of Research in Marketing 101  2990 22.   Psychological Review  100  1739 23.   Advances in Consumer Research  92  2420 24.  Journal of Experimental Psychology: Human Perception and Performance 92  1731 25.   Tourism Management  92  1592 2.10. Co‐Occurrence Analysis The use of cooccurrence data is very common in scientometric and cooccurrences of words may be used to construct so‐called co‐word maps, which are maps that provide a visual representation of the structure of a scientific field (Van Eck and Waltman 2009). In Figure 8 is shown the relatedness of items determined based on the number of documents in which they occur together (all keywords). The minimum  number  of  occurrences  of  a  keyword was  3. Of  the  3094  keywords,  332 met  the threshold. We identified 10 different clusters:  Cluster  1  Social  Media,  Machine  Learning  and  Artificial  Intelligence:  algorithms,  antecedents, behavior analysis, big data, blogosphere, brand monitoring, business intelligence, classification, cluster  analysis,  clustering,  community,  deep  neural  networks,  emotions,  engagement,  eye tracking  technology,  face  recognition,  Facebook,  framework,  identification,  image,  image classification,  in‐game advertising, influence,  information technology, Instagram,  intelligence, k‐means,  literature  review,  marketing  intelligence,  models,  naive  Bayes,  network,  object detection, opinion, pattern recognition, power, retrieval, satisfaction, sentiment, small business, social  influence,  social  media,  social  media  analytics,  social  media  mining,  social  media monitoring,  social  network,  social  network  analysis,  support  vector  machines,  systems, technology, text classification, tourism, trust, twitter, visual analytics, visualization, words.    Cluster 2 Product Design: advertisement, advertising, alcohol, allocation, avoidance, behavior, bias,  body  dissatisfaction,  brand  recall,  brands,  commercials,  communication,  consumption, drivers,  experience,  exposure,  eye,  eye  fixation,  eye movement,  eye  tracking,  health,  health warnings,  interactivity,  labels,  media,  messages,  nonsmokers,  online,  patterns,  perceptions, perspective, pictorial, pictures, products, recall, road safety, smokers, television, smoking, text, tobacco, trends, tv, united states, video, visual attention, warning labels, working memory.    Cluster  3  Design  Techniques:  advertisement,  algorithm,  attitudes,  augmented  reality,  color, computer vision, customer satisfaction, design, digital image processing, food, gender, grading, hidden  Markov  model,  image  processing,  image  segmentation,  integration,  knowledge, machine, machine vision, methodology, model, neural network, objects, optimization, orange, prediction,  purchase  intention,  quality,  regression,  segmentation,  sex  differences,  sorting, support vector machines, surface area, system, user experience, vision, website.  Cluster 4 Advertising and Message Impact: ads, animation, attention, attitude, capacity, capture, children, cognitive load, context, contextual advertising, distraction, features, human‐computer interaction,  information,  internet,  internet  use,  involvement,  looking,  mechanisms,  motion, movement, older drivers, online advertising, people, performance, persuasion, saliency, search, Soc. Sci. 2020, 9, 23  14  of  22  selective  attention,  strategies,  surveillance,  tracking, visual  attention, visual behavior, visual saliency, visual search, web design, websites, world wide web.  Cluster 5 Advertising and Neuroscience: advertising effectiveness, affective computing, arousal, biometrics, brain, consumer, cortex, cues, destination, digital signage, EEG, emotion recognition, face, familiarity, feature extraction, galvanic skin response, gaze, gender classification, images, interferences, memory, object recognition, perception, preference, recognition, representation, responses, scale, scenes, selection, stimuli, television commercials, valence, warnings, young.      Cluster  6  Social Network Analysis  (SNA):  approximation,  artificial  intelligence,  convolutional neural  networks,  corpus,  cross  domain,  customer  reviews,  data  mining,  deep  learning,  e‐commerce,  emotion  analysis,  feature  selection,  image  recognition,  information  extraction, lexicon  based, machine  learning, marketing,  natural  language  processing,  networks,  neural networks, NLP, ontology, opinion mining, product review, security, semantic web, sentiment analysis, sentiment classification, text analysis, text categorization, topic modelling, tweets, web, word embedding.    Cluster  7  Consumer  behavior:  analytics,  brand  image,  communities,  community  detection, congruity,  consumer  reviews,  dynamics,  experience,  field,  helpfulness,  hospitality  impact, knowledge discovery,  lexicon, management, moderating  role, news, online products  review, online  reputation management,  online  reviews, persuasion  knowledge,  product,  reputation, saccades,  sales,  semantics,  sentiments,  social  media  marketing,  support,  text  mining,  user‐generated content, word of mouth.    Cluster  8  Brand Analysis  and  Retail:  brand,  brand  attention,  brand  choice,  branding,  choice, consumer behavior, consumer choice, consumer neuroscience, decision making, display, eye tracking,  eye  movement,  facial  expression,  fixations,  gaze  bias,  implicit  memory,  in‐store decision making, mere exposure, neuromarketing, nutrition information, print advertisement, promotion, selective visual attention, time, time‐pressure, willingness to pay.    Cluster 9 Social Networks and Spam Detection: crowdsourcing, location, retailing, social networks, spam detection, web 2.0, web mining.    Cluster 10 Consumer Relationship Management: consumer, customer engagement, Hadoop, CRM, social media.  Figure 8. Co‐Occurrence analysis. The relatedness of  items  is determined based on  the number of documents  in which  they  occur  together  (all  keywords). Minimum  number  of  occurrences  of  a keyword was 3. Of the 3094 keywords, 332 met the threshold. Soc. Sci. 2020, 9, 23  15  of  22  2.11. Top 25 Times‐Cited Works in Opinion Mining and Sentiment Analysis in Marketing Communication The distribution of the most cited articles by year in OMSAMC is shown in Table 5. The first 25 studies are done according to their year publication, author, study title, total number of citations and ranking. There are 19 studies that have been cited at least 100 times. Table 5. The Top 25 Times‐Cited Works analysis. The relatedness of items is determined based on the number of times they are cited (publications). Title  Authors  Source  Publication Years  Citations Average Citation/Year 1. In the Eye of the Beholder: A Survey of Models for Eyes and Gaze Hansen, Dan Witzner; Ji, Qiang IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2010  565  56.5 2. Attention capture and transfer in advertising: Brand, pictorial, and text‐size effects Pieters, R; Wedel, M  JOURNAL OF MARKETING  2004  304  19 3. Does In‐Store Marketing Work? Effects of the Number and Position of Shelf Facings on Brand Attention and Evaluation at the Point of Purchase Chandon, Pierre; Hutchinson, J. Wesley; Bradlow, Eric T.; Young, Scott H. JOURNAL OF MARKETING  2009  249  22.64 4. More than words: Social networksʹ text mining for consumer brand sentiments Mostafa, Mohamed M. EXPERT SYSTEMS WITH APPLICATIONS  2013  188  26.86 5. Designing Ranking Systems for Hotels on Travel Search Engines by Mining User‐Generated and Crowdsourced Content Ghose, Anindya; Ipeirotis, Panagiotis G.; Li, Beibei MARKETING SCIENCE  2012  177  22.13 6. Eye fixations on advertisements and memory for brands: A model and findings Wedel, M; Pieters, R  MARKETING SCIENCE  2000  173  8.65 7. User generated content: the use of blogs for tourism organisations and tourism consumers Akehurst, Gary  SERVICE BUSINESS  2009  168  15.27 8. Breaking through the clutter: Benefits of advertisement originality and familiarity for brand attention and memory Pieters, R; Warlop, L; Wedel, M MANAGEMENT SCIENCE  2002  143  7.94 9. Understanding Transit Scenes: A Survey on Human Behavior‐Recognition Algorithms Candamo, Joshua; Shreve, Matthew; Goldgof, Dmitry B.; Sapper, Deborah B.; Kasturi, Rangachar IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS 2010  140  14 10. Shape Analysis of Agricultural Products: A Review of Recent Research Advances and Costa, Corrado; Antonucci, Francesca; Pallottino, Federico; Aguzzi, Jacopo; Sun, FOOD AND BIOPROCESS TECHNOLOGY 2011  139  15.44 Soc. Sci. 2020, 9, 23  16  of  22  Potential Application to Computer Vision Da‐Wen; Menesatti, Paolo 11. Sentic patterns: Dependency‐based rules for concept‐level sentiment analysis Poria, Soujanya; Cambria, Erik; Winterstein, Gregoire; Huang, Guang‐Bin KNOWLEDGE‐BASED SYSTEMS  2014  132  22 12. Using Twitter to Examine Smoking Behavior and Perceptions of Emerging Tobacco Products Myslin, Mark; Zhu, Shu‐Hong; Chapman, Wendy; Conway, Mike JOURNAL OF MEDICAL INTERNET RESEARCH 2013  125  17.86 13. Branding the brain: A critical review and outlook Plassmann, Hilke; Ramsoy, Thomas Zoega; Milosavljevic, Milica JOURNAL OF CONSUMER PSYCHOLOGY 2012  121  15.13 14. The impact of social and conventional media on firm equity value: A sentiment analysis approach Yu, Yang; Duan, Wenjing; Cao, Qing DECISION SUPPORT SYSTEMS  2013  118  16.86 15. Survey on mining subjective data on the web Tsytsarau, Mikalai; Palpanas, Themis DATA MINING AND KNOWLEDGE DISCOVERY 2012  118  14.75 16. A flexible model of consumer country‐of‐origin perceptions ‐ A cross‐cultural investigation Knight, GA; Calantone, RJ INTERNATIONAL MARKETING REVIEW  2000  116  5.8 17. Affective News: The Automated Coding of Sentiment in Political Texts Young, Lori; Soroka, Stuart POLITICAL COMMUNICATION  2012  113  14.13 18. Mining comparative opinions from customer reviews for Competitive Intelligence Xu, Kaiquan; Liao, Stephen Shaoyi; Li, Jiexun; Song, Yuxia DECISION SUPPORT SYSTEMS  2011  113  12.56 19. Visual attention to repeated print advertising: A test of scanpath theory Pieters, R; Rosbergen, E; Wedel, M JOURNAL OF MARKETING RESEARCH 1999  103  4.9 20. Building models for marketing decisions: Past, present and future Leeflang, PSH; Wittink, DR INTERNATIONAL JOURNAL OF RESEARCH IN MARKETING 2000  92  4.6 21. Goal control of attention to advertising: The Yarbus implication Pieters, Rik; Wedel, Michel JOURNAL OF CONSUMER RESEARCH 2007  90  6.92 22. What Do You See When Youʹre Surfing? Using Eye Tracking to Predict Salient Regions of Web Pages Buscher, Georg; Cutrell, Edward; Morris, Meredith Ringel CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1–4 2009  89  8.09 23. Going Native: Effects of Disclosure Position and Language on the Recognition and Evaluation of Online Native Advertising Wojdynski, Bartosz W.; Evans, Nathaniel J. JOURNAL OF ADVERTISING  2016  88  22 Soc. Sci. 2020, 9, 23  17  of  22  24. H‐ATLAS: PACS imaging for the Science Demonstration Phase Ibar, Edo; Ivison, R. J.; Cava, A.; Rodighiero, G.; Buttiglione, S.; Temi, P.; Frayer, D.; Fritz, J.; Leeuw, L.; Baes, M.; Rigby, E.; Verma, A.; Serjeant, S.; Mueller, T.; Auld, R.; Dariush, A.; Dunne, L.; Eales, S.; Maddox, S.; Panuzzo, P.; Pascale, E.; Pohlen, M.; Smith, D.; de Zotti, G.; Vaccari, M.; Hopwood, R.; Cooray, A.; Burgarella, D.; Jarvis, M. MONTHLY NOTICES OF THE ROYAL ASTRONOMICAL SOCIETY 2010  87  8.7 25. Predicting consumer sentiments from online text Bai, Xue  DECISION SUPPORT SYSTEMS  2011  85  9.44 The study with the most citations (565) of OMSAMC is “In the Eye of the Beholder: A Survey of Models for Eyes and Gaze”, published in 2010, and reviews the current progress in and state‐of‐the‐art of video‐based eye detection and tracking in order to identify promising techniques as well as issues  to  be  further  addressed.  The  study  presents  a  detailed  review  of  recent  eye models  and techniques for eye detection and tracking and survey methods for gaze estimation, comparing them based on their geometric properties and reported accuracies. The ratio of number of citations per year is approximately 56.9 citations.   “Attention capture and transfer in advertising: Brand, pictorial, and text‐size effects” published in 2004, follows, and because of the paper studies how the three key ad elements (brand, pictorial and text) each have unique superiority effects on attention to advertisements, which are on par with many commonly held ideas in marketing practice—the main conclusion of an analysis of 1363 print advertisements tested with infrared eye‐tracking methodology on more than 3600 consumers. The pictorial  is superior  in capturing attention,  independent of  its size. The authors discuss how  their findings can be used to render more effective decisions in advertising. This study has been cited 304 times and has a ratio equal to 19 citations per year.   Within the Top 3 is “Does In‐Store Marketing Work? Effects of the Number and Position of Shelf Facings on Brand Attention and Evaluation at the Point of Purchase”, published in 2009, which, like other studies within the table, covers the topic of interplay between in‐store and out‐of‐store factors on consumer attention to and evaluation of brands displayed on supermarket shelves. The results underscore the importance of combining eye‐tracking and purchase data to obtain a full picture of the effects of in‐store and out‐of‐store marketing at the point of purchase. This study has been cited 249 times and has a ratio equal to 22.64 citations per year.   The following studies cover other  issues such as text mining for consumer brand sentiments, design for ranking systems, user generated content and advertising or advertising and memory. 3. Discussion Sentiment  analysis  and  opinion  mining  in  marketing  communications  (OMSAMC)  is  a promising and growing research field. OMSAMC has been acquiring a crucial role in both research and commercial applications because of their probable applicability to numerous diverse fields, such as the identification of brand awareness, reputation and popularity at a specific moment or over time, the tracking of consumer reception of new products or features, the pinpoint targeting of an audience or the evaluation performance success of a marketing campaign. OMSAMC has experienced an exponential growth in the number of investigations (794) in the recent years, with a sum of citations of 9557 and with an average of 11.31 citations per paper. The H‐Soc. Sci. 2020, 9, 23  18  of  22  index  reveals  a  result  of  49  (49  studies  that  have  received  at  least  49  citations). To  explain  this development  there are  two main  topics that are  illustrious:  the  increase of researchers worldwide (United Nations, 2015) and the development of information technology and the Internet (Jasanoff and Pinch  2019)  that  permits  one  to  rapidly  acquire  a  greater  volume  of  information  connected  to OMSAMC and all global issues. The  study  shows which  are  the most prolific  authors  in  the  field  of OMSAMC  in  terms  of scientific collaboration: Pieters, R. (6 documents, 775 citations and total link strength of 6), Wedel, M. (6 documents, 775 citations and total link strength of 6) and Cambria, E. (8 documents, 306 citations and total link strength of 4). When analyzing the citation network, it was shown that although there are two distinct clusters, there is no real strong cluster structure within that citation network except for a group of more recent publications. Most of the group authors belongs to IEEE with a sum of 140 corporate authors. The  research  study  has  shown  that  in  terms  of  OMSAMC  international  co‐authorship productivity  the  first  place  is  for  USA  (211  documents  and  4939  citations),  followed  by  The Netherlands (36 documents and 1623 citations) and Italy (50 documents and 824 citations).   It  is  paradoxical  because  if  we  study  international  scientific  collaboration  in  all  areas  of knowledge, we observe that the United States is one of the most productive countries in terms of quality publications  (Q1  journals),  but  it  is  one  of  the  least productive  in  terms  of  international collaborations, since its tendency continues to be towards national collaborations. It is also interesting to talk about countries like the United Kingdom, Spain, Italy, France, Holland, Finland or Israel, with a lower volume of publications in Q1 journals, which are the countries that make more international collaborations.   The  study  of  international  collaboration  in  organizations  has  shown  that  the University  of Tilburg  (The Netherlands),  followed by  the University of Michigan  (USA)  and  the University of Pennsylvania (USA), are the three  institutions  that carry out research  in OMSAMC that have had more documents and citations.   The  funding  agencies  that have  funded more  studies  in OMSAMC have  been  the National Natural Science Foundation of China, followed by the National Institute of Health of NIH‐USA and the United States Department of Health Human Services. If we compare the funding agencies that have invested in research in OMSAMC with the funding agencies that have invested in research in all industries and scientific areas we see that the United States is the country that has produced the most documents that are located in Q1 journals and with the highest citation impact while China has produced many papers located in Q1 but has not had the same influence in terms of citation impact.   Results  prove  that  the  three most  representative  categories  of OMSAMC works  in Web  of Science  are Computer Science  (450  registers  and  53.254% of  845), Engineering  (235  registers  and 27.811%  of  845)  and Business Economics  (158  registers  and  18.698%  of  845),  and  the most  cited journals  in OMSAMC has a clear focus on marketing since it mostly cites marketing  journals. We have the Journal of Consumer Research (1st ranked with a sum of 521 citations), Journal of Marketing Research (2nd ranked with a sum of 371 citations) and Journal of Marketing (3rd ranked with a sum of 316 citations).   The results of the keyword analysis determine different clusters in which we find that the major areas of expertise in OMSAMC are opinion mining and sentiment analysis in computer science and neuroscience research areas. The  most  used  computational  intelligence  techniques  to  analyze  sentiment  and  opinion  in marketing are k‐means algorithms, Bayesian networks, clustering techniques, deep neural networks, convolutional neural networks, support vector machines, hidden Markov models as well as natural language  processing  and  ontology  developments.  From  the  neuroscience  field,  the  most  used techniques are eye tracking, EEG and galvanic skin response. On the one hand, there is a deep interest in the study and monitoring of brands, corporate reputation as well as the design of visual content and  advertising message. Several  studies  reveal  that  there  is new  research  in  areas of  consumer experience such as the impact and effect of alcohol and tobacco on buyers. On the other hand, the Soc. Sci. 2020, 9, 23  19  of  22  studies reflect that the most studied and analyzed social networks in OMSAMC research have been Instagram, Facebook and Twitter.   Finally, the research demonstrates that the studies that have had the greatest repercussion on OMSAMC in terms of times cited and impact citation have been “In the Eye of the Beholder: A Survey of Models for Eyes and Gaze “(565), which reviews the current progress in and the state‐of‐the‐art of video‐based eye detection and tracking in order to identify promising techniques as well as issues to be further addressed. “Attention capture and transfer in advertising: Brand, pictorial, and text‐size effects” (304) follows, and this paper studies how the three key ad elements (brand, pictorial, and text) each have unique superiority effects on attention to advertisements, which are on par with many commonly held ideas in marketing practice. And towards the end, “Does In‐Store Marketing Work?” (249), which  covers  the  topic of  interplay between  in‐store  and out‐of‐store  factors on  consumer attention to and evaluation of brands displayed on supermarket shelves. In future lines of research, it would be interesting to continue deepening in the OMSAMC cluster analysis, the dynamization of synergies in OMSAMC scientific collaboration networks, the increase of  the  research  in OMSAMC by  alternative metrics,  the  study  of opinion mining  and  sentiment analysis  in brand monitoring, political management, or sectorial analysis, as well as  the study of OMSAMC distribution resources and industrial collaborations by countries and organizations. 4. Materials and Methods   We  gathered  research  publications  indexed  in  Web  of  Science  (WoS)  on  OMSAMC  for  a significantly  large  timespan  of  20  years  (1998–2018),  which  almost  covers  the  whole  period  of beginning and development of OMSAMC research. We downloaded data for publications (article, book chapter, proceeding or review) on OMSAMC written in English. Table 6 illustrates the query, the selected  inclusion and exclusion criteria used, and  the  indexes, timespan and date of  the data downloaded. We  obtained  a  total  of  845  papers  as  a  result  of  the  query. Keywords  and  terms associated with opinion mining, sentiment analysis and emotion understanding were utilized in the subject search in blend with derived terms of advertising/marketing. Table 6. Details of dataset. Indexes  Timespan  Search  Results  Date Web of Science Core Collection: SCI‐EXPANDED, SSCI, A&HCI, CPCI‐S, CPCISSH, BKCI‐S, BKCI‐SSH, ESCI. 1998–2018 ((((((TS = (((“Sentiment Analysis”) OR (“Sentiment of Images”) OR (“Sentiment Classification”) OR (“Opinion Mining”) OR (“Opinion Classification”) OR (“Image Sentiment”) OR (“Image Emotion”) OR (“Image Processing”) OR (“Image Recognition”) OR (“Mining sentiment”) OR (“Visual Content”) OR (“Visual Attention”) OR (“Object Recognition”) OR (“Object Detection”) OR (“Image Classification”) OR (“Affect Analysis”) OR (“Affective Computing”)) AND (Advert* OR “Marketing”)))))))) AND LANGUAGES: (English) AND DOCUMENT TYPES: (Article OR Book Chapter OR Proceedings Paper OR Review) 845  03.12.2019 The  methodology  followed  the  science  mapping  analysis  approach  (Chen  2017),  a  generic process of domain analysis and scientometric visualization  (Egghe 1994). The scope of a scientific mapping study can be a research field, a scientific discipline or a thematic area related to specific investigation  issues  (Katz and Hicks 1997; Moya‐Anegón et al. 2013). The  study presents  several components, a selection of highlighted scientific works, a set of scientometric and visual mapping analytical  tools,  some  indicators and metrics  that can highlight potentially  significant  trends and patterns,  and  theories  of  scientific  change  that  can  lead  the  interpretation  and  exploration  of visualized dynamic patterns  and  intellectual  structures  (Ahlgren  et  al.  2013; Beaver  2001; De  las Heras‐Pedrosa et al. 2018; Glänzel 2001). All the publications were assessed in terms of following aspects: distribution of languages and publication year, distribution of countries, co‐authorship relations among countries, distribution of Soc. Sci. 2020, 9, 23  20  of  22  journals and subject categories, distribution of author keywords, distribution analysis of authors and institutions, authorship pattern analysis and co‐authorship relations among authors. All analyses and data visualizations referring to the document type, language, journal, country, institutes and author were performed using:    VOSviewer, a software tool for constructing and visualizing bibliometric networks (including individual publications, researchers, journals); being those constructed based on co‐authorship relations, co‐citation, bibliographic coupling, citation and co‐occurrence networks of important terms extracted from a body of scientific literature (Van Eck and Waltman 2010).    CitNetExplorer,  a  software  tool  for visualizing  and  analyzing  citation networks of  scientific publications by allowing us  to  identify clusters of closely  related publications  (Van Eck and Waltman 2014) .  InCites|Clarivate  Analytics  is  a  bibliometric  analysis  tool  that  gathers  all  the  scientific production of an institution included in the Web of Science from 1981 to the present. The tool allows  to analyze  the productivity of an  institution, compare  the performance of  researchers with other scientists in the world as well as determine emerging trends in research. Author Contributions: Conceptualization, Pablo Sánchez‐Núñez; Data curation, Pablo Sánchez‐Núñez; Formal analysis,  Pablo  Sánchez‐Núñez;  Funding  acquisition, Carlos de  las Heras‐Pedrosa  and  José  Ignacio  Peláez; Investigation, Pablo Sánchez‐Núñez; Methodology, Pablo Sánchez‐Núñez; Project administration, Carlos de las Heras‐Pedrosa  and  José  Ignacio  Peláez;  Resources,  Carlos  de  las  Heras‐Pedrosa  and  José  Ignacio  Peláez; Software, Carlos de las Heras‐Pedrosa and José Ignacio Peláez; Supervision, Carlos de las Heras‐Pedrosa and José  Ignacio  Peláez; Validation, Carlos  de  las Heras‐Pedrosa  and  José  Ignacio  Peláez; Visualization,  Pablo Sánchez‐Núñez; Writing – original draft, Pablo Sánchez‐Núñez; Writing – review & editing, Carlos de las Heras‐Pedrosa and José Ignacio Peláez. All authors have read and agreed to the published version of the manuscript. Funding: The research was  funded by Programa Operativo FEDER Andalucía 2014‐2020, grant number “La reputación de  las organizaciones en una sociedad digital. Elaboración de una Plataforma  Inteligente para  la Localización,  Identificación  y  Clasificación  de  Influenciadores  en  los  Medios  Sociales  Digitales  (UMA18‐FEDERJA‐148)” and The APC was funded by the same research grant. Conflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results. References Ahlgren, P., Persson, O., & Tijssen, R. (2013). Geographical distance in bibliometric relations within epistemic communities. Scientometrics, 95(2), 771–784. https://doi.org/10.1007/s11192‐012‐0819‐1 Álvarez‐Bornstein, B., Morillo, F., & Bordons, M.  (2017). Funding acknowledgments  in  the Web of Science: completeness  and  accuracy  of  collected  data.  Scientometrics,  112(3),  1793–1812. https://doi.org/10.1007/s11192‐017‐2453‐4 Baraybar‐Fernández, A., Baños‐González, M., Barquero‐Pérez, Ó., Goya‐Esteban, R., & De‐la‐Morena‐Gómez, A.  (2017).  Evaluation  of  Emotional  Responses  to  Television  Advertising  through  Neuromarketing. Comunicar, 25(52), 19–28. https://doi.org/10.3916/C52‐2017‐02 Baron, A. S., Zaltman, G., & Olson, J. (2017). Barriers to advancing the science and practice of marketing. Journal of Marketing Management, 33(11–12), 893–908. https://doi.org/10.1080/0267257X.2017.1323839 Beaver,  D.  D.  B.  (2001).  Reflections  on  scientific  collaboration  (and  its  study):  Past,  present,  and  future. Scientometrics, 52(3), 365–377. https://doi.org/10.1023/A:1014254214337 Bericat,  E.  (2016).  The  sociology  of  emotions:  Four  decades  of  progress.  Current  Sociology,  64(3),  491–513. https://doi.org/10.1177/0011392115588355 Chen, C. (2017). Science Mapping: A Systematic Review of the Literature. Journal of Data and Information Science, 2(2), 1–40. https://doi.org/10.1515/jdis‐2017‐0006 Soc. Sci. 2020, 9, 23  21  of  22  de las Heras‐Pedrosa, C., Martel‐Casado, T., & Jambrino‐Maldonado, C. (2018). Análisis de las redes académicas y tendencias científicas de la comunicación en las universidades españolas. Prisma Social, (22), 229–246. https://doi.org/http://doi.org/10.5281/zenodo.2603078 Eck, N. J. van, & Waltman, L. (2009). How to normalize cooccurrence data? An analysis of some well‐known similarity measures. Journal of the American Society for Information Science and Technology, 60(8), 1635–1651. https://doi.org/10.1002/asi.21075 Egghe,  L.  (1994).  Little  science,  big  science...  and  beyond.  Scientometrics,  30(2–3),  389–392. https://doi.org/10.1007/BF02018109 Fonseca, B. de P. F. e, Sampaio, R. B., Fonseca, M. V. de A., & Zicker, F. (2016). Co‐authorship network analysis in  health  research:  method  and  potential  use.  Health  Research  Policy  and  Systems,  14(1),  34. https://doi.org/10.1186/s12961‐016‐0104‐5 Glänzel, W.  (2001). National  characteristics  in  international  scientific  co‐authorship  relations.  Scientometrics, 51(1), 69–115. https://doi.org/10.1023/A:1010512628145 Jasanoff, S., & Pinch, T. (2019). Springer Handbook of Science and Technology Indicators. (W. Glänzel, H. F. Moed, U. Schmoch, & M. Thelwall, Eds.). Cham: Springer  International Publishing. https://doi.org/10.1007/978‐3‐030‐02511‐3 Kang, D., & Park, Y. (2014). Review‐based measurement of customer satisfaction in mobile service: Sentiment analysis  and  VIKOR  approach.  Expert  Systems  with  Applications,  41(4),  1041–1050. https://doi.org/10.1016/j.eswa.2013.07.101 Katz,  J.  S.,  &  Hicks,  D.  (1997).  How  much  is  a  collaboration  worth?  A  calibrated  bibliometric  model. Scientometrics, 40(3), 541–554. https://doi.org/10.1007/BF02459299 Kennedy, H. (2012). Perspectives on Sentiment Analysis. Journal of Broadcasting & Electronic Media, 56(4), 435–450. https://doi.org/10.1080/08838151.2012.732141 Liu, W., & Ji, R. (2018). Examining the Role of Online Reviews in Chinese Online Group Buying Context: The Moderating  Effect  of  Promotional  Marketing.  Social  Sciences,  7(8),  141. https://doi.org/10.3390/socsci7080141 Luukkonen,  T.,  Persson,  O.,  &  Sivertsen,  G.  (1992).  Understanding  Patterns  of  International  Scientific Collaboration.  Science,  Technology,  &  Human  Values,  17(1),  101–126. https://doi.org/10.1177/016224399201700106 Mäntylä, M. V., Graziotin, D., & Kuutila, M. (2018). The evolution of sentiment analysis—A review of research topics,  venues,  and  top  cited  papers.  Computer  Science  Review,  27,  16–32. https://doi.org/10.1016/j.cosrev.2017.10.002 Martínez‐López, F. J., Merigó, J. M., Valenzuela‐Fernández, L., & Nicolás, C. (2018). Fifty years of the European Journal  of  Marketing:  a  bibliometric  analysis.  European  Journal  of  Marketing,  52(1–2),  439–468. https://doi.org/10.1108/EJM‐11‐2017‐0853 Mostafa, M. M. (2013). More than words: Social networks’ text mining for consumer brand sentiments. Expert Systems with Applications, 40(10), 4241–4251. https://doi.org/10.1016/j.eswa.2013.01.019 Moya‐Anegón, F., Guerrero‐Bote, V. P., Bornmann, L., & Moed, H. F. (2013). The research guarantors of scientific papers  and  the  output  counting:  a  promising  new  approach.  Scientometrics,  97(2),  421–434. https://doi.org/10.1007/s11192‐013‐1046‐0 Newman,  M.  E.  J.  (2000).  The  structure  of  scientific  collaboration  networks,  2000. https://doi.org/https://doi.org/10.1073/pnas.98.2.404 Pang, B., & Lee, L.  (2008). Opinion Mining  and  Sentiment Analysis. Foundations  and Trends®  in  Information Soc. Sci. 2020, 9, 23  22  of  22  Retrieval, 2(1–2), 1–135. https://doi.org/10.1561/1500000011 Peláez,  J.  I.,  Martínez,  E.  A., &  Vargas,  L. G.  (2019).  Products  and  services  valuation  through  unsolicited information from social media. Soft Computing, 3. https://doi.org/10.1007/s00500‐019‐04005‐3 Peláez, José I., Martínez, E. A., & Vargas, L. G. (2019). Decision making in social media with consistent data. Knowledge‐Based Systems, 172, 33–41. https://doi.org/10.1016/j.knosys.2019.02.009 Peláez, Jose Ignacio, Cabrera, F. E., & Vargas, L. G. (2018). Estimating the importance of consumer purchasing criteria  in  digital  ecosystems.  Knowledge‐Based  Systems,  162(March),  252–264. https://doi.org/10.1016/j.knosys.2018.07.023 Piryani, R., Madhavi, D., & Singh, V. K. (2017). Analytical mapping of opinion mining and sentiment analysis research  during  2000–2015.  Information  Processing  &  Management,  53(1),  122–150. https://doi.org/10.1016/j.ipm.2016.07.001 Prabowo, R., & Thelwall, M. (2009). Sentiment analysis: A combined approach. Journal of Informetrics, 3(2), 143–157. https://doi.org/10.1016/j.joi.2009.01.003 Saaty, T. L., & Vargas, L. G. (2012). Models, Methods, Concepts & Applications of the Analytic Hierarchy Process (Vol. 175). Boston, MA: Springer US. https://doi.org/10.1007/978‐1‐4614‐3597‐6 Sebastian, V.  (2014). New Directions  in Understanding  the Decision‐making  Process: Neuroeconomics  and Neuromarketing.  Procedia  ‐  Social  and  Behavioral  Sciences,  127,  758–762. https://doi.org/10.1016/j.sbspro.2014.03.350 Thelwall, M., Haustein, S., & Larivie, V. (2013). Do Altmetrics Work ? Twitter and Ten Other Social Web Services, 8(5), 1–7. https://doi.org/https://doi.org/10.1371/journal.pone.0064841 United  Nations.  (2015).  UNESCO  Science  Report:  towards  2030.  Retrieved  from https://en.unesco.org/unescosciencereport van  Eck,  N.  J., &  Waltman,  L.  (2010).  Software  survey: VOSviewer,  a  computer  program  for  bibliometric mapping. Scientometrics, 84(2), 523–538. https://doi.org/10.1007/s11192‐009‐0146‐3 van Eck, N. J., & Waltman, L. (2014). CitNetExplorer: A new software tool for analyzing and visualizing citation networks. Journal of Informetrics, 8(4), 802–823. https://doi.org/10.1016/j.joi.2014.07.006 Wereda, W., & Woźniak, J. (2019). Building Relationships with Customer 4.0 in the Era of Marketing 4.0: The Case  Study  of  Innovative  Enterprises  in  Poland.  Social  Sciences,  8(6),  177. https://doi.org/10.3390/socsci8060177   © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).   ",
      "id": 7653684,
      "identifiers": [
        {
          "identifier": "oai:e-archivo.uc3m.es:10016/39404",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.3390/socsci9030023",
          "type": "DOI"
        },
        {
          "identifier": "oai:mdpi.com:/2076-0760/9/3/23/",
          "type": "OAI_ID"
        },
        {
          "identifier": "597637488",
          "type": "CORE_ID"
        },
        {
          "identifier": "540307207",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:dnet:riuma_______::d2a79740937b8612c995e531113c6ce1",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:rabida.uhu.es:10272/23932",
          "type": "OAI_ID"
        },
        {
          "identifier": "486309680",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.3390/socsci9030023.",
          "type": "DOI"
        },
        {
          "identifier": "323326686",
          "type": "CORE_ID"
        },
        {
          "identifier": "618762668",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:riuma.uma.es:10630/19377",
          "type": "OAI_ID"
        },
        {
          "identifier": "288162334",
          "type": "CORE_ID"
        }
      ],
      "title": "Opinion mining and sentiment analysis in marketing communications: a science mapping analysis in Web of Science (1998–2018)",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:dnet:riuma_______::d2a79740937b8612c995e531113c6ce1",
        "oai:mdpi.com:/2076-0760/9/3/23/",
        "oai:e-archivo.uc3m.es:10016/39404",
        "oai:rabida.uhu.es:10272/23932",
        "oai:riuma.uma.es:10630/19377"
      ],
      "publishedDate": "2020-01-01T00:00:00",
      "publisher": "'MDPI AG'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://rabida.uhu.es/dspace/bitstream/10272/23932/2/socsci-09-00023-v2.pdf",
        "https://riuma.uma.es/xmlui/bitstream/10630/19377/1/Opinion%20Mining%20and%20Sentiment%20Analysis%20in%20Marketing%20Communications-%20A%20Science%20Mapping%20Analysis%20in%20Web%20of%20Science%20%281998%e2%80%932018%29.pdf",
        "http://dx.doi.org/10.3390/socsci9030023",
        "https://e-archivo.uc3m.es/bitstream/10016/39404/2/opinion_SS_2020.pdf"
      ],
      "updatedDate": "2024-10-04T15:13:56",
      "yearPublished": 2020,
      "journals": [
        {
          "title": "Social Sciences",
          "identifiers": [
            "issn:2076-0760",
            "2076-0760"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/288162334.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/288162334"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/288162334/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/288162334/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/7653684"
        }
      ]
    },
    {
      "acceptedDate": "2016-07-14T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Al-Ayyoub, Mahmoud"
        },
        {
          "name": "Al-Smadi, Mohammad"
        },
        {
          "name": "Androutsopoulos, Ion"
        },
        {
          "name": "Apidianaki, Marianna"
        },
        {
          "name": "Bel Rafecas, Núria"
        },
        {
          "name": "Bel, Nuria"
        },
        {
          "name": "De Clercq, Orphée"
        },
        {
          "name": "Eryiğit, Gülşen"
        },
        {
          "name": "Galanis, Dimitris"
        },
        {
          "name": "Hoste, Véronique"
        },
        {
          "name": "Jiménez-Zafra, Salud María"
        },
        {
          "name": "Kotelnikov, Evgeniy"
        },
        {
          "name": "Loukachevitch, Natalia"
        },
        {
          "name": "Manandhar, Suresh"
        },
        {
          "name": "Papageorgiou, Haris"
        },
        {
          "name": "Pontiki, Maria"
        },
        {
          "name": "Qin, Bing"
        },
        {
          "name": "Tannier, Xavier"
        },
        {
          "name": "Zhao, Yanyan"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur (LIMSI) ; Université Paris-Sud - Paris 11 (UP11)-Sorbonne Université - UFR d'Ingénierie (UFR 919) ; Sorbonne Université (SU)-Sorbonne Université (SU)-Université Paris-Saclay-Centre National de la Recherche Scientifique (CNRS)-Université Paris Saclay (COmUE)",
        "Maria",
        "Steven"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/275785886",
        "https://api.core.ac.uk/v3/outputs/74663966",
        "https://api.core.ac.uk/v3/outputs/661702534",
        "https://api.core.ac.uk/v3/outputs/207821446",
        "https://api.core.ac.uk/v3/outputs/627173036",
        "https://api.core.ac.uk/v3/outputs/160076403"
      ],
      "createdDate": "2017-02-14T19:08:33",
      "dataProviders": [
        {
          "id": 320,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/320",
          "logo": "https://api.core.ac.uk/data-providers/320/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 1493,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1493",
          "logo": "https://api.core.ac.uk/data-providers/1493/logo"
        },
        {
          "id": 2314,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2314",
          "logo": "https://api.core.ac.uk/data-providers/2314/logo"
        },
        {
          "id": 2973,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2973",
          "logo": "https://api.core.ac.uk/data-providers/2973/logo"
        }
      ],
      "depositedDate": "2016-01-01T00:00:00",
      "abstract": "International audienceThis paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams",
      "documentType": "research",
      "doi": "10.18653/v1/s16-1002",
      "downloadUrl": "https://core.ac.uk/download/74663966.pdf",
      "fieldOfStudy": null,
      "fullText": "Proceedings of SemEval-2016, pages 19–30,\nSan Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics\nSemEval-2016 Task 5: Aspect Based Sentiment Analysis\nMaria Pontiki∗1, Dimitrios Galanis1, Haris Papageorgiou1, Ion Androutsopoulos1,2,\nSuresh Manandhar3,Mohammad AL-Smadi4,Mahmoud Al-Ayyoub4, Yanyan Zhao5,\nBing Qin5, Orphée De Clercq6, Véronique Hoste6,Marianna Apidianaki7,\nXavier Tannier7, Natalia Loukachevitch8, Evgeny Kotelnikov9,\nNuria Bel10, Salud María Jiménez-Zafra11, Gülşen Eryiğit12\n1Institute for Language and Speech Processing, Athena R.C., Athens, Greece,\n2Dept. of Informatics, Athens University of Economics and Business, Greece,\n3Dept. of Computer Science, University of York, UK,\n4Computer Science Dept., Jordan University of Science and Technology Irbid, Jordan,\n5Harbin Institute of Technology, Harbin, Heilongjiang, P.R. China,\n6LT3, Ghent University, Ghent, Belgium,\n7LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay, Orsay, France,\n8Lomonosov Moscow State University, Moscow, Russian Federation,\n9Vyatka State University, Kirov, Russian Federation,\n10Universitat Pompeu Fabra, Barcelona, Spain,\n11Dept. of Computer Science, Universidad de Jaén, Spain,\n12Dept. of Computer Engineering, Istanbul Technical University, Turkey\nAbstract\nThis paper describes the SemEval 2016 shared\ntask on Aspect Based Sentiment Analysis\n(ABSA), a continuation of the respective tasks\nof 2014 and 2015. In its third year, the task\nprovided 19 training and 20 testing datasets\nfor 8 languages and 7 domains, as well as a\ncommon evaluation procedure. From these\ndatasets, 25 were for sentence-level and 14 for\ntext-level ABSA; the latter was introduced for\nthe first time as a subtask in SemEval. The task\nattracted 245 submissions from 29 teams.\n1 Introduction\nMany consumers use the Web to share their experi-\nences about products, services or travel destinations\n(Yoo and Gretzel, 2008). Online opinionated texts\n(e.g., reviews, tweets) are important for consumer\ndecision making (Chevalier and Mayzlin, 2006) and\nconstitute a source of valuable customer feedback\nthat can help companies to measure satisfaction and\nimprove their products or services. In this setting,\nAspect Based Sentiment Analysis (ABSA) - i.e.,\nmining opinions from text about specific entities and\ntheir aspects (Liu, 2012) - can provide valuable in-\nsights to both consumers and businesses. An ABSA\n∗*Corresponding author: mpontiki@ilsp.gr.\nFigure 1: Table summarizing the average sentiment for each\naspect of an entity.\nmethod can analyze large amounts of unstructured\ntexts and extract (coarse- or fine-grained) informa-\ntion not included in the user ratings that are available\nin some review sites (e.g., Fig. 1).\nSentiment Analysis (SA) touches every aspect\n(e.g., entity recognition, coreference resolution,\nnegation handling) of Natural Language Processing\n(Liu, 2012) and as Cambria et al. (2013) mention “it\nrequires a deep understanding of the explicit and im-\nplicit, regular and irregular, and syntactic and se-\nmantic language rules”. Within the last few years\nseveral SA-related shared tasks have been organized\nin the context of workshops and conferences focus-\n19\ning on somewhat different research problems (Seki\net al., 2007; Seki et al., 2008; Seki et al., 2010;\nMitchell, 2013; Nakov et al., 2013; Rosenthal et al.,\n2014; Pontiki et al., 2014; Rosenthal et al., 2015;\nGhosh et al., 2015; Pontiki et al., 2015; Moham-\nmad et al., 2016; Recupero and Cambria, 2014; Rup-\npenhofer et al., 2014; Loukachevitch et al., 2015).\nSuch competitions provide training datasets and the\nopportunity for direct comparison of different ap-\nproaches on common test sets.\nCurrently, most of the available SA-related\ndatasets, whether released in the context of shared\ntasks or not (Socher et al., 2013; Ganu et al., 2009),\nare monolingual and usually focus on English texts.\nMultilingual datasets (Klinger and Cimiano, 2014;\nJiménez-Zafra et al., 2015) provide additional ben-\nefits enabling the development and testing of cross-\nlingual methods (Lambert, 2015). Following this di-\nrection, this year the SemEval ABSA task provided\ndatasets in a variety of languages.\nABSAwas introduced as a shared task for the first\ntime in the context of SemEval in 2014; SemEval-\n2014 Task 41 (SE-ABSA14) provided datasets of\nEnglish reviews annotated at the sentence level with\naspect terms (e.g., “mouse”, “pizza”) and their po-\nlarity for the laptop and restaurant domains, as well\nas coarser aspect categories (e.g., “food”) and their\npolarity only for restaurants (Pontiki et al., 2014).\nSemEval-2015 Task 122 (SE-ABSA15) built upon\nSE-ABSA14 and consolidated its subtasks into a\nunified framework in which all the identified con-\nstituents of the expressed opinions (i.e., aspects,\nopinion target expressions and sentiment polarities)\nmeet a set of guidelines and are linked to each other\nwithin sentence-level tuples (Pontiki et al., 2015).\nThese tuples are important since they indicate the\npart of text within which a specific opinion is ex-\npressed. However, a user might also be interested\nin the overall rating of the text towards a partic-\nular aspect. Such ratings can be used to estimate\nthe mean sentiment per aspect from multiple re-\nviews (McAuley et al., 2012). Therefore, in addition\nto sentence-level annotations, SE-ABSA163 accom-\nmodated also text-level ABSA annotations and pro-\nvided the respective training and testing data. Fur-\n1http://alt.qcri.org/semeval2014/task4/\n2http://alt.qcri.org/semeval2015/task12/\n3http://alt.qcri.org/semeval2016/task5/\nthermore, the SE-ABSA15 annotation framework\nwas extended to new domains and applied to lan-\nguages other than English (Arabic, Chinese, Dutch,\nFrench, Russian, Spanish, and Turkish).\nThe remainder of this paper is organized as fol-\nlows: the task set-up is described in Section 2. Sec-\ntion 3 provides information about the datasets and\nthe annotation process, while Section 4 presents the\nevaluation measures and the baselines. General in-\nformation about participation in the task is provided\nin Section 5. The evaluation scores of the participat-\ning systems are presented and discussed in Section\n6. The paper concludes with an overall assessment\nof the task.\n2 Task Description\nThe SE-ABSA16 task consisted of the following\nsubtasks and slots. Participants were free to choose\nthe subtasks, slots, domains and languages they\nwished to participate in.\nSubtask 1 (SB1): Sentence-level ABSA. Given\nan opinionated text about a target entity, identify all\nthe opinion tuples with the following types (tuple\nslots) of information:\n• Slot1: Aspect Category. Identification of the\nentity E and attribute A pairs towards which\nan opinion is expressed in a given sentence. E\nand A should be chosen from predefined in-\nventories4 of entity types (e.g., “restaurant”,\n“food”) and attribute labels (e.g., “price”,\n“quality”).\n• Slot2: Opinion Target Expression (OTE).\nExtraction of the linguistic expression used in\nthe given text to refer to the reviewed entity E\nof each E#A pair. The OTE is defined by its\nstarting and ending offsets. When there is no\nexplicit mention of the entity, the slot takes the\nvalue “null”. The identification of Slot2 val-\nues was required only in the restaurants, hotels,\nmuseums and telecommunications domains.\n• Slot3: Sentiment Polarity. Each identified\nE#A pair has to be assigned one of the following\npolarity labels: “positive”, “negative”, “neu-\ntral” (mildly positive or mildly negative).\n4The full inventories of the aspect category labels for each\ndomain are provided in Appendix A.\n20\nLang. Domain Subtask Train Test#Texts #Sent. #Tuples #Texts #Sent. #Tuples\nEN REST SB1 350 2000 2507 90 676 859\nEN REST SB2 335 1950 1435 90 676 404\nEN LAPT SB1 450 2500 2909 80 808 801\nEN LAPT SB2 395 2375 2082 80 808 545\nAR HOTE SB1 1839 4802 10509 452 1227 2604\nAR HOTE SB2 1839 4802 8757 452 1227 2158\nCH PHNS SB1 140 6330 1333 60 3191 529\nCH CAME SB1 140 5784 1259 60 2256 481\nDU REST SB1 300 1711 1860 100 575 613\nDU REST SB2 300 1711 1247 100 575 381\nDU PHNS SB1 200 1389 1393 70 308 396\nFR REST SB1 335 1733 2530 120 696 954\nFR MUSE SB3 - - - 162 686 891\nRU REST SB1 302 3490 4022 103 1209 1300\nRU REST SB2 302 3490 1545 103 1209 500\nES REST SB1 627 2070 2720 286 881 1072\nES REST SB2 627 2070 2121 286 881 881\nTU REST SB1 300 1104 1535 39 144 159\nTU REST SB2 300 1104 972 39 144 108\nTU TELC SB1 - 3000 4082 - 310 336\nTable 1: Datasets provided for SE-ABSA16.\nAn example of opinion tuples with Slot1-3 values\nfrom the restaurants domain is shown below: “Their\nsake list was extensive, but we were looking for Pur-\nple Haze, which wasn’t listed but made for us upon\nrequest!” → {cat: “drinks#style_options”, trg:\n“sake list”, fr: “6”, to: “15”, pol: “positive”}, {cat:\n“service#general”, trg: “null”, fr: “0”, to: “0”,\npol: “positive”}. The variable cat indicates the as-\npect category (Slot1), pol the polarity (Slot3), and\ntrg the ote (Slot2); fr, to are the starting/ending\noffsets of ote.\nSubtask 2 (SB2): Text-level ABSA. Given a cus-\ntomer review about a target entity, the goal was\nto identify a set of {cat, pol} tuples that summa-\nrize the opinions expressed in the review. cat can\nbe assigned the same values as in SB1 (E#A tu-\nple), while pol can be set to “positive”, “negative”,\n“neutral”, or “conflict”. For example, for the re-\nview text “The So called laptop Runs to Slow and\nI hate it! Do not buy it! It is the worst laptop\never ”, a system should return the following opin-\nion tuples: {cat: “laptop#general”, pol: “nega-\ntive”}, {cat: “laptop#operation_performance”,\npol: “negative”} .\nSubtask 3 (SB3): Out-of-domain ABSA. In SB3\nparticipants had the opportunity to test their systems\nin domains for which no training data was made\navailable; the domains remained unknown until the\nstart of the evaluation period. Test data for SB3 were\nprovided only for the museums domain in French.\n3 Datasets\n3.1 Data Collection and Annotation\nA total of 39 datasets were provided in the context of\nthe SE-ABSA16 task; 19 for training and 20 for test-\ning. The texts were from 7 domains and 8 languages;\nEnglish (en), Arabic (ar), Chinese (ch), Dutch (du),\nFrench (fr), Russian (ru), Spanish (es) and Turk-\nish (tu). The datasets for the domains of restaurants\n(rest), laptops (lapt), mobile phones (phns), digital\ncameras (came), hotels (hote) and museums (muse)\nconsist of customer reviews, whilst the telecommu-\nnication domain (telc) data consists of tweets. A to-\ntal of 70790 manually annotated ABSA tuples were\nprovided for training and testing; 47654 sentence-\nlevel annotations (SB1) in 8 languages for 7 do-\nmains, and 23136 text-level annotations (SB2) in 6\nlanguages for 3 domains. Table 1 provides more in-\nformation on the distribution of texts, sentences and\nannotated tuples per dataset.\nThe rest, hote, and lapt datasets were annotated\n21\nat the sentence-level (SB1) following the respective\nannotation schemas of SE-ABSA15 (Pontiki et al.,\n2015). Below are examples5 of annotated sentences\nfor the aspect category “service#general” in en\n(1), du (2), fr (3), ru (4), es (5), and tu (6) for the\nrest domain and in ar (7) for the hote domain:\n1. Service was slow, but the people were friendly.\n→ {trg: “Service”, pol: “negative”}, {trg:\n“people”, pol: “positive”}\n2. Snelle bediening en vriendelijk personeel moet\nook gemeld worden!! → {trg: “bediening”,\npol: “positive”}, {trg: “personeel”, pol: “posi-\ntive”}\n3. Le service est impeccable, personnel agréable.\n→ {trg: “service” , pol: “positive”}, {trg: “per-\nsonnel”, pol: “positive”}\n4. Про сервис ничего негативного не скажешь-\nбыстро подходят, все улябаются, подходят\nспрашивают, всё ли нравится. → {trg:\n“сервис”, pol: “neutral” }\n5. También la rapidez en el servicio. → {trg: “ser-\nvicio”, pol: “positive” }\n6. Servisi hızlı valesi var. → {trg: “Servisi”, pol:\n“positive”}\n7. .. ﺔﻌﯾﺮﺳ و اﺪﺟ ةﺪﯿﺟ ﺔﻣﺪﺨﻟا → {trg: ”ﺔﻣﺪﺨﻟا“ , pol:\n“positive”}\nThe lapt annotation schema was extended to two\nother domains of consumer electronics, came and\nphns. Examples of annotated sentences in the lapt\n(en), phns (du and ch) and came (ch) domains are\nshown below:\n1. It is extremely portable and easily connects to\nWIFI at the library and elsewhere. → {cat:\n“laptop#portability”, pol: “positive”} , {cat:\n“laptop#connectivity”, pol: “positive”}\n2. Apps starten snel op en werken\nvlot, internet gaat prima. → {cat:\n“software#operation_performance”, pol:\n“positive”}, {cat: “phone#connectivity”,\npol: “positive”}\n5The offsets of the opinion target expressions are omitted.\n3. 当然屏幕这么好→{cat: “display#quality”,\npol: “positive”}\n4. 更 轻 便 的 机 身 也 便 于 携 带。→ {cat:\n“camera# portability”, pol: “positive”}\nIn addition, the SE-ABSA15 framework was ex-\ntended to two new domains for which annotation\nguidelines were compiled: telc for tu and muse for\nfr. Below are two examples:\n1. #Internet kopuyor sürekli :( @turkcell→ {cat:\n“internet#coverage”, trg: “Internet”, pol:\n“positive”}\n2. 5€ pour les étudiants, ça vaut le coup. → {cat:\n“museum#prices”, “null”, “positive”}\nThe text-level (SB2) annotation task was based\non the sentence-level annotations; given a customer\nreview about a target entity (e.g., a restaurant) that\nincluded sentence-level annotations of ABSA tu-\nples, the goal was to identify a set of {cat, pol}\ntuples that summarize the opinions expressed in it.\nThis was not a simple summation/aggregation of the\nsentence-level annotations since an aspect may be\ndiscussed with different sentiment in different parts\nof the review. In such cases the dominant sentiment\nhad to be identified. In case of conflicting opin-\nions where the dominant sentiment was not clear, the\n”conflict” label was assigned. In addition, each re-\nview was assigned an overall sentiment label about\nthe target entity (e.g., “restaurant#general”,\n“laptop#general”), even if it was not included in\nthe sentence-level annotations.\n3.2 Annotation Process\nAll datasets for each language were prepared by one\nor more research groups as shown in Table 2. The\nen, du, fr, ru and es datasets were annotated using\nbrat (Stenetorp et al., 2012), a web-based annota-\ntion tool, which was configured appropriately for the\nneeds of the task. The tu datasets were annotated us-\ning a customized version of turksent (Eryigit et al.,\n2013), a sentiment annotation tool for social media.\nFor the ar and the ch data in-house tools6 were used.\n6The ar annotation tool was developed by the technical\nteam of the Advanced Arabic Text Mining group at Jordan Uni-\nversity of Science and Technology. The ch tool was developed\nby the Research Center for Social Computing and Information\nRetrieval at Harbin Institute of Technology.\n22\nLang. Research team(s)\nEnglish Institute for Language and Speech Processing, Athena R.C., Athens, GreeceDept. of Informatics, Athens University of Economics and Business, Greece\nArabic Computer Science Dept., Jordan University of Science and Technology Irbid, Jordan\nChinese Harbin Institute of Technology, Harbin, Heilongjiang, P.R. China\nDutch LT3, Ghent University, Ghent, Belgium\nFrench LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay, Orsay, France\nRussian Lomonosov Moscow State University, Moscow, Russian FederationVyatka State University, Kirov, Russian Federation\nSpanish Universitat Pompeu Fabra, Barcelona, SpainSINAI, Universidad de Jaén, Spain\nTurkish Dept. of Computer Engineering, Istanbul Technical University, Turkey\nTurkcell Global Bilgi, Turkey\nTable 2: Research teams that contributed to the creation of the datasets for each language.\nBelow are some further details about the annotation\nprocess for each language.\nEnglish. The SE-ABSA15 (Pontiki et al., 2015)\ntraining and test datasets (with some minor correc-\ntions) were merged and provided for training (rest\nand lapt domains). New data was collected and an-\nnotated from scratch for testing. In a first phase, the\nrest test data was annotated by an experienced7 lin-\nguist (annotator A), and the lapt data by 5 under-\ngraduate computer science students. The resulting\nannotations for both domains were then inspected\nand corrected (if needed) by a second expert linguist,\none of the task organizers (annotator B). Borderline\ncases were resolved collaboratively by annotators A\nand B.\nArabic. The hote dataset was annotated in re-\npeated cycles. In a first phase, the data was annotated\nby three native Arabic speakers, all with a computer\nscience background; then the output was validated\nby a senior researcher, one of the task organizers. If\nneeded (e.g. when inconsistencies were found) they\nwere given back to the annotators.\nChinese. The datasets presented by Zhao et al.\n(2015) were re-annotated by three native Chinese\nspeakers according to the SE-ABSA16 annotation\nschema and were provided for training and testing\n(phns and came domains).\nDutch. The rest and phns datasets (De Clercq\nand Hoste, 2016) were initially annotated by a\ntrained linguist, native speaker of Dutch. Then,\nthe output was verified by another Dutch linguist\nand disagreements were resolved between them. Fi-\n7Also annotator for SE-ABSA14 and 15.\nnally, the task organizers inspected collaboratively\nall the annotated data and corrections were made\nwhen needed.\nFrench. The train (rest) and test (rest, muse)\ndatasets were annotated from scratch by a linguist,\nnative speaker of French. When the annotator was\nnot confident, a decision was made collaboratively\nwith the organizers. In a second phase, the task or-\nganizers checked all the annotations for mistakes and\ninconsistencies and corrected them, when necessary.\nFor more information on the French datasets consult\nApidianaki et al. (2016).\nRussian. The rest datasets of the SentiRuEval-\n2015 task (Loukachevitch et al., 2015) were auto-\nmatically converted to the SE-ABSA16 annotation\nschema; then a linguist, native speaker of Russian,\nchecked them and added missing information. Fi-\nnally, the datasets were inspected by a second lin-\nguist annotator (also native speaker of Russian) for\nmistakes and inconsistencies, which were resolved\nalong with one of the task organizers.\nSpanish. Initially, 50 texts (134 sentences) from\nthe whole available data were annotated by 4 annota-\ntors. The inter-anotator agreement (IAA) in terms of\nF-1 was 91% for the identification of OTE, 88% for\nthe aspect category detection (E#A pair), and 80%\nfor opinion tuples extraction (E#A, OTE, polarity).\nProvided that the IAA was substantially high for all\nslots, the rest of the data was divided into 4 parts and\neach one was annotated by a different native Spanish\nspeakers (2 linguists and 2 software engineers). Sub-\nsequently, the resulting annotations were validated\nand corrected (if needed) by the task organizers.\n23\nTurkish. The telc dataset was based on the data\nused in (Yıldırım et al., 2015), while the rest dataset\nwas created from scratch. Both datasets were anno-\ntated simultaneously by two linguists. Then, one of\nthe organizers validated/inspected the resulting an-\nnotations and corrected them when needed.\n3.3 Datasets Format and Availability\nSimilarly to SE-ABSA14 and SE-ABSA15, the\ndatasets8 of SE-ABSA16 were provided in an XML\nformat and they are available under specific license\nterms through META-SHARE9, a repository de-\nvoted to the sharing and dissemination of language\nresources (Piperidis, 2012).\n4 Evaluation Measures and Baselines\nThe evaluation ran in two phases. In the first phase\n(Phase A), the participants were asked to return\nseparately the aspect categories (Slot1), the OTEs\n(Slot2), and the {Slot1, Slot2} tuples for SB1. For\nSB2 the respective text-level categories had to be\nidentified. In the second phase (Phase B), the gold\nannotations for the test sets of Phase A were pro-\nvided and participants had to return the respective\nsentiment polarity values (Slot3). Similarly to SE-\nABSA15, F-1 scores were calculated for Slot1, Slot2\nand {Slot1, Slot2} tuples, by comparing the anno-\ntations that a system returned to the gold annota-\ntions (using micro-averaging). For Slot1 evaluation,\nduplicate occurrences of categories were ignored in\nboth SB1 and SB2. For Slot2, the calculation for\neach sentence considered only distinct targets and\ndiscarded “null” targets, since they do not corre-\nspond to explicit mentions. To evaluate sentiment\npolarity classification (Slot3) in Phase B, we calcu-\nlated the accuracy of each system, defined as the\nnumber of correctly predicted polarity labels of the\n(gold) aspect categories, divided by the total num-\nber of the gold aspect categories. Furthermore, we\nimplemented and provided baselines for all slots of\nSB1 and SB2. In particular, the SE-ABSA15 base-\nlines that were implemented for the English language\n8The data are available at: http://metashare.ilsp.\ngr:8080/repository/search/?q=semeval+2016\n9META-SHARE (http://www.metashare.org/) was\nimplemented in the framework of the META-NET Network of\nExcellence (http://www.meta-net.eu/).\n(Pontiki et al., 2015), were adapted for the other lan-\nguages by using appropriate stopword lists and to-\nkenization functions. The baselines are briefly dis-\ncussed below:\nSB1-Slot1: For category (E#A) extraction, a Sup-\nport Vector Machine (SVM) with a linear kernel is\ntrained. In particular, n unigram features are ex-\ntracted from the respective sentence of each tuple\nthat is encountered in the training data. The cat-\negory value (e.g., “service#general”) of the tu-\nple is used as the correct label of the feature vec-\ntor. Similarly, for each test sentence s, a fea-\nture vector is built and the trained SVM is used\nto predict the probabilities of assigning each possi-\nble category to s (e.g., {“service#general”, 0.2},\n{“restaurant#general”, 0.4}. Then, a thresh-\nold10 t is used to decide which of the categories will\nbe assigned11 to s. As features, we use the 1,000\nmost frequent unigrams of the training data exclud-\ning stopwords.\nSB1-Slot2: The baseline uses the training\nreviews to create for each category c (e.g.,\n“service#general”) a list of OTEs (e.g.,\n“service#general” → {“staff”, “waiter”}).\nThese are extracted from the (training) opinion\ntuples whose category value is c . Then, given a test\nsentence s and an assigned category c, the baseline\nfinds in s the first occurrence of each OTE of c’s\nlist. The OTE slot is filled with the first of the target\noccurrences found in s. If no target occurrences are\nfound, the slot is assigned the value “null”.\nSB1-Slot3: For polarity prediction we trained a\nSVM classifier with a linear kernel. Again, as in\nSlot1, n unigram features are extracted from the re-\nspective sentence of each tuple of the training data.\nIn addition, an integer-valued feature12 that indicates\nthe category of the tuple is used. The correct label\nfor the extracted training feature vector is the corre-\nsponding polarity value (e.g., “positive”). Then, for\neach tuple {category, OTE} of a test sentence s, a\nfeature vector is built and classified using the trained\nSVM.\nSB2-Slot1: The sentence-level tuples returned by\nthe SB1 baseline are copied to the text level and du-\nplicates are removed.\n10The threshold t was set to 0.2 for all datasets.\n11We use the –b 1 option of LibSVM to obtain probabilities.\n12Each E#A pair has been assigned a distinct integer value.\n24\nLang./ Slot1 Slot2 {Slot1,Slot2} Slot3\nDom. F-1 F-1 F-1 Acc.\nEN/ NLANG./U/73.031 NLANG./U/72.34 NLANG./U/52.607 XRCE/C/88.126\nREST NileT./U/72.886 AUEB-./U/70.441 XRCE/C/48.891 IIT-T./U/86.729\nBUTkn./U/72.396 UWB/U/67.089 NLANG./C/45.724 NileT./U/85.448\nAUEB-./U/71.537 UWB/C/66.906 TGB/C/43.081* IHS-R./U/83.935\nBUTkn./C/71.494 GTI/U/66.553 bunji/U/41.113 ECNU/U/83.586\nSYSU/U/70.869 Senti./C/66.545 UWB/C/41.108 AUEB-./U/83.236\nXRCE/C/68.701 bunji/U/64.882 UWB/U/41.088 INSIG./U/82.072\nUWB/U/68.203 NLANG./C/63.861 DMIS/U/39.796 UWB/C/81.839\nINSIG./U/68.108 DMIS/C/63.495 DMIS/C/38.976 UWB/U/81.723\nESI/U/67.979 XRCE/C/61.98 basel./C/37.795 SeemGo/C/81.141\nUWB/C/67.817 AUEB-./C/61.552 IHS-R./U/35.608 bunji/U/81.024\nGTI/U/67.714 UWate./U/57.067 IHS-R./U/34.864 TGB/C/80.908*\nAUEB-./C/67.35 KnowC./U/56.816* UWate./U/34.536 ECNU/C/80.559\nNLANG./C/65.563 TGB/C/55.054* SeemGo/U/30.667 UWate./U/80.326\nLeeHu./C/65.455 BUAP/U/50.253 BUAP/U/18.428 INSIG./C/80.21\nTGB/C/63.919* basel./C/44.071 DMIS/C/79.977\nIIT-T./U/63.051 IHS-R./U/43.808 DMIS/U/79.627\nDMIS/U/62.583 IIT-T./U/42.603 IHS-R./U/78.696\nDMIS/C/61.754 SeemGo/U/34.332 Senti./U/78.114\nIIT-T./C/61.227 LeeHu./C/78.114\nbunji/U/60.145 basel./C/76.484\nbasel./C/59.928 bunji/C/76.251\nUFAL/U/59.3 SeemGo/U/72.992\nINSIG./C/58.303 AKTSKI/U/71.711\nIHS-R./U/55.034 COMMI./C/70.547\nIHS-R./U/53.149 SNLP/U/69.965\nSeemGo/U/50.737 GTI/U/69.965\nUWate./U/49.73 CENNL./C/63.912\nCENNL./C/40.578 BUAP/U/60.885\nBUAP/U/37.29\nTable 3: English REST results for SB1.\nSB2-Slot3: For each text-level aspect category c\nthe baseline traverses the predicted sentence-level\ntuples of the same category returned by the respec-\ntive SB1 baseline and counts the polarity labels (pos-\nitive, negative, neutral). Finally, the polarity label\nwith the highest frequency is assigned to the text-\nlevel category c. If there are no sentence-level tuples\nfor the same c, the polarity label is determined based\non all tuples regardless of c.\nThe baseline systems and evaluation scripts are\nimplemented in Java and are available for down-\nload from the SE-ABSA16 website13. The LibSVM\npackage14 (Chang and Lin, 2011) is used for SVM\ntraining and prediction. The scores of the baselines\n13http://alt.qcri.org/semeval2016/task5/index.\nphp?id=data-and-tools\n14http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nin the test datasets are presented in Section 6 along\nwith the system scores.\n5 Participation\nThe task attracted in total 245 submissions from 29\nteams. The majority of the submissions (216 runs)\nwere for SB1. The newly introduced SB2 attracted\n29 submissions from 5 teams in 2 languages (en and\nsp). Most of the submissions (168) were runs for\nthe rest domain. This was expected, mainly for two\nreasons; first, the rest classification schema is less\nfine-grained (complex) compared to the other do-\nmains (e.g., lapt). Secondly, this domain was sup-\nported for 6 languages enabling also multilingual or\nlanguage-agnostic approaches. The remaining sub-\nmissions were distributed as follows: 54 in lapt, 12\nin phns, 7 in came and 4 in hote.\n25\nLang./ Slot1 Slot2 {Slot1,Slot2} Slot3\nDom. F-1 F-1 F-1 Acc.\nES/ GTI/U/70.588 GTI/C/68.515 TGB/C/41.219* IIT-T./U/83.582\nREST GTI/C/70.027 GTI/U/68.387 basel./C/36.379 TGB/C/82.09*\nTGB/C/63.551* IIT-T./U/64.338 UWB/C/81.343\nUWB/C/61.968 TGB/C/55.764* INSIG./C/79.571\nINSIG./C/61.37 basel./C/51.914 basel./C/77.799\nIIT-T./U/59.899\nIIT-T./C/59.062\nUFAL/U/58.81\nbasel./C/54.686\nFR/ XRCE/C/61.207 IIT-T./U/66.667 XRCE/C/47.721 XRCE/C/78.826\nREST IIT-T./U/57.875 XRCE/C/65.316 basel./C/33.017 UWB/C/75.262\nIIT-T./C/57.033 basel./C/45.455 UWB/C/74.319\nINSIG./C/53.592 INSIG./C/73.166\nbasel./C/52.609 IIT-T./U/72.222\nUFAL/U/49.928 basel./C/67.4\nRU/ UFAL/U/64.825 basel./C/49.308 basel./C/39.441 MayAnd/U/77.923\nREST INSIG./C/62.802 Danii./U/33.472 Danii./U/22.591 INSIG./C/75.077\nIIT-T./C/62.689 Danii./C/30.618 Danii./C/22.107 IIT-T./U/73.615\nIIT-T./C/58.196 Danii./U/73.308\nbasel./C/55.882 Danii./C/72.538\nDanii./C/39.601 basel./C/71\nDanii./U/38.692\nDU/ TGB/C/60.153* IIT-T./U/56.986 TGB/C/45.167* TGB/C/77.814*\nREST INSIG./C/56 TGB/C/51.775* basel./C/30.916 IIT-T./U/76.998\nIIT-T./U/55.247 basel./C/50.64 INSIG./C/75.041\nIIT-T./C/54.98 basel./C/69.331\nUFAL/U/53.876\nbasel./C/42.816\nTU/ UFAL/U/61.029 basel./C/41.86 basel./C/28.152 IIT-T./U/84.277\nREST basel./C/58.896 INSIG./C/74.214\nIIT-T./U/56.627 basel./C/72.327\nIIT-T./C/55.728\nINSIG./C/49.123\nAR/ INSIG./C/52.114 basel./C/30.978 basel./C/18.806 INSIG./C/82.719\nHOTE UFAL/U/47.302 IIT-T./U/81.72\nbasel./C/40.336 basel./C/76.421\nTable 4: REST and HOTE results for SB1.\nAn interesting observation is that, unlike SE-\nABSA15, Slot1 (aspect category detection) attracted\nsignificantly more submissions than Slot2 (OTE ex-\ntraction); this may indicate a shift towards concept-\nlevel approaches. Regarding participation per lan-\nguage, the majority of the submissions (156/245)\nwere for en; see more information in Table 5. Most\nteams (20) submitted results only for one language\n(18 for en and 2 for ru). Of the remaining teams,\n3 submitted results for 2 languages, 5 teams submit-\nted results for 3-7 languages, while only one team\nparticipated in all languages.\n6 Evaluation Results\nThe evaluation results are presented in Tables 3\n(SB1: rest-en), 4 (SB1: rest-es, fr, ru, du, tu\n& hote-ar), 6 (SB1: lapt, came, phns), and 7\n(SB2)15. Each participating team was allowed to\nsubmit up to two runs per slot and domain in each\nphase; one constrained (C), where only the provided\ntraining data could be used, and one unconstrained\n(U), where other resources (e.g., publicly available\n15No submissions were made for sb3-muse-fr & sb1-telc-\ntu.\n26\nLanguage Teams Submissions\nEnglish 27 156\nArabic 3 4\nChinese 3 14\nDutch 4 16\nFrench 5 13\nRussian 5 15\nSpanish 6 21\nTurkish 3 6\nAll 29 245\nTable 5: Number of participating teams and submitted runs per\nlanguage.\nlexica) and additional data of any kind could be used\nfor training. In the latter case, the teams had to re-\nport the resources used. Delayed submissions (i.e.,\nruns submitted after the deadline and the release of\nthe gold annotations) are marked with “*”.\nAs revealed by the results, in both SB1 and SB2\nthe majority of the systems surpassed the baseline\nby a small or large margin and, as expected, the un-\nconstrained systems achieved better results than the\nconstrained ones. In SB1, the teams with the high-\nest scores for Slot1 and Slot2 achieved similar F-1\nscores (see Table 3) in most cases (e.g., en/rest,\nes/rest, du/rest, fr/rest), which shows that the\ntwo slots have a similar level of difficulty. How-\never, as expected, the {Slot1, Slot2} scores were sig-\nnificantly lower since the linking of the target ex-\npressions to the corresponding aspects is also re-\nquired. The highest scores in SB1 for all slots (Slot1,\nSlot2, {Slot1, Slot2}, Slot3) were achieved in the\nen/rest; this is probably due to the high participation\nand to the lower complexity of the rest annotation\nschema compared to the other domains. If we com-\npare the results for SB1 and SB2, we notice that the\nSB2 scores for Slot1 are significantly higher (e.g.,\nen/lapt, en/rest, es/rest) even though the respec-\ntive annotations are for the same (or almost the same)\nset of texts. This is due to the fact that it is easier to\nidentify whether a whole text discusses an aspect c\nthan finding all the sentences in the text discussing\nc . On the other hand, for Slot3, the SB2 scores are\nlower (e.g., en/rest, es/rest, ru/rest, en/lapt) than\nthe respective SB1 scores. This is mainly because an\naspect may be discussed at different points in a text\nand often with different sentiment. In such cases a\nsystem has to identify the dominant sentiment, which\nLang./ Slot1 Slot3\nDom. F-1 Acc.\nEN/ NLANG./U/51.937 IIT-T./U/82.772\nLAPT AUEB-./U/49.105 INSIG./U/78.402\nSYSU/U/49.076 ECNU/U/78.152\nBUTkn./U/48.396 IHS-R./U/77.903\nUWB/C/47.891 NileT./U/77.403\nBUTkn./C/47.527 AUEB-./U/76.904\nUWB/U/47.258 LeeHu./C/75.905\nNileT./U/47.196 Senti./U/74.282\nNLANG./C/46.728 INSIG./C/74.282\nINSIG./U/45.863 UWB/C/73.783\nAUEB-./C/45.629 UWB/U/73.783\nIIT-T./U/43.913 SeemGo/C/72.16\nLeeHu./C/43.754 UWate./U/71.286\nIIT-T./C/42.609 bunji/C/70.287\nSeemGo/U/41.499 bunji/U/70.162\nINSIG./C/41.458 ECNU/C/70.037\nbunji/U/39.586 basel./C/70.037\nIHS-R./U/39.024 COMMI./C/67.541\nbasel./C/37.481 GTI/U/67.291\nUFAL/U/26.984 BUAP/U/62.797\nCENNL./C/26.908 CENNL./C/59.925\nBUAP/U/26.787 SeemGo/U/40.824\nCH/ UWB/C/36.345 SeemGo/C/80.457\nCAME INSIG./C/25.581 INSIG./C/78.17\nbasel./C/18.434 UWB/C/77.755\nSeemGo/U/17.757 basel./C/74.428\nSeemGo/U/73.181\nCH/ UWB/C/22.548 SeemGo/C/73.346\nPHNS basel./C/17.03 INSIG./C/72.401\nINSIG./C/16.286 UWB/C/72.023\nSeemGo/U/10.43 basel./C/70.132\nSeemGo/U/65.406\nDU/ INSIG./C/45.551 INSIG./C/83.333\nPHNS IIT-T./U/45.443 IIT-T./U/82.576\nIIT-T./C/45.047 basel./C/80.808\nbasel./C/33.55\nTable 6: LAPT, CAME, and PHNS results for SB1.\nusually is not trivial.\n7 Conclusions\nIn its third year, the SemEval ABSA task provided\n19 training and 20 testing datasets, from 7 domains\nand 8 languages, attracting 245 submissions from\n29 teams. The use of the same annotation guide-\nlines for domains addressed in different languages\ngives the opportunity to experiment also with cross-\nlingual or language-agnostic approaches. In addi-\ntion, SE-ABSA16 included for the first time a text-\n27\nLang./ Slot1 Slot3\nDom. F-1 Acc.\nEN/ GTI/U/83.995 UWB/U/81.931\nREST UWB/C/80.965 ECNU/U/81.436\nUWB/U/80.163 UWB/C/80.941\nbunji/U/79.777 ECNU/C/78.713\nbasel./C/78.711 basel./C/74.257\nSYSU/U/68.841 bunji/U/70.545\nSYSU/U/68.841 bunji/C/66.584\nGTI/U/64.109\nES/ GTI/C/84.192 UWB/C/77.185\nREST GTI/U/84.044 basel./C/74.548\nbasel./C/74.548\nUWB/C/73.657\nRU/ basel./C/84.792 basel./C/70.6\nREST\nRU/ basel./C/84.792 basel./C/70.6\nREST\nDU/ basel./C/70.323 basel./C/73.228\nREST\nTU/ basel./C/72.642 basel./C/57.407\nREST\nAR/ basel./C/42.757 basel./C/73.216\nHOTE\nEN/ UWB/C/60.45 ECNU/U/75.046\nLAPT UWB/U/59.721 UWB/U/75.046\nbunji/U/54.723 UWB/C/74.495\nbasel./C/52.685 basel./C/73.028\nSYSU/U/48.889 ECNU/C/67.523\nSYSU/U/48.889 bunji/C/62.202\nbunji/U/60\nGTI/U/58.349\nTable 7: Results for SB2.\nlevel subtask. Future work will address the cre-\nation of datasets in more languages and domains and\nthe enrichment of the annotation schemas with other\ntypes of SA-related information like topics, events\nand figures of speech (e.g., irony, metaphor).\nAcknowledgments\nThe authors are grateful to all the annotators and\ncontributors for their valuable support to the task:\nKonstantina Papanikolaou, Juli Bakagianni, Omar\nQwasmeh, Nesreen Alqasem, AreenMagableh, Saja\nAlzoubi, Bashar Talafha, Zekui Li, Binbin Li,\nShengqiu Li, Aaron Gevaert, Els Lefever, Cécile\nRichart, Pavel Blinov, Maria Shatalova, M. Teresa\nMartín-Valdivia, Pilar Santolaria, Fatih Samet Çetin,\nEzgi Yıldırım, Can Özbey, Leonidas Valavanis,\nStavros Giorgis, Dionysios Xenos, Panos Theodor-\nakakos, and Apostolos Rousas. The work described\nin this paper is partially funded by the projects EOX\nGR07/3712 and “Research Programs for Excellence\n2014-2016 / CitySense-ATHENA R.I.C.”. The Ara-\nbic track was partially supported by the Jordan Uni-\nversity of Science and Technology, Research Grant\nNumber: 20150164. The Dutch track has been\npartly funded by the PARIS project (IWT-SBO-\nNr. 110067). The French track was partially sup-\nported by the French National Research Agency un-\nder project ANR-12-CORD-0015/TransRead. The\nRussian track was partially supported by the Rus-\nsian Foundation for Basic Research (RFBR) accord-\ning to the research projects No. 14-07-00682a, 16-\n07-00342a, and No. 16-37-00311mol_a. The Span-\nish track has been partially supported by a grant\nfrom theMinisterio de Educación, Cultura y Deporte\n(MECD - scholarship FPU014/00983) and REDES\nproject (TIN2015-65136-C2-1-R) from the Minis-\nterio de Economía y Competitividad. The Turk-\nish track was partially supported by TUBITAK-\nTEYDEB (The Scientific and Technological Re-\nsearch Council of Turkey – Technology and Inno-\nvation Funding Programs Directorate) project (grant\nnumber: 3140671).\nReferences\nMarianna Apidianaki, Xavier Tannier, and Cécile\nRichart. 2016. A Dataset for Aspect-Based Sentiment\nAnalysis in French. In Proceedings of the Interna-\ntional Conference on Language Resources and Eval-\nuation.\nErik Cambria, Björn W. Schuller, Yunqing Xia, and\nCatherine Havasi. 2013. New Avenues in Opinion\nMining and Sentiment Analysis. IEEE Intelligent Sys-\ntems, 28(2):15–21.\nChih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:\nA library for support vector machines. ACM TIST,\n2(3):27.\nJudith Chevalier and Dina Mayzlin. 2006. The effect\nof word of mouth on sales: Online book reviews. J.\nMarketing Res, pages 345–354.\nOrphée De Clercq and Véronique Hoste. 2016. Rude\nwaiter but mouthwatering pastries! An exploratory\nstudy into Dutch Aspect-Based Sentiment Analysis. In\nProceedings of the 10th International Conference on\nLanguage Resources and Evaluation.\nGülsen Eryigit, Fatih Samet Cetin, Meltem Yanık, Turk-\ncell Global Bilgi, Tanel Temel, and Ilyas Ciçekli.\n28\n2013. TURKSENT: A Sentiment Annotation Tool for\nSocial Media. In Proceedings of the 7th Linguistic\nAnnotation Workshop and Interoperability with Dis-\ncourse.\nGayatree Ganu, Noemie Elhadad, and Amélie Marian.\n2009. Beyond the Stars: Improving Rating Predic-\ntions using Review Text Content. In Proceedings of\nWebDB.\nAniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso,\nEkaterina Shutova, John Barnden, and Antonio Reyes.\n2015. SemEval-2015 Task 11: Sentiment Analysis of\nFigurative Language in Twitter. In Proceedings of the\n9th International Workshop on Semantic Evaluation,\nDenver, Colorado.\nSaludM. Jiménez-Zafra, Giacomo Berardi, Andrea Esuli,\nDiego Marcheggiani, María Teresa Martín-Valdivia,\nand Alejandro Moreo Fernández. 2015. A Multi-\nlingual Annotated Dataset for Aspect-Oriented Opin-\nion Mining. In Proceedings of Empirical Methods in\nNatural Language Processing, pages 2533–2538.\nRoman Klinger and Philipp Cimiano. 2014. The USAGE\nReview Corpus for Fine Grained Multi Lingual Opin-\nion Analysis. In Proceedings of the Ninth Interna-\ntional Conference on Language Resources and Eval-\nuation, Reykjavik, Iceland.\nPatrik Lambert. 2015. Aspect-Level Cross-lingual Sen-\ntiment Classification with Constrained SMT. In Pro-\nceedings of the Association for Computational Linguis-\ntics and the International Joint Conference on Natu-\nral Language Processing, 2015, Beijing, China, pages\n781–787.\nBing Liu. 2012. Sentiment Analysis and Opinion Mining.\nSynthesis Lectures onHuman Language Technologies.\nMorgan & Claypool Publishers.\nNatalia Loukachevitch, Pavel Blinov, Evgeny Kotel-\nnikov, Yulia Rubtsova, Vladimir Ivanov, and Elena\nTutubalina. 2015. SentiRuEval: Testing Object-\noriented Sentiment Analysis Systems in Russian. In\nProceedings of International Conference Dialog.\nJulian J. McAuley, Jure Leskovec, and Dan Jurafsky.\n2012. Learning Attitudes and Attributes from Multi-\naspect Reviews. In 12th IEEE International Confer-\nence on Data Mining, ICDM 2012, Brussels, Belgium,\nDecember 10-13, 2012, pages 1020–1025.\nMargaret Mitchell. 2013. Overview of the TAC2013\nKnowledge Base Population Evaluation English Senti-\nment Slot Filling. In Proceedings of the 6th Text Anal-\nysis Conference, Gaithersburg, Maryland, USA.\nSaif M Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, andColin Cherry. 2016. SemEval-\n2016 Task 6: Detecting Stance in Tweets. In Proceed-\nings of the 10th International Workshop on Semantic\nEvaluation, San Diego, California.\nPreslav Nakov, Sara Rosenthal, Zornitsa Kozareva,\nVeselin Stoyanov, Alan Ritter, and Theresa Wilson.\n2013. SemEval-2013 Task 2: Sentiment Analysis in\nTwitter. In Proceedings of the 7th International Work-\nshop on Semantic Evaluation, Atlanta, Georgia.\nStelios Piperidis. 2012. The META-SHARE Language\nResources Sharing Infrastructure: Principles, Chal-\nlenges, Solutions. In Proceedings of the 8th Interna-\ntional Conference on Language Resources and Evalu-\nation.\nMaria Pontiki, Dimitrios Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. SemEval-2014 Task 4: Aspect\nBased Sentiment Analysis. In Proceedings of the\n8th International Workshop on Semantic Evaluation,\nDublin, Ireland.\nMaria Pontiki, Dimitrios Galanis, Harris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemEval-2015 Task 12: Aspect Based Sentiment\nAnalysis. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation, Denver, Colorado.\nDiego Reforgiato Recupero and Erik Cambria. 2014.\nEswc’14 challenge on concept-level sentiment analy-\nsis. In Semantic Web Evaluation Challenge - SemWe-\nbEval 2014 at ESWC 2014, Anissaras, Crete, Greece,\npages 3–20.\nSara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin\nStoyanov. 2014. SemEval-2014 Task 4: Sentiment\nAnalysis in Twitter. In Proceedings of the 8th Interna-\ntional Workshop on Semantic Evaluation, Dublin, Ire-\nland.\nSara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,\nSaif M Mohammad, Alan Ritter, and Veselin Stoy-\nanov. 2015. SemEval-2015 Task 10: Sentiment Anal-\nysis in Twitter. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation, Denver, Colorado.\nJosef Ruppenhofer, Roman Klinger, Julia Maria Struß,\nJonathan Sonntag, and Michael Wiegand. 2014. IG-\nGSA Shared Tasks on German Sentiment Analysis\n(GESTALT). In Workshop Proceedings of the 12th\nEdition of the KONVENS Conference, pages 164–173.\nYohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi\nChen, Noriko Kando, and Chin-Yew Lin. 2007.\nOverview of Opinion Analysis Pilot Task at NTCIR-\n6. In Proceedings of the 6th NTCIR Workshop, Tokyo,\nJapan.\nYohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,\nHsin-Hsi Chen, and Noriko Kando. 2008. Overview\nof Multilingual Opinion Analysis Task at NTCIR-7.\nIn Proceedings of the 7th NTCIR Workshop, Tokyo,\nJapan.\nYohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and\nNoriko Kando. 2010. Overview of Multilingual Opin-\nion Analysis Task at NTCIR-8: A Step Toward Cross\n29\nLingual Opinion Analysis. In Proceedings of the 8th\nNTCIR Workshop, Tokyo, Japan, pages 209–220.\nRichard Socher, Alex Perelygin, JeanWu, Jason Chuang,\nChristopher D. Manning, Andrew Y. Ng, and Christo-\npher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of Empirical Methods in Natural\nLanguage Processing, pages 1631–1642, Stroudsburg,\nPA.\nPontus Stenetorp, Sampo Pyysalo, Goran Topic, Tomoko\nOhta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012.\nBRAT: AWeb-based Tool for NLP-Assisted Text An-\nnotation. In Proceedings of the European Chapter of\nthe Association for Computational Linguistics, pages\n102–107.\nEzgi Yıldırım, Fatih Samet Çetin, Gülşen Eryiğit,\nand Tanel Temel. 2015. The impact of nlp\non turkish sentiment analysis. TÜRKİYE BİLİŞİM\nVAKFI BİLGİSAYARBİLİMLERİ veMÜHENDİSLİĞİ\nDERGİSİ, 7(1 (Basılı 8).\nKyung Hyan Yoo and Ulrike Gretzel. 2008. What Moti-\nvates Consumers to Write Online Travel Reviews? J.\nof IT & Tourism, 10(4):283–295.\nYanyan Zhao, Bing Qin, and Ting Liu. 2015. Creating a\nFine-Grained Corpus for Chinese Sentiment Analysis.\nIEEE Intelligent Systems, 30(1):36–43.\nAppendix A. Aspect inventories for all domains\nEntity Labels\nlaptop, display, keyboard, mouse, motherboard,\ncpu, fans_cooling, ports, memory, power_supply\noptical_drives, battery, graphics, hard_disk,\nmultimedia_devices, hardware, software, os,\nwarranty, shipping, support, company\nAttribute Labels\ngeneral, price, quality, design_features,\noperation_performance, usability, portability,\nconnectivity, miscellaneous\nTable 8: Laptops.\nEntity Labels\nphone, display, keyboard, cpu, ports, memory,\npower_supply, hard_disk, multimedia_devices,\nbattery, hardware, software, os, warranty,\nshipping, support, company\nAttribute Labels\nSame as in Laptops (Table 8) with the exception of\nportability that is included in the design_features\nlabel and does not apply as a separate attribute type.\nTable 9: Mobile Phones.\nEntity Labels\ncamera, display, keyboard, cpu, ports, memory,\npower_supply, battery, multimedia_devices,\nhardware, software, os, warranty, shipping,\nsupport, company, lens, photo, focus\nAttribute Labels\nSame as in Laptops (Table 8).\nTable 10: Digital Cameras.\nEntity Labels\nrestaurant, food, drinks, ambience,\nservice, location\nAttribute Labels\ngeneral, prices, quality,\nstyle_options, miscellaneous\nTable 11: Restaurants.\nEntity Labels\nhotel, rooms, facilities, room_amenities,\nservice, location, food_drinks\nAttribute Labels\ngeneral, price, comfort, cleanliness, quality,\nstyle_options, design_features, miscellaneous\nTable 12: Hotels.\nEntity Labels\ntelecom operator, device, internet,\ncustomer_services, application_service\nAttribute Labels\ngeneral, price_invoice, coverage,\nspeed, campaign_advertisement, miscellaneous\nTable 13: Telecommunications.\nEntity Labels\nmuseum, collections, facilities, service,\ntour_guiding, location\nAttribute Labels\ngeneral, prices, comfort, activities,\narchitecture, interest, set up, miscellaneous\nTable 14: Museums.\n30\n",
      "id": 37978496,
      "identifiers": [
        {
          "identifier": "10.18653/v1/s16-1002",
          "type": "DOI"
        },
        {
          "identifier": "661702534",
          "type": "CORE_ID"
        },
        {
          "identifier": "627173036",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:archive.ugent.be:8131987",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:repositori.upf.edu:10230/68305",
          "type": "OAI_ID"
        },
        {
          "identifier": "74663966",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:repositori-api.upf.edu:10230/68305",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:hal:hal-02407165v1",
          "type": "OAI_ID"
        },
        {
          "identifier": "160076403",
          "type": "CORE_ID"
        },
        {
          "identifier": "207821446",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:hal:hal-01838537v1",
          "type": "OAI_ID"
        },
        {
          "identifier": "624090453",
          "type": "CORE_ID"
        },
        {
          "identifier": "275785886",
          "type": "CORE_ID"
        },
        {
          "identifier": "627655992",
          "type": "CORE_ID"
        }
      ],
      "title": "SemEval-2016 task 5 : aspect based sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:archive.ugent.be:8131987",
        "oai:hal:hal-02407165v1",
        "oai:repositori.upf.edu:10230/68305",
        "oai:repositori-api.upf.edu:10230/68305",
        "oai:hal:hal-01838537v1"
      ],
      "publishedDate": "2016-01-01T00:00:00",
      "publisher": "'Association for Computational Linguistics (ACL)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://hal.science/hal-01838537v1/document",
        "https://biblio.ugent.be/publication/8131987/file/8502211.pdf",
        "https://hal.science/hal-02407165v1/document"
      ],
      "updatedDate": "2025-07-13T18:00:08",
      "yearPublished": 2016,
      "journals": [
        {
          "title": null,
          "identifiers": [
            "issn:0736-587X",
            "0736-587x"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/74663966.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/74663966"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/74663966/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/74663966/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/37978496"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "cs/0409058",
      "authors": [
        {
          "name": "Lee, Lillian"
        },
        {
          "name": "Pang, Bo"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/24682462"
      ],
      "createdDate": "2012-04-13T14:21:49",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;\nan example application is classifying a movie review as \"thumbs up\" or \"thumbs\ndown\". To determine this sentiment polarity, we propose a novel\nmachine-learning method that applies text-categorization techniques to just the\nsubjective portions of the document. Extracting these portions can be\nimplemented using efficient techniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence contextual constraints.Comment: Data available at\n  http://www.cs.cornell.edu/people/pabo/movie-review-data",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/cs/0409058",
      "fieldOfStudy": null,
      "fullText": "ar\nX\niv\n:c\ns/0\n40\n90\n58\nv1\n  [\ncs\n.C\nL]\n  2\n9 S\nep\n 20\n04\nA Sentimental Education: Sentiment Analysis Using Subjectivity\nSummarization Based on Minimum Cuts\nBo Pang and Lillian Lee\nDepartment of Computer Science\nCornell University\nIthaca, NY 14853-7501\n{pabo,llee}@cs.cornell.edu\nAbstract\nSentiment analysis seeks to identify the view-\npoint(s) underlying a text span; an example appli-\ncation is classifying a movie review as “thumbs up”\nor “thumbs down”. To determine this sentiment po-\nlarity, we propose a novel machine-learning method\nthat applies text-categorization techniques to just\nthe subjective portions of the document. Extracting\nthese portions can be implemented using efficient\ntechniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence\ncontextual constraints.\nPublication info: Proceedings of the ACL, 2004.\n1 Introduction\nThe computational treatment of opinion, sentiment,\nand subjectivity has recently attracted a great deal\nof attention (see references), in part because of its\npotential applications. For instance, information-\nextraction and question-answering systems could\nflag statements and queries regarding opinions\nrather than facts (Cardie et al., 2003). Also, it has\nproven useful for companies, recommender sys-\ntems, and editorial sites to create summaries of peo-\nple’s experiences and opinions that consist of sub-\njective expressions extracted from reviews (as is\ncommonly done in movie ads) or even just a re-\nview’s polarity — positive (“thumbs up”) or neg-\native (“thumbs down”).\nDocument polarity classification poses a sig-\nnificant challenge to data-driven methods, re-\nsisting traditional text-categorization techniques\n(Pang, Lee, and Vaithyanathan, 2002). Previous ap-\nproaches focused on selecting indicative lexical fea-\ntures (e.g., the word “good”), classifying a docu-\nment according to the number of such features that\noccur anywhere within it. In contrast, we propose\nthe following process: (1) label the sentences in\nthe document as either subjective or objective, dis-\ncarding the latter; and then (2) apply a standard\nmachine-learning classifier to the resulting extract.\nThis can prevent the polarity classifier from consid-\nering irrelevant or even potentially misleading text:\nfor example, although the sentence “The protagonist\ntries to protect her good name” contains the word\n“good”, it tells us nothing about the author’s opin-\nion and in fact could well be embedded in a negative\nmovie review. Also, as mentioned above, subjectiv-\nity extracts can be provided to users as a summary\nof the sentiment-oriented content of the document.\nOur results show that the subjectivity extracts\nwe create accurately represent the sentiment in-\nformation of the originating documents in a much\nmore compact form: depending on choice of down-\nstream polarity classifier, we can achieve highly sta-\ntistically significant improvement (from 82.8% to\n86.4%) or maintain the same level of performance\nfor the polarity classification task while retaining\nonly 60% of the reviews’ words. Also, we ex-\nplore extraction methods based on a minimum cut\nformulation, which provides an efficient, intuitive,\nand effective means for integrating inter-sentence-\nlevel contextual information with traditional bag-of-\nwords features.\n2 Method\n2.1 Architecture\nOne can consider document-level polarity classi-\nfication to be just a special (more difficult) case\nof text categorization with sentiment- rather than\ntopic-based categories. Hence, standard machine-\nlearning classification techniques, such as sup-\nport vector machines (SVMs), can be applied to\nthe entire documents themselves, as was done by\nPang, Lee, and Vaithyanathan (2002). We refer to\nsuch classification techniques as default polarity\nclassifiers.\nHowever, as noted above, we may be able to im-\nprove polarity classification by removing objective\nsentences (such as plot summaries in a movie re-\nview). We therefore propose, as depicted in Figure\n1, to first employ a subjectivity detector that deter-\nmines whether each sentence is subjective or not:\ndiscarding the objective ones creates an extract that\nshould better represent a review’s subjective content\nto a default polarity classifier.\ns1\ns2\ns3\ns4\ns_n\n+/-\ns4\ns1\nsu\nbje\ncti\nvit\ny\nde\nte\nct\nor\nyes\nno\nno\nyes\nn-sentence review\nsubjective\nsentence? m-sentence extract\n(m<=n) review?\npositive or negative\nde\nfa\nul\nt\ncl\nas\nsif\nie\nr\npo\nla\nrit\ny\nsubjectivity extraction\nFigure 1: Polarity classification via subjectivity detec-\ntion.\nTo our knowledge, previous work has not\nintegrated sentence-level subjectivity detec-\ntion with document-level sentiment polarity.\nYu and Hatzivassiloglou (2003) provide methods\nfor sentence-level analysis and for determining\nwhether a document is subjective or not, but do not\ncombine these two types of algorithms or consider\ndocument polarity classification. The motivation\nbehind the single-sentence selection method of\nBeineke et al. (2004) is to reveal a document’s\nsentiment polarity, but they do not evaluate the\npolarity-classification accuracy that results.\n2.2 Context and Subjectivity Detection\nAs with document-level polarity classification, we\ncould perform subjectivity detection on individual\nsentences by applying a standard classification algo-\nrithm on each sentence in isolation. However, mod-\neling proximity relationships between sentences\nwould enable us to leverage coherence: text spans\noccurring near each other (within discourse bound-\naries) may share the same subjectivity status, other\nthings being equal (Wiebe, 1994).\nWe would therefore like to supply our algorithms\nwith pair-wise interaction information, e.g., to spec-\nify that two particular sentences should ideally re-\nceive the same subjectivity label but not state which\nlabel this should be. Incorporating such informa-\ntion is somewhat unnatural for classifiers whose\ninput consists simply of individual feature vec-\ntors, such as Naive Bayes or SVMs, precisely be-\ncause such classifiers label each test item in isola-\ntion. One could define synthetic features or fea-\nture vectors to attempt to overcome this obstacle.\nHowever, we propose an alternative that avoids the\nneed for such feature engineering: we use an ef-\nficient and intuitive graph-based formulation rely-\ning on finding minimum cuts. Our approach is in-\nspired by Blum and Chawla (2001), although they\nfocused on similarity between items (the motiva-\ntion being to combine labeled and unlabeled data),\nwhereas we are concerned with physical proximity\nbetween the items to be classified; indeed, in com-\nputer vision, modeling proximity information via\ngraph cuts has led to very effective classification\n(Boykov, Veksler, and Zabih, 1999).\n2.3 Cut-based classification\nFigure 2 shows a worked example of the concepts\nin this section.\nSuppose we have n items x1, . . . , xn to divide\ninto two classes C1 and C2, and we have access to\ntwo types of information:\n• Individual scores indj(xi): non-negative esti-\nmates of each xi’s preference for being in Cj based\non just the features of xi alone; and\n• Association scores assoc(xi, xk): non-negative\nestimates of how important it is that xi and xk be in\nthe same class.1\nWe would like to maximize each item’s “net hap-\npiness”: its individual score for the class it is as-\nsigned to, minus its individual score for the other\nclass. But, we also want to penalize putting tightly-\nassociated items into different classes. Thus, after\nsome algebra, we arrive at the following optimiza-\ntion problem: assign the xis to C1 and C2 so as to\nminimize the partition cost∑\nx∈C1\nind2(x)+\n∑\nx∈C2\nind1(x)+\n∑\nxi∈C1,\nxk∈C2\nassoc(xi, xk).\nThe problem appears intractable, since there are\n2n possible binary partitions of the xi’s. How-\never, suppose we represent the situation in the fol-\nlowing manner. Build an undirected graph G with\nvertices {v1, . . . , vn, s, t}; the last two are, respec-\ntively, the source and sink. Add n edges (s, vi), each\nwith weight ind1(xi), and n edges (vi, t), each with\nweight ind2(xi). Finally, add\n(\nn\n2\n)\nedges (vi, vk),\neach with weight assoc(xi, xk). Then, cuts in G\nare defined as follows:\nDefinition 1 A cut (S, T ) of G is a partition of its\nnodes into sets S = {s} ∪ S′ and T = {t} ∪ T ′,\nwhere s 6∈ S′, t 6∈ T ′. Its cost cost(S, T ) is the sum\n1Asymmetry is allowed, but we used symmetric scores.\n[]\ns t\nY\nM\nN\n2ind (Y) [.2]1ind (Y) [.8]\n2ind (M) [.5]1ind (M) [.5]\n[.1]assoc(Y,N)\n2ind (N) [.9]1ind (N)\nassoc(M,N)\nassoc(Y,M)\n[.2]\n[1.0]\n[.1]\nC1 Individual Association Cost\npenalties penalties\n{Y,M} .2 + .5 + .1 .1 + .2 1.1\n(none) .8 + .5 + .1 0 1.4\n{Y,M,N} .2 + .5 + .9 0 1.6\n{Y} .2 + .5 + .1 1.0 + .1 1.9\n{N} .8 + .5 + .9 .1 + .2 2.5\n{M} .8 + .5 + .1 1.0 + .2 2.6\n{Y,N} .2 + .5 + .9 1.0 + .2 2.8\n{M,N} .8 + .5 + .9 1.0 + .1 3.3\nFigure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to\nbe probabilities. Based on individual scores alone, we would put Y (“yes”) in C1, N (“no”) in C2, and be undecided\nabout M (“maybe”). But the association scores favor cuts that put Y and M in the same class, as shown in the table.\nThus, the minimum cut, indicated by the dashed line, places M together with Y in C1.\nof the weights of all edges crossing from S to T . A\nminimum cut of G is one of minimum cost.\nObserve that every cut corresponds to a partition of\nthe items and has cost equal to the partition cost.\nThus, our optimization problem reduces to finding\nminimum cuts.\nPractical advantages As we have noted, formulat-\ning our subjectivity-detection problem in terms of\ngraphs allows us to model item-specific and pair-\nwise information independently. Note that this is\na very flexible paradigm. For instance, it is per-\nfectly legitimate to use knowledge-rich algorithms\nemploying deep linguistic knowledge about sen-\ntiment indicators to derive the individual scores.\nAnd we could also simultaneously use knowledge-\nlean methods to assign the association scores. In-\nterestingly, Yu and Hatzivassiloglou (2003) com-\npared an individual-preference classifier against a\nrelationship-based method, but didn’t combine the\ntwo; the ability to coordinate such algorithms is\nprecisely one of the strengths of our approach.\nBut a crucial advantage specific to the uti-\nlization of a minimum-cut-based approach is\nthat we can use maximum-flow algorithms with\npolynomial asymptotic running times — and\nnear-linear running times in practice — to ex-\nactly compute the minimum-cost cut(s), despite\nthe apparent intractability of the optimization\nproblem (Cormen, Leiserson, and Rivest, 1990;\nAhuja, Magnanti, and Orlin, 1993).2 In con-\ntrast, other graph-partitioning problems\nthat have been previously used to formu-\n2Code available at http://www.avglab.com/andrew/soft.html.\nlate NLP classification problems3 are NP-\ncomplete (Hatzivassiloglou and McKeown, 1997;\nAgrawal et al., 2003; Joachims, 2003).\n3 Evaluation Framework\nOur experiments involve classifying movie re-\nviews as either positive or negative, an appeal-\ning task for several reasons. First, as mentioned\nin the introduction, providing polarity informa-\ntion about reviews is a useful service: witness\nthe popularity of www.rottentomatoes.com. Sec-\nond, movie reviews are apparently harder to clas-\nsify than reviews of other products (Turney, 2002;\nDave, Lawrence, and Pennock, 2003). Third, the\ncorrect label can be extracted automatically from\nrating information (e.g., number of stars). Our data4\ncontains 1000 positive and 1000 negative reviews\nall written before 2002, with a cap of 20 reviews per\nauthor (312 authors total) per category. We refer to\nthis corpus as the polarity dataset.\nDefault polarity classifiers We tested support vec-\ntor machines (SVMs) and Naive Bayes (NB). Fol-\nlowing Pang et al. (2002), we use unigram-presence\nfeatures: the ith coordinate of a feature vector is\n1 if the corresponding unigram occurs in the input\ntext, 0 otherwise. (For SVMs, the feature vectors\nare length-normalized). Each default document-\nlevel polarity classifier is trained and tested on the\nextracts formed by applying one of the sentence-\nlevel subjectivity detectors to reviews in the polarity\ndataset.\n3Graph-based approaches to general clustering problems\nare too numerous to mention here.\n4Available at www.cs.cornell.edu/people/pabo/movie-\nreview-data/ (review corpus version 2.0).\nSubjectivity dataset To train our detectors, we\nneed a collection of labeled sentences. Riloff and\nWiebe (2003) state that “It is [very hard] to ob-\ntain collections of individual sentences that can be\neasily identified as subjective or objective”; the\npolarity-dataset sentences, for example, have not\nbeen so annotated.5 Fortunately, we were able\nto mine the Web to create a large, automatically-\nlabeled sentence corpus6. To gather subjective\nsentences (or phrases), we collected 5000 movie-\nreview snippets (e.g., “bold, imaginative, and im-\npossible to resist”) from www.rottentomatoes.com.\nTo obtain (mostly) objective data, we took 5000 sen-\ntences from plot summaries available from the In-\nternet Movie Database (www.imdb.com). We only\nselected sentences or snippets at least ten words\nlong and drawn from reviews or plot summaries of\nmovies released post-2001, which prevents overlap\nwith the polarity dataset.\nSubjectivity detectors As noted above, we can use\nour default polarity classifiers as “basic” sentence-\nlevel subjectivity detectors (after retraining on the\nsubjectivity dataset) to produce extracts of the orig-\ninal reviews. We also create a family of cut-based\nsubjectivity detectors; these take as input the set of\nsentences appearing in a single document and de-\ntermine the subjectivity status of all the sentences\nsimultaneously using per-item and pairwise rela-\ntionship information. Specifically, for a given doc-\nument, we use the construction in Section 2.2 to\nbuild a graph wherein the source s and sink t cor-\nrespond to the class of subjective and objective sen-\ntences, respectively, and each internal node vi cor-\nresponds to the document’s ith sentence si. We can\nset the individual scores ind1(si) to PrNBsub (si) and\nind2(si) to 1 − PrNBsub (si), as shown in Figure 3,\nwhere PrNBsub (s) denotes Naive Bayes’ estimate of\nthe probability that sentence s is subjective; or, we\ncan use the weights produced by the SVM classi-\nfier instead.7 If we set all the association scores\nto zero, then the minimum-cut classification of the\n5We therefore could not directly evaluate sentence-\nclassification accuracy on the polarity dataset.\n6Available at www.cs.cornell.edu/people/pabo/movie-\nreview-data/ , sentence corpus version 1.0.\n7We converted SVM output di, which is a signed distance\n(negative=objective) from the separating hyperplane, to non-\nnegative numbers by\nind1(si)\ndef\n=\n{\n1 di > 2;\n(2 + di)/4 −2 ≤ di ≤ 2;\n0 di < −2.\nand ind2(si) = 1 − ind1(si). Note that scaling is employed\nonly for consistency; the algorithm itself does not require prob-\nabilities for individual scores.\nsentences is the same as that of the basic subjectiv-\nity detector. Alternatively, we incorporate the de-\ngree of proximity between pairs of sentences, con-\ntrolled by three parameters. The threshold T spec-\nifies the maximum distance two sentences can be\nseparated by and still be considered proximal. The\nnon-increasing function f(d) specifies how the in-\nfluence of proximal sentences decays with respect to\ndistance d; in our experiments, we tried f(d) = 1,\ne1−d, and 1/d2. The constant c controls the relative\ninfluence of the association scores: a larger c makes\nthe minimum-cut algorithm more loath to put prox-\nimal sentences in different classes. With these in\nhand8, we set (for j > i)\nassoc(si, sj)\ndef\n=\n{\nf(j − i) · c if (j − i) ≤ T ;\n0 otherwise.\n4 Experimental Results\nBelow, we report average accuracies computed by\nten-fold cross-validation over the polarity dataset.\nSection 4.1 examines our basic subjectivity extrac-\ntion algorithms, which are based on individual-\nsentence predictions alone. Section 4.2 evaluates\nthe more sophisticated form of subjectivity extrac-\ntion that incorporates context information via the\nminimum-cut paradigm.\nAs we will see, the use of subjectivity extracts\ncan in the best case provide satisfying improve-\nment in polarity classification, and otherwise can\nat least yield polarity-classification accuracies indis-\ntinguishable from employing the full review. At the\nsame time, the extracts we create are both smaller\non average than the original document and more\neffective as input to a default polarity classifier\nthan the same-length counterparts produced by stan-\ndard summarization tactics (e.g., first- or last-N sen-\ntences). We therefore conclude that subjectivity ex-\ntraction produces effective summaries of document\nsentiment.\n4.1 Basic subjectivity extraction\nAs noted in Section 3, both Naive Bayes and SVMs\ncan be trained on our subjectivity dataset and then\nused as a basic subjectivity detector. The former has\nsomewhat better average ten-fold cross-validation\nperformance on the subjectivity dataset (92% vs.\n90%), and so for space reasons, our initial discus-\nsions will focus on the results attained via NB sub-\njectivity detection.\n8Parameter training is driven by optimizing the performance\nof the downstream polarity classifier rather than the detector\nitself because the subjectivity dataset’s sentences come from\ndifferent reviews, and so are never proximal.\n.\n.\n.\n.\n.\n.\nsubsub\nNB NB\ns1\ns2\ns3\ns4\ns_n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\u0001\nconstruct\ngraph\ncompute\nmin. cut\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\u0001\nextract\ncreate\ns1\ns4\nm−sentence extract\n(m<=n)\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0001\u0001\u0001\n\u0001\u0001\u0001\nn−sentence review\nv2\nv3\nv1\ns t\nv n\nv1\nv2\ns\nv3\nedge crossing the cut\nv n\nt\nproximity link\nindividual subjectivity−probability linkPr\n1−Pr   (s1)Pr   (s1)\n\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\nFigure 3: Graph-cut-based creation of subjective extracts.\nEmploying Naive Bayes as a subjectivity detec-\ntor (ExtractNB) in conjunction with a Naive Bayes\ndocument-level polarity classifier achieves 86.4%\naccuracy.9 This is a clear improvement over the\n82.8% that results when no extraction is applied\n(Full review); indeed, the difference is highly sta-\ntistically significant (p < 0.01, paired t-test). With\nSVMs as the polarity classifier instead, the Full re-\nview performance rises to 87.15%, but comparison\nvia the paired t-test reveals that this is statistically\nindistinguishable from the 86.4% that is achieved by\nrunning the SVM polarity classifier on ExtractNB\ninput. (More improvements to extraction perfor-\nmance are reported later in this section.)\nThese findings indicate10 that the extracts pre-\nserve (and, in the NB polarity-classifier case, appar-\nently clarify) the sentiment information in the orig-\ninating documents, and thus are good summaries\nfrom the polarity-classification point of view. Fur-\nther support comes from a “flipping” experiment:\nif we give as input to the default polarity classifier\nan extract consisting of the sentences labeled ob-\njective, accuracy drops dramatically to 71% for NB\nand 67% for SVMs. This confirms our hypothesis\nthat sentences discarded by the subjectivity extrac-\ntion process are indeed much less indicative of sen-\ntiment polarity.\nMoreover, the subjectivity extracts are much\nmore compact than the original documents (an im-\nportant feature for a summary to have): they contain\non average only about 60% of the source reviews’\nwords. (This word preservation rate is plotted along\nthe x-axis in the graphs in Figure 5.) This prompts\nus to study how much reduction of the original doc-\numents subjectivity detectors can perform and still\n9This result and others are depicted in Figure 5; for now,\nconsider only the y-axis in those plots.\n10Recall that direct evidence is not available because the po-\nlarity dataset’s sentences lack subjectivity labels.\naccurately represent the texts’ sentiment informa-\ntion.\nWe can create subjectivity extracts of varying\nlengths by taking just the N most subjective sen-\ntences11 from the originating review. As one\nbaseline to compare against, we take the canoni-\ncal summarization standard of extracting the first\nN sentences — in general settings, authors of-\nten begin documents with an overview. We also\nconsider the last N sentences: in many docu-\nments, concluding material may be a good sum-\nmary, and www.rottentomatoes.com tends to se-\nlect “snippets” from the end of movie reviews\n(Beineke et al., 2004). Finally, as a sanity check,\nwe include results from the N least subjective sen-\ntences according to Naive Bayes.\nFigure 4 shows the polarity classifier results as\nN ranges between 1 and 40. Our first observation\nis that the NB detector provides very good “bang\nfor the buck”: with subjectivity extracts containing\nas few as 15 sentences, accuracy is quite close to\nwhat one gets if the entire review is used. In fact,\nfor the NB polarity classifier, just using the 5 most\nsubjective sentences is almost as informative as the\nFull review while containing on average only about\n22% of the source reviews’ words.\nAlso, it so happens that at N = 30, performance\nis actually slightly better than (but statistically in-\ndistinguishable from) Full review even when the\nSVM default polarity classifier is used (87.2% vs.\n87.15%).12 This suggests potentially effective ex-\ntraction alternatives other than using a fixed proba-\n11These are the N sentences assigned the highest probability\nby the basic NB detector, regardless of whether their probabil-\nities exceed 50% and so would actually be classified as subjec-\ntive by Naive Bayes. For reviews with fewer than N sentences,\nthe entire review will be returned.\n12Note that roughly half of the documents in the polarity\ndataset contain more than 30 sentences (average=32.3, standard\ndeviation 15).\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 1  5  10  15  20  25  30  35  40\nAv\ner\nag\ne \nac\ncu\nra\ncy\nN\nAccuracy for N-sentence abstracts (def =  NB)\nN most subjective sentences\nlast N sentences\nfirst N sentences\nN least subjective sentences\nFull review\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 1  5  10  15  20  25  30  35  40\nAv\ner\nag\ne \nac\ncu\nra\ncy\nN\nAccuracy for N-sentence abstracts (def = SVM)\nN most subjective sentences\nlast N sentences\nfirst N sentences\nN least subjective sentences\nFull review\nFigure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.\n 83\n 83.5\n 84\n 84.5\n 85\n 85.5\n 86\n 86.5\n 87\n 0.6  0.7  0.8  0.9  1  1.1\nAv\ner\nag\ne \nac\ncu\nra\ncy\n% of words extracted\nAccuracy for subjective abstracts (def = NB)\ndifference in accuracy \nExtractSVM+Prox\nExtractNB+Prox\nExtractNB\nExtractSVM\nnot statistically significant\nFull Review\nindicates statistically significant\nimprovement in accuracy\n 83\n 83.5\n 84\n 84.5\n 85\n 85.5\n 86\n 86.5\n 87\n 0.6  0.7  0.8  0.9  1  1.1\nAv\ner\nag\ne \nac\ncu\nra\ncy\n% of words extracted\nAccuracy for subjective abstracts (def = SVM)\ndifference in accuracy ExtractNB+Prox\nExtractSVM+Prox\nExtractSVM\nExtractNB\nnot statistically significant\nFull Review\nimprovement in accuracy\nindicates statistically significant\nFigure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.\nAlso indicated are results for some statistical significance tests.\nbility threshold (which resulted in the lower accu-\nracy of 86.4% reported above).\nFurthermore, we see in Figure 4 that the N most-\nsubjective-sentences method generally outperforms\nthe other baseline summarization methods (which\nperhaps suggests that sentiment summarization can-\nnot be treated the same as topic-based summariza-\ntion, although this conjecture would need to be veri-\nfied on other domains and data). It’s also interesting\nto observe how much better the last N sentences are\nthan the first N sentences; this may reflect a (hardly\nsurprising) tendency for movie-review authors to\nplace plot descriptions at the beginning rather than\nthe end of the text and conclude with overtly opin-\nionated statements.\n4.2 Incorporating context information\nThe previous section demonstrated the value of\nsubjectivity detection. We now examine whether\ncontext information, particularly regarding sentence\nproximity, can further improve subjectivity extrac-\ntion. As discussed in Section 2.2 and 3, con-\ntextual constraints are easily incorporated via the\nminimum-cut formalism but are not natural inputs\nfor standard Naive Bayes and SVMs.\nFigure 5 shows the effect of adding in\nproximity information. ExtractNB+Prox and\nExtractSVM+Prox are the graph-based subjectivity\ndetectors using Naive Bayes and SVMs, respec-\ntively, for the individual scores; we depict the\nbest performance achieved by a single setting of\nthe three proximity-related edge-weight parameters\nover all ten data folds13 (parameter selection was\nnot a focus of the current work). The two compar-\nisons we are most interested in are ExtractNB+Prox\nversus ExtractNB and ExtractSVM+Prox versus\n13Parameters are chosen from T ∈ {1, 2, 3}, f(d) ∈\n{1, e1−d, 1/d2}, and c ∈ [0, 1] at intervals of 0.1.\nExtractSVM.\nWe see that the context-aware graph-based sub-\njectivity detectors tend to create extracts that are\nmore informative (statistically significant so (paired\nt-test) for SVM subjectivity detectors only), al-\nthough these extracts are longer than their context-\nblind counterparts. We note that the performance\nenhancements cannot be attributed entirely to the\nmere inclusion of more sentences regardless of\nwhether they are subjective or not — one counter-\nargument is that Full review yielded substantially\nworse results for the NB default polarity classifier—\nand at any rate, the graph-derived extracts are still\nsubstantially more concise than the full texts.\nNow, while incorporating a bias for assigning\nnearby sentences to the same category into NB and\nSVM subjectivity detectors seems to require some\nnon-obvious feature engineering, we also wish\nto investigate whether our graph-based paradigm\nmakes better use of contextual constraints that can\nbe (more or less) easily encoded into the input of\nstandard classifiers. For illustrative purposes, we\nconsider paragraph-boundary information, looking\nonly at SVM subjectivity detection for simplicity’s\nsake.\nIt seems intuitively plausible that paragraph\nboundaries (an approximation to discourse bound-\naries) loosen coherence constraints between nearby\nsentences. To capture this notion for minimum-cut-\nbased classification, we can simply reduce the as-\nsociation scores for all pairs of sentences that oc-\ncur in different paragraphs by multiplying them by\na cross-paragraph-boundary weight w ∈ [0, 1]. For\nstandard classifiers, we can employ the trick of hav-\ning the detector treat paragraphs, rather than sen-\ntences, as the basic unit to be labeled. This en-\nables the standard classifier to utilize coherence be-\ntween sentences in the same paragraph; on the other\nhand, it also (probably unavoidably) poses a hard\nconstraint that all of a paragraph’s sentences get the\nsame label, which increases noise sensitivity.14 Our\nexperiments reveal the graph-cut formulation to be\nthe better approach: for both default polarity clas-\nsifiers (NB and SVM), some choice of parameters\n(including w) for ExtractSVM+Prox yields statisti-\ncally significant improvement over its paragraph-\nunit non-graph counterpart (NB: 86.4% vs. 85.2%;\nSVM: 86.15% vs. 85.45%).\n5 Conclusions\nWe examined the relation between subjectivity de-\ntection and polarity classification, showing that sub-\n14For example, in the data we used, boundaries may have\nbeen missed due to malformed html.\njectivity detection can compress reviews into much\nshorter extracts that still retain polarity information\nat a level comparable to that of the full review. In\nfact, for the Naive Bayes polarity classifier, the sub-\njectivity extracts are shown to be more effective in-\nput than the originating document, which suggests\nthat they are not only shorter, but also “cleaner” rep-\nresentations of the intended polarity.\nWe have also shown that employing the\nminimum-cut framework results in the develop-\nment of efficient algorithms for sentiment analy-\nsis. Utilizing contextual information via this frame-\nwork can lead to statistically significant improve-\nment in polarity-classification accuracy. Directions\nfor future research include developing parameter-\nselection techniques, incorporating other sources of\ncontextual cues besides sentence proximity, and in-\nvestigating other means for modeling such informa-\ntion.\nAcknowledgments\nWe thank Eric Breck, Claire Cardie, Rich Caruana,\nYejin Choi, Shimon Edelman, Thorsten Joachims,\nJon Kleinberg, Oren Kurland, Art Munson, Vincent\nNg, Fernando Pereira, Ves Stoyanov, Ramin Zabih,\nand the anonymous reviewers for helpful comments.\nThis paper is based upon work supported in part\nby the National Science Foundation under grants\nITR/IM IIS-0081334 and IIS-0329064, a Cornell\nGraduate Fellowship in Cognitive Studies, and by\nan Alfred P. Sloan Research Fellowship. Any opin-\nions, findings, and conclusions or recommendations\nexpressed above are those of the authors and do not\nnecessarily reflect the views of the National Science\nFoundation or Sloan Foundation.\nReferences\nAgrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-\nnan Srikant, and Yirong Xu. 2003. Mining news-\ngroups using networks arising from social behav-\nior. In WWW, pages 529–535.\nAhuja, Ravindra, Thomas L. Magnanti, and\nJames B. Orlin. 1993. Network Flows: Theory,\nAlgorithms, and Applications. Prentice Hall.\nBeineke, Philip, Trevor Hastie, Christopher Man-\nning, and Shivakumar Vaithyanathan. 2004.\nExploring sentiment summarization. In AAAI\nSpring Symposium on Exploring Attitude and Af-\nfect in Text: Theories and Applications (AAAI\ntech report SS-04-07).\nBlum, Avrim and Shuchi Chawla. 2001. Learning\nfrom labeled and unlabeled data using graph min-\ncuts. In Intl. Conf. on Machine Learning (ICML),\npages 19–26.\nBoykov, Yuri, Olga Veksler, and Ramin Zabih.\n1999. Fast approximate energy minimization via\ngraph cuts. In Intl. Conf. on Computer Vision\n(ICCV), pages 377–384. Journal version in IEEE\nTrans. Pattern Analysis and Machine Intelligence\n(PAMI) 23(11):1222–1239, 2001.\nCardie, Claire, Janyce Wiebe, Theresa Wilson, and\nDiane Litman. 2003. Combining low-level and\nsummary representations of opinions for multi-\nperspective question answering. In AAAI Spring\nSymposium on New Directions in Question An-\nswering, pages 20–27.\nCormen, Thomas H., Charles E. Leiserson, and\nRonald L. Rivest. 1990. Introduction to Algo-\nrithms. MIT Press.\nDas, Sanjiv and Mike Chen. 2001. Yahoo! for\nAmazon: Extracting market sentiment from stock\nmessage boards. In Asia Pacific Finance Associ-\nation Annual Conf. (APFA).\nDave, Kushal, Steve Lawrence, and David M. Pen-\nnock. 2003. Mining the peanut gallery: Opinion\nextraction and semantic classification of product\nreviews. In WWW, pages 519–528.\nDini, Luca and Giampaolo Mazzini. 2002. Opin-\nion classification through information extraction.\nIn Intl. Conf. on Data Mining Methods and\nDatabases for Engineering, Finance and Other\nFields, pages 299–310.\nDurbin, Stephen D., J. Neal Richter, and Doug\nWarner. 2003. A system for affective rating of\ntexts. In KDD Wksp. on Operational Text Classi-\nfication Systems (OTC-3).\nHatzivassiloglou, Vasileios and Kathleen Mc-\nKeown. 1997. Predicting the semantic orienta-\ntion of adjectives. In 35th ACL/8th EACL, pages\n174–181.\nJoachims, Thorsten. 2003. Transductive learning\nvia spectral graph partitioning. In Intl. Conf. on\nMachine Learning (ICML).\nLiu, Hugo, Henry Lieberman, and Ted Selker.\n2003. A model of textual affect sensing using\nreal-world knowledge. In Intelligent User Inter-\nfaces (IUI), pages 125–132.\nMontes-y-Go´mez, Manuel, Aurelio Lo´pez-Lo´pez,\nand Alexander Gelbukh. 1999. Text mining as a\nsocial thermometer. In IJCAI Wksp. on Text Min-\ning, pages 103–107.\nMorinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi,\nand Toshikazu Fukushima. 2002. Mining prod-\nuct reputations on the web. In KDD, pages 341–\n349. Industry track.\nPang, Bo, Lillian Lee, and Shivakumar\nVaithyanathan. 2002. Thumbs up? Senti-\nment classification using machine learning\ntechniques. In EMNLP, pages 79–86.\nQu, Yan, James Shanahan, and Janyce Wiebe, edi-\ntors. 2004. AAAI Spring Symposium on Explor-\ning Attitude and Affect in Text: Theories and Ap-\nplications. AAAI technical report SS-04-07.\nRiloff, Ellen and Janyce Wiebe. 2003. Learning\nextraction patterns for subjective expressions. In\nEMNLP.\nRiloff, Ellen, Janyce Wiebe, and Theresa Wilson.\n2003. Learning subjective nouns using extraction\npattern bootstrapping. In Conf. on Natural Lan-\nguage Learning (CoNLL), pages 25–32.\nSubasic, Pero and Alison Huettner. 2001. Af-\nfect analysis of text using fuzzy semantic typing.\nIEEE Trans. Fuzzy Systems, 9(4):483–496.\nTong, Richard M. 2001. An operational system for\ndetecting and tracking opinions in on-line discus-\nsion. SIGIR Wksp. on Operational Text Classifi-\ncation.\nTurney, Peter. 2002. Thumbs up or thumbs down?\nSemantic orientation applied to unsupervised\nclassification of reviews. In ACL, pages 417–424.\nWiebe, Janyce M. 1994. Tracking point of view in\nnarrative. Computational Linguistics, 20(2):233–\n287.\nYi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu,\nand Wayne Niblack. 2003. Sentiment analyzer:\nExtracting sentiments about a given topic using\nnatural language processing techniques. In IEEE\nIntl. Conf. on Data Mining (ICDM).\nYu, Hong and Vasileios Hatzivassiloglou. 2003.\nTowards answering opinion questions: Separat-\ning facts from opinions and identifying the polar-\nity of opinion sentences. In EMNLP.\n",
      "id": 944740,
      "identifiers": [
        {
          "identifier": "oai:citeseerx.psu:10.1.1.9.9144",
          "type": "OAI_ID"
        },
        {
          "identifier": "2420044",
          "type": "CORE_ID"
        },
        {
          "identifier": "cs/0409058",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:cs/0409058",
          "type": "OAI_ID"
        },
        {
          "identifier": "24682462",
          "type": "CORE_ID"
        }
      ],
      "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity\n  Summarization Based on Minimum Cuts",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:citeseerx.psu:10.1.1.9.9144",
        "oai:arxiv.org:cs/0409058"
      ],
      "publishedDate": "2004-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.9144",
        "http://arxiv.org/abs/cs/0409058"
      ],
      "updatedDate": "2020-12-24T15:49:12",
      "yearPublished": 2004,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/cs/0409058"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/944740"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Orroquia Barea, Aroa"
        },
        {
          "name": "Salles. Bernal, Soluna"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/214843773",
        "https://api.core.ac.uk/v3/outputs/323335734"
      ],
      "createdDate": "2019-07-09T14:24:16",
      "dataProviders": [
        {
          "id": 2072,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2072",
          "logo": "https://api.core.ac.uk/data-providers/2072/logo"
        },
        {
          "id": 11082,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11082",
          "logo": "https://api.core.ac.uk/data-providers/11082/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The use of linguistic resources beyond the scope of language studies, i.e. commercial purposes, has become commonplace since the availability of massive amounts of data and the development of tools to process them. An interesting focus on these materials is provided by Sentiment Analysis (SA) tools and methodologies, which attempt to identify the polarity or semantic orientation of a text, i.e., its positive, negative, or neutral value. Two main approaches have been made in this sense, one based on complex machine-learning algorithms and the other relying principally on lexical knowledge (Taboada et al., 2011). Lingmotif is an example of lexicon-based SA tool offering polarity classification and other related metrics, together with an analysis of the target segments evaluated (Moreno-Ortiz, 2017). Sentiment has been shown to be domain-specific to a large extent (Choi & Cardie, 2008) and it is therefore necessary to study and describe how sentiment is expressed not only in general language, but also in specialized domains. The availability of annotated, domain-specific corpora could greatly enhance the capacity of SA tools.\r\nFurthermore, the demand for a more fine-grained approach requires the identification of specific domain terminology, allowing the recognition of target terms associated with the polarity (Liu, 2012). Most available SA corpora are annotated at the document level, which allows systems to be trained to return the overall orientation of the text. However, more detail is necessary: what aspects exactly are being praised or criticized? This type SA is known as Aspect-Based Sentiment Analysis (ABSA), and attempts to extract more fined-grained knowledge. ABSA has attracted the attention of recent SemEval shared-tasks (Pontiki et al., 2015)",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/214843773.pdf",
      "fieldOfStudy": null,
      "fullText": "SentiTur: Building Linguistic Resources for Aspect-Based Sentiment Analysis in the Tourism Sector  Soluna Salles and Aroa Orrequia-Barea   The use of linguistic resources beyond the scope of language studies, i.e. commercial purposes, has become commonplace since the availability of massive amounts of data and the development of tools to process them.  An interesting focus on these materials is provided by Sentiment Analysis (SA) tools and methodologies, which attempt to identify the polarity or semantic orientation of a text, i.e., its positive, negative, or neutral value. Two main approaches have been made in this sense, one based on complex machine-learning algorithms and the other relying principally on lexical knowledge (Taboada et al., 2011). Lingmotif is an example of lexicon-based SA tool offering polarity classification and other related metrics, together with an analysis of the target segments evaluated (Moreno-Ortiz, 2017). Sentiment has been shown to be domain-specific to a large extent (Choi & Cardie, 2008) and it is therefore necessary to study and describe how sentiment is expressed not only in general language, but also in specialized domains. The availability of annotated, domain-specific corpora could greatly enhance the capacity of SA tools.   Furthermore, the demand for a more fine-grained approach requires the identification of specific domain terminology, allowing the recognition of target terms associated with the polarity (Liu, 2012). Most available SA corpora are annotated at the document level, which allows systems to be trained to return the overall orientation of the text. However, more detail is necessary: what aspects exactly are being praised or criticized? This type SA is known as Aspect-Based Sentiment Analysis (ABSA), and attempts to extract more fined-grained knowledge. ABSA has attracted the attention of recent SemEval shared-tasks  (Pontiki et al., 2015).  We propose the creation of the SentiTur corpus, a bilingual (Spanish-English), aspect-annotated corpus of user reviews covering three sectors of the tourism industry: accommodation, catering, and car rental. Reviews obtained from online platforms (Tripavisor and Booking, among others) are being annotated according to an annotation schema by means of the collaborative annotation tool Brat (Stenetorp et al., 2012). Five annotators work initially in a preliminary dataset, so as to validate the schema and the methodology proposed. Inter annotator agreement (IAA) is computed measuring the pairwise agreement among annotators by the Kappa coefficient (K) (Siegel, 1988). These results are used to revise and adjust the annotation schema before it is employed to annotated the general corpus. Finally, the corpus is offered in standard XML format suitable as input of Sentiment Analysis tools.  Key words: SentiTur, Aspect-based Sentiment Analysis, Sentiment Analysis, tourism.  References  Choi, Y., & Cardie, C. (2008). Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis, (October), 793–801. Liu, B. (2012). Sentiment Aanalysis and Opinion Mining: Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers. Moreno-Ortiz, A. (2017). Lingmotif: Sentiment Analysis for the Digital Humanities. In Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017 (pp. 73–76). Pontiki, M., Galanis, D., & Papageorgiou, H. (2015). SemEval-2015 Task 12: Aspect-Based Sentiment Analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado, June 4-5, 2015. Siegel, S. (1988). Nonparametric Statistics for the Behavioral Science. McGraw-Hill. Stenetorp, P., Pyysalo, S., Topi, G., Ohta, T., Ananiadou, S., & Tsujii, J. (2012). BRAT: a Web-based Tool for NLP-Assisted Text Annotation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, Avignon, France, April 23-27 2012. (pp. 102–107). Taboada, M., Brooke, J., & Voll, K. (2011). Lexicon-Based Methods for Sentiment Analysis, (September 2010).   ",
      "id": 7649256,
      "identifiers": [
        {
          "identifier": "oai:riuma.uma.es:10630/15645",
          "type": "OAI_ID"
        },
        {
          "identifier": "214843773",
          "type": "CORE_ID"
        },
        {
          "identifier": "323335734",
          "type": "CORE_ID"
        }
      ],
      "title": "SentiTur: Building Linguistic Resources for Aspect-Based Sentiment Analysis in the Tourism Sector",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:riuma.uma.es:10630/15645"
      ],
      "publishedDate": "2018-04-30T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://riuma.uma.es/xmlui/bitstream/10630/15645/1/Salles_Aesla_2018.pdf"
      ],
      "updatedDate": "2022-03-20T00:45:59",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/214843773.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/214843773"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/214843773/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/214843773/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/7649256"
        }
      ]
    }
  ],
  "searchId": "e5ba88327b593e72b250122e40ab319c"
}