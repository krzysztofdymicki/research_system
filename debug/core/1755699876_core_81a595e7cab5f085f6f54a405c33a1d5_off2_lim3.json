{
  "totalHits": 535056,
  "limit": 3,
  "offset": 2,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": "1509.06041",
      "authors": [
        {
          "name": "Jin, Hailin"
        },
        {
          "name": "Luo, Jiebo"
        },
        {
          "name": "Yang, Jianchao"
        },
        {
          "name": "You, Quanzeng"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/386117133",
        "https://api.core.ac.uk/v3/outputs/103268817"
      ],
      "createdDate": "2016-08-03T02:20:56",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms.Comment: 9 pages, 5 figures, AAAI 201",
      "documentType": "research",
      "doi": "10.1609/aaai.v29i1.9179",
      "downloadUrl": "http://arxiv.org/abs/1509.06041",
      "fieldOfStudy": null,
      "fullText": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\nTransferred Deep Networks\nQuanzeng You and Jiebo Luo\nDepartment of Computer Science\nUniversity of Rochester\nRochester, NY 14623\n{qyou, jluo}@cs.rochester.edu\nHailin Jin and Jianchao Yang\nAdobe Research\n345 Park Avenue\nSan Jose, CA 95110\n{hljin, jiayang}@adobe.com\nAbstract\nSentiment analysis of online user generated content is\nimportant for many social media analytics tasks. Re-\nsearchers have largely relied on textual sentiment anal-\nysis to develop systems to predict political elections,\nmeasure economic indicators, and so on. Recently, so-\ncial media users are increasingly using images and\nvideos to express their opinions and share their expe-\nriences. Sentiment analysis of such large scale visual\ncontent can help better extract user sentiments toward\nevents or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is comple-\nmentary to textual sentiment analysis. Motivated by the\nneeds in leveraging large scale yet noisy training data to\nsolve the extremely challenging problem of image sen-\ntiment analysis, we employ Convolutional Neural Net-\nworks (CNN). We first design a suitable CNN archi-\ntecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment\nalgorithm to label Flickr images. To make use of such\nnoisy machine labeled data, we employ a progressive\nstrategy to fine-tune the deep network. Furthermore, we\nimprove the performance on Twitter images by induc-\ning domain transfer with a small number of manually\nlabeled Twitter images. We have conducted extensive\nexperiments on manually labeled Twitter images. The\nresults show that the proposed CNN can achieve better\nperformance in image sentiment analysis than compet-\ning algorithms.\nIntroduction\nOnline social networks are providing more and more con-\nvenient services to their users. Today, social networks have\ngrown to be one of the most important sources for people to\nacquire information on all aspects of their lives. Meanwhile,\nevery online social network user is a contributor to such\nlarge amounts of information. Online users love to share\ntheir experiences and to express their opinions on virtually\nall events and subjects.\nAmong the large amount of online user generated data, we\nare particularly interested in people’s opinions or sentiments\ntowards specific topics and events. There have been many\nCopyright c© 2015, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Examples of Flickr images related to the 2012\nUnited States presidential election.\nworks on using online users’ sentiments to predict box-\noffice revenues for movies (Asur and Huberman 2010), po-\nlitical elections (O’Connor et al. 2010; Tumasjan et al. 2010)\nand economic indicators (Bollen, Mao, and Zeng 2011;\nZhang, Fuehres, and Gloor 2011). These works have sug-\ngested that online users’ opinions or sentiments are closely\ncorrelated with our real-world activities. All of these results\nhinge on accurate estimation of people’s sentiments accord-\ning to their online generated content. Currently all of these\nworks only rely on sentiment analysis from textual content.\nHowever, multimedia content, including images and videos,\nhas become prevalent over all online social networks. In-\ndeed, online social network providers are competing with\neach other by providing easier access to their increasingly\npowerful and diverse services. Figure 1 shows example im-\nages related to the 2012 United States presidential election.\nClearly, images in the top and bottom rows convey opposite\nsentiments towards the two candidates.\nA picture is worth a thousand words. People with differ-\nent backgrounds can easily understand the main content of\nan image or video. Apart from the large amount of easily\navailable visual content, today’s computational infrastruc-\nture is also much cheaper and more powerful to make the\nanalysis of computationally intensive visual content analy-\nsis feasible. In this era of big data, it has been shown that\nthe integration of visual content can provide us more reli-\nable or complementary online social signals (Jin et al. 2010;\nYuan et al. 2013).\nTo the best of our knowledge, little attention has been paid\nto the sentiment analysis of visual content. Only a few recent\nworks attempted to predict visual sentiment using features\nar\nX\niv\n:1\n50\n9.\n06\n04\n1v\n1 \n [c\ns.C\nV]\n  2\n0 S\nep\n 20\n15\nfrom images (Siersdorfer et al. 2010; Borth et al. 2013b;\nBorth et al. 2013a; Yuan et al. 2013) and videos (Morency,\nMihalcea, and Doshi 2011). Visual sentiment analysis is ex-\ntremely challenging. First, image sentiment analysis is in-\nherently more challenging than object recognition as the lat-\nter is usually well defined. Image sentiment involves a much\nhigher level of abstraction and subjectivity in the human\nrecognition process (Joshi et al. 2011), on top of a wide vari-\nety of visual recognition tasks including object, scene, action\nand event recognition. In order to use supervised learning, it\nis imperative to collect a large and diverse labeled training\nset perhaps on the order of millions of images. This is an\nalmost insurmountable hurdle due to the tremendous labor\nrequired for image labeling. Second, the learning schemes\nneed to have high generalizability to cover more different\ndomains. However, the existing works use either pixel-level\nfeatures or a limited number of predefined attribute features,\nwhich is difficult to adapt the trained models to images from\na different domain.\nThe deep learning framework enables robust and accurate\nfeature learning, which in turn produces the state-of-the-art\nperformance on digit recognition (LeCun et al. 1989; Hin-\nton, Osindero, and Teh 2006), image classification (Cires¸an\net al. 2011; Krizhevsky, Sutskever, and Hinton 2012), mu-\nsical signal processing (Hamel and Eck 2010) and natural\nlanguage processing (Maas et al. 2011). Both the academia\nand industry have invested a huge amount of effort in build-\ning powerful neural networks. These works suggested that\ndeep learning is very effective in learning robust features in a\nsupervised or unsupervised fashion. Even though deep neu-\nral networks may be trapped in local optima (Hinton 2010;\nBengio 2012), using different optimization techniques, one\ncan achieve the state-of-the-art performance on many chal-\nlenging tasks mentioned above.\nInspired by the recent successes of deep learning, we are\ninterested in solving the challenging visual sentiment anal-\nysis task using deep learning algorithms. For images related\ntasks, Convolutional Neural Network (CNN) are widely\nused due to the usage of convolutional layers. It takes into\nconsideration the locations and neighbors of image pixels,\nwhich are important to capture useful features for visual\ntasks. Convolutional Neural Networks (LeCun et al. 1998;\nCires¸an et al. 2011; Krizhevsky, Sutskever, and Hinton\n2012) have been proved very powerful in solving computer\nvision related tasks. We intend to find out whether applying\nCNN to visual sentiment analysis provides advantages over\nusing a predefined collection of low-level visual features or\nvisual attributes, which have been done in prior works.\nTo that end, we address in this work two major challenges:\n1) how to learn with large scale weakly labeled training\ndata, and 2) how to generalize and extend the learned model\nacross domains. In particular, we make the following contri-\nbutions.\n• We develop an effective deep convolutional network ar-\nchitecture for visual sentiment analysis. Our architecture\nemploys two convolutional layers and several fully con-\nnected layers for the prediction of visual sentiment labels.\n• Our model attempts to address the weakly labeled nature\nof the training image data, where such labels are machine\ngenerated, by leveraging a progressive training strategy\nand a domain transfer strategy to fine-tune the neural net-\nwork. Our evaluation results suggest that this strategy is\neffective for improving the performance of neural net-\nwork in terms of generalizability.\n• In order to evaluate our model as well as competing algo-\nrithms, we build a large manually labeled visual sentiment\ndataset using Amazon Mechanical Turk. This dataset will\nbe released to the research community to promote further\ninvestigations on visual sentiment.\nRelated Work\nIn this section, we review literature closely related to our\nstudy on visual sentiment analysis, particularly in sentiment\nanalysis and Convolutional Neural Networks.\nSentiment Analysis\nSentiment analysis is a very challenging task (Liu et al.\n2003; Li et al. 2010). Researchers from natural language\nprocessing and information retrieval have developed differ-\nent approaches to solve this problem, achieving promising\nor satisfying results (Pang and Lee 2008). In the context of\nsocial media, there are several additional unique challenges.\nFirst, there are huge amounts of data available. Second, mes-\nsages on social networks are by nature informal and short.\nThird, people use not only textual messages, but also images\nand videos to express themselves.\nTumasjan et al. (2010) and Bollen et al. (2011) employed\npre-defined dictionaries for measuring the sentiment level\nof Tweets. The volume or percentage of sentiment-bearing\nwords can produce an estimate of the sentiment of one par-\nticular tweet. Davidov et al. (2010) used the weak labels\nfrom a large amount of Tweets. In contrast, they manually\nselected hashtags with strong positive and negative senti-\nments and ASCII smileys are also utilized to label the sen-\ntiments of tweets. Furthermore, Hu et al. (2013) incorpo-\nrated social signals into their unsupervised sentiment anal-\nysis framework. They defined and integrated both emotion\nindication and correlation into a framework to learn param-\neters for their sentiment classifier.\nThere are also several recent works on visual sentiment\nanalysis. Siersdorfer et al. (2010) proposes a machine learn-\ning algorithm to predict the sentiment of images using pixel-\nlevel features. Motivated by the fact that sentiment involves\nhigh-level abstraction, which may be easier to explain by\nobjects or attributes in images, both (Borth et al. 2013a)\nand (Yuan et al. 2013) propose to employ visual entities or\nattributes as features for visual sentiment analysis. In (Borth\net al. 2013a), 1200 adjective noun pairs (ANP), which may\ncorrespond to different levels of different emotions, are ex-\ntracted. These ANPs are used as queries to crawl images\nfrom Flickr. Next, pixel-level features of images in each\nANP are employed to train 1200 ANP detectors. The re-\nsponses of these 1200 classifiers can then be considered as\nmid-level features for visual sentiment analysis. The work\nin (Yuan et al. 2013) employed a similar mechanism. The\nmain difference is that 102 scene attributes are used instead.\n256\n256\n227\n227\n3\n3\n227\n227\n11\n11\n5\n5\n96\n55\n55\n256\n27\n27\n512 512\n24\n2\nFigure 2: Convolutional Neural Network for Visual Sentiment Analysis.\nConvolutional Neural Networks\nConvolutional Neural Networks (CNN) have been very suc-\ncessful in document recognition (LeCun et al. 1998). CNN\ntypically consists of several convolutional layers and sev-\neral fully connected layers. Between the convolutional lay-\ners, there may also be pooling layers and normalization lay-\ners. CNN is a supervised learning algorithm, where parame-\nters of different layers are learned through back-propagation.\nDue to the computational complexity of CNN, it has only be\napplied to relatively small images in the literature. Recently,\nthanks to the increasing computational power of GPU, it is\nnow possible to train a deep convolutional neural network on\na large scale image dataset (Krizhevsky, Sutskever, and Hin-\nton 2012). Indeed, in the past several years, CNN has been\nsuccessfully applied to scene parsing (Grangier, Bottou, and\nCollobert 2009), feature learning (LeCun, Kavukcuoglu, and\nFarabet 2010), visual recognition (Kavukcuoglu et al. 2010)\nand image classification (Krizhevsky, Sutskever, and Hinton\n2012). In our work, we intend to use CNN to learn features\nwhich are useful for visual sentiment analysis.\nVisual Sentiment Analysis\nWe propose to develop a suitable convolutional neural net-\nwork architecture for visual sentiment analysis. Moreover,\nwe employ a progressive training strategy that leverages the\ntraining results of convolutional neural network to further\nfilter out (noisy) training data. The details of the proposed\nframework will be described in the following sections.\nVisual Sentiment Analysis with regular CNN\nCNN has been proven to be effective in image classifica-\ntion tasks, e.g., achieving the state-of-the-art performance\nin ImageNet Challenge (Krizhevsky, Sutskever, and Hin-\nton 2012). Visual sentiment analysis can also be treated\nas an image classification problem. It may seem to be a\nmuch easier problem than image classification from Ima-\ngeNet (2 classes vs. 1000 classes in ImageNet). However,\nvisual sentiment analysis is quite challenging because senti-\nments or opinions correspond to high level abstractions from\na given image. This type of high level abstraction may re-\nquire viewer’s knowledge beyond the image content itself.\nMeanwhile, images in the same class of ImageNet mainly\ncontain the same type of object. In sentiment analysis, each\nclass contains much more diverse images. It is therefore ex-\ntremely challenging to discover features which can distin-\nguish much more diverse classes from each other. In addi-\ntion, people may have totally different sentiments over the\nsame image. This adds difficulties to not only our classifi-\ncation task, but also the acquisition of labeled images. In\nother words, it is nontrivial to obtain highly reliable labeled\ninstances, let alone a large number of them. Therefore, we\nneed a supervised learning engine that is able to tolerate a\nsignificant level of noise in the training dataset.\nThe architecture of the CNN we employ for sentiment\nanalysis is shown in Figure 2. Each image is resized to\n256 × 256 (if needed, we employ center crop, which first\nresizes the shorter dimension to 256 and then crops the mid-\ndle section of the resized image). The resized images are\nprocessed by two convolutional layers. Each convolutional\nlayer is also followed by max-pooling layers and normaliza-\ntion layers. The first convolutional layer has 96 kernels of\nsize 11 × 11 × 3 with a stride of 4 pixels. The second con-\nvolutional layer has 256 kernels of size 5 × 5 with a stride\nof 2 pixels. Furthermore, we have four fully connected lay-\ners. Inspired by (C¸aglar Gu¨lc¸ehre et al. 2013), we constrain\nthe second to last fully connected layer to have 24 neurons.\nAccording to the Plutchik’s wheel of emotions (Plutchik\n1984), there are a total of 24 emotions belonging to two cate-\ngories: positive emotions and negative emotions. Intuitively.\nwe hope these 24 nodes may help the network to learn the 24\nemotions from a given image and then classify each image\ninto positive or negative class according to the responses of\nthese 24 emotions.\nThe last layer is designed to learn the parameter w by\nmaximizing the following conditional log likelihood func-\ntion (xi and yi are the feature vector and label for the i-th\ninstance respectively):\nl(w) =\nn∑\ni=1\nln p(yi = 1|xi, w) + (1− yi) ln p(yi = 0|xi, w)\n(1)\nwhere\np(yi|xi, w) =\nexp(w0 +\n∑k\nj=1 wjxij)\nyi\n1 + exp(w0 +\n∑k\nj=1 wjxij)\nyi\n(2)\n... ...\n... f(·)PredictCNN\nPCNN\n1) Input\nTrain convolutional Neural Network\n2) CNN model\n3) 4) Sampling\n5) Fine-tune\n6) PCNN model\nFigure 3: Progressive CNN (PCNN) for visual sentiment analysis.\nVisual Sentiment Analysis with Progressive CNN\nSince the images are weakly labeled, it is possible that the\nneural network can get stuck in a bad local optimum. This\nmay lead to poor generalizability of the trained neural net-\nwork. On the other hand, we found that the neural network\nis still able to correctly classify a large proportion of the\ntraining instances. In other words, the neural network has\nlearned knowledge to distinguish the training instances with\nrelatively distinct sentiment labels. Therefore, we propose to\nprogressively select a subset of the training instances to re-\nduce the impact of noisy training instances. Figure 3 shows\nthe overall flow of the proposed progressive CNN (PCNN).\nWe first train a CNN on Flickr images. Next, we select train-\ning samples according to the prediction score of the trained\nmodel on the training data itself. Instead of training from the\nbeginning, we further fine-tune the trained model using these\nnewly selected, and potentially cleaner training instances.\nThis fine-tuned model will be our final model for visual sen-\ntiment analysis.\nAlgorithm 1 Progressive CNN training for Visual Sentiment\nAnalysis\nInput: X = {x1, x2, . . . , xn} a set of images of size 256×\n256\nY = {y1, y2, . . . , yn} sentiment labels of X\n1: Train convolutional neural network CNN with input X\nand Y\n2: Let S ∈ Rn×2 be the sentiment scores of X predicted\nusing CNN\n3: for si ∈ S do\n4: Delete xi from X with probability pi (Eqn.(3))\n5: end for\n6: Let X ′ ⊂ X be the remaining training images, Y ′ be\ntheir sentiment labels\n7: Fine-tune CNN with input X ′ and Y ′ to get PCNN\n8: return PCNN\nIn particular, we employ a probabilistic sampling algo-\nrithm to select the new training subset. The intuition is that\nwe want to keep instances with distinct sentiment scores\nbetween the two classes with a high probability, and con-\nversely remove instances with similar sentiment scores for\nboth classes with a high probability. Let si = (si1, si2) be\nthe prediction sentiment scores for the two classes of in-\nstance i. We choose to remove the training instance i with\nprobability pi given by Eqn.(3). Algorithm 1 summarizes the\nsteps of the proposed framework.\npi = max (0, 2− exp(|si1 − si2|)) (3)\nWhen the difference between the predicted sentiment scores\nof one training instance are large enough, this training in-\nstance will be kept in the training set. Otherwise, the smaller\nthe difference between the predicted sentiment scores be-\ncome, the larger the probability of this instance being re-\nmoved from the training set.\nExperiments\nWe choose to use the same half million Flickr images\nfrom SentiBank1 to train our Convolutional Neural Network.\nThese images are only weakly labeled since each image be-\nlongs to one adjective noun pair (ANP). There are a total\nof 1200 ANPs. According to the Plutchik’s Wheel of Emo-\ntions (Plutchik 1984), each ANP is generated by the combi-\nnation of adjectives with strong sentiment values and nouns\nfrom tags of images and videos (Borth et al. 2013b). These\nANPs are then used as queries to collect related images\nfor each ANP. The released SentiBank contains 1200 ANPs\nwith about half million Flickr images. We train our convolu-\ntional neural network mainly on this image dataset. We im-\nplement the proposed architecture of CNN on the publicly\navailable implementation Caffe (Jia 2013). All of our exper-\niments are evaluated on a Linux X86 64 machine with 32G\nRAM and two NVIDIA GTX Titan GPUs.\nComparisons of different CNN architectures\nThe architecture of our model is shown in Figure 2. How-\never, we also evaluate other architectures for the visual sen-\ntiment analysis task. Table 1 summarizes the performance\nof different architectures on a randomly chosen Flickr test-\ning dataset. In Table 1, iCONV-jFC indicates that there are\n1http://visual-sentiment-ontology.appspot.com/\ni convolutional layers and j fully connected layers in the ar-\nchitecture. The model in Figure 2 shows slightly better per-\nformance than other models in terms of F1 and accuracy. In\nthe following experiments, we mainly focus on the evalua-\ntion of CNN using the architecture in Figure 2.\nTable 1: Summary of performance of different architectures\non randomly chosen testing data.\nArchitecture Precision Recall F1 Accuracy\n3CONV-4FC 0.679 0.845 0.753 0.644\n3CONV-2FC 0.69 0.847 0.76 0.657\n2CONV-3FC 0.679 0.874 0.765 0.654\n2CONV-4FC 0.688 0.875 0.77 0.665\nBaselines\nWe compare the performance of PCNN with three other\nbaselines or competing algorithms for image sentiment clas-\nsification.\nLow-level Feature-based Siersdorfer et al. (2010) defined\nboth global and local visual features. Specifically, the global\ncolor histograms (GCH) features consist of 64-bin RGB his-\ntogram. The local color histogram features (LCH) first di-\nvided the image into 16 blocks and used the 64-bin RGB\nhistogram for each block. They also employed SIFT features\nto learn a visual word dictionary. Next, they defined bag of\nvisual word features (BoW) for each image.\nMid-level Feature-based Damian et al. (2013a; 2013b)\nproposed a framework to build visual sentiment ontology\nand SentiBank according to the previously discussed 1200\nANPs. With the trained 1200 ANP detectors, they are able\nto generate 1200 responses for any given test image using\nthese pre-trained 1200 ANP detectors. A sentiment classifier\nis built on top of these mid-level features according to the\nsentiment label of training images. Sentribute (Yuan et al.\n2013) also employed mid-level features for sentiment pre-\ndiction. However, instead of using adjective noun pairs, they\nemployed scene-based attributes (Patterson and Hays 2012)\nto define the mid-level features.\nDeep Learning on Flickr Dataset\nWe randomly choose 90% images from the half million\nFlickr images as our training dataset. The remaining 10%\nimages are our testing dataset. We train the convolutional\nneural network with 300,000 iterations of mini-batches\n(each mini-batch contains 256 images). We employ the sam-\npling probability in Eqn.(3) to filter the training images ac-\ncording to the prediction score of CNN on its training data.\nIn the fine-tuning stage of PCNN, we run another 100,000\niterations of mini-batches using the filtered training dataset.\nTable 2 gives a summary of the number of data instances in\nour experiments. Figure 4 shows the filters learned in the\nfirst convolutional layer of CNN and PCNN, respectively.\nThere are some differences between 4(a) and 4(b). While\nit is somewhat inconclusive that the neural networks have\nreached a better local optimum, at least we can conclude that\nthe fine-tuning stage using a progressively cleaner training\nTable 2: Statistics of the number of Flickr image dataset.\nModels training testing # of iterations\nCNN 401,739 44,637 300,000\nPCNN 369,828 44,637 100,000\nTable 3: Performance on the Testing Dataset by CNN and\nPCNN.\nAlgorithm Precision Recall F1 Accuracy\nCNN 0.714 0.729 0.722 0.718\nPCNN 0.759 0.826 0.791 0.781\ndataset has prompted the neural networks to learn different\nknowledge. Indeed, the evaluation results suggest that this\nfine-tuning leads to the improvement of performance.\nTable 3 shows the performance of both CNN and PCNN\non the 10% randomly chosen testing data. PCNN outper-\nformed CNN in terms of Precision, Recall, F1 and Accu-\nracy. The results in Table 3 and the filters from Figure 4\nshows that the fine-tuning stage of PCNN can help the neu-\nral network to search for a better local optimum.\n(a) Filters learned from CNN\n(b) Filters learned from PCNN\nFigure 4: Filters of the first convolutional layer.\nTwitter Testing Dataset\nWe also built a new image dataset from image tweets. Im-\nage tweets refer to those tweets that contain images. We\nbuilt a total of 1269 images as our candidate testing im-\nages. We employed crowd intelligence, Amazon Mechani-\ncal Turk (AMT), to generate sentiment labels for these test-\ning images, in a similar fashion to (Borth et al. 2013b). We\nrecruited 5 AMT workers for each of the candidate image.\nTable 4 shows the statistics of the labeling results from the\nAmazon Mechanical Turk. In the table, “five agree” indi-\ncates that all the 5 AMT workers gave the same sentiment\nlabel for a given image. Only a small portion of the images,\n153 out of 1269, had significant disagreements between the\nTable 5: Performance of different algorithms on the Twitter image dataset (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc\nCNN 0.749 0.869 0.805 0.722 0.707 0.839 0.768 0.686 0.691 0.814 0.747 0.667\nPCNN 0.77 0.878 0.821 0.747 0.733 0.845 0.785 0.714 0.714 0.806 0.757 0.687\nTable 4: Summary of AMT labeled results for the Twitter\ntesting dataset.\nSentiment Five Agree At Least FourAgree\nAt Least\nThree Agree\nPositive 581 689 769\nNegative 301 427 500\nSum 882 1116 1269\n5 workers (3 vs. 2). We evaluate the performance of Con-\nvolutional Neural Networks on this manually labeled image\ndataset according to the model trained on Flickr images. Ta-\nble 5 shows the performance of the two frameworks. Not\nsurprisingly, both models perform better on the less ambigu-\nous image set (“five agree” by AMT). Meanwhile, PCNN\nshows better performance than CNN on all the three label-\ning sets in terms of both F1 and accuracy. This suggests that\nthe fine-tuning stage of CNN effectively improves the gen-\neralizability extensibility of the neural networks.\nTransfer Learning\nHalf million Flickr images are used in our CNN training.\nThe features learned are generic features on these half mil-\nlion images. Table 5 shows that these generic features also\nhave the ability to predict visual sentiment of images from\nother domains. The question we ask is whether we can fur-\nther improve the performance of visual sentiment analysis\non Twitter images by inducing transfer learning. In this sec-\ntion, we conduct experiments to answer this question.\nThe users of Flickr are more likely to spend more time\non taking high quality pictures. Twitter users are likely to\nshare the moment with the world. Thus, most of the Twitter\nimages are casually taken snapshots. Meanwhile, most of the\nimages are related to current trending topics and personal\nexperiences, making the images on Twitter much diverse in\ncontent as well as quality.\nIn this experiment, we fine-tune the pre-trained neural net-\nwork model in the following way to achieve transfer learn-\ning. We randomly divide the Twitter images into 5 equal par-\ntitions. Every time, we use 4 of the 5 partitions to fine-tune\nour pre-trained model from the half million Flickr images\nand evaluate the new model on the remaining partition. The\naveraged evaluation results are reported. The algorithm is\ndetailed in Algorithm 2.\nSimilar to (Borth et al. 2013b), we also employ 5-fold\ncross-validation to evaluate the performance of all the base-\nline algorithms. Table 6 summarizes the averaged perfor-\nmance results of different baseline algorithms and our two\nCNN models. Overall, both CNN models outperform the\nbaseline algorithms. In the baseline algorithms, Sentribute\ngives slightly better results than the other two baseline al-\nFigure 5: Positive (top block) and Negative (bottom block)\nexamples. Each column shows the negative example im-\nages for each algorithm (PCNN, CNN, Sentribute, Sen-\ntibank, GCH, LCH, GCH+BoW, LCH+BoW). The images\nare ranked by the prediction score from top to bottom in a\ndecreasing order.\nAlgorithm 2 Transfer Learning to fine-tune CNN\nInput: X = {x1, x2, . . . , xn} a set of images of size 256×\n256\nY = {y1, y2, . . . , yn} sentiment labels of X\nPre-trained CNN model M\n1: Randomly partition X and Y into 5 equal groups\n{(X1, Y1), . . . , (X5, Y5)}.\n2: for i from 1 to 5 do\n3: Let (X ′, Y ′) = (X,Y )− (Xi, Yi)\n4: Fine-tune M with input (X ′, Y ′) to obtain model Mi\n5: Evaluate the performance of Mi on (Xi, Yi)\n6: end for\n7: return The averaged performance of Mi on (Xi, Yi) (i\nfrom 1 to 5)\ngorithms. Interestingly, even the combination of using low-\nTable 6: 5-Fold Cross-Validation Performance of different algorithms on the Twitter image dataset. Note that compared with\nTable 5, both fine-tuned CNN models have been improved due to domain transfer learning (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc\nGCH 0.708 0.888 0.787 0.684 0.687 0.84 0.756 0.665 0.678 0.836 0.749 0.66\nLCH 0.764 0.809 0.786 0.71 0.725 0.753 0.739 0.671 0.716 0.737 0.726 0.664\nGCH + BoW 0.724 0.904 0.804 0.71 0.703 0.849 0.769 0.685 0.683 0.835 0.751 0.665\nLCH + BoW 0.771 0.811 0.79 0.717 0.751 0.762 0.756 0.697 0.722 0.726 0.723 0.664\nSentiBank 0.785 0.768 0.776 0.709 0.742 0.727 0.734 0.675 0.720 0.723 0.721 0.662\nSentribute 0.789 0.823 0.805 0.738 0.75 0.792 0.771 0.709 0.733 0.783 0.757 0.696\nCNN 0.795 0.905 0.846 0.783 0.773 0.855 0.811 0.755 0.734 0.832 0.779 0.715\nPCNN 0.797 0.881 0.836 0.773 0.786 0.842 0.811 0.759 0.755 0.805 0.778 0.723\nlevel features local color histogram (LCH) and bag of visual\nwords (BoW) shows better results than SentiBank on our\nTwitter dataset. Both fine-tuned CNN models have been im-\nproved. This improvement is significant given that we only\nuse four fifth of the 1269 images for domain adaptation.\nBoth neural network models have similar performance on\nall the three sets of the Twitter testing data. This suggests\nthat the fine-tuning stage helps both models to find a better\nlocal minimum. In particular, the knowledge from the Twit-\nter images starts to determine the performance of both neural\nnetworks. The previously trained model only determines the\nstart position of the fine-tuned model.\nMeanwhile, for each model, we respectively select the top\n5 positive and top 5 negative examples from the 1269 Twit-\nter images according to the evaluation scores. Figure show\nthose examples for each model. In both figures, each column\ncontains the images for one model. A green solid box means\nthe prediction label of the image agrees with the human la-\nbel. Otherwise, we use a red dashed box. The labels of top\nranked images in both neural network models are all cor-\nrectly predicted. However, the images are not all the same.\nThis on the other hand suggests that even though the two\nmodels achieve similar results after fine-tuning, they may\nhave arrived at somewhat different local optima due to the\ndifferent starting positions, as well as the transfer learning\nprocess. For all the baseline models, it is difficult to say\nwhich kind of images are more likely to be correctly clas-\nsified according to these images. However, we observe that\nthere are several mistakenly classified images in common\namong the models using low-level features (the four right-\nmost columns in Figure ). Similarly, for Sentibank and Sen-\ntribute, several of the same images are also in the top ranked\nsamples. This indicates that there are some common learned\nknowledge in the low-level feature models and mid-level\nfeature models.\nConclusions\nVisual sentiment analysis is a challenging and interesting\nproblem. In this paper, we adopt the recent developed con-\nvolutional neural networks to solve this problem. We have\ndesigned a new architecture, as well as new training strate-\ngies to overcome the noisy nature of the large-scale train-\ning samples. Both progressive training and transfer learning\ninducted by a small number of confidently labeled images\nfrom the target domain have yielded notable improvements.\nThe experimental results suggest that convolutional neural\nnetworks that are properly trained can outperform both clas-\nsifiers that use predefined low-level features or mid-level vi-\nsual attributes for the highly challenging problem of visual\nsentiment analysis. Meanwhile, the main advantage of us-\ning convolutional neural networks is that we can transfer\nthe knowledge to other domains using a much simpler fine-\ntuning technique than those in the literature e.g., (Duan et al.\n2012).\nIt is important to reiterate the significance of this work\nover the state-of-the-art (Siersdorfer et al. 2010; Borth et al.\n2013b; Yuan et al. 2013). We are able to directly leverage\na much larger weakly labeled data set for training, as well\nas a larger manually labeled dataset for testing. The larger\ndata sets, along with the proposed deep CNN and its training\nstrategies, give rise to better generalizability of the trained\nmodel and higher confidence of such generalizability. In the\nfuture, we plan to develop robust multimodality models that\nemploy both the textual and visual content for social me-\ndia sentiment analysis. We also hope our sentiment analysis\nresults can encourage further research on online user gener-\nated content.\nWe believe that sentiment analysis on large scale online\nuser generated content is quite useful since it can provide\nmore robust signals and information for many data analytics\ntasks, such as using social media for prediction and forecast-\ning. In the future, we plan to develop robust multimodal-\nity models that employ both the textual and visual content\nfor social media sentiment analysis. We also hope our senti-\nment analysis results can encourage further research on on-\nline user generated content.\nAcknowledgments\nThis work was generously supported in part by Adobe Re-\nsearch. We would like to thank Digital Video and Multime-\ndia (DVMM) Lab at Columbia University for providing the\nhalf million Flickr images and their machine-generated la-\nbels.\nReferences\n[Asur and Huberman 2010] Asur, S., and Huberman, B. A.\n2010. Predicting the future with social media. In WI-IAT,\nvolume 1, 492–499. IEEE.\n[Bengio 2012] Bengio, Y. 2012. Practical recommendations\nfor gradient-based training of deep architectures. In Neural\nNetworks: Tricks of the Trade. Springer. 437–478.\n[Bollen, Mao, and Pepe 2011] Bollen, J.; Mao, H.; and Pepe,\nA. 2011. Modeling public mood and emotion: Twitter sen-\ntiment and socio-economic phenomena. In ICWSM.\n[Bollen, Mao, and Zeng 2011] Bollen, J.; Mao, H.; and\nZeng, X. 2011. Twitter mood predicts the stock market.\nJournal of Computational Science 2(1):1–8.\n[Borth et al. 2013a] Borth, D.; Chen, T.; Ji, R.; and Chang,\nS.-F. 2013a. Sentibank: large-scale ontology and classifiers\nfor detecting sentiment and emotions in visual content. In\nACM MM, 459–460. ACM.\n[Borth et al. 2013b] Borth, D.; Ji, R.; Chen, T.; Breuel, T.;\nand Chang, S.-F. 2013b. Large-scale visual sentiment ontol-\nogy and detectors using adjective noun pairs. In ACM MM,\n223–232. ACM.\n[C¸aglar Gu¨lc¸ehre et al. 2013] C¸aglar Gu¨lc¸ehre; Cho, K.; Pas-\ncanu, R.; and Bengio, Y. 2013. Learned-norm pooling for\ndeep neural networks. CoRR abs/1311.1780.\n[Cires¸an et al. 2011] Cires¸an, D. C.; Meier, U.; Masci, J.;\nGambardella, L. M.; and Schmidhuber, J. 2011. Flexible,\nhigh performance convolutional neural networks for image\nclassification. In IJCAI, 1237–1242. AAAI Press.\n[Davidov, Tsur, and Rappoport 2010] Davidov, D.; Tsur, O.;\nand Rappoport, A. 2010. Enhanced sentiment learning using\ntwitter hashtags and smileys. In ICL, 241–249. Association\nfor Computational Linguistics.\n[Duan et al. 2012] Duan, L.; Xu, D.; Tsang, I.-H.; and Luo,\nJ. 2012. Visual event recognition in videos by learning from\nweb data. IEEE PAMI 34(9):1667–1680.\n[Grangier, Bottou, and Collobert 2009] Grangier, D.; Bot-\ntou, L.; and Collobert, R. 2009. Deep convolutional net-\nworks for scene parsing. In ICML 2009 Deep Learning\nWorkshop, volume 3. Citeseer.\n[Hamel and Eck 2010] Hamel, P., and Eck, D. 2010. Learn-\ning features from music audio with deep belief networks. In\nISMIR, 339–344.\n[Hinton, Osindero, and Teh 2006] Hinton, G. E.; Osindero,\nS.; and Teh, Y.-W. 2006. A fast learning algorithm for deep\nbelief nets. Neural computation 18(7):1527–1554.\n[Hinton 2010] Hinton, G. 2010. A practical guide to training\nrestricted boltzmann machines. Momentum 9(1):926.\n[Hu et al. 2013] Hu, X.; Tang, J.; Gao, H.; and Liu, H. 2013.\nUnsupervised sentiment analysis with emotional signals. In\nWWW, 607–618. International World Wide Web Confer-\nences Steering Committee.\n[Jia 2013] Jia, Y. 2013. Caffe: An open source convolutional\narchitecture for fast feature embedding. http://caffe.\nberkeleyvision.org/.\n[Jin et al. 2010] Jin, X.; Gallagher, A.; Cao, L.; Luo, J.; and\nHan, J. 2010. The wisdom of social multimedia: using flickr\nfor prediction and forecast. In ACMMM, 1235–1244. ACM.\n[Joshi et al. 2011] Joshi, D.; Datta, R.; Fedorovskaya, E.; Lu-\nong, Q.-T.; Wang, J. Z.; Li, J.; and Luo, J. 2011. Aesthetics\nand emotions in images. IEEE Signal Processing Magazine\n28(5):94–115.\n[Kavukcuoglu et al. 2010] Kavukcuoglu, K.; Sermanet, P.;\nBoureau, Y.-L.; Gregor, K.; Mathieu, M.; and LeCun, Y.\n2010. Learning convolutional feature hierarchies for visual\nrecognition. In NIPS, 5.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classifi-\ncation with deep convolutional neural networks. In NIPS,\n4.\n[LeCun et al. 1989] LeCun, Y.; Boser, B.; Denker, J. S.; Hen-\nderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. D.\n1989. Backpropagation applied to handwritten zip code\nrecognition. Neural computation 1(4):541–551.\n[LeCun et al. 1998] LeCun, Y.; Bottou, L.; Bengio, Y.; and\nHaffner, P. 1998. Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE 86(11):2278–\n2324.\n[LeCun, Kavukcuoglu, and Farabet 2010] LeCun, Y.;\nKavukcuoglu, K.; and Farabet, C. 2010. Convolutional\nnetworks and applications in vision. In ISCAS, 253–256.\nIEEE.\n[Li et al. 2010] Li, G.; Hoi, S. C.; Chang, K.; and Jain, R.\n2010. Micro-blogging sentiment detection by collaborative\nonline learning. In ICDM, 893–898. IEEE.\n[Liu et al. 2003] Liu, B.; Dai, Y.; Li, X.; Lee, W. S.; and Yu,\nP. S. 2003. Building text classifiers using positive and unla-\nbeled examples. In ICDM, 179–186. IEEE.\n[Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.;\nHuang, D.; Ng, A. Y.; and Potts, C. 2011. Learning word\nvectors for sentiment analysis. In ACL, 142–150.\n[Morency, Mihalcea, and Doshi 2011] Morency, L.-P.; Mi-\nhalcea, R.; and Doshi, P. 2011. Towards multimodal senti-\nment analysis: Harvesting opinions from the web. In ICMI,\n169–176. New York, NY, USA: ACM.\n[O’Connor et al. 2010] O’Connor, B.; Balasubramanyan, R.;\nRoutledge, B. R.; and Smith, N. A. 2010. From tweets to\npolls: Linking text sentiment to public opinion time series.\nICWSM 11:122–129.\n[Pang and Lee 2008] Pang, B., and Lee, L. 2008. Opinion\nmining and sentiment analysis. Foundations and trends in\ninformation retrieval 2(1-2):1–135.\n[Patterson and Hays 2012] Patterson, G., and Hays, J. 2012.\nSun attribute database: Discovering, annotating, and recog-\nnizing scene attributes. In CVPR.\n[Plutchik 1984] Plutchik, R. 1984. Emotions: A general psy-\nchoevolutionary theory. Approaches to emotion 1984:197–\n219.\n[Siersdorfer et al. 2010] Siersdorfer, S.; Minack, E.; Deng,\nF.; and Hare, J. 2010. Analyzing and predicting sentiment\nof images on the social web. In ACM MM, 715–718. ACM.\n[Tumasjan et al. 2010] Tumasjan, A.; Sprenger, T. O.; Sand-\nner, P. G.; and Welpe, I. M. 2010. Predicting elections\nwith twitter: What 140 characters reveal about political sen-\ntiment. ICWSM 178–185.\n[Yuan et al. 2013] Yuan, J.; Mcdonough, S.; You, Q.; and\nLuo, J. 2013. Sentribute: image sentiment analysis from\na mid-level perspective. In Proceedings of the Second In-\nternational Workshop on Issues of Sentiment Discovery and\nOpinion Mining, 10. ACM.\n[Zhang, Fuehres, and Gloor 2011] Zhang, X.; Fuehres, H.;\nand Gloor, P. A. 2011. Predicting stock market indicators\nthrough twitter i hope it is not as bad as i fear. Procedia-\nSocial and Behavioral Sciences 26:55–62.\n",
      "id": 24729442,
      "identifiers": [
        {
          "identifier": "10.1609/aaai.v29i1.9179",
          "type": "DOI"
        },
        {
          "identifier": "386117133",
          "type": "CORE_ID"
        },
        {
          "identifier": "103268817",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/9179",
          "type": "OAI_ID"
        },
        {
          "identifier": "42637726",
          "type": "CORE_ID"
        },
        {
          "identifier": "1509.06041",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.687.4407",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1509.06041",
          "type": "OAI_ID"
        }
      ],
      "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1509.06041",
        "oai:citeseerx.psu:10.1.1.687.4407",
        "oai:ojs.aaai.org:article/9179"
      ],
      "publishedDate": "2015-02-09T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://www.cs.rochester.edu/u/qyou/papers/sentiment_analysis_final.pdf",
        "http://arxiv.org/abs/1509.06041"
      ],
      "updatedDate": "2024-02-09T07:01:46",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1509.06041"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24729442"
        }
      ]
    }
  ],
  "searchId": "81a595e7cab5f085f6f54a405c33a1d5"
}