{
  "totalHits": 535056,
  "limit": 100,
  "offset": 0,
  "results": [
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Chambers, L."
        },
        {
          "name": "Gaber, M."
        },
        {
          "name": "Pechenizkiy, M."
        },
        {
          "name": "Tromp, E."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/29582286",
        "https://api.core.ac.uk/v3/outputs/52396296"
      ],
      "createdDate": "2015-09-29T10:12:02",
      "dataProviders": [
        {
          "id": 695,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/695",
          "logo": "https://api.core.ac.uk/data-providers/695/logo"
        },
        {
          "id": 1899,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1899",
          "logo": "https://api.core.ac.uk/data-providers/1899/logo"
        }
      ],
      "depositedDate": "",
      "abstract": null,
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/29582286.pdf",
      "fieldOfStudy": null,
      "fullText": "Mobile Sentiment Analysis \nLorraine Chambers1, Erik Tromp2, Mykola Pechenizkiy2, Mohamed Medhat Gaber1 \n1School of Computing  \nUniversity of Portsmouth \nPortsmouth, Hampshire, PO1 3HE, England, UK \n{lorraine.chambers@myport.ac.uk, mohamed.gaber@port.ac.uk} \n2Department of Computer Science  \nEindhoven University of Technology \nP.O. Box 513, 5600 MB Eindhoven, the Netherlands \n{e.t.tromp@gmail.com,m.pechenizkiy@tue.nl} \nAbstract. Mobile devices play a significant part in a user’s communication me-\nthods and much data that they read and write is received and sent via mobile \nphones, for instance SMS messages, e-mails, Twitter tweets and social media \nnetworking feeds.  One of the main goals is to make people aware of how much \nnegative and positive content they read and write via their mobile phones.  Ex-\nisting sentiment analysis applications perform sentiment analysis on down-\nloaded data from mobile phones or use an application installed on another com-\nputer to perform the analysis.  The sentiment analysis described in this paper is \nto be performed locally on the mobile phone enabling immediate and private \nanalysis of personal messages and social media contents, allowing the users to \nbe able to reason about their mood and stress level that may be affected by what \nthey had been receiving. Experimental results showed the effectiveness of the \nproposed system on Android smartphones with varying computational capabili-\nties. \nKeywords. Sentiment analysis, data mining, and mobile computing \n1 Introduction \nSentiment analysis is a branch of natural language processing and one stage in the \nprocess of opinion mining.  To achieve opinion mining there are five tasks that need \nto be carried out, these are: Entity extraction and grouping, aspect orientation and \ngrouping, opinion holder and time extraction, aspect sentiment classification and opi-\nnion quintuple generation for summarization of the opinions.  The stage of aspect \nsentiment classification attempts to classify the sentiment of a particular aspect of a \nsentence; in the context of this paper, the general sentiment of the text will be consi-\ndered and not the sentiment of a particular aspect of the sentence, as the aim is to \nprovide the user with a general view of the sentiment of the texts within the mobile \ndevice and not the sentiments of particular products or services. \nSMS messages are a means of sending short text messages not longer than 160 \ncharacters for the Latin alphabet.  Although nowadays most mobile phones have the \ncapability to split longer texts into multiple messages and the recipient’s phone to \nreceive this as one message, an SMS message is typically one or two sentences long.  \nThis is similar to tweets on Twitter which allows text-based posts of up to 140 charac-\nters and has been described as “the SMS of the internet” [14]; because of these simi-\nlarities, SMS messages will be considered to be the same as tweets.  \nNowadays most personal communication is recorded digitally in mediums such as \nSMS text messages, e-mails, tweets, Facebook updates and other social media net-\nworking sites.  All of these are accessible from mobile phones and as these become \nfaster and more powerful, the possibility of performing sentiment analysis on mobile \ndevices has become more attractive, enabling the user to have sentiment analysis per-\nformed locally, rather than having personal information sent to a server or down-\nloaded to another application on a computer to be analyzed.  The overall goal is to \nperform sentiment analysis locally on the user’s mobile phones to enable the user to \nreflect upon how much general positive or negative content they are reading or writ-\ning. \nWe have developed a mobile application for the Android operating system that per-\nforms sentiment analysis locally on the mobile phone for SMS messages.  Due to the \nsimilarity of Twitter messages to SMS messages, the work reported in this paper gen-\nerally target both Twitter tweets and SMS messages. It is based on the SentiCorr [2] \nsystem which performs multi-lingual sentiment analysis of personal correspondence \non e-mails, Twitter tweets, Facebook and other social networking media.  The multi-\nlingual aspect is beyond the scope of this paper, and the only language that shall be \nconsidered for SMS messages in this paper is English. It is worth noting that the sys-\ntem can be easily extended to handle all text received on the mobile phone. Our sys-\ntem reported in this paper has extended the SentiCorr system in the following aspects: \n(1) applying a number of POS taggers and experimentally assessing their computa-\ntional performance and accuracy; (2) experimental study of the performance with \nrespect to the system configuration of the mobile device; and (3) a temporal aspect of \nthe sentiment has also been developed allowing the users to reason about the effect on \nthe sentiment on their level of stress. \nIn the following section work related to the different methods of sentiment analysis \nand the applications of sentiment analysis to mobile applications are considered.  \nSection 3 describes our approach to mobile sentiment analysis, while section 4 sum-\nmarizes our experimental study. Finally, section 5 concludes the findings and suggests \nfurther work. \n2 Related Work \nThere has been much work done on sentiment analysis, especially in the last five \nyears due to the abundance of data being available due to the explosion of social me-\ndia sites and blogging sites like Twitter.  This is reflected in the differences in Liu’s \nopinion mining and sentiment analysis chapters in subsequent versions [6] published \nin 2006 and [7] published in 2011. There has also been a comprehensive survey of \nsentiment analysis techniques as summarized by Pang [1].  In this paper sentiment \nanalysis shall be considered at the sentence level where there are two tasks to be per-\nformed; subjectivity classification to determine whether it is a subjective sentence or \nan objective sentence and sentiment classification – if the sentence is subjective, de-\ntermine if it contains a positive or negative opinion.  The assumption at this level is \nthat the sentence expresses a single opinion from a single opinion holder.  Both of \nthese tasks are classification problems, so supervised learning methods are applicable.  \nEarly methods used the Naïve Bayes classifier [9], subsequent methods have applied \nregression models [11] and support vector regression and boosting [12].  Semi-\nsupervised [10] and unsupervised techniques have also been applied [13]. \nFor mobile devices, sentiment analysis has been utilized for reflection of personal \ninformatics; analyzing SMS messages and emails to provide a general overview of a \nperson’s life at a particular point, but this is not performed on the mobile device but \nby another application to which the data is downloaded [3], and sentiment analysis \nhas been employed with SMS messages to gather feedback about teaching, where \nSMS messages are sent by students at the end of a lecture where they are automatical-\nly analyzed at the server-end to provide information about the teaching [8].  There are \nmobile phone applications available that involve sentiment analysis; for instance, for \nthe iPhone there is an application available that tells you what people think about your \narea by analyzing the sentiment of tweets around your location by sending them to an \nanalytics engine for sentiment analysis [4]. There is also an Android mobile applica-\ntion that analyses tweets and tries to determine if the attitude of the writer is positive \nor negative; it uses the internet and returns the results that the twitter sentiment analy-\nsis engine has generated [5].  To our knowledge there are no mobile applications that \nlocally execute sentiment analysis on the mobile phone. \n3 Mobile Sentiment Analysis System Description \nThe target platform selected for the mobile sentiment analysis application is An-\ndroid due to its “open” nature, availability of development resources and its wide \nusage. The sentiment analysis is to be performed using the principles of the SentiCorr \nsentiment analysis engine; firstly it will be described how SentiCorr achieves senti-\nment analysis and then the different aspects of enabling it to work on the Android \nplatform will be further discussed. \nSentiCorr achieves sentiment classification at the sentence level by using POS (Po-\nsition Of Speech) tagging to identify the types of the words in the sentence; the sub-\njectivity detection stage then uses the POS tags to identify opinion lexicon and hence \nif the sentence is subjective or objective; the polarity detection stage also utilizes the \nPOS tags to search for patterns in the sentence that indicate positive or negative ex-\npressions.   \nFor the POS tagging stage, Stuttgart University’s Tree Tagger was employed; \nAdaBoost for subjectivity detection and an in-house method called Rule-Based Emis-\nsion Model (RBEM) was developed for the polarity detection stage.  It is not the aim \nof this paper to propose new solutions for the sentiment analysis as extensive experi-\nments have been conducted in comparing these classification techniques with others \nsuch as Majority class, Prior Polarity, Naïve Bayes and Support Vector Machines, in \nwhich AdaBoost and RBEM outperform these other methods [15].  The following \nparagraphs summarize the algorithms used for subjectivity detection and polarity \ndetection, a comprehensive description can be found in [15]. \nThe principle employed for subjectivity detection is boosting, by use of the Ada-\nBoost (adaptive boosting algorithm) [19] and is a general method for generating a \nstrong classifier from a set of weak classifiers.  Each time an instance is incorrectly \nclassified, it is given a greater weight for use in the next round of classification.  This \nprocess continues until the maximum number of rounds is reached or the weighted \nerror is more than 50%.  The weak learners used are decision stumps of the form if f \npresent then label = a else label = b where f is a feature and a and b are labels.  Fea-\ntures utilized are POS tags, pre-defined lexicons that contain positive, negative and \nnegation words, the presence of exactly one positive word, the presence of multiple \npositive words, the presence of exactly one negative word and the presence of mul-\ntiple negative words, and whenever a positive or negative word is directly preceded \nby a word from the negation list, its polarity is flipped. \nThe principle employed for polarity detection is RBEM which uses rules to define \nan emissive model.  The rules emerge from eight different pattern groups which are \npositive patterns, negative patterns, amplifier patterns, attenuator patterns, right flip \npatterns, left flip patterns, continuator patterns and stop patterns.  These patterns are \ncombined with rules to define an emissive model.  A model is constructed by \nrepresenting patterns as lists of words and corresponding POS tags. In the patterns, \nword wildcards are allowed which means that wildcards for a word can appear at any \nposition of a pattern. Single-position wildcards are allowed so that a single entity in \nthe pattern can be any word and any POS tag.  Multi-position wildcards are also al-\nlowed so that any number of word tag pairs can occur in-between two elements that \nare not multi-position or single position wildcards.  The model consists of a set of \npatterns per pattern group, each pattern except for the positive and negative patterns \nadhere to an action radius, which is set to 4 in this case. \nWhen classifying previously unseen data, all of the patterns that match the sen-\ntence are collected from the model, and a rule associated with each pattern group is \napplied to each pattern in the message.  All patterns of all groups are evaluated for a \nmatch within the sentence; if there is a match, the start position and the end position \nof the pattern in the sentence is recorded.  Some patterns may occur within other pat-\nterns in the sentence, if so these subsumed patterns are removed from the final pattern \ncollection.  Once the patterns that occur in the sentence have been collected, the rules \nfor each pattern group are applied.  The rules must be applied in the correct order as \noutlined in the following paragraph. \nThe first rule to be applied is Setting Stops; this sets a stop at the starting position \nof all the left flip and stop patterns.  The second rule to be applied is Removing Stops; \nif there is a stop to the left of a continuator pattern within the pre-set action radius it is \nremoved.  The third rule to be applied is Positive Sentiment Emission; for each posi-\ntive pattern an emission value is calculated based on the distance of the elements in \nthe sentence from the centre of the positive pattern, which decays the further the ele-\nment is from the centre of the pattern, e-x is used as the decaying function, this is cal-\nculated for each element until stops are reached.   \nThe fourth rule to be applied is Negative Sentiment Emission; this is handled the \nsame way as Positive Sentiment Emission except that the decay function is –e-x.  The \nfifth rule to be applied is Amplifying Sentiment; amplifier patterns amplify sentiment \nemitted by positive or negative patterns and similarly to the positive and negative \npatterns, amplification reduces over distance.  The function used is 1+ e-x where x is \nthe distance within the action radius.  The sixth rule to be applied is Attenuating Sen-\ntiment; this performs the reverse of Amplifying Sentiment and the decay function \napplied is 1 – e-x.   \nThe seventh rule to be applied is Right Flipping Sentiment; if there is a right flip \npattern the emission of sentiment is flipped to the right and if there is a stop at the \nexact centre of the right flip pattern, it is ignored. The eighth rule to be applied is Left \nFlipping Sentiment; this mirrors the effect of the right flip pattern.  \nOnce the rules have been applied, every element of the sentence has an emission \nvalue and the final polarity of the message is calculated by summing the emission \nvalues for each element. If the final polarity of the sentence is greater than zero, the \nsentence is positive; if it is less than zero the sentence is negative, if it is zero the po-\nlarity of the sentence is unknown due to insufficient patterns in the sentence model. \nThe main aim of mobile sentiment analysis is to perform sentiment analysis on \nSMS messages which were earlier likened to tweets and that language identification \nwas beyond the scope of this paper, this reduces the original SentiCorr framework to \nthat of the POS tagger, Subjectivity Detection and Polarity Detection. Although we \nhave focused in this paper on short text sentiment analysis as applied to SMS messag-\nes and tweets received onboard the mobile phone, the work could be easily genera-\nlized to other social media items like Facebook and LinkedIn. \nAndroid mobile phones use ARM processors (Advanced RISC Machine) and the \noriginal POS tagger (TreeTagger) could not be used on the Android operating system \nas the source code was not available for re-compilation suitable for an ARM proces-\nsor.  This reduced the POS taggers available as they are constrained to POS taggers \nwritten in Java for which either a library where all the components and dependencies \nare capable of executing on an ARM compiler or the source code is available so it can \nbe compiled to execute on an ARM processor. The shortlist of POS taggers tested \nwere Stanford POS tagger and OpenNLP POS tagger. \nThe subjectivity detection stage and the polarity detection stages required that the \nPOS tags were in the format of the Penn Tree Bank set 1 for the software to operate \ncorrectly with the pre-trained models used within the subjectivity and polarity detec-\ntion stages.  \nPOS taggers are usually supervised and as such require a model.  Loading these \nmodels contributed to how the software was architected.  On a PC, the time to load \nthese models is small compared to the time to load them on a mobile device.  In Sen-\ntiCorr, the language of the text is assessed at the sentence level and if the sentence is \nin a different language to the previous one, a different model needs to be loaded for \nthe POS tagging, subjectivity and polarity classification stages.  As we are not yet \nconsidering the multi-lingual aspect, the model is loaded once at startup of the appli-\ncation and the same language is assumed throughout each usage of the application and \nthe model is loaded only once per application usage. These constraints shaped the \nworkflow of the mobile sentiment analysis system as shown in Figure 1. \n \n \nFig. 1. Mobile Sentiment Analysis Workflow \nIn the mobile sentiment analysis solution the POS taggers are dynamically inter-\nchangeable for analysis purposes and can be loaded via settings menus and the output \nform each POS tagger are transformed into a standard output so that the interface to \nthe rest of the application remains constant regardless of the POS tagger.  Tagging of \nthe sentence splits the text into sentences and tags them using the selected POS tag-\nger.  The tagged sentences are then passed to the subjectivity detection stage where \nthe algorithm operates in the same way as within SentiCorr as described in the pre-\nvious paragraphs.   \nOnce the subjectivity has been determined, if it is subjective, the sentence is then \npassed to the polarity detection stage where the algorithm operates in the same way as \nthe SentiCorr algorithm also as summarized in the previous paragraphs; if the sen-\ntence is objective, no further processing is applied to it. \nThe sentiment analysis code is implemented as a standard Java library that the An-\ndroid application uses, ensuring its use is not limited to an Android mobile applica-\ntion. The models for the subjectivity and polarity detection stages are stored in XML \nformat and serializable directly into Java objects using Simple XML [20] so that it has \nthe possibility to be extended to allow creation of new models that can be stored in \nthe correct format for later use. \nHaving discussed the technical and implementation details of our mobile sentiment \nanalysis system, the following section provides an experimental study of the system \nproving empirically its feasibility and efficiency. \n4 Experimental Results \nThe mobile sentiment analysis application was installed on three mobile phones, \nthe specifications of which are shown in Table 1. The aim of varying the mobile \nphones is to conduct stress testing, so that to reveal the minimum configuration of \ncomputational power that is able to run our system.  \nThe load time of the POS tagger models was the major factor in the duration of the \nexecution time of the application.  The POS taggers evaluated were OpenNLP and \nStanford POS Tagger. Each of these taggers has a number of models which are sum-\nmarized in the following paragraphs; these models are also listed in Table 2 along \nwith the time taken to load these models for each phone. Each POS tagger model was \nloaded ten times and an average taken to give the model load times in Table 2. \nThe Open NLP POS tagger comes with two different types of models: maximum \nentropy model and a perceptron model, each with the option of using a dictionary.  A \ntoken can have many tag possibilities depending on the token and the context. Open \nNLP uses a probability model to guess the correct POS tag out of the tag set, to limit \nthe possible tags, a dictionary can be used.  The maximum entropy model with a dic-\ntionary produced the best results at 87%.  The perceptron model is a linear classifier \nand relies on Viterbi decoding of training examples [18]. \nThe Stanford POS tagger comes with trained English tagger models and taggers \ntrained on the Wall Street Journal corpus from the Penn Treebank Project., as de-\npicted by ‘wsj’ in the model name; the ‘left3’ in the model name means that the mod-\nel uses the left3 words architecture and includes word shape features; the ‘distsim’ \npart of the tagger name means that the model includes distributional similarity fea-\ntures.  All of the Stanford POS tagger models utilize a maximum entropy method of \ntagging where a probability is assigned for every tag in a set of possible tags for a \nword where the possible tags are determined from the sequence of words preceding \nthe word that is to be tagged [16].  The Stanford POS tagger model that performed the \nbest on the tested data was the english-left3words-distsim model and the wsj-0-18-\nleft3words model, showing that in general, the left3 words architecture was a success-\nful model for the data.  The bidirectional models use a cyclic dependency networks to \nachieve bi-directional traversing of the words in a given sentence [17]. \nAs the sentiment analysis code is implemented as a standard java library, we were \nable to calculate the accuracy of the sentiment analysis for each model on a PC where \nthe accuracy is taken as the number of correctly tagged tokens divided by the total \nnumber of tokens and represented as a percentage.  The sentiment was analyzed on \npart of the original data used in the evaluation of SentiCorr and is based on 60 texts \nand utilizes the POS tags from the original data and uses this as the gauge for accura-\ncy. These 60 texts were obtained from Twitter by scraping all public data and manual-\nly labeling 20 negative, 20 objective and 20 positive tweets, according to the tweet's \ntext. The results are shown in table 2.  The peak memory usage of the application \nduring the loading of the POS tagger models was also recorded and is shown in Table \n2. \nTable 1. POS tagger model load times on mobile phone \nMobile Phone \nModel A\nn\ndr\no\nid\n \n \nV\ner\nsio\nn\n \nSD\n \nC\na\nrd\n \nA\nv\na\nila\nbl\ne \nSp\na\nce\n \n(G\nB)\n \nIn\nte\nrn\na\nl \nPh\no\nn\ne \nSt\no\nra\nge\n \n(G\nB)\n \nTo\nta\nl A\nv\na\nila\n-\nbl\ne \nM\nem\no\nry\n \n(G\nB)\n \nGT540 2.1 1.77 0.099 1.869 \nHTC Desire HD 2.3.5 3.0 0.815 3.815 \nGalaxy Nexus 4.0.2 N/A N/A 13.33 \nTable 2. POS tagger model size and accuracy  \nPOS Tagger Model M\no\nde\nl S\niz\ne \n(M\nB)\n \nA\ncc\nu\nra\ncy\n \n(%\n) \nPe\na\nk \nM\nem\no\nry\n \n \nU\nsa\nge\n \n(M\nB)\n \nModel Load Times (s) MinSpec \nG\nT5\n40\n \nH\nTC\n \nD\nes\nir\ne \nH\nD\n \nG\na\nla\nx\ny \nN\nex\nu\ns \nM\nin\n \n \nA\nn\ndr\no\nid\n \nV\ner\n \nSt\no\nra\nge\n \nSp\na\nce\n \n(M\nB)\n \nH\nea\np \nSp\na\nce\n \n(M\nB)\n \nOpen \nNLP \nMaxent no dictio-\nnary \n5.46 85 105 OSM 17.78 23.03 2.1 180 61 \nMaxent with \ndictionary \n5.56 87 - Invalid format for dictionary \nfile \n- - - \nPerceptron no \ndictionary \n3.78 83 105 OSM 17.78 22.21 2.1 178 62 \nPerceptron with \ndictionary \n3.88 85 - Invalid format for dictionary \nfile \n- - - \nStan-\nford \nenglish-\nleft3words-distsim \n20.15 90 230 OSM OHM 250.79 3.0 182 233 \nenglish-caseless-\nleft3words-distsim \n19.77 88 225 OSM OHM 180.26 3.0 182 229 \nwsj-0-18-\nleft3words-distsim \n17.38 87 207 OSM OHM 205.44 3.0 181 207 \nwsj-0-18-\nleft3words \n7.98 90 110 OSM OHM 50.71 3.0 177 110 \nwsj-0-18-caseless-\nleft3words-distsim \n17.12 88 201 OSM OHM 179.71 3.0 180 201 \nwsj-0-18-\nbidirectional-\ndistsim \n31.65 87 285 OSM OHM OHM 3.0 - - \nOSM = Out of Storage Memory, OHM = Out of Heap Memory \n \nThe accuracy of the models ranges from 83% to 90% across the Stanford and the \nOpen NLP POS tagger models, with the most accurate being the Stanford English left \n3 words and the Stanford wsj-0-18left3words which also had a low load time but only \nran on the Galaxy Nexus phone.  In fact, no Stanford model could be loaded on a \nphone that had an Android version of lower than 3.0 because the Android application \nsetting largeHeap was required to be set to true in the Android manifest file to \nallow more memory to be dynamically allocated to the application, hence no results \nfor the Stanford POS tagger for the LG GT540 or the HTC Desire HD mobile phones \ncould be recorded for the Stanford POS taggers.  No results could be recorded for the  \nGT540 phone as there was not enough storage memory.  From this information mini-\nmum phone specifications for each tagger have been calculated and included in Table \n2.  Overall, the Stanford POS tagger model was more accurate but requires more mo-\nbile phone resources such as memory and load time, which limits the targets it can be \ninstalled on. In most cases, it took nearly ten times as long to load the Stanford mod-\nels as it did the Open NLP model, for a 2% increase in accuracy. \nThe duration of the loading of the POS models has been focused on but the Ada-\nBoost and Emission Miner models also have an overhead when they are loaded.  The \nAdaBoost and Emission Miner models use the POS tags that are produced by the POS \ntaggers.  The original POS tagger – TreeTagger output Penn tree bank I tag set whe-\nreas the Stanford POS tagger and Open NLP POS tagger both output Penn tree bank \nII tag set.  The difference in these tag sets are that in tree bank II the individual tags \nfor “VH” tags are now included in “VB” tags as re “VV” tags, meaning that the verbs \n“to have” and “to take” are now included under the general verb tags.  This meant that \nthe existing AdaBoost and Emission Miner models had to be adjusted to take this into \naccount.  The result is that if the POS tagger model is changed to one which uses a \ndifferent tag set, then different AdaBoost and Emission Miner models are also re-\nquired to be loaded.  The time it takes to load these models will be critical if a dynam-\nic multi-language approach is required as in the original SentiCorr system. \n5 Conclusion and Future Work \nSentiment analysis can be performed locally on a mobile phone, as we have proved \nempirically in this paper.  To the best of our knowledge, the proposed and imple-\nmented system reported in this paper is the first mobile sentiment analyzer. The \namount of time and resources this takes largely depends upon the POS tagger model \nthat is utilized.  The Stanford POS tagger is more accurate, but takes longer to load \nand requires more memory. In a non-language dynamic environment, the model is \nonly loaded once per start-up of the application but limits the user to the same lan-\nguage unless this is specifically changed by the user.  The change of a POS tagger \ncould affect the subsequent subjectivity and polarity detection stages, meaning that \nnew models for these stages may also need to be loaded.  If the application was to \nincorporate dynamic language selection then the load time of the models would be-\ncome critical.   \nFuture work is to include experimentation on a wider set of Android mobile phones \nwith differing memory and Android operating systems, and inclusion of analysis on \nother POS taggers implemented in Java and those specifically aimed at Twitter data \nsuch as Tweet NLP.  Experimentation could also be extended to the training of small-\ner models to widen the target mobile phones of the application such that the applica-\ntion could be extended to dynamically interrogate and change the language during \nsentiment classification. \nReferences \n1. Pang, B., Lillian, L.: Opinion Mining and Sentiment Analysis. In: Foundations and Trends \nin Information Retrieval 2, pp. 1-135 (2008) \n2. Tromp, E., Pechenizkiy, M.: SentiCorr: Multilingual Sentiment Analysis of Personal Cor-\nrespondence. In: IEEE 11th International Conference on Data Mining Workshops, pp. \n1247-1250 (2011) \n3. Hangal, S., Lam, M.: Sentiment Analysis on Personal Email Archives. Proceedings of the \n24th annual ACM symposium on User interface software and technology (2011) \n4. Chatterbox analytics, http://cbanalytics.co.uk/apps/sentimental \n5. SmmMobile – Sentiment Analyser \nhttps://market.android.com/details?id=com.cyhex.smmMobile \n6. Liu, B.: Chapter 11 Opinion Mining. In: Web Data Mining. pp. 411-448. Springer , Hei-\ndelberg (2006) \n7. Liu, B.: Chapter 11 Opinion Mining. In: Web Data Mining. pp. 459-526. Springer , Hei-\ndelberg (2011) \n8. Leong, C.K., Lee, Y.H.: Mining sentiments in SMS texts for teaching evaluation. In Expert \nSystems with Applications 39, pp. 2584-2589 (2012) \n9. Yu, H.: Towards Answering Opinion Questions: Separating facts from Opinions and iden-\ntifying the Polarity of Opinion Sentences.  In: EMNLP ’03 Proceedings of the 2003 confe-\nrence on Empirical methods in natural language processing pp. 129-136 (2003) \n10. Dasgupta, S.: Mine the easy, classify the hard: A semi-supervised approach to Automatic \nSentiment Classification. In: ACL ’09 Proceedings of the joint Conference of the 47th An-\nnual Meeting of the ACL and the 4th International Joint Conference on Natural Language \nProcessing of the AFNLP 2 pp. 701-709 (2009) \n11. Hatzivassiloglou, V.: Predicting the semantic orientation of Adjectives. In: ACL ’98 Pro-\nceedings of the 35th Annual Meeting of the Association for Computational Linguistics and \nEighth Conference of the European Chapter of the Association for Computational Linguis-\ntics. Pp. 174-181 (1997) \n12. Wilson, T., Wiebe, J., Hwa, R.: Just How Mad Are you? Finding Strong and Weak Opi-\nnion Clauses. In: Proceedings of the National Conference on Artificial Intelligence. 10 \npp.761-769 (2004) \n13. Turner, D.: Thunbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised \nClassification of Reviews. In: Proceedings of the 40th Annual Meeting of the Association \nfor Computational Linguistics (ACL). pp. 417-424 (2002) \n14. Wikipedia – Twitter. http://en.wikipedia.org/wiki/Twitter \n15. Tromp, E.: Multilingual Sentiment Analysis on Social Media. \nhttp://alexandria.tue.nl/extra1/afstversl/wsk-i/tromp2011.pdf \n16. Toutanova, K., Manning, C.: Enriching the Knowledge Sources Used in a Maximum En-\ntropy Part-of-Speech Tagger. In: Proceedings of the Joint SIGDAT Conference on Empiri-\ncal Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-\n2000), pp. 63-70 (2000) \n17. Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. In: Feature-Rich \nPart-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-\nNAACL 2003, pp. 252-259 (2003) \n18. Collins, M.: Discriminative Training Methods for Hidden Markov Models: Theory and \nExperiments with Perceptron Algorithms. In: Proceedings of the Conference on Empirical \nMethods in Natural Language Processing (EMNLP). pp. 1-8 (2002) \n19. Freund, Y., Shapire, R.: A decision-theoretic generalization of on-line learning and an ap-\nplication to boosting. In: Proceedings of the Second European Conference on Computa-\ntional Learning Theory, pp. 23-37 (1995) \n20. Gallagher, N.: Simple Framework for XML. \nhttp://simple.sourceforge.net/home.php \n",
      "id": 18098239,
      "identifiers": [
        {
          "identifier": "oai:researchportal.port.ac.uk:publications/782ac36b-80c7-4842-aebe-69c9406ba3b4",
          "type": "OAI_ID"
        },
        {
          "identifier": "29582286",
          "type": "CORE_ID"
        },
        {
          "identifier": "52396296",
          "type": "CORE_ID"
        }
      ],
      "title": "Mobile sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:researchportal.port.ac.uk:publications/782ac36b-80c7-4842-aebe-69c9406ba3b4"
      ],
      "publishedDate": "2012-09-10T01:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 38740203,
          "title": "A decision-theoretic generalization of on-line learning and an application to boosting. In:",
          "authors": [],
          "date": "1995",
          "doi": "10.1006/jcss.1997.1504",
          "raw": "Freund, Y., Shapire, R.: A decision-theoretic generalization of on-line learning and an application to boosting. In: Proceedings of the Second European Conference on Computational Learning Theory, pp. 23-37 (1995)",
          "cites": null
        },
        {
          "id": 38740188,
          "title": "Chapter 11 Opinion Mining. In: Web Data Mining.",
          "authors": [],
          "date": "2011",
          "doi": "10.1007/978-3-642-19460-3_11",
          "raw": "Liu, B.: Chapter 11 Opinion Mining. In: Web Data Mining. pp. 459-526. Springer , Heidelberg (2011)",
          "cites": null
        },
        {
          "id": 38740202,
          "title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In:",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1118693.1118694",
          "raw": "Collins, M.: Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1-8 (2002)",
          "cites": null
        },
        {
          "id": 38740198,
          "title": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In:",
          "authors": [],
          "date": "2000",
          "doi": "10.3115/1117794.1117802",
          "raw": "Toutanova, K., Manning, C.: Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In: Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC2000), pp. 63-70 (2000)",
          "cites": null
        },
        {
          "id": 38740195,
          "title": "How Mad Are you? Finding Strong and Weak Opinion Clauses. In:",
          "authors": [],
          "date": "2004",
          "doi": null,
          "raw": "Wilson, T., Wiebe, J., Hwa, R.: Just How Mad Are you? Finding Strong and Weak Opinion Clauses. In: Proceedings of the National Conference on Artificial Intelligence. 10 pp.761-769 (2004)",
          "cites": null
        },
        {
          "id": 38740199,
          "title": "In: Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.",
          "authors": [],
          "date": "2003",
          "doi": "10.3115/1073445.1073478",
          "raw": "Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. In: Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLTNAACL 2003, pp. 252-259 (2003)",
          "cites": null
        },
        {
          "id": 38740193,
          "title": "Mine the easy, classify the hard: A semi-supervised approach to Automatic Sentiment Classification. In:",
          "authors": [],
          "date": "2009",
          "doi": "10.3115/1690219.1690244",
          "raw": "Dasgupta, S.: Mine the easy, classify the hard: A semi-supervised approach to Automatic Sentiment Classification. In: ACL ’09 Proceedings of the joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP 2 pp. 701-709 (2009)",
          "cites": null
        },
        {
          "id": 38740197,
          "title": "Multilingual Sentiment Analysis on Social Media.",
          "authors": [],
          "date": null,
          "doi": "10.3115/v1/w14-2611",
          "raw": "Tromp, E.: Multilingual Sentiment Analysis on Social Media. http://alexandria.tue.nl/extra1/afstversl/wsk-i/tromp2011.pdf",
          "cites": null
        },
        {
          "id": 38740184,
          "title": "Opinion Mining and Sentiment Analysis. In:",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "Pang, B., Lillian, L.: Opinion Mining and Sentiment Analysis. In: Foundations and Trends in Information Retrieval 2, pp. 1-135 (2008)",
          "cites": null
        },
        {
          "id": 38740194,
          "title": "Predicting the semantic orientation of Adjectives. In:",
          "authors": [],
          "date": "1997",
          "doi": "10.3115/976909.979640",
          "raw": "Hatzivassiloglou, V.: Predicting the semantic orientation of Adjectives. In: ACL ’98 Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics. Pp. 174-181 (1997)",
          "cites": null
        },
        {
          "id": 38740186,
          "title": "SentiCorr: Multilingual Sentiment Analysis of Personal Correspondence. In:",
          "authors": [],
          "date": "2011",
          "doi": "10.1109/icdmw.2011.152",
          "raw": "Tromp, E., Pechenizkiy, M.: SentiCorr: Multilingual Sentiment Analysis of Personal Correspondence. In: IEEE 11th International Conference on Data Mining Workshops, pp. 1247-1250 (2011)",
          "cites": null
        },
        {
          "id": 38740187,
          "title": "Sentiment Analysis on Personal Email Archives.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Hangal, S., Lam, M.: Sentiment Analysis on Personal Email Archives. Proceedings of the 24th annual ACM symposium on User interface software and technology (2011)",
          "cites": null
        },
        {
          "id": 38740204,
          "title": "Simple Framework for XML.",
          "authors": [],
          "date": null,
          "doi": "10.4324/9780080522210",
          "raw": "Gallagher, N.: Simple Framework for XML. http://simple.sourceforge.net/home.php",
          "cites": null
        },
        {
          "id": 38740196,
          "title": "Thunbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In:",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1073083.1073153",
          "raw": "Turner, D.: Thunbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). pp. 417-424 (2002)",
          "cites": null
        },
        {
          "id": 38740192,
          "title": "Towards Answering Opinion Questions: Separating facts from Opinions and identifying the Polarity of Opinion Sentences. In:",
          "authors": [],
          "date": "2003",
          "doi": "10.3115/1119355.1119372",
          "raw": "Yu, H.: Towards Answering Opinion Questions: Separating facts from Opinions and identifying the Polarity of Opinion Sentences.  In: EMNLP ’03 Proceedings of the 2003 conference on Empirical methods in natural language processing pp. 129-136 (2003)",
          "cites": null
        },
        {
          "id": 38740191,
          "title": "Y.H.: Mining sentiments in SMS texts for teaching evaluation.",
          "authors": [],
          "date": "2012",
          "doi": "10.1016/j.eswa.2011.08.113",
          "raw": "Leong, C.K., Lee, Y.H.: Mining sentiments in SMS texts for teaching evaluation. In Expert Systems with Applications 39, pp. 2584-2589 (2012)",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "https://researchportal.port.ac.uk/portal/services/downloadRegister/152710/k12gen-092.pdf"
      ],
      "updatedDate": "2021-07-22T16:23:48",
      "yearPublished": 2012,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/29582286.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/29582286"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/29582286/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/29582286/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/18098239"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1509.06041",
      "authors": [
        {
          "name": "Jin, Hailin"
        },
        {
          "name": "Luo, Jiebo"
        },
        {
          "name": "Yang, Jianchao"
        },
        {
          "name": "You, Quanzeng"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/386117133",
        "https://api.core.ac.uk/v3/outputs/103268817"
      ],
      "createdDate": "2016-08-03T02:20:56",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms.Comment: 9 pages, 5 figures, AAAI 201",
      "documentType": "research",
      "doi": "10.1609/aaai.v29i1.9179",
      "downloadUrl": "http://arxiv.org/abs/1509.06041",
      "fieldOfStudy": null,
      "fullText": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\nTransferred Deep Networks\nQuanzeng You and Jiebo Luo\nDepartment of Computer Science\nUniversity of Rochester\nRochester, NY 14623\n{qyou, jluo}@cs.rochester.edu\nHailin Jin and Jianchao Yang\nAdobe Research\n345 Park Avenue\nSan Jose, CA 95110\n{hljin, jiayang}@adobe.com\nAbstract\nSentiment analysis of online user generated content is\nimportant for many social media analytics tasks. Re-\nsearchers have largely relied on textual sentiment anal-\nysis to develop systems to predict political elections,\nmeasure economic indicators, and so on. Recently, so-\ncial media users are increasingly using images and\nvideos to express their opinions and share their expe-\nriences. Sentiment analysis of such large scale visual\ncontent can help better extract user sentiments toward\nevents or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is comple-\nmentary to textual sentiment analysis. Motivated by the\nneeds in leveraging large scale yet noisy training data to\nsolve the extremely challenging problem of image sen-\ntiment analysis, we employ Convolutional Neural Net-\nworks (CNN). We first design a suitable CNN archi-\ntecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment\nalgorithm to label Flickr images. To make use of such\nnoisy machine labeled data, we employ a progressive\nstrategy to fine-tune the deep network. Furthermore, we\nimprove the performance on Twitter images by induc-\ning domain transfer with a small number of manually\nlabeled Twitter images. We have conducted extensive\nexperiments on manually labeled Twitter images. The\nresults show that the proposed CNN can achieve better\nperformance in image sentiment analysis than compet-\ning algorithms.\nIntroduction\nOnline social networks are providing more and more con-\nvenient services to their users. Today, social networks have\ngrown to be one of the most important sources for people to\nacquire information on all aspects of their lives. Meanwhile,\nevery online social network user is a contributor to such\nlarge amounts of information. Online users love to share\ntheir experiences and to express their opinions on virtually\nall events and subjects.\nAmong the large amount of online user generated data, we\nare particularly interested in people’s opinions or sentiments\ntowards specific topics and events. There have been many\nCopyright c© 2015, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Examples of Flickr images related to the 2012\nUnited States presidential election.\nworks on using online users’ sentiments to predict box-\noffice revenues for movies (Asur and Huberman 2010), po-\nlitical elections (O’Connor et al. 2010; Tumasjan et al. 2010)\nand economic indicators (Bollen, Mao, and Zeng 2011;\nZhang, Fuehres, and Gloor 2011). These works have sug-\ngested that online users’ opinions or sentiments are closely\ncorrelated with our real-world activities. All of these results\nhinge on accurate estimation of people’s sentiments accord-\ning to their online generated content. Currently all of these\nworks only rely on sentiment analysis from textual content.\nHowever, multimedia content, including images and videos,\nhas become prevalent over all online social networks. In-\ndeed, online social network providers are competing with\neach other by providing easier access to their increasingly\npowerful and diverse services. Figure 1 shows example im-\nages related to the 2012 United States presidential election.\nClearly, images in the top and bottom rows convey opposite\nsentiments towards the two candidates.\nA picture is worth a thousand words. People with differ-\nent backgrounds can easily understand the main content of\nan image or video. Apart from the large amount of easily\navailable visual content, today’s computational infrastruc-\nture is also much cheaper and more powerful to make the\nanalysis of computationally intensive visual content analy-\nsis feasible. In this era of big data, it has been shown that\nthe integration of visual content can provide us more reli-\nable or complementary online social signals (Jin et al. 2010;\nYuan et al. 2013).\nTo the best of our knowledge, little attention has been paid\nto the sentiment analysis of visual content. Only a few recent\nworks attempted to predict visual sentiment using features\nar\nX\niv\n:1\n50\n9.\n06\n04\n1v\n1 \n [c\ns.C\nV]\n  2\n0 S\nep\n 20\n15\nfrom images (Siersdorfer et al. 2010; Borth et al. 2013b;\nBorth et al. 2013a; Yuan et al. 2013) and videos (Morency,\nMihalcea, and Doshi 2011). Visual sentiment analysis is ex-\ntremely challenging. First, image sentiment analysis is in-\nherently more challenging than object recognition as the lat-\nter is usually well defined. Image sentiment involves a much\nhigher level of abstraction and subjectivity in the human\nrecognition process (Joshi et al. 2011), on top of a wide vari-\nety of visual recognition tasks including object, scene, action\nand event recognition. In order to use supervised learning, it\nis imperative to collect a large and diverse labeled training\nset perhaps on the order of millions of images. This is an\nalmost insurmountable hurdle due to the tremendous labor\nrequired for image labeling. Second, the learning schemes\nneed to have high generalizability to cover more different\ndomains. However, the existing works use either pixel-level\nfeatures or a limited number of predefined attribute features,\nwhich is difficult to adapt the trained models to images from\na different domain.\nThe deep learning framework enables robust and accurate\nfeature learning, which in turn produces the state-of-the-art\nperformance on digit recognition (LeCun et al. 1989; Hin-\nton, Osindero, and Teh 2006), image classification (Cires¸an\net al. 2011; Krizhevsky, Sutskever, and Hinton 2012), mu-\nsical signal processing (Hamel and Eck 2010) and natural\nlanguage processing (Maas et al. 2011). Both the academia\nand industry have invested a huge amount of effort in build-\ning powerful neural networks. These works suggested that\ndeep learning is very effective in learning robust features in a\nsupervised or unsupervised fashion. Even though deep neu-\nral networks may be trapped in local optima (Hinton 2010;\nBengio 2012), using different optimization techniques, one\ncan achieve the state-of-the-art performance on many chal-\nlenging tasks mentioned above.\nInspired by the recent successes of deep learning, we are\ninterested in solving the challenging visual sentiment anal-\nysis task using deep learning algorithms. For images related\ntasks, Convolutional Neural Network (CNN) are widely\nused due to the usage of convolutional layers. It takes into\nconsideration the locations and neighbors of image pixels,\nwhich are important to capture useful features for visual\ntasks. Convolutional Neural Networks (LeCun et al. 1998;\nCires¸an et al. 2011; Krizhevsky, Sutskever, and Hinton\n2012) have been proved very powerful in solving computer\nvision related tasks. We intend to find out whether applying\nCNN to visual sentiment analysis provides advantages over\nusing a predefined collection of low-level visual features or\nvisual attributes, which have been done in prior works.\nTo that end, we address in this work two major challenges:\n1) how to learn with large scale weakly labeled training\ndata, and 2) how to generalize and extend the learned model\nacross domains. In particular, we make the following contri-\nbutions.\n• We develop an effective deep convolutional network ar-\nchitecture for visual sentiment analysis. Our architecture\nemploys two convolutional layers and several fully con-\nnected layers for the prediction of visual sentiment labels.\n• Our model attempts to address the weakly labeled nature\nof the training image data, where such labels are machine\ngenerated, by leveraging a progressive training strategy\nand a domain transfer strategy to fine-tune the neural net-\nwork. Our evaluation results suggest that this strategy is\neffective for improving the performance of neural net-\nwork in terms of generalizability.\n• In order to evaluate our model as well as competing algo-\nrithms, we build a large manually labeled visual sentiment\ndataset using Amazon Mechanical Turk. This dataset will\nbe released to the research community to promote further\ninvestigations on visual sentiment.\nRelated Work\nIn this section, we review literature closely related to our\nstudy on visual sentiment analysis, particularly in sentiment\nanalysis and Convolutional Neural Networks.\nSentiment Analysis\nSentiment analysis is a very challenging task (Liu et al.\n2003; Li et al. 2010). Researchers from natural language\nprocessing and information retrieval have developed differ-\nent approaches to solve this problem, achieving promising\nor satisfying results (Pang and Lee 2008). In the context of\nsocial media, there are several additional unique challenges.\nFirst, there are huge amounts of data available. Second, mes-\nsages on social networks are by nature informal and short.\nThird, people use not only textual messages, but also images\nand videos to express themselves.\nTumasjan et al. (2010) and Bollen et al. (2011) employed\npre-defined dictionaries for measuring the sentiment level\nof Tweets. The volume or percentage of sentiment-bearing\nwords can produce an estimate of the sentiment of one par-\nticular tweet. Davidov et al. (2010) used the weak labels\nfrom a large amount of Tweets. In contrast, they manually\nselected hashtags with strong positive and negative senti-\nments and ASCII smileys are also utilized to label the sen-\ntiments of tweets. Furthermore, Hu et al. (2013) incorpo-\nrated social signals into their unsupervised sentiment anal-\nysis framework. They defined and integrated both emotion\nindication and correlation into a framework to learn param-\neters for their sentiment classifier.\nThere are also several recent works on visual sentiment\nanalysis. Siersdorfer et al. (2010) proposes a machine learn-\ning algorithm to predict the sentiment of images using pixel-\nlevel features. Motivated by the fact that sentiment involves\nhigh-level abstraction, which may be easier to explain by\nobjects or attributes in images, both (Borth et al. 2013a)\nand (Yuan et al. 2013) propose to employ visual entities or\nattributes as features for visual sentiment analysis. In (Borth\net al. 2013a), 1200 adjective noun pairs (ANP), which may\ncorrespond to different levels of different emotions, are ex-\ntracted. These ANPs are used as queries to crawl images\nfrom Flickr. Next, pixel-level features of images in each\nANP are employed to train 1200 ANP detectors. The re-\nsponses of these 1200 classifiers can then be considered as\nmid-level features for visual sentiment analysis. The work\nin (Yuan et al. 2013) employed a similar mechanism. The\nmain difference is that 102 scene attributes are used instead.\n256\n256\n227\n227\n3\n3\n227\n227\n11\n11\n5\n5\n96\n55\n55\n256\n27\n27\n512 512\n24\n2\nFigure 2: Convolutional Neural Network for Visual Sentiment Analysis.\nConvolutional Neural Networks\nConvolutional Neural Networks (CNN) have been very suc-\ncessful in document recognition (LeCun et al. 1998). CNN\ntypically consists of several convolutional layers and sev-\neral fully connected layers. Between the convolutional lay-\ners, there may also be pooling layers and normalization lay-\ners. CNN is a supervised learning algorithm, where parame-\nters of different layers are learned through back-propagation.\nDue to the computational complexity of CNN, it has only be\napplied to relatively small images in the literature. Recently,\nthanks to the increasing computational power of GPU, it is\nnow possible to train a deep convolutional neural network on\na large scale image dataset (Krizhevsky, Sutskever, and Hin-\nton 2012). Indeed, in the past several years, CNN has been\nsuccessfully applied to scene parsing (Grangier, Bottou, and\nCollobert 2009), feature learning (LeCun, Kavukcuoglu, and\nFarabet 2010), visual recognition (Kavukcuoglu et al. 2010)\nand image classification (Krizhevsky, Sutskever, and Hinton\n2012). In our work, we intend to use CNN to learn features\nwhich are useful for visual sentiment analysis.\nVisual Sentiment Analysis\nWe propose to develop a suitable convolutional neural net-\nwork architecture for visual sentiment analysis. Moreover,\nwe employ a progressive training strategy that leverages the\ntraining results of convolutional neural network to further\nfilter out (noisy) training data. The details of the proposed\nframework will be described in the following sections.\nVisual Sentiment Analysis with regular CNN\nCNN has been proven to be effective in image classifica-\ntion tasks, e.g., achieving the state-of-the-art performance\nin ImageNet Challenge (Krizhevsky, Sutskever, and Hin-\nton 2012). Visual sentiment analysis can also be treated\nas an image classification problem. It may seem to be a\nmuch easier problem than image classification from Ima-\ngeNet (2 classes vs. 1000 classes in ImageNet). However,\nvisual sentiment analysis is quite challenging because senti-\nments or opinions correspond to high level abstractions from\na given image. This type of high level abstraction may re-\nquire viewer’s knowledge beyond the image content itself.\nMeanwhile, images in the same class of ImageNet mainly\ncontain the same type of object. In sentiment analysis, each\nclass contains much more diverse images. It is therefore ex-\ntremely challenging to discover features which can distin-\nguish much more diverse classes from each other. In addi-\ntion, people may have totally different sentiments over the\nsame image. This adds difficulties to not only our classifi-\ncation task, but also the acquisition of labeled images. In\nother words, it is nontrivial to obtain highly reliable labeled\ninstances, let alone a large number of them. Therefore, we\nneed a supervised learning engine that is able to tolerate a\nsignificant level of noise in the training dataset.\nThe architecture of the CNN we employ for sentiment\nanalysis is shown in Figure 2. Each image is resized to\n256 × 256 (if needed, we employ center crop, which first\nresizes the shorter dimension to 256 and then crops the mid-\ndle section of the resized image). The resized images are\nprocessed by two convolutional layers. Each convolutional\nlayer is also followed by max-pooling layers and normaliza-\ntion layers. The first convolutional layer has 96 kernels of\nsize 11 × 11 × 3 with a stride of 4 pixels. The second con-\nvolutional layer has 256 kernels of size 5 × 5 with a stride\nof 2 pixels. Furthermore, we have four fully connected lay-\ners. Inspired by (C¸aglar Gu¨lc¸ehre et al. 2013), we constrain\nthe second to last fully connected layer to have 24 neurons.\nAccording to the Plutchik’s wheel of emotions (Plutchik\n1984), there are a total of 24 emotions belonging to two cate-\ngories: positive emotions and negative emotions. Intuitively.\nwe hope these 24 nodes may help the network to learn the 24\nemotions from a given image and then classify each image\ninto positive or negative class according to the responses of\nthese 24 emotions.\nThe last layer is designed to learn the parameter w by\nmaximizing the following conditional log likelihood func-\ntion (xi and yi are the feature vector and label for the i-th\ninstance respectively):\nl(w) =\nn∑\ni=1\nln p(yi = 1|xi, w) + (1− yi) ln p(yi = 0|xi, w)\n(1)\nwhere\np(yi|xi, w) =\nexp(w0 +\n∑k\nj=1 wjxij)\nyi\n1 + exp(w0 +\n∑k\nj=1 wjxij)\nyi\n(2)\n... ...\n... f(·)PredictCNN\nPCNN\n1) Input\nTrain convolutional Neural Network\n2) CNN model\n3) 4) Sampling\n5) Fine-tune\n6) PCNN model\nFigure 3: Progressive CNN (PCNN) for visual sentiment analysis.\nVisual Sentiment Analysis with Progressive CNN\nSince the images are weakly labeled, it is possible that the\nneural network can get stuck in a bad local optimum. This\nmay lead to poor generalizability of the trained neural net-\nwork. On the other hand, we found that the neural network\nis still able to correctly classify a large proportion of the\ntraining instances. In other words, the neural network has\nlearned knowledge to distinguish the training instances with\nrelatively distinct sentiment labels. Therefore, we propose to\nprogressively select a subset of the training instances to re-\nduce the impact of noisy training instances. Figure 3 shows\nthe overall flow of the proposed progressive CNN (PCNN).\nWe first train a CNN on Flickr images. Next, we select train-\ning samples according to the prediction score of the trained\nmodel on the training data itself. Instead of training from the\nbeginning, we further fine-tune the trained model using these\nnewly selected, and potentially cleaner training instances.\nThis fine-tuned model will be our final model for visual sen-\ntiment analysis.\nAlgorithm 1 Progressive CNN training for Visual Sentiment\nAnalysis\nInput: X = {x1, x2, . . . , xn} a set of images of size 256×\n256\nY = {y1, y2, . . . , yn} sentiment labels of X\n1: Train convolutional neural network CNN with input X\nand Y\n2: Let S ∈ Rn×2 be the sentiment scores of X predicted\nusing CNN\n3: for si ∈ S do\n4: Delete xi from X with probability pi (Eqn.(3))\n5: end for\n6: Let X ′ ⊂ X be the remaining training images, Y ′ be\ntheir sentiment labels\n7: Fine-tune CNN with input X ′ and Y ′ to get PCNN\n8: return PCNN\nIn particular, we employ a probabilistic sampling algo-\nrithm to select the new training subset. The intuition is that\nwe want to keep instances with distinct sentiment scores\nbetween the two classes with a high probability, and con-\nversely remove instances with similar sentiment scores for\nboth classes with a high probability. Let si = (si1, si2) be\nthe prediction sentiment scores for the two classes of in-\nstance i. We choose to remove the training instance i with\nprobability pi given by Eqn.(3). Algorithm 1 summarizes the\nsteps of the proposed framework.\npi = max (0, 2− exp(|si1 − si2|)) (3)\nWhen the difference between the predicted sentiment scores\nof one training instance are large enough, this training in-\nstance will be kept in the training set. Otherwise, the smaller\nthe difference between the predicted sentiment scores be-\ncome, the larger the probability of this instance being re-\nmoved from the training set.\nExperiments\nWe choose to use the same half million Flickr images\nfrom SentiBank1 to train our Convolutional Neural Network.\nThese images are only weakly labeled since each image be-\nlongs to one adjective noun pair (ANP). There are a total\nof 1200 ANPs. According to the Plutchik’s Wheel of Emo-\ntions (Plutchik 1984), each ANP is generated by the combi-\nnation of adjectives with strong sentiment values and nouns\nfrom tags of images and videos (Borth et al. 2013b). These\nANPs are then used as queries to collect related images\nfor each ANP. The released SentiBank contains 1200 ANPs\nwith about half million Flickr images. We train our convolu-\ntional neural network mainly on this image dataset. We im-\nplement the proposed architecture of CNN on the publicly\navailable implementation Caffe (Jia 2013). All of our exper-\niments are evaluated on a Linux X86 64 machine with 32G\nRAM and two NVIDIA GTX Titan GPUs.\nComparisons of different CNN architectures\nThe architecture of our model is shown in Figure 2. How-\never, we also evaluate other architectures for the visual sen-\ntiment analysis task. Table 1 summarizes the performance\nof different architectures on a randomly chosen Flickr test-\ning dataset. In Table 1, iCONV-jFC indicates that there are\n1http://visual-sentiment-ontology.appspot.com/\ni convolutional layers and j fully connected layers in the ar-\nchitecture. The model in Figure 2 shows slightly better per-\nformance than other models in terms of F1 and accuracy. In\nthe following experiments, we mainly focus on the evalua-\ntion of CNN using the architecture in Figure 2.\nTable 1: Summary of performance of different architectures\non randomly chosen testing data.\nArchitecture Precision Recall F1 Accuracy\n3CONV-4FC 0.679 0.845 0.753 0.644\n3CONV-2FC 0.69 0.847 0.76 0.657\n2CONV-3FC 0.679 0.874 0.765 0.654\n2CONV-4FC 0.688 0.875 0.77 0.665\nBaselines\nWe compare the performance of PCNN with three other\nbaselines or competing algorithms for image sentiment clas-\nsification.\nLow-level Feature-based Siersdorfer et al. (2010) defined\nboth global and local visual features. Specifically, the global\ncolor histograms (GCH) features consist of 64-bin RGB his-\ntogram. The local color histogram features (LCH) first di-\nvided the image into 16 blocks and used the 64-bin RGB\nhistogram for each block. They also employed SIFT features\nto learn a visual word dictionary. Next, they defined bag of\nvisual word features (BoW) for each image.\nMid-level Feature-based Damian et al. (2013a; 2013b)\nproposed a framework to build visual sentiment ontology\nand SentiBank according to the previously discussed 1200\nANPs. With the trained 1200 ANP detectors, they are able\nto generate 1200 responses for any given test image using\nthese pre-trained 1200 ANP detectors. A sentiment classifier\nis built on top of these mid-level features according to the\nsentiment label of training images. Sentribute (Yuan et al.\n2013) also employed mid-level features for sentiment pre-\ndiction. However, instead of using adjective noun pairs, they\nemployed scene-based attributes (Patterson and Hays 2012)\nto define the mid-level features.\nDeep Learning on Flickr Dataset\nWe randomly choose 90% images from the half million\nFlickr images as our training dataset. The remaining 10%\nimages are our testing dataset. We train the convolutional\nneural network with 300,000 iterations of mini-batches\n(each mini-batch contains 256 images). We employ the sam-\npling probability in Eqn.(3) to filter the training images ac-\ncording to the prediction score of CNN on its training data.\nIn the fine-tuning stage of PCNN, we run another 100,000\niterations of mini-batches using the filtered training dataset.\nTable 2 gives a summary of the number of data instances in\nour experiments. Figure 4 shows the filters learned in the\nfirst convolutional layer of CNN and PCNN, respectively.\nThere are some differences between 4(a) and 4(b). While\nit is somewhat inconclusive that the neural networks have\nreached a better local optimum, at least we can conclude that\nthe fine-tuning stage using a progressively cleaner training\nTable 2: Statistics of the number of Flickr image dataset.\nModels training testing # of iterations\nCNN 401,739 44,637 300,000\nPCNN 369,828 44,637 100,000\nTable 3: Performance on the Testing Dataset by CNN and\nPCNN.\nAlgorithm Precision Recall F1 Accuracy\nCNN 0.714 0.729 0.722 0.718\nPCNN 0.759 0.826 0.791 0.781\ndataset has prompted the neural networks to learn different\nknowledge. Indeed, the evaluation results suggest that this\nfine-tuning leads to the improvement of performance.\nTable 3 shows the performance of both CNN and PCNN\non the 10% randomly chosen testing data. PCNN outper-\nformed CNN in terms of Precision, Recall, F1 and Accu-\nracy. The results in Table 3 and the filters from Figure 4\nshows that the fine-tuning stage of PCNN can help the neu-\nral network to search for a better local optimum.\n(a) Filters learned from CNN\n(b) Filters learned from PCNN\nFigure 4: Filters of the first convolutional layer.\nTwitter Testing Dataset\nWe also built a new image dataset from image tweets. Im-\nage tweets refer to those tweets that contain images. We\nbuilt a total of 1269 images as our candidate testing im-\nages. We employed crowd intelligence, Amazon Mechani-\ncal Turk (AMT), to generate sentiment labels for these test-\ning images, in a similar fashion to (Borth et al. 2013b). We\nrecruited 5 AMT workers for each of the candidate image.\nTable 4 shows the statistics of the labeling results from the\nAmazon Mechanical Turk. In the table, “five agree” indi-\ncates that all the 5 AMT workers gave the same sentiment\nlabel for a given image. Only a small portion of the images,\n153 out of 1269, had significant disagreements between the\nTable 5: Performance of different algorithms on the Twitter image dataset (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc\nCNN 0.749 0.869 0.805 0.722 0.707 0.839 0.768 0.686 0.691 0.814 0.747 0.667\nPCNN 0.77 0.878 0.821 0.747 0.733 0.845 0.785 0.714 0.714 0.806 0.757 0.687\nTable 4: Summary of AMT labeled results for the Twitter\ntesting dataset.\nSentiment Five Agree At Least FourAgree\nAt Least\nThree Agree\nPositive 581 689 769\nNegative 301 427 500\nSum 882 1116 1269\n5 workers (3 vs. 2). We evaluate the performance of Con-\nvolutional Neural Networks on this manually labeled image\ndataset according to the model trained on Flickr images. Ta-\nble 5 shows the performance of the two frameworks. Not\nsurprisingly, both models perform better on the less ambigu-\nous image set (“five agree” by AMT). Meanwhile, PCNN\nshows better performance than CNN on all the three label-\ning sets in terms of both F1 and accuracy. This suggests that\nthe fine-tuning stage of CNN effectively improves the gen-\neralizability extensibility of the neural networks.\nTransfer Learning\nHalf million Flickr images are used in our CNN training.\nThe features learned are generic features on these half mil-\nlion images. Table 5 shows that these generic features also\nhave the ability to predict visual sentiment of images from\nother domains. The question we ask is whether we can fur-\nther improve the performance of visual sentiment analysis\non Twitter images by inducing transfer learning. In this sec-\ntion, we conduct experiments to answer this question.\nThe users of Flickr are more likely to spend more time\non taking high quality pictures. Twitter users are likely to\nshare the moment with the world. Thus, most of the Twitter\nimages are casually taken snapshots. Meanwhile, most of the\nimages are related to current trending topics and personal\nexperiences, making the images on Twitter much diverse in\ncontent as well as quality.\nIn this experiment, we fine-tune the pre-trained neural net-\nwork model in the following way to achieve transfer learn-\ning. We randomly divide the Twitter images into 5 equal par-\ntitions. Every time, we use 4 of the 5 partitions to fine-tune\nour pre-trained model from the half million Flickr images\nand evaluate the new model on the remaining partition. The\naveraged evaluation results are reported. The algorithm is\ndetailed in Algorithm 2.\nSimilar to (Borth et al. 2013b), we also employ 5-fold\ncross-validation to evaluate the performance of all the base-\nline algorithms. Table 6 summarizes the averaged perfor-\nmance results of different baseline algorithms and our two\nCNN models. Overall, both CNN models outperform the\nbaseline algorithms. In the baseline algorithms, Sentribute\ngives slightly better results than the other two baseline al-\nFigure 5: Positive (top block) and Negative (bottom block)\nexamples. Each column shows the negative example im-\nages for each algorithm (PCNN, CNN, Sentribute, Sen-\ntibank, GCH, LCH, GCH+BoW, LCH+BoW). The images\nare ranked by the prediction score from top to bottom in a\ndecreasing order.\nAlgorithm 2 Transfer Learning to fine-tune CNN\nInput: X = {x1, x2, . . . , xn} a set of images of size 256×\n256\nY = {y1, y2, . . . , yn} sentiment labels of X\nPre-trained CNN model M\n1: Randomly partition X and Y into 5 equal groups\n{(X1, Y1), . . . , (X5, Y5)}.\n2: for i from 1 to 5 do\n3: Let (X ′, Y ′) = (X,Y )− (Xi, Yi)\n4: Fine-tune M with input (X ′, Y ′) to obtain model Mi\n5: Evaluate the performance of Mi on (Xi, Yi)\n6: end for\n7: return The averaged performance of Mi on (Xi, Yi) (i\nfrom 1 to 5)\ngorithms. Interestingly, even the combination of using low-\nTable 6: 5-Fold Cross-Validation Performance of different algorithms on the Twitter image dataset. Note that compared with\nTable 5, both fine-tuned CNN models have been improved due to domain transfer learning (Acc stands for Accuracy).\nAlgorithms Five Agree At Least Four Agree At Least Three AgreePrecision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc\nGCH 0.708 0.888 0.787 0.684 0.687 0.84 0.756 0.665 0.678 0.836 0.749 0.66\nLCH 0.764 0.809 0.786 0.71 0.725 0.753 0.739 0.671 0.716 0.737 0.726 0.664\nGCH + BoW 0.724 0.904 0.804 0.71 0.703 0.849 0.769 0.685 0.683 0.835 0.751 0.665\nLCH + BoW 0.771 0.811 0.79 0.717 0.751 0.762 0.756 0.697 0.722 0.726 0.723 0.664\nSentiBank 0.785 0.768 0.776 0.709 0.742 0.727 0.734 0.675 0.720 0.723 0.721 0.662\nSentribute 0.789 0.823 0.805 0.738 0.75 0.792 0.771 0.709 0.733 0.783 0.757 0.696\nCNN 0.795 0.905 0.846 0.783 0.773 0.855 0.811 0.755 0.734 0.832 0.779 0.715\nPCNN 0.797 0.881 0.836 0.773 0.786 0.842 0.811 0.759 0.755 0.805 0.778 0.723\nlevel features local color histogram (LCH) and bag of visual\nwords (BoW) shows better results than SentiBank on our\nTwitter dataset. Both fine-tuned CNN models have been im-\nproved. This improvement is significant given that we only\nuse four fifth of the 1269 images for domain adaptation.\nBoth neural network models have similar performance on\nall the three sets of the Twitter testing data. This suggests\nthat the fine-tuning stage helps both models to find a better\nlocal minimum. In particular, the knowledge from the Twit-\nter images starts to determine the performance of both neural\nnetworks. The previously trained model only determines the\nstart position of the fine-tuned model.\nMeanwhile, for each model, we respectively select the top\n5 positive and top 5 negative examples from the 1269 Twit-\nter images according to the evaluation scores. Figure show\nthose examples for each model. In both figures, each column\ncontains the images for one model. A green solid box means\nthe prediction label of the image agrees with the human la-\nbel. Otherwise, we use a red dashed box. The labels of top\nranked images in both neural network models are all cor-\nrectly predicted. However, the images are not all the same.\nThis on the other hand suggests that even though the two\nmodels achieve similar results after fine-tuning, they may\nhave arrived at somewhat different local optima due to the\ndifferent starting positions, as well as the transfer learning\nprocess. For all the baseline models, it is difficult to say\nwhich kind of images are more likely to be correctly clas-\nsified according to these images. However, we observe that\nthere are several mistakenly classified images in common\namong the models using low-level features (the four right-\nmost columns in Figure ). Similarly, for Sentibank and Sen-\ntribute, several of the same images are also in the top ranked\nsamples. This indicates that there are some common learned\nknowledge in the low-level feature models and mid-level\nfeature models.\nConclusions\nVisual sentiment analysis is a challenging and interesting\nproblem. In this paper, we adopt the recent developed con-\nvolutional neural networks to solve this problem. We have\ndesigned a new architecture, as well as new training strate-\ngies to overcome the noisy nature of the large-scale train-\ning samples. Both progressive training and transfer learning\ninducted by a small number of confidently labeled images\nfrom the target domain have yielded notable improvements.\nThe experimental results suggest that convolutional neural\nnetworks that are properly trained can outperform both clas-\nsifiers that use predefined low-level features or mid-level vi-\nsual attributes for the highly challenging problem of visual\nsentiment analysis. Meanwhile, the main advantage of us-\ning convolutional neural networks is that we can transfer\nthe knowledge to other domains using a much simpler fine-\ntuning technique than those in the literature e.g., (Duan et al.\n2012).\nIt is important to reiterate the significance of this work\nover the state-of-the-art (Siersdorfer et al. 2010; Borth et al.\n2013b; Yuan et al. 2013). We are able to directly leverage\na much larger weakly labeled data set for training, as well\nas a larger manually labeled dataset for testing. The larger\ndata sets, along with the proposed deep CNN and its training\nstrategies, give rise to better generalizability of the trained\nmodel and higher confidence of such generalizability. In the\nfuture, we plan to develop robust multimodality models that\nemploy both the textual and visual content for social me-\ndia sentiment analysis. We also hope our sentiment analysis\nresults can encourage further research on online user gener-\nated content.\nWe believe that sentiment analysis on large scale online\nuser generated content is quite useful since it can provide\nmore robust signals and information for many data analytics\ntasks, such as using social media for prediction and forecast-\ning. In the future, we plan to develop robust multimodal-\nity models that employ both the textual and visual content\nfor social media sentiment analysis. We also hope our senti-\nment analysis results can encourage further research on on-\nline user generated content.\nAcknowledgments\nThis work was generously supported in part by Adobe Re-\nsearch. We would like to thank Digital Video and Multime-\ndia (DVMM) Lab at Columbia University for providing the\nhalf million Flickr images and their machine-generated la-\nbels.\nReferences\n[Asur and Huberman 2010] Asur, S., and Huberman, B. A.\n2010. Predicting the future with social media. In WI-IAT,\nvolume 1, 492–499. IEEE.\n[Bengio 2012] Bengio, Y. 2012. Practical recommendations\nfor gradient-based training of deep architectures. In Neural\nNetworks: Tricks of the Trade. Springer. 437–478.\n[Bollen, Mao, and Pepe 2011] Bollen, J.; Mao, H.; and Pepe,\nA. 2011. Modeling public mood and emotion: Twitter sen-\ntiment and socio-economic phenomena. In ICWSM.\n[Bollen, Mao, and Zeng 2011] Bollen, J.; Mao, H.; and\nZeng, X. 2011. Twitter mood predicts the stock market.\nJournal of Computational Science 2(1):1–8.\n[Borth et al. 2013a] Borth, D.; Chen, T.; Ji, R.; and Chang,\nS.-F. 2013a. Sentibank: large-scale ontology and classifiers\nfor detecting sentiment and emotions in visual content. In\nACM MM, 459–460. ACM.\n[Borth et al. 2013b] Borth, D.; Ji, R.; Chen, T.; Breuel, T.;\nand Chang, S.-F. 2013b. Large-scale visual sentiment ontol-\nogy and detectors using adjective noun pairs. In ACM MM,\n223–232. ACM.\n[C¸aglar Gu¨lc¸ehre et al. 2013] C¸aglar Gu¨lc¸ehre; Cho, K.; Pas-\ncanu, R.; and Bengio, Y. 2013. Learned-norm pooling for\ndeep neural networks. CoRR abs/1311.1780.\n[Cires¸an et al. 2011] Cires¸an, D. C.; Meier, U.; Masci, J.;\nGambardella, L. M.; and Schmidhuber, J. 2011. Flexible,\nhigh performance convolutional neural networks for image\nclassification. In IJCAI, 1237–1242. AAAI Press.\n[Davidov, Tsur, and Rappoport 2010] Davidov, D.; Tsur, O.;\nand Rappoport, A. 2010. Enhanced sentiment learning using\ntwitter hashtags and smileys. In ICL, 241–249. Association\nfor Computational Linguistics.\n[Duan et al. 2012] Duan, L.; Xu, D.; Tsang, I.-H.; and Luo,\nJ. 2012. Visual event recognition in videos by learning from\nweb data. IEEE PAMI 34(9):1667–1680.\n[Grangier, Bottou, and Collobert 2009] Grangier, D.; Bot-\ntou, L.; and Collobert, R. 2009. Deep convolutional net-\nworks for scene parsing. In ICML 2009 Deep Learning\nWorkshop, volume 3. Citeseer.\n[Hamel and Eck 2010] Hamel, P., and Eck, D. 2010. Learn-\ning features from music audio with deep belief networks. In\nISMIR, 339–344.\n[Hinton, Osindero, and Teh 2006] Hinton, G. E.; Osindero,\nS.; and Teh, Y.-W. 2006. A fast learning algorithm for deep\nbelief nets. Neural computation 18(7):1527–1554.\n[Hinton 2010] Hinton, G. 2010. A practical guide to training\nrestricted boltzmann machines. Momentum 9(1):926.\n[Hu et al. 2013] Hu, X.; Tang, J.; Gao, H.; and Liu, H. 2013.\nUnsupervised sentiment analysis with emotional signals. In\nWWW, 607–618. International World Wide Web Confer-\nences Steering Committee.\n[Jia 2013] Jia, Y. 2013. Caffe: An open source convolutional\narchitecture for fast feature embedding. http://caffe.\nberkeleyvision.org/.\n[Jin et al. 2010] Jin, X.; Gallagher, A.; Cao, L.; Luo, J.; and\nHan, J. 2010. The wisdom of social multimedia: using flickr\nfor prediction and forecast. In ACMMM, 1235–1244. ACM.\n[Joshi et al. 2011] Joshi, D.; Datta, R.; Fedorovskaya, E.; Lu-\nong, Q.-T.; Wang, J. Z.; Li, J.; and Luo, J. 2011. Aesthetics\nand emotions in images. IEEE Signal Processing Magazine\n28(5):94–115.\n[Kavukcuoglu et al. 2010] Kavukcuoglu, K.; Sermanet, P.;\nBoureau, Y.-L.; Gregor, K.; Mathieu, M.; and LeCun, Y.\n2010. Learning convolutional feature hierarchies for visual\nrecognition. In NIPS, 5.\n[Krizhevsky, Sutskever, and Hinton 2012] Krizhevsky, A.;\nSutskever, I.; and Hinton, G. E. 2012. Imagenet classifi-\ncation with deep convolutional neural networks. In NIPS,\n4.\n[LeCun et al. 1989] LeCun, Y.; Boser, B.; Denker, J. S.; Hen-\nderson, D.; Howard, R. E.; Hubbard, W.; and Jackel, L. D.\n1989. Backpropagation applied to handwritten zip code\nrecognition. Neural computation 1(4):541–551.\n[LeCun et al. 1998] LeCun, Y.; Bottou, L.; Bengio, Y.; and\nHaffner, P. 1998. Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE 86(11):2278–\n2324.\n[LeCun, Kavukcuoglu, and Farabet 2010] LeCun, Y.;\nKavukcuoglu, K.; and Farabet, C. 2010. Convolutional\nnetworks and applications in vision. In ISCAS, 253–256.\nIEEE.\n[Li et al. 2010] Li, G.; Hoi, S. C.; Chang, K.; and Jain, R.\n2010. Micro-blogging sentiment detection by collaborative\nonline learning. In ICDM, 893–898. IEEE.\n[Liu et al. 2003] Liu, B.; Dai, Y.; Li, X.; Lee, W. S.; and Yu,\nP. S. 2003. Building text classifiers using positive and unla-\nbeled examples. In ICDM, 179–186. IEEE.\n[Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.;\nHuang, D.; Ng, A. Y.; and Potts, C. 2011. Learning word\nvectors for sentiment analysis. In ACL, 142–150.\n[Morency, Mihalcea, and Doshi 2011] Morency, L.-P.; Mi-\nhalcea, R.; and Doshi, P. 2011. Towards multimodal senti-\nment analysis: Harvesting opinions from the web. In ICMI,\n169–176. New York, NY, USA: ACM.\n[O’Connor et al. 2010] O’Connor, B.; Balasubramanyan, R.;\nRoutledge, B. R.; and Smith, N. A. 2010. From tweets to\npolls: Linking text sentiment to public opinion time series.\nICWSM 11:122–129.\n[Pang and Lee 2008] Pang, B., and Lee, L. 2008. Opinion\nmining and sentiment analysis. Foundations and trends in\ninformation retrieval 2(1-2):1–135.\n[Patterson and Hays 2012] Patterson, G., and Hays, J. 2012.\nSun attribute database: Discovering, annotating, and recog-\nnizing scene attributes. In CVPR.\n[Plutchik 1984] Plutchik, R. 1984. Emotions: A general psy-\nchoevolutionary theory. Approaches to emotion 1984:197–\n219.\n[Siersdorfer et al. 2010] Siersdorfer, S.; Minack, E.; Deng,\nF.; and Hare, J. 2010. Analyzing and predicting sentiment\nof images on the social web. In ACM MM, 715–718. ACM.\n[Tumasjan et al. 2010] Tumasjan, A.; Sprenger, T. O.; Sand-\nner, P. G.; and Welpe, I. M. 2010. Predicting elections\nwith twitter: What 140 characters reveal about political sen-\ntiment. ICWSM 178–185.\n[Yuan et al. 2013] Yuan, J.; Mcdonough, S.; You, Q.; and\nLuo, J. 2013. Sentribute: image sentiment analysis from\na mid-level perspective. In Proceedings of the Second In-\nternational Workshop on Issues of Sentiment Discovery and\nOpinion Mining, 10. ACM.\n[Zhang, Fuehres, and Gloor 2011] Zhang, X.; Fuehres, H.;\nand Gloor, P. A. 2011. Predicting stock market indicators\nthrough twitter i hope it is not as bad as i fear. Procedia-\nSocial and Behavioral Sciences 26:55–62.\n",
      "id": 24729442,
      "identifiers": [
        {
          "identifier": "10.1609/aaai.v29i1.9179",
          "type": "DOI"
        },
        {
          "identifier": "386117133",
          "type": "CORE_ID"
        },
        {
          "identifier": "103268817",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/9179",
          "type": "OAI_ID"
        },
        {
          "identifier": "42637726",
          "type": "CORE_ID"
        },
        {
          "identifier": "1509.06041",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.687.4407",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1509.06041",
          "type": "OAI_ID"
        }
      ],
      "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1509.06041",
        "oai:citeseerx.psu:10.1.1.687.4407",
        "oai:ojs.aaai.org:article/9179"
      ],
      "publishedDate": "2015-02-09T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://www.cs.rochester.edu/u/qyou/papers/sentiment_analysis_final.pdf",
        "http://arxiv.org/abs/1509.06041"
      ],
      "updatedDate": "2024-02-09T07:01:46",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1509.06041"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24729442"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Alani, Harith"
        },
        {
          "name": "Fernández, Miriam"
        },
        {
          "name": "He, Yulan"
        },
        {
          "name": "Saif, Hassan"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/20667326",
        "https://api.core.ac.uk/v3/outputs/102833506"
      ],
      "createdDate": "2014-10-06T20:14:35",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 86,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/86",
          "logo": "https://api.core.ac.uk/data-providers/86/logo"
        }
      ],
      "depositedDate": "2014-08-05T15:30:00",
      "abstract": "Sentiment analysis over Twitter offers organisations and individuals a fast and effective way to monitor the publics' feelings towards them and their competitors. To assess the performance of sentiment analysis methods over Twitter a small set of evaluation datasets have been released in the last few years. In this paper we present an overview of eight publicly available and manually annotated evaluation datasets for Twitter sentiment analysis. Based on this review, we show that a common limitation of most of these datasets, when assessing sentiment analysis at target (entity) level, is the lack of distinctive sentiment annotations among the tweets and the entities contained in them. For example, the tweet \"I love iPhone, but I hate iPad\" can be annotated with a mixed sentiment label, but the entity iPhone within this tweet should be annotated with a positive sentiment label. Aiming to overcome this limitation, and to complement current evaluation datasets, we present STS-Gold, a new evaluation dataset where tweets and targets (entities) are annotated individually and therefore may present different sentiment labels. This paper also provides a comparative study of the various datasets along several dimensions including: total number of tweets, vocabulary size and sparsity. We also investigate the pair-wise correlation among these dimensions as well as their correlations to the sentiment classification performance on different datasets",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/20667326.pdf",
      "fieldOfStudy": null,
      "fullText": "Open Research OnlineThe Open University’s repository of research publicationsand other research outputsEvaluation datasets for Twitter sentiment analysis: asurvey and a new dataset, the STS-GoldConference or Workshop ItemHow to cite:Saif, Hassan; Ferna´ndez, Miriam; He, Yulan and Alani, Harith (2013). Evaluation datasets for Twitter sentimentanalysis: a survey and a new dataset, the STS-Gold. In: 1st Interantional Workshop on Emotion and Sentiment inSocial and Expressive Media: Approaches and Perspectives from AI (ESSEM 2013), 3 Dec 2013, Turin, Italy.For guidance on citations see FAQs.c© [not recorded]Version: Accepted ManuscriptLink(s) to article on publisher’s website:http://www.di.unito.it/ patti/essem13/index.htmlCopyright and Moral Rights for the articles on this site are retained by the individual authors and/or other copyrightowners. For more information on Open Research Online’s data policy on reuse of materials please consult the policiespage.oro.open.ac.ukEvaluation Datasets forTwitter Sentiment AnalysisA survey and a new dataset, the STS-GoldHassan Saif1, Miriam Fernandez1, Yulan He2 and Harith Alani11 Knowledge Media Institute, The Open University, United Kingdom{h.saif, m.fernandez, h.alani}@open.ac.uk2 School of Engineering and Applied Science, Aston University, UKy.he@cantab.netAbstract. Sentiment analysis over Twitter offers organisations and indi-viduals a fast and effective way to monitor the publics’ feelings towardsthem and their competitors. To assess the performance of sentimentanalysis methods over Twitter a small set of evaluation datasets havebeen released in the last few years. In this paper we present an overviewof eight publicly available and manually annotated evaluation datasets forTwitter sentiment analysis. Based on this review, we show that a commonlimitation of most of these datasets, when assessing sentiment analysisat target (entity) level, is the lack of distinctive sentiment annotationsamong the tweets and the entities contained in them. For example, thetweet “I love iPhone, but I hate iPad” can be annotated with a mixedsentiment label, but the entity iPhone within this tweet should be anno-tated with a positive sentiment label. Aiming to overcome this limitation,and to complement current evaluation datasets, we present STS-Gold, anew evaluation dataset where tweets and targets (entities) are annotatedindividually and therefore may present different sentiment labels. Thispaper also provides a comparative study of the various datasets alongseveral dimensions including: total number of tweets, vocabulary sizeand sparsity. We also investigate the pair-wise correlation among thesedimensions as well as their correlations to the sentiment classificationperformance on different datasets.Keywords: Sentiment Analysis, Twitter, Datasets1 IntroductionWith the emergence of social media, the performance of sentiment analysis toolshas become increasingly critical. In the current commercial competition, designers,developers, vendors and sales representatives of new information products need tocarefully study whether and how do their products offer competitive advantages.Twitter, with over 500 million registered users and over 400 million messages perday,3 has become a gold mine for organisations to monitor their reputation and3 http://www.alexa.com/topsitesbrands by extracting and analysing the sentiment of the tweets posted by thepublic about them, their markets, and competitors.Developing accurate sentiment analysis methods requires the creation ofevaluation datasets that can be used to assess their performances. In the last fewyears several evaluation datasets for Twitter sentiment analysis have been madepublicly available. The general evaluation dataset consists of a set of tweets, whereeach tweet is annotated with a sentiment label [1,8,16,22]. The most commonsentiment labels are positive, negative and neutral, but some evaluation datasetsconsider additional sentiment labels such as mixed, other or irrelevant [1,23].Instead of the final sentiment labels associated to the tweets, some datasetsprovide a numeric sentiment strength between -5 and 5 defining a range fromnegative to positive polarity [24,25]. In addition to sentiment labels associated tothe tweets some evaluation datasets also provide sentiment labels associated totargets (entities) within the tweets. However, these datasets do not distinguishbetween the sentiment label of the tweet and the sentiment labels of the entitiescontained within it [23]. For example, the tweet “iPhone 5 is awesome, but Ican’t upgrade :(” presents a negative sentiment. However, the entity “iPhone5” should receive a positive sentiment.Aiming to overcome this limitation, we present STS-Gold, an evaluationdataset for Twitter sentiment analysis that targets sentiment annotation atboth, tweet and entity levels. The annotation process allows a dissimilar polarityannotation between the tweet and the entities contained within it. To create thisdataset a subset of tweets was selected from the Standford Twitter SentimentCorpus [8] and entities were extracted from this subset of tweets by using athird-party entity extraction tool. Tweets and entities were manually annotatedby three different human evaluators. The final evaluation dataset contains 2,206tweets and 58 entities with associated sentiment labels. The purpose of this datasetis therefore to complement current state of the art datasets by providing entitysentiment labels, therefore supporting the evaluation of sentiment classificationmodels at entity as well as tweet level.Along with the description of the STS-Gold dataset, this paper summariseseight publicly available and manually annotated evaluation datasets for Twittersentiment analysis. Our goal is to provide the reader with an overview of theexisting evaluation datasets and their characteristics. To this aim, we providea comparison of these datasets along different dimensions including: the totalnumber of tweets, the vocabulary size and the degree of data sparsity. We alsoinvestigate the pair-wise correlation among these dimensions as well as theircorrelations to the sentiment classification performance on all datasets. Ourstudy shows that the correlation between the sparsity and the classificationperformance is intrinsic, meaning that it might exists within the dataset itself,but not necessarily across the datasets. We also show that the correlationsbetween sparsity, vocabulary size and number of tweets are all strong. However,the large number of tweets in a dataset is not always an indication for a largevocabulary size or a high sparsity degree.The rest of the paper is structured as follows: Section2 presents an overview ofthe existing evaluation datasets for Twitter sentiment analysis. Section 3 describesSTS-Gold, our proposed evaluation dataset. Section 4 presents a comparisonstudy across the evaluation datasets. We conclude the paper in Section 5.2 Twitter Sentiment Analysis DatasetsIn this section we present 8 different datasets widely used in the Twitter senti-ment analysis literature. We have focused our selection on those datasets thatare: (i) publicly available to the research community, (ii) manually annotated,providing a reliable set of judgements over the tweets and, (iii) used to evaluateseveral sentiment sentiment analysis models. Tweets in these datasets have beenannotated with different sentiment labels including: Negative, Neutral, Positive,Mixed, Other and Irrelevant. Table 1 displays the distribution of tweets in theeight selected datasets according to these sentiment labels.Variations of the evaluation datasets are due to the particularities of thedifferent sentiment analysis tasks. Sentiment analysis on Twitter spans multipletasks, such as polarity detection (positive vs. negative), subjectivity detection(polar vs. neutral) or sentiment strength detection. These tasks can also beperformed either at tweet level or at target (entity) level. In the followingsubsections, we provide an overview of the available evaluation datasets and thedifferent sentiment tasks for which they are used.Dataset No. of Tweets #Negative #Neutral #Positive #Mixed #Other #IrrelevantSTS-Test 498 177 139 182 - - -HCR 2,516 1,381 470 541 - 45 79OMD 3,238 1,196 - 710 245 1,087 -SS-Twitter 4,242 1,037 1,953 1,252 - - -Sanders 5,513 654 2,503 570 - - 1,786GASP 12,771 5,235 6,268 1,050 - 218 -WAB 13,340 2,580 3,707 2,915 - 420 3,718SemEval 13,975 2,186 6,440 5,349 - - -Table 1. Total number of tweets and the tweet sentiment distribution in all datasetsStanford Twitter Sentiment Test Set (STS-Test)The Stanford Twitter sentiment corpus (http://help.sentiment140.com/),introduced by Go et al. [8] consists of two different sets, training and test. Thetraining set contains 1.6 million tweets automatically labelled as positive ornegative based on emotions. For example, a tweet is labelled as positive if itcontains :), :-), : ), :D, or =) and is labelled as negative if it contains :(, :-(, or : (.Although automatic sentiment annotation of tweets using emoticons is fast, itsaccuracy is arguable because emoticons might not reflect the actual sentimentof tweets. In this study, we focus on those datasets that have been manuallyannotated. Therefore, although we acknowledge the relevance of the STS trainingdataset for building sentiment analysis models, we discard it from the rest of ourstudy.The test set (STS-Test), on the other hand, is manually annotated andcontains 177 negative, 182 positive and 139 neutrals tweets. These tweets werecollected by searching Twitter API with specific queries including names ofproducts, companies and people. Although the STS-Test dataset is relativelysmall, it has been widely used in the literature in different evaluation tasks.For example, Go et al. [8], Saif et al. [19,20], Speriosu et al. [23], and Bakliwalet al. [2] use it to evaluate their models for polarity classification (positive vs.negative). In addition to polarity classification, Marquez et al. [3] use this datasetfor evaluating subjectivity classification (neutral vs. polar).Health Care Reform (HCR)The Health Care Reform (HCR) dataset was built by crawling tweets containingthe hashtag “#hcr” (health care reform) in March 2010 [23]. A subset of thiscorpus was manually annotated by the authors with 5 labels (positive, negative,neutral, irrelevant, unsure(other)) and split into training (839 tweets), develop-ment (838 tweets) and test (839 tweets) sets. The authors also assigned sentimentlabels to 8 different targets extracted from all the three sets (Health Care Re-form, Obama, Democrats, Republicans, Tea Party, Conservatives, Liberals, andStupak). However, both the tweet and the targets within it, were assigned thesame sentiment label, as can be found in the published version of this dataset(https://bitbucket.org/speriosu/updown). In this paper, we consider all thethree subsets (training, development and test) as one unique dataset for theanalysis (see Section 4). The final datasets, as shown in Table 1, consists of 2,516tweets including 1,381 negative, 470 neutral and 541 positive tweets.The HCR dataset has been used to evaluate polarity classification [23,21] butcan also be used to evaluate subjectivity classification since it identifies neutraltweets.Obama-McCain Debate (OMD)The Obama-McCain Debate (OMD) dataset was constructed from 3,238 tweetscrawled during the first U.S. presidential TV debate in September 2008 [22].Sentiment labels were acquired for these tweets using Amazon Mechanical Turk,where each tweet was rated by at least three annotators as either positive,negative, mixed, or other. The authors in [6] reported an inter-annotator agreementof 0.655, which shows a relatively good agreement between annotators. Thedataset is provided at https://bitbucket.org/speriosu/updown along withthe annotators’ votes on each tweet. We considered those sentiment labels, whichtwo-third of the voters agree on, as final labels of the tweets. This resulted in aset of 1,196 negative, 710 positive and 245 mixed tweets.The OMD dataset is a popular dataset, which has been used to evaluatevarious supervised learning methods [10,23,21], as well as unsupervised methods[9] for polarity classification of tweets. Tweets’ sentiments in this dataset werealso used to characterize the Obama-McCain debate event in 2008 [6].Sentiment Strength Twitter Dataset (SS-Tweet)This dataset consists of 4,242 tweets manually labelled with their positive andnegative sentiment strengths. i.e., a negative strength is a number between -1(not negative) and -5 (extremely negative). Similarly, a positive strength is anumber between 1 (not positive) and 5 (extremely positive). The dataset wasconstructed by [24] to evaluate SentiStrenth (http://sentistrength.wlv.ac.uk/), a lexicon-based method for sentiment strength detection.In this paper we propose re-annotating tweets in this dataset with sentimentlabels (negative, positive, neutral) rather than sentiment strengths, which willallow using this dataset for subjectivity classification in addition to sentimentstrength detection. To this end, we assign a single sentiment label to each tweetbased on the following two rules inspired by the way SentiStrength works:4 (i) atweet is considered neutral if the absolute value of the tweet’s negative to positivestrength ratio is equals to 1, (ii) a tweet is positive if its positive sentimentstrength is 1.5 times higher than the negative one, and negative otherwise. Thefinal dataset, as shown in table 1, consists of 1,037 negative, 1,953 neutral and1,252 positive tweets.The original dataset is publicly available at http://sentistrength.wlv.ac.uk/documentation/ along with other 5 datasets from different social mediaplatforms including MySpace, Digg, BBC forum, Runners World forum, andYouTube.Sanders Twitter DatasetThe Sanders dataset consists of 5,512 tweets on four different topics (Apple,Google, Microsoft, Twitter). Each tweet was manually labelled by one annotatoras either positive, negative, neutral, or irrelevant with respect to the topic. Theannotation process resulted in 654 negative, 2,503 neutral, 570 positive and 1,786irrelevant tweets.The dataset has been used in [3,12,5] for polarity and subjectivity classificationof tweets.The Sanders dataset is available at http://www.sananalytics.com/labThe Dialogue Earth Twitter CorpusThe Dialogue Earth Twitter corpus consists of three subsets of tweets. Thefirst two sets (WA, WB) contain 4,490 and 8,850 tweets respectively aboutthe weather, while the third set (GASP) contains 12,770 tweets about gasprices. These datasets were constructed as a part of the Dialogue Earth Project5(www.dialogueearth.org) and were hand labelled by several annotators withfive labels: positive, negative, neutral, not related and can’t tell (other). In thiswork we merge the two sets about the weather in one dataset (WAB) for ouranalysis study in Section 4. This results in 13,340 tweets with 2,580 negative,3,707 neutral, and 2,915 positive tweets. The GASP dataset on the other handconsists of 5,235 negative, 6,268 neutral and 1,050 positive tweets.The WAB and the GASP datasets have been used to evaluate several machinelearning classifiers (e.g., Naive Bayes, SVM, KNN) for polarity classification oftweets [1].4 http://sentistrength.wlv.ac.uk/documentation/SentiStrengthJavaManual.doc5 Dialogue Earth, is former program of the Institute on the Environment at theUniversity of MinnesotaSemEval-2013 Dataset (SemEval)This dataset was constructed for the Twitter sentiment analysis task (Task 2) [16]in the Semantic Evaluation of Systems challenge (SemEval-2013).6 The originalSemEval dataset consists of 20K tweets split into training, development and testsets. All the tweets were manually annotated by 5 Amazon Mechanical Turkworkers with negative, positive and neutral labels. The turkers were also askedto annotate expressions within the tweets as subjective or objective. Using a listof the dataset’s tweet ids provided by [16], we managed to retrieve 13,975 tweetswith 2,186 negative, 6,440 neutrals and 5,349 positives tweets.Participants in the SemEval-2013 Task 2 used this dataset to evaluate theirsystems for expression-level subjectivity detection[15,4], as well as tweet-levelsubjectivity detection[14,18].Summary: Based on the above reviews we can identify two main shortcomingsof these datasets when using them to assess the performance of Twitter sentimentanalysis models. The first shortcoming is the lack of specifications provided bysome datasets (e.g., STS-Test, HCR, Sanders) about the annotation methodologyused to assign sentiment labels to the tweets. For example [8] do not report thenumber of annotators. Similarly [23] do not report annotation agreement amongannotators. The second shortcoming is that most of these datasets are focusedon assessing the performance of sentiment analysis models working at tweet levelbut not at entity level (i.e., they provide human annotations for tweets but notfor entities). In the few cases where the annotation process also targets entitiesas in the HCR dataset, these entities are assigned similar sentiment labels to thelabel of the tweet they belong to. Entity sentiment analysis is however a highlyrelevant task, since it is closely related to the problem of mining the reputationof individuals and brands in Twitter.3 STS-Gold DatasetIn the following subsections we described our proposed dataset, STS-Gold. Thegoal of this dataset is to complement existing Twitter sentiment analysis evalua-tion datasets by providing a new dataset where tweets and entities are annotatedindependently, allowing for different sentiment labels between the tweet andthe entities contained within it. The purpose is to support the performanceassessment for entity-based sentiment analysis models, which is currently hardlyaddressed in the datasets that have been released to date (see Section 2).3.1 Data AcquisitionTo construct this dataset, we first extracted all named entities from a collectionof 180K tweets randomly selected from the original Stanford Twitter corpus (seeSection 2). To this end, we used AlchemyAPI,7 an online service that allowsfor the extraction of entities from text along with their associated semanticconcept class (e.g., Person, Company, City). After that, we identified the topmost frequent semantic concepts and, selected under each of them, the top 26 http://www.cs.york.ac.uk/semeval-2013/task2/7 www.alchemyapi.commost frequent and 2 mid-frequent entities. For example, for the semantic conceptPerson we selected the top most frequent entities (Taylor Swift and Obama)as well as two mid frequent entities (Oprah and Lebron). This resulted in 28different entities along with their 7 associated concepts as shown in Table 2.Concept Top 2 Entities Mid 2 EntitiesPerson Taylor Swift, Obama Oprah, LebronCompany Facebook, Youtube Starbucks, McDonaldsCity London, Vegas Sydney, SeattleCountry England, US Brazil, ScotlandOrganisation Lakers, Cavs Nasa, UNTechnology iPhone, iPod Xbox, PSPHealthCondition Headache, Flu Cancer, FeverTable 2. 28 Entities, with their semantic concepts, used to build STS-Gold.The next step was to construct and prepare a collection of tweets for sentimentannotation, ensuring that each tweet in the collection contains one or more of the28 entities listed in Table 2. To this aim, we randomly selected 100 tweets fromthe remaining part of the STS corpus for each of the 28 entities, i.e., a total of2,800 tweets. We further added another 200 tweets without specific reference toany entities to add up a total of 3,000 tweets. Afterwards, we applied AlchemyAPIon the selected 3,000 tweets. Apart from the initial 28 entities the extraction toolreturned 119 additional entities, providing a total of 147 entities for the 3,000selected tweets.3.2 Data AnnotationWe asked three graduate students to manually label each of the 3,000 tweets withone of the five classes: (Negative, Positive, Neutral, Mixed and Other).The “Mixed” label was assigned to tweets containing mixed sentiment and“Other” to those that were difficult to decide on a proper label. The studentswere also asked to annotate each entity contained in a tweet with the same fivesentiment classes. The students were provided with a booklet explaining boththe tweet-level and the entity-level annotation tasks. The booklet also contains alist of key instructions as shown in this paper’s appendix. It is worth noting thatthe annotation was done using Tweenator,8 an online tool that we previouslybuilt to annotate tweet messages [20].We measured the inter-annotation agreement using the Krippendorff’s alphametric [11], obtaining an agreement of αt = 0.765 for the tweet-level annotationtask. For the entity-level annotation task, if we measured sentiment of entity foreach individual tweet, we only obtained αe = 0.416 which is relatively low for theannotated data to be used. However, if we measured the aggregated sentimentfor each entity, we got a very high inter-annotator agreement of αe = 0.964.To construct the final STS-Gold dataset we selected those tweets and entitiesfor which our three annotators agreed on the sentiment labels, discarding any8 http://tweenator.compossible noisy data from the constructed dataset. As shown in Table 3 the STS-Gold dataset contains 13 negative, 27 positive and 18 neutral entities as well as1,402 negative, 632 positive and 77 neutral tweets. The STS-Gold dataset containsindependent sentiment labels for tweets and entities, supporting the evaluationof tweet-based as well as entity-based Twitter sentiment analysis models.Class Negative Positive Neutral Mixed OtherNo. of Entities 13 27 18 - -No. of Tweets 1402 632 77 90 4Table 3. Number of tweets and entities under each class4 Comparative study of Twitter Sentiment AnalysisDatasetsIn this section, we present a comparison of the described datasets accordingto three different dimensions: the vocabulary size, the total number of tweets,and the data sparsity. We also study the pair-wise intrinsic correlation betweenthese dimensions as well as their correlation with the sentiment classificationperformance (correlation are computed using the Pearson correlation coefficient).To this end, we perform a binary sentiment classification (positive vs. negative)on all the datasets using a Maximum Entropy classifier (MaxEnt). Note thatno stemming or filtering was applied to the data since our aim by providingthis comparison is not to build better classifiers. Instead, we aim at showingthe particularities of each dataset and how these particularities may affect theperformance of sentiment classifiers.Vocabulary SizeThe vocabulary size of a dataset is commonly determined by the number ofthe unique word unigrams that the dataset contains. To extract the number ofunigrams, we use the TweetNLP tokenizer [7], which is specifically built to workon tweets data.9 Note that we considered all tokens found in the tweets includingwords, numbers, URLs, emoticons, and speical characters (e.g., question marks,intensifiers, hashtags, etc).Figure 1 depicts the correlation between the the vocabulary size and thetotal number of tweets in the datasets. Although the correlation between thetwo quantities seems to be positively strong (ρ = 0.95), increasing the numberof tweets does not always lead to increasing the vocabulary size. For example,the OMD dataset has higher number of tweets than the HCR dataset, yet theformer has a smaller vocabulary size than the latter.Data SparsityDataset sparsity is an important factor that affects the overall performance oftypical machine learning classifiers [17]. According to Saif et al. [20], tweets data9 The TweetNLP tokenizer can be downloaded from http://www.ark.cs.cmu.edu/TweetNLP/Fig. 1. Total number of tweets and the vocabulary size of each dataset.are sparser than other types of data (e.g., movie review data) due to a largenumber of infrequent words in tweets.In this section, we aim to compare the presented datasets with respect totheir sparsity. To calculate the sparsity degree of a given dataset we use thefollowing formula from [13]:Sd = 1−∑ni Nin× |V | (1)Where Ni is the the number of distinct words in tweet i, n is the number oftweets in the dataset and |V | the vocabulary size.According to Figure 2, all datasets have a high sparsity degree, with SemEvalbeing the sparsest. It is also worth noticing that there is a strong correlationbetween the sparsity degree and the total number of tweets in a dataset (ρ = 0.71)and an even stronger correlation between the sparsity degree and the vocabularysize of the dataset (ρ = 0.77).Fig. 2. Sparsity degree, vocabulary size and the total number of tweets across thedatasetsClassification PerformanceWe perform a binary sentiment classification on all the datasets using a MaxEntclassifier from Mallet.10 To this end, we selected for each dataset only the subsetof positive and negative tweets.Table 4 reports the classification results (using 10-fold cross validation) inaccuracy and the average F-measure (F-average) on all datasets. The highestaccuracy is achieved on the GASP dataset with 90.897%, while the highestaverage F-measure of 84.621% is obtained on the WAB dataset. It is also worthnoticing that the per-class performance is highly affected by the distributionof positive and negative tweets in the dataset. For example, F-measure fordetecting positive tweets (F-positive) is higher than F-measure for detectingnegative tweets (F-negative) for positive datasets (i.e., datasets that have highernumber of positive tweets than negative ones) such as STS-Test, SS-Twitter,WAB and SemEval. Similarly, F-negative score is higher than F-positive fornegative datasets (i.e., datasets that have higher number of negative tweets thanpositive ones). However, the average accuracy for negative datasets is 84.53%,while it is 80.37% for positive tweets, suggesting that detecting positive tweets ismore difficult than detecting negative tweets.Dataset STS-Test STS-Gold HCR OMD SS-Twitter Sanders GASP WAB SemEvalAccuracy 80.171 85.69 78.679 82.661 73.399 83.84 90.897 84.668 83.257F-negative 79.405 89.999 85.698 86.617 69.179 84.964 94.617 83.745 68.668F-positive 81.21 74.909 58.23 75.47 76.621 82.548 70.682 85.498 88.578F-average 80.307 82.454 71.964 81.044 72.9 83.756 82.65 84.621 78.623Table 4. Accuracy and the average harmonic mean (F measure) obtained from identi-fying positive and negative sentiment.Makrehchi and Kamel [13] showed that the performance trend of text classifierscan be estimated using the sparsity degree of the dataset. In particular, theyfound that reducing the sparsity of a given dataset enhances the performance ofa SVM classifier. Their observation is based on changing the sparsity degree ofthe same dataset by removing/keeping specific terms.Figure 3 illustrates the correlation across all datasets between Accuracy andF-measure on the one hand, and the dataset sparsity on the other hand. Asillustrated by this figure, there is almost no correlation (ρacc = −0.06, ρf1 = 0.23)between the classification performance and the sparsity degree across the datasets.In other words, the sparsity-performance correlation is intrinsic, meaning that itmight exists within the dataset itself, but not necessarily across the datasets. Thisis not surprising given that there are other dataset characteristics in additionto data sparsity, such as polarity class distribution, which may also affect theoverall performance as we discussed earlier in this section.10 http://mallet.cs.umass.edu/Fig. 3. F-Measure and the Sparsity degree of the datasets5 ConclusionsIn this paper, we provided an overview of eight publicly available and manuallyannotated evaluation datasets for Twitter sentiment analysis. Based on ourreview, we found that unlike the tweet level, very few annotation efforts werespent towards providing datasets for evaluating sentiment classifiers at the entitylevel. This motivated us to build a new evaluation dataset, STS-Gold, whichallows for the evaluation of sentiment classification models at both the entity andthe tweet levels. Our dataset, unlike most of the other datasets, distinguishesbetween the sentiment of a tweet and the sentiment of entities mentioned withinit.We also provided a comparative study across all the reported datasets interms of different characteristics including the vocabulary size, the total numberof tweets and the degree of sparsity. Finally, we studied the various pair-wisecorrelations among these characteristics as well as the correlation between thedata sparsity degree and the sentiment classification performance across thedatasets. Our study showed that the large number of tweets in a dataset isnot always an indication for a large vocabulary size although the correlationbetween these two characteristics is relatively strong. We also showed that thesparsity-performance correlation is intrinsic, where it might exists within thedataset itself, but not necessarily across the datasets.AcknowledgmentThe work of the authors was supported by the EU-FP7 projects: ROBUST (grantno. 257859) and SENSE4US (grant no. 611242).Appendix: Annotation BookletWe need to manually annotate 3000 tweets with their sentiment label (Negative,Positive, Neutral, Mixed) using the online annotation tool “Tweenator.com”.The task consists of two subtasks:Task A. Tweet-Level Sentiment Annotation Given a tweet message, de-cide weather it has a positive, negative, neutral or mixed sentiment.Task B. Entity-Level Sentiment Annotation Given a tweet message anda named entity, decided weather the entity received a negative, positive or neutralsentiment. The named entities to annotate are highlighted in yellow within thetweets.Please note that:– A Tweet could have a different sentiment from an entity within it. For ex-ample, the tweet “iPhone 5 is very nice phone, but I can’t upgrade:(” has a negative sentiment. However, the entity “iPhone 5” receives apositive sentiment.– “Mixed” label refers to a tweet that has mixed sentiment. For example, the“Kobe is the best in the world not Lebron” has a mixed sentiment.– Some tweets might have emoticons such as :), :-), :(, or :-(. Please give lessattention to the emoticons and focus more on the content of the tweets.Emoticons can be very misleading indicators sometimes.– Try to be objective with your judgement and feel free to take a break wheneveryou feel tired or bored.References1. Asiaee T, A., Tepper, M., Banerjee, A., Sapiro, G.: If you are happy and you knowit... tweet. In: Proceedings of the 21st ACM international conference on Informationand knowledge management. pp. 1602–1606. ACM (2012)2. Bakliwal, A., Arora, P., Madhappan, S., Kapre, N., Singh, M., Varma, V.: Miningsentiments from tweets. Proceedings of the WASSA 12 (2012)3. Bravo-Marquez, F., Mendoza, M., Poblete, B.: Combining strengths, emotions andpolarities for boosting twitter sentiment analysis. In: Proceedings of the SecondInternational Workshop on Issues of Sentiment Discovery and Opinion Mining.ACM (2013)4. Chalothorn, T., Ellman, J.: Tjp: Using twitter to analyze the polarity of contexts.In: In Proceedings of the seventh international workshop on Semantic EvaluationExercises (SemEval-2013), Atlanta, Georgia, USA, June 2013. (2013)5. Deitrick, W., Hu, W.: Mutually enhancing community detection and sentimentanalysis on twitter networks. Journal of Data Analysis and Information Processing1, 19–29 (2013)6. Diakopoulos, N., Shamma, D.: Characterizing debate performance via aggregatedtwitter sentiment. In: Proceedings of the 28th international conference on Humanfactors in computing systems. ACM (2010)7. Gimpel, K., Schneider, N., O’Connor, B., Das, D., Mills, D., Eisenstein, J., Heilman,M., Yogatama, D., Flanigan, J., Smith, N.A.: Part-of-speech tagging for twitter:Annotation, features, and experiments. Tech. rep., DTIC Document (2010)8. Go, A., Bhayani, R., Huang, L.: Twitter sentiment classification using distantsupervision. CS224N Project Report, Stanford (2009)9. Hu, X., Tang, J., Gao, H., Liu, H.: Unsupervised sentiment analysis with emotionalsignals. In: Proceedings of the 22nd international conference on World Wide Web.pp. 607–618. International World Wide Web Conferences Steering Committee (2013)10. Hu, X., Tang, L., Tang, J., Liu, H.: Exploiting social relations for sentiment analysisin microblogging. In: Proceedings of the sixth ACM international conference onWeb search and data mining. pp. 537–546. ACM (2013)11. Krippendorff, K.: Content analysis: an introduction to its methodology. (1980)12. Liu, K.L., Li, W.J., Guo, M.: Emoticon smoothed language models for twittersentiment analysis. In: AAAI (2012)13. Makrehchi, M., Kamel, M.S.: Automatic extraction of domain-specific stopwordsfrom labeled documents. In: Advances in information retrieval, pp. 222–233. Springer(2008)14. Martınez-Ca´mara, E., Montejo-Ra´ez, A., Martın-Valdivia, M., Urena-Lo´pez, L.:Sinai: Machine learning and emotion of the crowd for sentiment analysis in mi-croblogs (2013)15. Mohammad, S.M., Kiritchenko, S., Zhu, X.: Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets. In: In Proceedings of the seventh internationalworkshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia,USA, June 2013. (2013)16. Nakov, P., Rosenthal, S., Kozareva, Z., Stoyanov, V., Ritter, A., Wilson, T.: Semeval-2013 task 2: Sentiment analysis in twitter. In: In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation. Association for Computational Linguistics.(2013)17. Phan, X.H., Nguyen, L.M., Horiguchi, S.: Learning to classify short and sparse text& web with hidden topics from large-scale data collections. In: Proceedings of the17th international conference on World Wide Web. pp. 91–100. ACM (2008)18. Remus, R.: Asvuniofleipzig: Sentiment analysis in twitter using data-driven machinelearning techniques (2013)19. Saif, H., He, Y., Alani, H.: Semantic Smoothing for Twitter Sentiment Analysis. In:Proceeding of the 10th International Semantic Web Conference (ISWC) (2011)20. Saif, H., He, Y., Alani, H.: Alleviating data sparsity for twitter sentiment analysis.In: Proceedings, 2nd Workshop on Making Sense of Microposts (#MSM2012) inconjunction with WWW 2012. Layon, France (2012)21. Saif, H., He, Y., Alani, H.: Semantic sentiment analysis of twitter. In: Proceedingsof the 11th international conference on The Semantic Web. Boston, MA (2012)22. Shamma, D., Kennedy, L., Churchill, E.: Tweet the debates: understanding com-munity annotation of uncollected sources. In: Proceedings of the first SIGMMworkshop on Social media. pp. 3–10. ACM (2009)23. Speriosu, M., Sudan, N., Upadhyay, S., Baldridge, J.: Twitter polarity classificationwith label propagation over lexical links and the follower graph. In: Proceedingsof the EMNLP First workshop on Unsupervised Learning in NLP. Edinburgh,Scotland (2011)24. Thelwall, M., Buckley, K., Paltoglou, G.: Sentiment strength detection for the socialweb. Journal of the American Society for Information Science and Technology 63(1),163–173 (2012)25. Thelwall, M., Buckley, K., Paltoglou, G., Cai, D., Kappas, A.: Sentiment strengthdetection in short informal text. Journal of the American Society for InformationScience and Technology 61(12), 2544–2558 (2010)",
      "id": 8061796,
      "identifiers": [
        {
          "identifier": "oai:oro.open.ac.uk:40660",
          "type": "OAI_ID"
        },
        {
          "identifier": "23731877",
          "type": "CORE_ID"
        },
        {
          "identifier": "20667326",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.402.7222",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.657.2853",
          "type": "OAI_ID"
        },
        {
          "identifier": "102833506",
          "type": "CORE_ID"
        }
      ],
      "title": "Evaluation datasets for Twitter sentiment analysis: a survey and a new dataset, the STS-Gold",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:oro.open.ac.uk:40660",
        "oai:citeseerx.psu:10.1.1.657.2853",
        "oai:citeseerx.psu:10.1.1.402.7222"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://oro.open.ac.uk/40660/1/paper1.pdf",
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.402.7222"
      ],
      "updatedDate": "2022-04-18T06:13:37",
      "yearPublished": 2013,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/20667326.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/20667326"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/20667326/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/20667326/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/8061796"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "De Koninck, Joseph"
        },
        {
          "name": "Matwin, Stan"
        },
        {
          "name": "Nadeau, David"
        },
        {
          "name": "Sabourin, Catherine"
        },
        {
          "name": "Turney, Peter D."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/86618",
        "https://api.core.ac.uk/v3/outputs/141210571"
      ],
      "createdDate": "2012-07-01T04:55:32",
      "dataProviders": [
        {
          "id": 21,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/21",
          "logo": "https://api.core.ac.uk/data-providers/21/logo"
        },
        {
          "id": 2890,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2890",
          "logo": "https://api.core.ac.uk/data-providers/2890/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "In this position paper, we propose a first step toward automatic analysis of sentiments in dreams. 100 dreams were sampled from a dream bank created for a normative study of dreams. Two human judges assigned a score to describe dream sentiments. We ran four baseline algorithms in an attempt to automate the rating of sentiments in dreams. Particularly, we compared the General Inquirer (GI) tool, the Linguistic Inquiry and Word Count (LIWC), a weighted version of the GI lexicon and of the HM lexicon and a standard bag-of-words. We show that machine learning allows automating the human judgment with accuracy superior to majority class choice",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/pdf/86618.pdf",
      "fieldOfStudy": null,
      "fullText": "Automatic Dream Sentiment Analysis \nDavid Nadeau1, 3, Catherine Sabourin2, Joseph De Koninck2, Stan Matwin1 and Peter D. Turney3 \n \n1School of Information Technology and Engineering \n2School of Psychology \nUniversity of Ottawa \nOttawa, Canada \n3Institute for Information Technology \nNational Research Council Canada \nGatineau and Ottawa, Canada \n {david.nadeau, peter.turney}@nrc-cnrc.gc.ca, {jdekonin, stan, csabo069}@uottawa.ca  \n \n \nAbstract \nIn this position paper, we propose a first step toward \nautomatic analysis of sentiments in dreams. 100 dreams \nwere sampled from a dream bank created for a normative \nstudy of dreams. Two human judges assigned a score to \ndescribe dream sentiments. We ran four baseline algorithms \nin an attempt to automate the rating of sentiments in dreams. \nParticularly, we compared the General Inquirer (GI) tool, \nthe Linguistic Inquiry and Word Count (LIWC), a weighted \nversion of the GI lexicon and of the HM lexicon and a \nstandard bag-of-words. We show that machine learning \nallows automating the human judgment with accuracy \nsuperior to majority class choice. \nIntroduction  \nResearch in psychology shows that emotion is a prominent \nfeature of dreams [2], [6], [11]. Typically, the level of \nemotions, or sentiments, is assessed in dreams by content \nanalysis made by human judges using scales of various \nlevels, or by dreamers themselves. In this work, we show \nhow to automatically obtain equivalent measures. We used \na value from 0 to 3 to estimate both the positive and \nnegative content of dreams, as applied by independent \njudges and we compared it to an automatic analysis.  \nThe granularity of our scale (4 levels) was chosen to reflect \nthe variety of sentiment experience and to maintain \nsimplicity. One envisioned application of this measurement \nis the assessment of the stress experienced by the dreamer. \nPrevious work aiming at drawing a link between negative \nsentiments in dreams and dreamer stress relied on content \nanalysis of written dreams [1].  \nA more general application of automatically analyzing \ndream sentiments would be the mining of large dream \nbanks and discovery of unsuspected data about sentiments \nin dreams of individual of different age, social status, etc.  \nFrom a machine learning perspective, the task of dream \nsentiment analysis is expressed as a classification problem \nwith labels {0, 1, 2, 3}. The goal of this work is to create a \nsystem that can reliably replace human in analyzing \nsentiments in dreams.  \nThe next three sections go as follow: first, the dream \ncorpus is detailed, then our experiments in automatic \ndream sentiment analysis are presented and, finally, related \nworks are discussed.   \nDream Bank \nDreams were gathered from a dream bank created during a \nnormative study conducted at the Sleep Research \nLaboratory of the University of Ottawa (UofO). The ethics \ncommittee of UofO has approved this normative study as \nwell as the use of the dream bank for future studies. \nVolunteers were informed that their dreams could be used \nin other studies on dreams and they all gave their consent. \nTheir participation mainly consist of completing a brief \ndream diary at home during a maximum of three weeks, \nand to write down all the dreams they remembered when \nwaking up, until a maximum of four dreams. A sample of \n100 dreams, from 29 individuals of varied age and sex, was \nused in this study.  \nManual Sentiment Analysis \nThe second author of this paper annotated the 100 dreams \nwith two scores ranging from 0-3. One score is for the \npositive orientation of the dream and the other one is for its \nnegative orientation. The third author of this paper \nindependently annotated 26 dreams. With this second \nannotation, we calculated the inter-judge agreement shown \nin Table 1. We also report the mean squared error (MSE) \non the agreement. MSE is presented and discussed in the \nresult section. Judges based their rating on example dream \npassages like in Table 2. \n \nScale Inter-judge agreement MSE \nPositive  57.7% 0.54 \nNegative  80.8% 0.19 \nTable 1: Inter-judge agreement on 26 dreams. \nAt this point, we dropped the positive scale. The reason is \ntwofold. First, the agreement between annotators is too low \nto extract any meaningful results. As a matter of \ncomparison, a majority class rule would have performed at \nthe same level (56% of positive examples were rated ‘0’ on \nour scale). Second, works in dream analysis often \nconcentrate on the negative sentiments in dreams since \nthey are typically more present and differentiated than \npositive sentiments [3], [4]. The negative scale can \ntherefore be useful in isolation. \n \nNegative orientation \nLevel Description Sample passage \n0 Neutral “I was back in Halifax with some \nof my high school friends and we \nwere just waking around.” \n1 Lightly \nnegative \n“I then got on the street beside a \nbus stop.  The bus I was supposed \nto take past by without stopping to \nlet me in.” \n2 Moderately \nnegative \n“I ran to the car and it wouldn’t \nstart.  So I ran to the bus stop.  The \nbus finally came and I started \ndriving it.  When we got to \ncampus, I spent 25 minutes trying \nto find parking.” \n3 Highly \nnegative \n“When we got there we were in \nthe bad part of town.  We asked \nfor directions and they pulled a \ngun out at us.” \nTable 2: Description of the negative scales. \nAutomatic Dream Analysis \nThe algorithmic framework presented in this section make \nuse of the online version1 of the General Inquirer (GI) [10], \nthe online version2 of the Linguistic Inquiry and Word \nCount (LIWC) [9], the weighted GI and HM lexicons \nintroduced by Turney and Littman [13], and a bag-of-\nwords approach making use of the Balie3 text pre-\nprocessing software. Results are computed using the Weka \nmachine learning toolkit [15].  \nThe General Inquirer \nThe first analysis is performed using the General Inquirer \n[10]. This resource contains 3,600 words labeled “positive” \nor “negative” (respectively “Pos” and “Neg” tags in GI). \nMoreover, each word is paired with disambiguation rules \nthat allow identifying if a specific occurrence refers to the \nsentiment or not. For instance, if the word “kind” is used as \nan adjective, it means “benevolent, charitable” and has a \npositive orientation. In the case the word “kind” is a noun, \nit has no specific orientation. For a particular dream, for \nexample, we obtain “Neg” = 1,6%, meaning that 1,6% of \n                                                \n1\n http://www.webuse.umd.edu:9090/ \n2\n http://www.liwc.net/liwcresearch.php \n3\n http://balie.sourceforge.net \nthe words has an unambiguous negative orientation (e.g., \nANGRY,  DISTURB,  …)  From a machine learning point \nof view, we create a dataset with the following the features. \nNote that even if these features are used to score the \nnegative content of dreams, we still use the positive cues \nthat may be useful. \n1. the number of positive words in GI \n2. the number of negative words in GI \n3. the percentage of positive words in GI \n4. the percentage of negative words in GI \n5. the difference 1-2 \n6. the log ratio 1/2 \n7. the difference 3-4 \n8. the log ratio 3/4 \n9. the negative orientation level {0,1,2,3} \nFeatures 1 to 4 are taken directly from GI output. The \nfeatures 5 and 7 give the difference, which is the \n“remaining” positive or negative strength of a dream. The \nfeatures 6 and 8 give the log ratio, a value related to the \ndifference but that is less sensitive to the magnitude of the \ncompared features. \nThe Linguistic Inquiry and Word Count \nThe second resource we analyzed is the Linguistic Inquiry \nand Word Count [9] software. The LIWC offers measures \nof the percentage of positive and negative words in texts. \nThe LIWC dictionary is composed of 2290 words and \nword stems. In contrasts with the GI, this resource makes \nno use of disambiguation rule; it relies on simple word \ncount. The richness of LIWC is its scrupulous choice of \nwords made by multiple experts that came to near perfect \nagreement. We used the following features: \n1. the percentage of positive words in LIWC \n2. the percentage of negative words in LIWC \n3. the difference 1-2 \n4. the log ratio 1/2 \n5. the negative orientation level {0,1,2,3} \nAgain, we use a feature for the difference of percentage \nscores and a feature for the log ratio.  \nThe Weighted GI and HM \nA third strategy is to use the weighted GI and HM lexicons \nas described in Turney and Littman [13]. The HM lexicon \noriginates from work by Hatzivassiloglou and McKeown \n[5] that evaluates the semantic orientation of 1600 \nadjectives. The GI lexicon is derived from the General \nInquirer used in the previous section. In both resources, \nwords have a weight that represents their orientation and \nstrength, in the general case. For instance, in the weighted \nGI, the word “kind” has a weight of +0.056. The sign ‘+’ \nmeans the orientation is positive and the absolute value \nthat is near 0 means the word is almost neutral (maybe \nbecause of its meaning as a noun). For the matter of \ncomparison, an unambiguous word such as “outstanding” \nhas a weight of +13.41 while “broken-hearted” has a \nweight of -14.29.  \nWe parsed each dream using Balie and count each time the \ntoken canonic version exactly match an entry of the GI \nlexicon or the HM lexicon. We choose to use the following \nfeatures for both lexicon (GI and HM): \n1. the sum of positive weights \n2. the sum of negative weights \n3. the average of  positive weights \n4. the average of  negative weights \n5. the maximal positive weight \n6. the maximal negative weight \n7. the negative orientation level {0,1,2,3} \nFor each pair (1-2, 3-4 and 5-6), we also add a feature for \nthe difference and a feature for the log ratio.  \nThe Bag-of-Words \nWe experiment a Bag-of-words (Bow) approach as a fourth \nstrategy to classify dreams. The Bow approach consists in \nusing as feature every unique word appearing in any \ndream. Our dream sample is composed of 2758 unique \ntokens that turns out to be 2758 features. A particular \ndream (a textual document) is represented by a Boolean \nvector of length 2758 for which the value of element j is 1 \nif the token j appears in the document, and 0 otherwise. \nThis technique is often used in text classification. It allows \nlinking a class (ex.: 1, on the negative scale) to some \nspecific words (ex.: dark, cold, night, etc.) \nResults \nTwo metrics are required in our experiments. First, we \ncalculate classifiers accuracy – the sum of correct guesses \nover the total number of guesses – i.e. their performance at \nexactly finding the right label (e.g., human rates 3, \nmachine guess 3). Second, we calculate the mean squared \nerror of classifier – the average of the squares of the \ndifferences between the human labels and the machine \npredictions. This metric is low when a classifier guesses \nnear the human (e.g., human rates 3, machine guesses 2) \nand becomes high if the classifier is far from human \njudgment (e.g., human rates 3, machine guesses 0). \nIn Table 3, we report the accuracy percentage (ACC) and \nmean squared error (MSE) of every strategy. Results are \nfor stratified 10-fold cross-validations. \n \n Linear \nregression \nwith GI \nLinear \nregression \nwith \nLIWC \nLinear \nregression \nweighted  \nGI & HM \nNaive \nBayes \nwith \nBOW \nACC 50% 48% 35% 38% \nMSE 0.577 0.608 0.865 1.392 \nTable 3: Accuracy and mean squared error of various \nstrategies on analysis of dream negative sentiments. \nThe baseline accuracy is given by a classifier that always \nguesses the majority class. In our dataset, 33% of dreams \nwere rated with label “2” and this is the majority class. \nGuessing always “2” results in 33% accuracy. The baseline \nmean squared error is given by a classifier that always \nguesses the average of classes. The average of all classes is \n1.37 in our dataset. It results in a mean squared error of \n0.993. \nFeatures from the General Inquirer outperform other \nstrategies accuracy (highest number of correct guesses) and \nmean squared error (lowest difference with human \njudgment when incorrectly guessing). LIWC is considered \nas good as GI since there is no statistically significant \ndifference between both resources. \nWe tried many different supervised learning algorithms, \nbut the best result was linear regression. Standard \nclassification algorithms have the downside of resulting in \nbad mean squared errors. In Table 3, the last column \n(BOW) is for a Naïve Bayes algorithm known to perform \nwell in text classification. Even if the accuracy is not the \nlowest, the mean squared error is the worst.  \nDiscussion \nThe best features to automate dream sentiment analysis are \nfrom the GI tool [10] and the LIWC tool [9]. We believe \nthis constitutes a significant first step in this field. Even if \n50% of accuracy may appear to be a poor score, it is \nstatistically better than the baseline accuracy (majority \nclass guessing) with 95% confidence.  \nThe MSE of 0.577 for an accuracy of 50% means that most \nerrors have a difference of 1 on the scale (e.g.: human rates \n3, machine guesses 2). As a matter of comparison, if every \nerror was for a difference of 1, it would result in a MSE of \n0.5 (50 dreams out of 100 with an error of 1 or -1 = 50 time \na squared error of 1 out of 100 = mean squared error of \n0.5). If every error was for a difference of 2, the MSE \nwould be 2. \nRelated Works \nDream Analysis in Psychology \nSentiment analysis is an important component for the \nstudies of dreams since emotions are considered by many \nas responsible for structuring the content of dreams [4], \n[8]. Recent findings from brain imaging studies have \nshown an increased activation of limbic and paralimbic \nareas during Rapid-Eye Movement (REM) sleep [7]. \nDreams being strongly associated with this sleep phase, \nthis may account for the emotional intensity of dreams [2]. \nHowever, further studies are still needed to better \nunderstand the origin as well as the potential role of the \nemotionality of dreams. \nUntil now, most of the recent studies on dreams use the \nclassical scales of Hall and Van de Castle [3], which are \nconsidered as being the most detailed and complete coding \nsystem available for scoring dreams [2]. It comprises \nvarious scales measuring both positive and negative \ncontent, such as the presence of friendly or aggressive \ninteractions, emotions, good fortunes or misfortunes, and \nsuccesses or failures. However, this system is time \nconsuming and depends on the rater’s judgment. It is of \ngreatest interest to develop objective means of scoring \ndreams that are independent of a human judgment and that \ncan be reproduced across laboratories. So far, automatic \nanalysis has not been used in studies of emotions in \ndreams. The development of this technology could \nimprove our knowledge on dreams and be a major \nbreakthrough in this research area. \nSentiment Analysis in AI \nIn this work, we classify whole texts using 4-level scales. \nIn most related literature, texts are analyzed at the \nsentence-level. This representation would be an interesting \nalternative for our work but, unfortunately, the UofO \ndream bank is not annotated at the sentence-level at this \ntime. Moreover, many works (e.g., Turney [12]) formulate \nthe problem as classifying texts as positive or negative \n(binary classification). This formulation differs from our 4-\nlevel scale that we motivate by the need of fine grain \nanalysis of sentiment strength for further processing (e.g., \nanalyzing stress level of dreamers). We believe our \nproblem formulation is more difficult than the binary \nclassification but gives more flexibility.  \nThe most severe limitation of our work is the rather limited \nuse of context. In [14], negations and modalities handling \nis added to a model making use of the GI and the HM \nlexicons. It allows recognizing when the context changes \nthe polarity of a word (for instance the passage “is not \nkind” means the opposite of benevolent, charitable.) This \nimprovement is reported to future work items. \nConclusion and Future Work \nIn this paper, we show how to automate dream sentiment \nanalysis. We specifically experimented with techniques \naiming at rating a dream on a 4-level negative scale. We \nreached accuracy of 50% with a mean squared error of \n0.577, a statistically significant improvement over the \nmajority class guessing. We found that the GI and LIWC \nresources offer the best features from an automatic dream \nsentiment analysis point of view.  \nIn our future work, we will first extend our dataset. We \nexpect that this will significantly improve our results, \ngiven that we have a 4-class problem and only a very \nlimited set of labeled instances. We will also improve the \nhandling of negations and modalities that can completely \nchange the polarity of words in our current framework. The \nlong-term research goal would be to support further \nprocessing in the dream analysis field such as stress \nanalysis. \nAcknowledgement \nWe would like to thank Sonia Matwin who introduced us \nto the LIWC resource. \nReferences \n[1] Delorme, M.-A., Lortie-Lussier, M., and De Koninck, \nJ. (2002). Stress and Coping in the Waking and \nDreaming States during an Examination Period. \nDreaming, 12(4), pp. 171-183. \n[2] Domhoff, G. W. (2003). The Scientific Study of \nDreams: Neural Networks, Cognitive Development, and \nContent Analysis. American Psychological Association, \nNew York. \n[3] Hall, C. S. and Van de Castle, R. L. (1966). The \nContent Analysis of Dreams. Meredith Publishing \nCompany, New York. \n[4] Hartmann, E. (1998). Dreams and Nightmares: The \nNew Theory on the Origin and Meaning of Dreams. \nPlenum, New York, \n[5] Hatzivassiloglou, V. and McKeown, K. R. (1997) \nPredicting the Semantic Orientation of Adjectives. Proc. \nConference of the European Chapter of the ACL. \n[6] Hobson, J. A., Stickgold, R. and Pace-Schott, E. F. \n(1998). The Neuropsychology of REM Sleep Dreaming. \nNeuroReport, Vol. 9, pp. R1-R14. \n[7] Maquet, P., Péters, J. M., Aerts, J., Delfiore, G., \nDegueldre, C, Luxen, A. and Franck, G. (1996). \nFunctional Neuroanatomy of Human Rapid-Eye-\nMovement Sleep and Dreaming, Nature, Vol. 383, \npp.163-166 \n[8] Nielsen, T.  A. and Strenstrom, P. (2005). What are \nthe Memory Sources of Dreaming? Nature, Vol. 437, \npp.1286-1289. \n[9] Pennebaker, J. W., Francis, M. E. and Booth, R.J. \n(2001) Linguistic Inquiry and Word Count LIWC2001. \nErlbaum Publishers, Mahwah, NJ. \n[10] Stone, P. J., Dunphy, D. C., Smith, M. S., Ogilvie, D. \nM. (1966) The General Inquirer: A Computer Approach \nto Content Analysis, The MIT Press. \n[11] St-Onge, M., Lortie-Lussier, M., Mercier, P., Grenier, \nJ. and De Koninck, J. (2005). Emotions in the Diary and \nREM Dreams of Young and Late Adulthood Women and \nTheir Relation to Life Satisfaction. Dreaming, Vol. 15, \npp.116-128. \n[12] Turney, P. (2002) Thumbs Up or Thumbs Down? \nSemantic Orientation Applied to Unsupervised \nClassification of Reviews. Proc. 40th Annual Meeting of \nthe Association for Computational Linguistics. \n[13] Turney, P. and Littman, M. (2003) Measuring Praise \nand Criticism: Inference of Semantic Orientation from \nAssociation. ACM Transactions on Information Systems \n(TOIS). 21(4). pp. 315-346. \n[14] Wilson, T., Wiebe, J. and Hoffman, P. (2005). \nRecognizing Contextual Polarity in Phrase-Level \nSentiment Analysis. Proceedings of HLT/EMNLP. \n[15] Witten, I. H. and Frank, E. (2005) Data Mining: \nPractical machine learning tools and techniques, 2nd \nEdition, Morgan Kaufmann, San Francisco. \n",
      "id": 259612,
      "identifiers": [
        {
          "identifier": "141210571",
          "type": "CORE_ID"
        },
        {
          "identifier": "86618",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:cogprints.org:5030",
          "type": "OAI_ID"
        }
      ],
      "title": "Automatic Dream Sentiment Analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:cogprints.org:5030"
      ],
      "publishedDate": "2006-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 544009,
          "title": "Data Mining: Practical machine learning tools and techniques, 2nd Edition,",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230309,
          "title": "Dreams and Nightmares: The New Theory on the Origin and Meaning of Dreams.",
          "authors": [],
          "date": "1998",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230314,
          "title": "Measuring Praise and Criticism: Inference of Semantic Orientation from Association.",
          "authors": [],
          "date": "2003",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230310,
          "title": "Predicting the Semantic Orientation of Adjectives.",
          "authors": [],
          "date": "1997",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230315,
          "title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230306,
          "title": "Stress and Coping in the Waking and Dreaming States during an Examination Period.",
          "authors": [],
          "date": "2002",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230308,
          "title": "The Content Analysis of Dreams.",
          "authors": [],
          "date": "1966",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230312,
          "title": "The General Inquirer: A Computer Approach to Content Analysis,",
          "authors": [],
          "date": "1966",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230307,
          "title": "The Scientific Study of Dreams: Neural Networks, Cognitive Development, and Content Analysis.",
          "authors": [],
          "date": "2003",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230313,
          "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.",
          "authors": [],
          "date": "2002",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 230311,
          "title": "What are the Memory Sources of Dreaming?",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": null,
          "cites": null
        }
      ],
      "sourceFulltextUrls": [],
      "updatedDate": "2018-01-23T07:50:05",
      "yearPublished": 2006,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/pdf/86618.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/86618"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/86618/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/86618/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/259612"
        }
      ]
    },
    {
      "acceptedDate": "2010-10-26T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Adam",
        "Jimmy"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/280985129",
        "https://api.core.ac.uk/v3/outputs/143909368",
        "https://api.core.ac.uk/v3/outputs/11309792",
        "https://api.core.ac.uk/v3/outputs/357360191",
        "https://api.core.ac.uk/v3/outputs/147599841"
      ],
      "createdDate": "2013-07-10T11:53:34",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        }
      ],
      "depositedDate": "2010-01-01T00:00:00",
      "abstract": "Microblogs as a new textual domain offer a unique proposition for sentiment analysis. Their short document length suggests any sentiment they contain is compact and explicit. However, this short length coupled with their noisy nature can pose difficulties for standard machine learning document representations. In this work we examine the hypothesis that it is easier to classify the sentiment in these short form documents than in longer form documents. Surprisingly, we find classifying sentiment in microblogs easier than in blogs and make a number of observations pertaining to the challenge of supervised learning for sentiment analysis in microblogs",
      "documentType": "research",
      "doi": "10.1145/1871437.1871741",
      "downloadUrl": "https://core.ac.uk/download/11309792.pdf",
      "fieldOfStudy": null,
      "fullText": "Classifying Sentiment in Microblogs:\nIs Brevity an Advantage?\nAdam Bermingham & Alan Smeaton\nCLARITY: Centre for Sensor Web Technologies\nSchool of Computing\nDublin City University\n{abermingham, asmeaton}@computing.dcu.ie\nABSTRACT\nMicroblogs as a new textual domain o\u000ber a unique propo-\nsition for sentiment analysis. Their short document length\nsuggests any sentiment they contain is compact and explicit.\nHowever, this short length coupled with their noisy nature\ncan pose di\u000eculties for standard machine learning document\nrepresentations. In this work we examine the hypothesis\nthat it is easier to classify the sentiment in these short form\ndocuments than in longer form documents. Surprisingly,\nwe \fnd classifying sentiment in microblogs easier than in\nblogs and make a number of observations pertaining to the\nchallenge of supervised learning for sentiment analysis in\nmicroblogs.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Text Mining\nGeneral Terms\nAlgorithms, Experimentation\n1. INTRODUCTION\nMicroblogging has become a popular method for Internet\nusers to publish thoughts and information in real-time. Au-\ntomated sentiment analysis of microblog posts is of interest\nto many, allowing monitoring of public sentiment towards\npeople, products and events, as they happen.\nThe short length of microblog documents means they can\nbe easily published and read on a variety of platforms and\nmodalities. This brevity constraint has led to the use of non-\nstandard textual artefacts such as emoticons and informal\nlanguage. The resulting text is often considered \\noisy\".\nIt is reasonable to assume that the short document length\nintroduces a succinctness to the content. The focused nature\nof the text and higher density of sentiment-bearing terms\nmay bene\ft automated sentiment analysis techniques. On\nthe other hand, it may also be that the shorter length and\nlanguage conventions used mean there is not enough context\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nCIKM’10, October 26–29, 2010, Toronto, Ontario, Canada.\nCopyright 2010 ACM 978-1-4503-0099-5/10/10 ...$10.00.\nfor sentiment to be accurately detected. It is unclear which\nof these is true.\nThese issues motivate our research questions: (i) How does\nsentiment classi\fcation accuracy in the microblogging do-\nmain compare to that for microreviews, another short-form\ntextual domain? How do these accuracies compare to those\nfor their long-form counterparts? and (ii) How do di\u000berent\nfeature vector representations and classi\fers a\u000bect sentiment\nclassi\fcation accuracy for microblogs? How does this com-\npare to the corpora explored in (i)?\n2. RELATEDWORK\nSentiment analysis has been successfully used to analyse and\nextract opinion from text in recent years [12]. Some ex-\nploratory works have been completed in the microblog do-\nmain. Diakapolous and Shamma used manual annotations\nto characterize the sentiment reactions to various issues in\na political debate [5]. They \fnd that sentiment is useful as\na measure for identifying controversy. Jansen et al. studied\nthe Word-Of-Mouth e\u000bect on Twitter using an adjective-\nbased sentiment classi\fer, \fnding it useful for brand analyt-\nics on Twitter. Bollen et al. analysed sentiment on Twitter\naccording to a six-dimensional mood representation [2] \fnd-\ning that sentiment on Twitter correlates with real-world val-\nues such as stock prices and coincides with cultural events.\nThe latter two studies report positive results from using au-\ntomated sentiment analysis techniques on Twitter data.\nNoise in Computer-Mediated-Content has been the sub-\nject of much research. Tagliamonte and Denis studied in-\nstant messaging [13], \fnding that the penetration of non-\nstandard English language and punctuation is far less than\nis reported in the media. In a study of classi\fcation of cus-\ntomer feedback, Gamon found a high level of accuracy for\nsupervised sentiment classi\fcation despite their noisy nature\n[7]. One strategy to deal with noise in the domain put for-\nward by Choudhury et al. is to use Hidden Markov Models\nto decode text into standard English [4] reporting a high\nlevel of success for SMS data. Agarwal et al. showed that\nby simulating noise in text classi\fcation, a good classi\fer\nshould perform well up to about 40% noise [1] suggesting\nthat although noise may be present in text, this may not\nprove to be important for supervised learning tasks. Car-\nvalho et al. found that non-standard surface features such\nas a heavy punctuation and emoticons are key to detecting\nirony in user-generated content [3].\nCollectively, these studies all support our assumption that\nnew textual domains exhibit domain speci\fc features. We\nalso see that there is signi\fcant value in being able to model\nTable 1: Microblog annotation labels and associated\ndocument counts.\nLabel #Documents\nRelevant, Positive 1,410\nRelevant, Negative 1,040\nRelevant, Neutral 2,597\nRelevant, Mixed 146\nNot relevant 498\nUnannotatable 603\nUnclear 530\nTotal 6,824\nsentiment in these domains. To our knowledge, this is the\n\frst work to explore the challenges that the shortness of mi-\ncroblog documents present to feature vector representations\nand supervised sentiment classi\fcation.\n3. METHODOLOGY\nThe microblog posts used in these experiments are taken\nfrom a collection of over 60 million posts which we gathered\nfrom the Twitter public data API1 from February to May\n2009. We examined the trending topics from this period and\nidenti\fed \fve recurring themes: Entertainment, Products\nand Services, Sport, Current A\u000bairs and Companies. We\nselected 10 trends from each of these categories to be used\nas sentiment targets. By making the topic set diverse and\nchallenging, we hope to better test the performance of our\napproach and build a classi\fer representative of a real world\ngeneric sentiment classi\fcation scenario.\nIn the annotation process, Wilson's de\fnition of sentiment\nwas used: \\Sentiment analysis is the task of identifying posi-\ntive and negative opinions, emotions, and evaluations.\" [14]\nOur team of annotators consisted of 9 PhD students and\nPostdoctoral researchers. To ensure su\u000ecient agreement\namong the annotators, the annotation was preceded by a\nnumber of training iterations, consisting of group meetings,\nconsensus annotations and one-on-one discussions. See Ta-\nble 3 for a breakdown of annotations by label.\nIn total, 9 annotators annotated 17 documents for each\nof the 50 topics. 463 documents were doubly annotated for\ninter-annotator agreement (6.78%). For the 7 labels, the\nKappa agreement was 0.65. For the 3 classes which we use\nfor training (positive, negative and neutral) Kappa was 0.72.\nIf we just consider the binary sentiment classes, positive and\nnegative, this increases to 0.94. These relatively high values\nfor kappa are consistent with our previous annotation of\nblogs [10].\nTo contrast with our microblogs corpus, we derive a cor-\npus of blog posts from the TREC Blogs06 corpus [8]. We\nuse a templating approach to extract positive, negative and\nneutral blog post content and comments from the corpus,\nusing the TREC relevance judgments as labels.\nAs much of sentiment analysis literature concerns review\nclassi\fcation, in parallel to our experiments on the microblog\nand blog corpora, we also conduct our experiments on a cor-\npus of microreviews and a corpus of reviews. In January\n2010 we collected microreview documents from the microre-\nview website, Blippr2. Blippr reviews bear a similarity to\n1http://apiwiki.twitter.com\n2http://www.blippr.com\nmicroblog posts in that they share the same character limit\nof 140 characters. Reviews on Blippr are given one of four\nratings by the author, in order from most negative to most\npositive: hate, dislike, like and love. In our corpus we use\nonly reviews with strongly polarised sentiment: hate and\nlove. We have made our microreview and microblog corpora\navailable for other researchers3.\nThe reviews corpus we use as comparison is perhaps the\nmostly widely studied sentiment corpus, Pang and Lee's\nmovie review corpus [11]. This corpus contains archival\nmovie reviews from USENET. We refer to the microblog and\nmicroreview datasets as the short-form document corpora\nand the blog and movie review datasets as the long-form\ndocument corpora.\nOur datasets are limited to exactly 1000 documents per\nclass in line with the movie review corpus. This allows us\nto eliminate any underlying sentiment bias which may be\npresent. While this is obviously a consideration for a real-\nworld system, in our experiments we wish to examine the\nchallenges of the classi\fcation without biasing our evalua-\ntion towards any particular class. As the sentiment distri-\nbution is di\u000berent in each of the domains, this also makes\naccuracies comparable across datasets.\nFor our experiments we use Support Vector Machine (SVM)\nand Multinomial Naive Bayes (MNB) classi\fers, giving us an\naccurate representation of the state-of-the-art in text clas-\nsi\fcation. We use an SVM with a linear kernel and the\nparameter c set to 1. In preliminary experiments we found\nbinary feature vectors more e\u000bective than frequency-based\nvectors and found no bene\ft from stopwording or stemming.\nWhere possible, we replaced topics with pseudo-terms to\navoid learning topic-sentiment bias. We also replace URLs\nand usernames with pseudo-terms to avoid confusion during\ntokenization and POS tagging. Each feature vector is L2\nNormalized and for the the long-form corpora only features\nwhich occurred 4 or more times were used, as Pang and Lee\ndid in their original movie review experiments. Accuracy\nwas measured using 10 fold cross-validation and the folds\nwere \fxed for all experiments.\nAs a baseline for binary (positve/negative) classi\fcation\nwe use a classi\fer based on a sentiment lexicon, SentiWord-\nNet [6]. This unsupervised classi\fer calssi\fes documents\nusing the mean sentiment scores of the synsets its words be-\nlong to. Despite their naivety, this type of classi\fer is often\nused as it does not require expensive training data.\n4. RESULTS AND DISCUSSION\nUnigram binary (positive/negative) classi\fcation accuracy\nfor microblogs is 74.85% using an SVM. This is an encour-\naging accuracy given the diversity in the sentiment topics.\nAs we have balanced datasets, a trivial classi\fer achieves\n50% accuracy for binary classi\fcation. For microreviews,\nthe accuracy is considerably higher at 82.25% using an SVM.\nAs expected, the classi\fer \fnds it easier to distinguish be-\ntween polarised reviews than to identify sentiment in arbi-\ntrary posts.\nSentiment classi\fcation of the long-form documents yields\nsome surprising results. Blog classi\fcation accuracy is sig-\nni\fcantly lower than for microblogs. However, movie re-\nview classi\fcation is higher than for microreviews, con\frm-\ning Pang and Lee's results of 87.15% for SVM with unigram\n3http://www.computing.dcu.ie/~abermingham/data/\nFigure 1: Accuracies for unigram features.\nfeatures. At \frst this may seem contradictory | surely\nthe classi\fer should perform consistently across textual do-\nmains? We speculate that this behaviour is due to within-\ndocument topic drift. In the two review corpora the text\nof the document has a high density of sentiment informa-\ntion about the topic, and a low noise density. In the blogs\ndataset, this is not necessarily the case; the sentiment in\na blog post may be an isolated reference in a subsection\nof the document. Although topic drift also occurs in the\nmicroblog corpus, there is less opportunity for non-relevant\ninformation to enter the feature vector and our classi\fer is\nnot as adversely a\u000bected as in the blog domain.\nOur unsupervised lexicon-based classi\fer performs poorly\nacross all datasets. For the blogs corpus, it is outperformed\nby a trivial classi\fer. The accuracy gap between supervised\nand unsupervised classi\fcation accuracy in the long-form\ncorpora is much more pronounced. This makes intuitive\nsense as the probability of the polarity of a word in a doc-\nument expressing sentiment towards a topic is again much\nhigher in the short-form domains.\nOf the two supervised classi\fers, SVM outperforms MNB\nin the long-form domains, whereas the opposite is true in\nthe short-form domains. SVMs scale better with larger vec-\ntor dimensionality so this is most likely the reason for this\nobservation; the number of unique terms in the longer doc-\numents is over three times their shorter counterparts, even\nwhen infrequent features have been excluded.\nHaving established a reasonable performance in sentiment\nclassi\fcation of microblog posts, we wish to explore whether\nwe can improve the standard bag of words feature set by\nadding more sophisticated features. Using sequences of terms,\nor n-grams, we can capture some of the information lost in\nthe bag-of-words model. We evaluated two feature sets: (un-\nigrams + bigrams) and (unigrams + bigrams + trigrams).\nWe found that although an increase in classi\fcation accu-\nracy is observed for the movie reviews, this is not the case\nfor any of the other datasets (see Table 2). We also ex-\namined POS-based n-grams in conjuction with a unigram\nmodel and observed a decrease in accuracy across all cor-\npora. This indicates that the syntactic patterns represented\nby the POS n-gram features do contain information which\nis more discriminative than unigrams.\nThe most promising results came from a POS-based stop-\nwording approach proposed by Matsumoto et al. ([9]). This\napproach (which Matsumoto et al. refer to as \\word sub-\nsequences\") consists of an n-gram model, where terms have\nTable 3: Most discriminative unigrams, bigrams and\ntrigrams according to Information Gain Ratio for\nbinary classi\fcation.\nMicroblogs Blogs Microreviews Reviews\n1 ! witherspoon great bad\n2 <Url> joaquin boring worst\n3 <Topic> reese best stupid\nwitherspoon\n4 amazing joaquin terrible boring\nphoenix\n5 . . sharon the best the worst\n6 ! ! ledger worst waste\n7 ? heath n't ridiculous\nledger\n8 ! ! ! heath love wasted\n9 love johnny loved awful\ncash\n10 <Topic> ! palestinians ? ?\nTable 4: Ternary Unigram Classi\fcation Accuracies:\nPositive, Negative, Neutral\nMNB SVM #features\nMicroblogs 61.3 59.5 8132\nBlogs 52.13 57.6 28805\nbeen stopworded based on their POS. We use the same POS\nlist as Matsumoto. These features increase accuracy across\nall corpora for unigrams + POS-stopworded bigrams. This\nsuggests that a better understanding of the linguistic con-\ntext of terms is similarly advantageous in all domains.\nTo examine the performance of individual features, we use\na standard measure of discriminability, Information Gain\nRatio (see Table 3). Immediately obvious is the signi\fcant\nrole that punctuation plays in expressing sentiment in mi-\ncroblog posts. This suggests that these are being used specif-\nically in microblog posts to express sentiment, perhaps as in-\ndicators for intonation. The discriminative features for both\nthe reviews and microreviews are largely similar in nature,\ntypically polarised adjectives. The blog classi\fer appears to\nhave learned a certain amount of entity bias as many of the\ndiscriminative features are people or places. Note that none\nof these entities are topic terms (topic terms were removed\nin pre-processing), though they do appear to be entities as-\nsociated with topics.\nResults of our ternary classi\fcation on microblogs and\nblogs can be seen in Table 4. The accuracy is, as expected,\nsigni\fcantly less than for binary classi\fcation with SVMs\nagain outperforming MNB on the longer blog documents.\n5. CONCLUSION\nThe results of our experiments on the whole are encourag-\ning for the task of analysing sentiment in microblogs. We\nachieve an accuracy of 74.85% for binary classi\fcation for\na diverse set of topics indicating we can classify microblog\ndocuments with a moderate degree of con\fdence. In both\nof our short-form corpora we \fnd it di\u000ecult to improve per-\nformance by extending a unigram feature representation.\nThis is contrary to the long-form corpora which respond\nfavourably to enriched feature representations. We do how-\never see promise in sophisticated POS-based features across\nTable 2: Binary accuracy summary (\fgures as %)\nMicroblogs Blogs Microreviews Movies\nFeature Set MNB SVM MNB SVM MNB SVM MNB SVM\nUnigram 74.85 72.95 64.6 68.75 82.25 80.8 82.95 87.15\nUnigram+Bigram 74.35 72.95 64.6 68.45 82.15 81.4 85.25 87.9\nUnigram+Bigram+Trigram 73.7 72.8 64.6 68.5 81.95 80.85 84.8 87.9\nUnigram+POS n-gram (n=1) 73.25 71.6 64.7 68.45 80.8 79.5 82.4 86.95\nUnigram+POS n-gram (n=1,2) 70.25 70.05 62.6 66.25 80.8 79.5 81.8 84.95\nUnigram+POS n-gram (n=1,2,3) 68.8 69.7 62.45 64.6 74.7 76.9 79.95 82\nUnigram+POS-stopworded Bigram 74.15 73.25 64.5 69 82.5 81.05 85.35 87.5\nUnigram+POS-stopworded Bigram+Trigram 74.4 73.45 64.85 68.7 82.15 80.6 85.5 87.8\nall datasets and speculate that engineering features based\non deeper linguistic representations such as dependencies\nand parse trees may work for microblogs as they have been\nshown to do for movie reviews. In analysing discrimina-\ntive features, we found that a signi\fcant role is played by\npunctuation. As a future direction for this work we hope to\nexplore this notion with a view to incorporating it into the\nfeature engineering process. It is surprising to see that this\nis not a pattern seen in our microreviews corpus indicating\nthat this is not an artefact of the brevity of the platforms.\nWe conclude that although the shortness of the documents\nhas a bearing on which feature sets and classi\fer will pro-\nvide optimum performance, the sparsity of information in\nthe documents does not hamper our ability to classify them.\nOn the contrary, we \fnd classifying these short documents a\nmuch easier task than their longer counterparts, blogs. Also,\nthe \\noisy\" artefacts of the microblog domain such as infor-\nmal punctuation turn out to be a bene\ft to the classi\fers.\nThese results provide a compelling argument to encourage\nthe community to focus on microblogs in future sentiment\nanalysis research.\nAcknowledgments\nThis work is supported by Science Foundation Ireland under\ngrant 07/CE/I1147\n6. REFERENCES\n[1] S. Agarwal, S. Godbole, D. Punjani, and S. Roy. How\nmuch noise is too much: A study in automatic text\nclassi\fcation. In ICDM, pages 3{12, 2007.\n[2] J. Bollen, A. Pepe, and H. Mao. Modeling public mood\nand emotion: Twitter sentiment and socio-economic\nphenomena. CoRR, abs/0911.1583, 2009.\n[3] P. Carvalho, L. Sarmento, M. J. Silva, and\nE. de Oliveira. Clues for detecting irony in\nuser-generated contents: oh...!! it's \"so easy\" ;-). In\nTSA '09: Proceeding of the 1st international CIKM\nworkshop on Topic-sentiment analysis for mass\nopinion, pages 53{56, New York, NY, USA, 2009.\nACM.\n[4] M. Choudhury, R. Saraf, V. Jain, A. Mukherjee,\nS. Sarkar, and A. Basu. Investigation and modeling of\nthe structure of texting language. IJDAR,\n10(3-4):157{174, 2007.\n[5] N. A. Diakopoulos and D. A. Shamma. Characterizing\ndebate performance via aggregated Twitter sentiment.\nIn Conference on Human Factors in Computing\nSystems (CHI 2010), 2010.\n[6] A. Esuli and F. Sebastiani. Sentiwordnet: A publicly\navailable lexical resource for opinion mining. In In\nProceedings of the 5th Conference on Language\nResources and Evaluation (LREC-06), pages 417{422,\n2006.\n[7] M. Gamon. Sentiment classi\fcation on customer\nfeedback data: noisy data, large feature vectors, and\nthe role of linguistic analysis. In COLING '04:\nProceedings of the 20th international conference on\nComputational Linguistics, page 841, Morristown, NJ,\nUSA, 2004. Association for Computational Linguistics.\n[8] C. MacDonald and I. Ounis. The TREC Blogs06\ncollection : Creating and analysing a blog test\ncollection. Technical report, University of Glasgow,\nDepartment of Computing Science, 2006.\n[9] S. Matsumoto, H. Takamura, and M. Okumura.\nSentiment classi\fcation using word sub-sequences and\ndependency sub-trees. In Proceedings of PAKDD'05,\nthe 9th Paci\fc-Asia Conference on Advances in\nKnowledge Discovery and Data Mining, 2005.\n[10] N. O'Hare, M. Davy, A. Bermingham, P. Ferguson,\nP. Sheridan, C. Gurrin, and A. F. Smeaton.\nTopic-dependent sentiment analysis of \fnancial blogs.\nIn In: TSA 2009 - 1st International CIKM Workshop\non Topic-Sentiment Analysis for Mass Opinion\nMeasurement, Hong Kong, China, 6 Nov 2009.\n[11] B. Pang and L. Lee. A sentimental education:\nsentiment analysis using subjectivity summarization\nbased on minimum cuts. In ACL '04: Proceedings of\nthe 42nd Annual Meeting on Association for\nComputational Linguistics, page 271, Morristown, NJ,\nUSA, 2004. Association for Computational Linguistics.\n[12] B. Pang and L. Lee. Opinion mining and sentiment\nanalysis. Foundation and Trends in Information\nRetrieval, 2(1-2):1{135, 2008.\n[13] S. A. Tagliamonte and D. Denis. LINGUISTIC RUIN?\nLOL! INSTANT MESSAGING AND TEEN\nLANGUAGE. American Speech, 83(1):3{34, 2008.\n[14] T. Wilson, J. Wiebe, and P. Ho\u000bmann. Recognizing\ncontextual polarity in phrase-level sentiment analysis.\nProceedings of the 2005 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 347{354, 2005.\n",
      "id": 4762917,
      "identifiers": [
        {
          "identifier": "11309792",
          "type": "CORE_ID"
        },
        {
          "identifier": "280985129",
          "type": "CORE_ID"
        },
        {
          "identifier": "2171645516",
          "type": "MAG_ID"
        },
        {
          "identifier": "143909368",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.1145/1871437.1871741",
          "type": "DOI"
        },
        {
          "identifier": "oai:doras.dcu.ie:15663",
          "type": "OAI_ID"
        },
        {
          "identifier": "147599841",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.1052.103",
          "type": "OAI_ID"
        },
        {
          "identifier": "357360191",
          "type": "CORE_ID"
        }
      ],
      "title": "Classifying sentiment in microblogs: is brevity an advantage?",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:citeseerx.psu:10.1.1.1052.103",
        "oai:doras.dcu.ie:15663"
      ],
      "publishedDate": "2010-01-01T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [
        {
          "id": 16547101,
          "title": "A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.",
          "authors": [],
          "date": "2004",
          "doi": "10.3115/1218955.1218990",
          "raw": "B. Pang and L. Lee. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 271, Morristown, NJ, USA, 2004. Association for Computational Linguistics.",
          "cites": null
        },
        {
          "id": 16547073,
          "title": "Characterizing debate performance via aggregated Twitter sentiment.",
          "authors": [],
          "date": "2010",
          "doi": "10.1145/1753326.1753504",
          "raw": "N. A. Diakopoulos and D. A. Shamma. Characterizing debate performance via aggregated Twitter sentiment. In Conference on Human Factors in Computing Systems (CHI 2010), 2010.",
          "cites": null
        },
        {
          "id": 16547061,
          "title": "Clues for detecting irony in user-generated contents: oh...!! it’s ”so easy” ;-).",
          "authors": [],
          "date": "2009",
          "doi": "10.1145/1651461.1651471",
          "raw": "P. Carvalho, L. Sarmento, M. J. Silva, and E. de Oliveira. Clues for detecting irony in user-generated contents: oh...!! it’s ”so easy” ;-). In TSA ’09: Proceeding of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 53–56, New York, NY, USA, 2009. ACM.",
          "cites": null
        },
        {
          "id": 16547051,
          "title": "How much noise is too much: A study in automatic text classiﬁcation.",
          "authors": [],
          "date": "2007",
          "doi": "10.1109/icdm.2007.21",
          "raw": "S. Agarwal, S. Godbole, D. Punjani, and S. Roy. How much noise is too much: A study in automatic text classiﬁcation. In ICDM, pages 3–12, 2007.",
          "cites": null
        },
        {
          "id": 16547066,
          "title": "Investigation and modeling of the structure of texting language.",
          "authors": [],
          "date": "2007",
          "doi": "10.1007/s10032-007-0054-0",
          "raw": "M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar, and A. Basu. Investigation and modeling of the structure of texting language. IJDAR, 10(3-4):157–174, 2007.",
          "cites": null
        },
        {
          "id": 16547056,
          "title": "Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "J. Bollen, A. Pepe, and H. Mao. Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena. CoRR, abs/0911.1583, 2009.",
          "cites": null
        },
        {
          "id": 16547103,
          "title": "Opinion mining and sentiment analysis. Foundation and Trends in Information Retrieval,",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundation and Trends in Information Retrieval, 2(1-2):1–135, 2008.",
          "cites": null
        },
        {
          "id": 16547106,
          "title": "Recognizing contextual polarity in phrase-level sentiment analysis.",
          "authors": [],
          "date": "2005",
          "doi": "10.3115/1220575.1220619",
          "raw": "T. Wilson, J. Wiebe, and P. Hoﬀmann. Recognizing contextual polarity in phrase-level sentiment analysis. Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 347–354, 2005.",
          "cites": null
        },
        {
          "id": 16547082,
          "title": "Sentiment classiﬁcation on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis.",
          "authors": [],
          "date": "2004",
          "doi": "10.3115/1220355.1220476",
          "raw": "M. Gamon. Sentiment classiﬁcation on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 841, Morristown, NJ, USA, 2004. Association for Computational Linguistics.",
          "cites": null
        },
        {
          "id": 16547091,
          "title": "Sentiment classiﬁcation using word sub-sequences and dependency sub-trees.",
          "authors": [],
          "date": "2005",
          "doi": "10.1007/11430919_37",
          "raw": "S. Matsumoto, H. Takamura, and M. Okumura. Sentiment classiﬁcation using word sub-sequences and dependency sub-trees. In Proceedings of PAKDD’05, the 9th Paciﬁc-Asia Conference on Advances in Knowledge Discovery and Data Mining, 2005.",
          "cites": null
        },
        {
          "id": 16547076,
          "title": "Sentiwordnet: A publicly available lexical resource for opinion mining. In",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "A. Esuli and F. Sebastiani. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC-06), pages 417–422, 2006.",
          "cites": null
        },
        {
          "id": 16547088,
          "title": "The TREC Blogs06 collection : Creating and analysing a blog test collection.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "C. MacDonald and I. Ounis. The TREC Blogs06 collection : Creating and analysing a blog test collection. Technical report, University of Glasgow, Department of Computing Science, 2006.",
          "cites": null
        },
        {
          "id": 16547096,
          "title": "Topic-dependent sentiment analysis of ﬁnancial blogs. In In:",
          "authors": [],
          "date": "2009",
          "doi": "10.1145/1651461.1651464",
          "raw": "N. O’Hare, M. Davy, A. Bermingham, P. Ferguson, P. Sheridan, C. Gurrin, and A. F. Smeaton. Topic-dependent sentiment analysis of ﬁnancial blogs. In In: TSA 2009 - 1st International CIKM Workshop on Topic-Sentiment Analysis for Mass Opinion Measurement, Hong Kong, China, 6 Nov 2009.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/15663/1/cikm1079-bermingham.pdf",
        "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=D6D9313401D92AE58BC98D0FF9F111B6?doi=10.1.1.1052.103&rep=rep1&type=pdf"
      ],
      "updatedDate": "2021-12-13T06:11:26",
      "yearPublished": 2010,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11309792.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11309792"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11309792/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11309792/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4762917"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1805.09016",
      "authors": [
        {
          "name": "Barnes, Jeremy"
        },
        {
          "name": "Klinger, Roman"
        },
        {
          "name": "Walde, Sabine Schulte im"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Jeremy",
        "Iryna"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/618049396",
        "https://api.core.ac.uk/v3/outputs/470620469"
      ],
      "createdDate": "2018-06-16T12:29:31",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 2520,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2520",
          "logo": "https://api.core.ac.uk/data-providers/2520/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Sentiment analysis in low-resource languages suffers from a lack of annotated\ncorpora to estimate high-performing models. Machine translation and bilingual\nword embeddings provide some relief through cross-lingual sentiment approaches.\nHowever, they either require large amounts of parallel data or do not\nsufficiently capture sentiment information. We introduce Bilingual Sentiment\nEmbeddings (BLSE), which jointly represent sentiment information in a source\nand target language. This model only requires a small bilingual lexicon, a\nsource-language corpus annotated for sentiment, and monolingual word embeddings\nfor each language. We perform experiments on three language combinations\n(Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment\nclassification and find that our model significantly outperforms\nstate-of-the-art methods on four out of six experimental setups, as well as\ncapturing complementary information to machine translation. Our analysis of the\nresulting embedding space provides evidence that it represents sentiment\ninformation in the resource-poor target language without any annotated data in\nthat language.Comment: Accepted to ACL 2018 (Long Papers",
      "documentType": "research",
      "doi": "10.18653/v1/p18-1231",
      "downloadUrl": "https://core.ac.uk/download/618049396.pdf",
      "fieldOfStudy": null,
      "fullText": "Bilingual Sentiment Embeddings:\nJoint Projection of Sentiment Across Languages\nJeremy Barnes, Roman Klinger, and Sabine Schulte im Walde\nInstitut fu¨r Maschinelle Sprachverarbeitung\nUniversity of Stuttgart\nPfaffenwaldring 5b, 70569 Stuttgart, Germany\n{barnesjy,klinger,schulte}@ims.uni-stuttgart.de\nAbstract\nSentiment analysis in low-resource lan-\nguages suffers from a lack of annotated\ncorpora to estimate high-performing mod-\nels. Machine translation and bilingual word\nembeddings provide some relief through\ncross-lingual sentiment approaches. How-\never, they either require large amounts of\nparallel data or do not sufficiently capture\nsentiment information. We introduce Bilin-\ngual Sentiment Embeddings (BLSE), which\njointly represent sentiment information in a\nsource and target language. This model\nonly requires a small bilingual lexicon,\na source-language corpus annotated for\nsentiment, and monolingual word embed-\ndings for each language. We perform ex-\nperiments on three language combinations\n(Spanish, Catalan, Basque) for sentence-\nlevel cross-lingual sentiment classification\nand find that our model significantly out-\nperforms state-of-the-art methods on four\nout of six experimental setups, as well as\ncapturing complementary information to\nmachine translation. Our analysis of the re-\nsulting embedding space provides evidence\nthat it represents sentiment information in\nthe resource-poor target language without\nany annotated data in that language.\n1 Introduction\nCross-lingual approaches to sentiment analysis are\nmotivated by the lack of training data in the vast\nmajority of languages. Even languages spoken\nby several million people, such as Catalan, often\nhave few resources available to perform sentiment\nanalysis in specific domains. We therefore aim\nto harness the knowledge previously collected in\nresource-rich languages.\nPrevious approaches for cross-lingual sentiment\nanalysis typically exploit machine translation based\nmethods or multilingual models. Machine trans-\nlation (MT) can provide a way to transfer senti-\nment information from a resource-rich to resource-\npoor languages (Mihalcea et al., 2007; Balahur and\nTurchi, 2014). However, MT-based methods re-\nquire large parallel corpora to train the translation\nsystem, which are often not available for under-\nresourced languages.\nExamples of multilingual methods that have\nbeen applied to cross-lingual sentiment analysis\ninclude domain adaptation methods (Prettenhofer\nand Stein, 2011), delexicalization (Almeida et al.,\n2015), and bilingual word embeddings (Mikolov\net al., 2013; Hermann and Blunsom, 2014; Artetxe\net al., 2016). These approaches however do not in-\ncorporate enough sentiment information to perform\nwell cross-lingually, as we will show later.\nWe propose a novel approach to incorporate sen-\ntiment information in a model, which does not have\nthese disadvantages. Bilingual Sentiment Embed-\ndings (BLSE) are embeddings that are jointly opti-\nmized to represent both (a) semantic information in\nthe source and target languages, which are bound\nto each other through a small bilingual dictionary,\nand (b) sentiment information, which is annotated\non the source language only. We only need three\nresources: (i) a comparably small bilingual lexicon,\n(ii) an annotated sentiment corpus in the resource-\nrich language, and (iii) monolingual word embed-\ndings for the two involved languages.\nWe show that our model outperforms previous\nstate-of-the-art models in nearly all experimental\nsettings across six benchmarks. In addition, we\noffer an in-depth analysis and demonstrate that our\nmodel is aware of sentiment. Finally, we provide a\nqualitative analysis of the joint bilingual sentiment\nspace. Our implementation is publicly available at\nhttps://github.com/jbarnesspain/blse.\nar\nX\niv\n:1\n80\n5.\n09\n01\n6v\n1 \n [c\ns.C\nL]\n  2\n3 M\nay\n 20\n18\n2 Related Work\nMachine Translation: Early work in cross-lingual\nsentiment analysis found that machine translation\n(MT) had reached a point of maturity that enabled\nthe transfer of sentiment across languages. Re-\nsearchers translated sentiment lexicons (Mihalcea\net al., 2007; Meng et al., 2012) or annotated corpora\nand used word alignments to project sentiment an-\nnotation and create target-language annotated cor-\npora (Banea et al., 2008; Duh et al., 2011; Demirtas\nand Pechenizkiy, 2013; Balahur and Turchi, 2014).\nSeveral approaches included a multi-view repre-\nsentation of the data (Banea et al., 2010; Xiao and\nGuo, 2012) or co-training (Wan, 2009; Demirtas\nand Pechenizkiy, 2013) to improve over a naive\nimplementation of machine translation, where only\nthe translated data is used. There are also ap-\nproaches which only require parallel data (Meng\net al., 2012; Zhou et al., 2016; Rasooli et al., 2017),\ninstead of machine translation.\nAll of these approaches, however, require large\namounts of parallel data or an existing high qual-\nity translation tool, which are not always available.\nA notable exception is the approach proposed by\nChen et al. (2016), an adversarial deep averaging\nnetwork, which trains a joint feature extractor for\ntwo languages. They minimize the difference be-\ntween these features across languages by learning\nto fool a language discriminator, which requires\nno parallel data. It does, however, require large\namounts of unlabeled data.\nBilingual Embedding Methods: Recently pro-\nposed bilingual embedding methods (Hermann and\nBlunsom, 2014; Chandar et al., 2014; Gouws et al.,\n2015) offer a natural way to bridge the language\ngap. These particular approaches to bilingual em-\nbeddings, however, require large parallel corpora\nin order to build the bilingual space, which are not\navailable for all language combinations.\nAn approach to create bilingual embeddings that\nhas a less prohibitive data requirement is to create\nmonolingual vector spaces and then learn a projec-\ntion from one to the other. Mikolov et al. (2013)\nfind that vector spaces in different languages have\nsimilar arrangements. Therefore, they propose a\nlinear projection which consists of learning a rota-\ntion and scaling matrix. Artetxe et al. (2016, 2017)\nimprove upon this approach by requiring the pro-\njection to be orthogonal, thereby preserving the\nmonolingual quality of the original word vectors.\nGiven source embeddings S, target embed-\ndings T , and a bilingual lexicon L, Artetxe et al.\n(2016) learn a projection matrix W by minimizing\nthe square of Euclidean distances\nargmin\nW\n∑\ni\n||S′W − T ′||2F , (1)\nwhere S′ ∈ S and T ′ ∈ T are the word embedding\nmatrices for the tokens in the bilingual lexicon L.\nThis is solved using the Moore-Penrose pseudoin-\nverse S′+ = (S′TS′)−1S′T as W = S′+T ′, which\ncan be computed using SVD. We refer to this ap-\nproach as ARTETXE.\nGouws and Søgaard (2015) propose a method to\ncreate a pseudo-bilingual corpus with a small task-\nspecific bilingual lexicon, which can then be used\nto train bilingual embeddings (BARISTA). This\napproach requires a monolingual corpus in both\nthe source and target languages and a set of trans-\nlation pairs. The source and target corpora are\nconcatenated and then every word is randomly kept\nor replaced by its translation with a probability of\n0.5. Any kind of word embedding algorithm can be\ntrained with this pseudo-bilingual corpus to create\nbilingual word embeddings.\nThese last techniques have the advantage of re-\nquiring relatively little parallel training data while\ntaking advantage of larger amounts of monolingual\ndata. However, they are not optimized for senti-\nment.\nSentiment Embeddings: Maas et al. (2011) first\nexplored the idea of incorporating sentiment in-\nformation into semantic word vectors. They pro-\nposed a topic modeling approach similar to latent\nDirichlet allocation in order to collect the semantic\ninformation in their word vectors. To incorporate\nthe sentiment information, they included a second\nobjective whereby they maximize the probability\nof the sentiment label for each word in a labeled\ndocument.\nTang et al. (2014) exploit distantly annotated\ntweets to create Twitter sentiment embeddings. To\nincorporate distributional information about tokens,\nthey use a hinge loss and maximize the likelihood\nof a true n-gram over a corrupted n-gram. They\ninclude a second objective where they classify the\npolarity of the tweet given the true n-gram. While\nthese techniques have proven useful, they are not\neasily transferred to a cross-lingual setting.\nZhou et al. (2015) create bilingual sentiment\nembeddings by translating all source data to the\ntarget language and vice versa. This requires the\nexistence of a machine translation system, which is\na prohibitive assumption for many under-resourced\nlanguages, especially if it must be open and freely\naccessible. This motivates approaches which can\nuse smaller amounts of parallel data to achieve\nsimilar results.\n3 Model\nIn order to project not only semantic similarity and\nrelatedness but also sentiment information to our\ntarget language, we propose a new model, namely\nBilingual Sentiment Embeddings (BLSE), which\njointly learns to predict sentiment and to minimize\nthe distance between translation pairs in vector\nspace. We detail the projection objective in Sec-\ntion 3.1, the sentiment objective in Section 3.2, and\nthe full objective in Section 3.3. A sketch of the\nmodel is depicted in Figure 1.\n3.1 Cross-lingual Projection\nWe assume that we have two precomputed vector\nspaces S = Rv×d and T = Rv′×d′ for our source\nand target languages, where v (v′) is the length of\nthe source vocabulary (target vocabulary) and d\n(d′) is the dimensionality of the embeddings. We\nalso assume that we have a bilingual lexicon L\nof length n which consists of word-to-word trans-\nlation pairs L = {(s1, t1), (s2, t2), . . . , (sn, tn)}\nwhich map from source to target.\nIn order to create a mapping from both origi-\nnal vector spaces S and T to shared sentiment-\ninformed bilingual spaces z and zˆ, we employ two\nlinear projection matrices, M and M ′. During\ntraining, for each translation pair in L, we first look\nup their associated vectors, project them through\ntheir associated projection matrix and finally mini-\nmize the mean squared error of the two projected\nvectors. This is very similar to the approach taken\nby Mikolov et al. (2013), but includes an additional\ntarget projection matrix.\nThe intuition for including this second matrix is\nthat a single projection matrix does not support the\ntransfer of sentiment information from the source\nlanguage to the target language. Without M ′, any\nsignal coming from the sentiment classifier (see\nSection 3.2) would have no affect on the target\nembedding space T , and optimizing M to predict\nsentiment and projection would only be detrimental\nto classification of the target language. We analyze\nthis further in Section 6.3. Note that in this con-\nfiguration, we do not need to update the original\nvector spaces, which would be problematic with\nsuch small training data.\nThe projection quality is ensured by minimizing\nthe mean squared error12\nMSE =\n1\nn\nn∑\ni=1\n(zi − zˆi)2 , (2)\nwhere zi = Ssi ·M is the dot product of the embed-\nding for source word si and the source projection\nmatrix and zˆi = Tti ·M ′ is the same for the target\nword ti.\n3.2 Sentiment Classification\nWe add a second training objective to optimize\nthe projected source vectors to predict the senti-\nment of source phrases. This inevitably changes\nthe projection characteristics of the matrix M , and\nconsequently M ′ and encourages M ′ to learn to\npredict sentiment without any training examples in\nthe target language.\nTo train M to predict sentiment, we re-\nquire a source-language corpus Csource =\n{(x1, y1), (x2, y2), . . . , (xi, yi)} where each sen-\ntence xi is associated with a label yi.\nFor classification, we use a two-layer feed-\nforward averaging network, loosely following Iyyer\net al. (2015)3. For a sentence xi we take the word\nembeddings from the source embedding S and av-\nerage them to ai ∈ Rd. We then project this vector\nto the joint bilingual space zi = ai ·M . Finally,\nwe pass zi through a softmax layer P to get our\nprediction yˆi = softmax(zi · P ).\nTo train our model to predict sentiment, we min-\nimize the cross-entropy error of our predictions\nH = −\nn∑\ni=1\nyi log yˆi − (1− yi) log(1− yˆi) . (3)\n3.3 Joint Learning\nIn order to jointly train both the projection com-\nponent and the sentiment component, we combine\nthe two loss functions to optimize the parameter\n1We omit parameters in equations for better readability.\n2We also experimented with cosine distance, but found\nthat it performed worse than Euclidean distance.\n3Our model employs a linear transformation after the aver-\naging layer instead of including a non-linearity function. We\nchoose this architecture because the weightsM andM ′ are\nalso used to learn a linear cross-lingual projection.\nThis hotel is  nice fun  No   está  muy bien\nEmbedding \nLayer\nAveraging Layer\nProjection Layer\nSoftmax Layer\ndivertido\nSource Language \nAnnotated Sentences Translation Dictionary\nTarget Language\nUnnanotated Sentences\nMinimize\nEuclidean\nDistance\nTRAINING TEST\nMinimize\nCrossentropy \nLoss\nFigure 1: Bilingual Sentiment Embedding Model (BLSE)\nEN ES CA EU\nB\nin\nar\ny + 1258 1216 718 956\n− 473 256 467 173\nTotal 1731 1472 1185 1129\n4-\ncl\nas\ns\n++ 379 370 256 384\n+ 879 846 462 572\n− 399 218 409 153\n−− 74 38 58 20\nTotal 1731 1472 1185 1129\nTable 1: Statistics for the OpeNER English (EN)\nand Spanish (ES) as well as the MultiBooked Cata-\nlan (CA) and Basque (EU) datasets.\nmatrices M , M ′, and P by\nJ =\n∑\n(x,y)∈Csource\n∑\n(s,t)∈L\nαH(x, y)+(1−α) ·MSE(s, t) ,\n(4)\nwhere α is a hyperparameter that weights sentiment\nloss vs. projection loss.\n3.4 Target-language Classification\nFor inference, we classify sentences from a target-\nlanguage corpus Ctarget. As in the training proce-\ndure, for each sentence, we take the word embed-\ndings from the target embeddings T and average\nthem to ai ∈ Rd. We then project this vector to the\njoint bilingual space zˆi = ai ·M ′. Finally, we pass\nSpanish Catalan Basque\nSentences 23 M 9.6 M 0.7 M\nTokens 610 M 183 M 25 M\nEmbeddings 0.83 M 0.4 M 0.14 M\nTable 2: Statistics for the Wikipedia corpora and\nmonolingual vector spaces.\nzˆi through a softmax layer P to get our prediction\nyˆi = softmax(zˆi · P ).\n4 Datasets and Resources\n4.1 OpeNER and MultiBooked\nTo evaluate our proposed model, we conduct ex-\nperiments using four benchmark datasets and three\nbilingual combinations. We use the OpeNER En-\nglish and Spanish datasets (Agerri et al., 2013)\nand the MultiBooked Catalan and Basque datasets\n(Barnes et al., 2018). All datasets contain hotel\nreviews which are annotated for aspect-level senti-\nment analysis. The labels include Strong Negative\n(−−), Negative (−), Positive (+), and Strong Pos-\nitive (++). We map the aspect-level annotations\nto sentence level by taking the most common label\nand remove instances of mixed polarity. We also\ncreate a binary setup by combining the strong and\nweak classes. This gives us a total of six experi-\nments. The details of the sentence-level datasets\nare summarized in Table 1. For each of the experi-\nFigure 2: Binary and four class macro F1 on Span-\nish (ES), Catalan (CA), and Basque (EU).\nments, we take 70 percent of the data for training,\n20 percent for testing and the remaining 10 percent\nare used as development data for tuning.\n4.2 Monolingual Word Embeddings\nFor BLSE, ARTETXE, and MT, we require monolin-\ngual vector spaces for each of our languages. For\nEnglish, we use the publicly available GoogleNews\nvectors4. For Spanish, Catalan, and Basque, we\ntrain skip-gram embeddings using the Word2Vec\ntoolkit4 with 300 dimensions, subsampling of 10−4,\nwindow of 5, negative sampling of 15 based on a\n2016 Wikipedia corpus5 (sentence-split, tokenized\nwith IXA pipes (Agerri et al., 2014) and lower-\ncased). The statistics of the Wikipedia corpora are\ngiven in Table 2.\n4.3 Bilingual Lexicon\nFor BLSE, ARTETXE, and BARISTA, we also re-\nquire a bilingual lexicon. We use the sentiment\nlexicon from Hu and Liu (2004) (to which we refer\nin the following as Bing Liu) and its translation\ninto each target language. We translate the lexicon\nusing Google Translate and exclude multi-word ex-\npressions.6 This leaves a dictionary of 5700 trans-\nlations in Spanish, 5271 in Catalan, and 4577 in\nBasque. We set aside ten percent of the translation\npairs as a development set in order to check that the\ndistances between translation pairs not seen during\ntraining are also minimized during training.\n4https://code.google.com/archive/p/word2vec/\n5http://attardi.github.io/wikiextractor/\n6Note that we only do that for convenience. Using a ma-\nchine translation service to generate this list could easily be\nreplaced by a manual translation, as the lexicon is comparably\nsmall.\n5 Experiments\n5.1 Setting\nWe compare BLSE (Sections 3.1–3.3) to ARTETXE\n(Section 2) and BARISTA (Section 2) as baselines,\nwhich have similar data requirements and to ma-\nchine translation (MT) and monolingual (MONO)\nupper bounds which request more resources. For\nall models (MONO, MT, ARTETXE, BARISTA),\nwe take the average of the word embeddings in\nthe source-language training examples and train a\nlinear SVM7. We report this instead of using the\nsame feed-forward network as in BLSE as it is the\nstronger upper bound. We choose the parameter c\non the target language development set and evalu-\nate on the target language test set.\nUpper Bound MONO. We set an empirical up-\nper bound by training and testing a linear SVM\non the target language data. As mentioned in Sec-\ntion 5.1, we train the model on the averaged em-\nbeddings from target language training data, tuning\nthe c parameter on the development data. We test\non the target language test data.\nUpper Bound MT. To test the effectiveness of\nmachine translation, we translate all of the senti-\nment corpora from the target language to English\nusing the Google Translate API8. Note that this\napproach is not considered a baseline, as we as-\nsume not to have access to high-quality machine\ntranslation for low-resource languages of interest.\nBaseline ARTETXE. We compare with the ap-\nproach proposed by Artetxe et al. (2016) which\nhas shown promise on other tasks, such as word\nsimilarity. In order to learn the projection matrix\nW , we need translation pairs. We use the same\nword-to-word bilingual lexicon mentioned in Sec-\ntion 3.1. We then map the source vector space\nS to the bilingual space Sˆ = SW and use these\nembeddings.\nBaseline BARISTA. We also compare with the\napproach proposed by Gouws and Søgaard (2015).\nThe bilingual lexicon used to create the pseudo-\nbilingual corpus is the same word-to-word bilin-\ngual lexicon mentioned in Section 3.1. We follow\nthe authors’ setup to create the pseudo-bilingual\ncorpus. We create bilingual embeddings by train-\ning skip-gram embeddings using the Word2Vec\ntoolkit on the pseudo-bilingual corpus using the\nsame parameters from Section 4.2.\n7LinearSVC implementation from scikit-learn.\n8https://translate.google.com\nOur method: BLSE. We implement our model\nBLSE in Pytorch (Paszke et al., 2016) and initial-\nize the word embeddings with the pretrained word\nembeddings S and T mentioned in Section 4.2.\nWe use the word-to-word bilingual lexicon from\nSection 4.3, tune the hyperparameters α, training\nepochs, and batch size on the target development\nset and use the best hyperparameters achieved on\nthe development set for testing. ADAM (Kingma\nand Ba, 2014) is used in order to minimize the\naverage loss of the training batches.\nBinary 4-class\nES CA EU ES CA EU\nU\npp\ner\nB\nou\nnd\ns\nM\nO\nN\nO P 75.0 79.0 74.0 55.2 50.0 48.3\nR 72.3 79.6 67.4 42.8 50.9 46.5\nF1 73.5 79.2 69.8 45.5 49.9 47.1\nM\nT\nP 82.3 78.0 75.6 51.8 58.9 43.6\nR 76.6 76.8 66.5 48.5 50.5 45.2\nF1 79.0 77.2 69.4 48.8 52.7 43.6\nB\nL\nSE\nP 72.1 **72.8 **67.5 **60.0 38.1 *42.5\nR **80.1 **73.0 **72.7 *43.4 38.1 37.4\nF1 **74.6 **72.9 **69.3 *41.2 35.9 30.0\nB\nas\nel\nin\nes Ar\nte\ntx\ne P 75.0 60.1 42.2 40.1 21.6 30.0\nR 64.3 61.2 49.5 36.9 29.8 35.7\nF1 67.1 60.7 45.6 34.9 23.0 21.3\nB\nar\nis\nta P 64.7 65.3 55.5 44.1 36.4 34.1\nR 59.8 61.2 54.5 37.9 38.5 34.3\nF1 61.2 60.1 54.8 39.5 36.2 33.8\nE\nns\nem\nbl\ne\nA\nrt\net\nxe P 65.3 63.1 70.4 43.5 46.5 50.1\nR 61.3 63.3 64.3 44.1 48.7 50.7\nF1 62.6 63.2 66.4 43.8 47.6 49.9\nB\nar\nis\nta P 60.1 63.4 50.7 48.3 52.8 50.8\nR 55.5 62.3 50.4 46.6 53.7 49.8\nF1 56.0 62.5 49.8 47.1 53.0 47.8\nB\nL\nSE\nP 79.5 84.7 80.9 49.5 54.1 50.3\nR 78.7 85.5 69.9 51.2 53.9 51.4\nF1 80.3 85.0 73.5 50.3 53.9 50.5\nTable 3: Precision (P), Recall (R), and macro F1 of\nfour models trained on English and tested on Span-\nish (ES), Catalan (CA), and Basque (EU). The bold\nnumbers show the best results for each metric per\ncolumn and the highlighted numbers show where\nBLSE is better than the other projection methods,\nARTETXE and BARISTA (** p < 0.01, * p < 0.05).\nEnsembles We create an ensemble of MT\nand each projection method (BLSE, ARTETXE,\nBARISTA) by training a random forest classifier\non the predictions from MT and each of these ap-\nproaches. This allows us to evaluate to what extent\neach projection model adds complementary infor-\nmation to the machine translation approach.\n5.2 Results\nIn Figure 2, we report the results of all four meth-\nods. Our method outperforms the other projection\nmethods (the baselines ARTETXE and BARISTA)\non four of the six experiments substantially. It per-\nforms only slightly worse than the more resource-\ncostly upper bounds (MT and MONO). This is espe-\ncially noticeable for the binary classification task,\nwhere BLSE performs nearly as well as machine\ntranslation and significantly better than the other\nmethods. We perform approximate randomization\ntests (Yeh, 2000) with 10,000 runs and highlight\nthe results that are statistically significant (**p <\n0.01, *p < 0.05) in Table 3.\nIn more detail, we see that MT generally per-\nforms better than the projection methods (79–69\nF1 on binary, 52–44 on 4-class). BLSE (75–69\non binary, 41–30 on 4-class) has the best perfor-\nmance of the projection methods and is comparable\nwith MT on the binary setup, with no significant\ndifference on binary Basque. ARTETXE (67–46\non binary, 35–21 on 4-class) and BARISTA (61–\n55 on binary, 40–34 on 4-class) are significantly\nworse than BLSE on all experiments except Cata-\nlan and Basque 4-class. On the binary experiment,\nARTETXE outperforms BARISTA on Spanish (67.1\nvs. 61.2) and Catalan (60.7 vs. 60.1) but suffers\nmore than the other methods on the four-class ex-\nperiments, with a maximum F1 of 34.9. BARISTA\nModel vo\nc\nm\nod\nne\ng\nkn\now\not\nhe\nr\nto\nta\nl\nMT\nbi 49 26 19 14 5 113\n4 147 94 19 21 12 293\nARTETXE\nbi 80 44 27 14 7 172\n4 182 141 19 24 19 385\nBARISTA\nbi 89 41 27 20 7 184\n4 191 109 24 31 15 370\nBLSE\nbi 67 45 21 15 8 156\n4 146 125 29 22 19 341\nTable 4: Error analysis for different phenomena.\nSee text for explanation of error classes.\nFigure 3: Macro F1 for translation pairs in the\nSpanish 4-class setup.\nis relatively stable across languages.\nENSEMBLE performs the best, which shows that\nBLSE adds complementary information to MT. Fi-\nnally, we note that all systems perform successively\nworse on Catalan and Basque. This is presum-\nably due to the quality of the word embeddings, as\nwell as the increased morphological complexity of\nBasque.\n6 Model and Error Analysis\nWe analyze three aspects of our model in further\ndetail: (i) where most mistakes originate, (ii) the ef-\nfect of the bilingual lexicon, and (iii) the effect and\nnecessity of the target-language projection matrix\nM ′.\n6.1 Phenomena\nIn order to analyze where each model struggles, we\ncategorize the mistakes and annotate all of the test\nphrases with one of the following error classes: vo-\ncabulary (voc), adverbial modifiers (mod), negation\n(neg), external knowledge (know) or other. Table 4\nshows the results.\nVocabulary: The most common way to express\nsentiment in hotel reviews is through the use of\npolar adjectives (as in “the room was great) or the\nmention of certain nouns that are desirable (“it\nhad a pool”). Although this phenomenon has the\nlargest total number of mistakes (an average of\n71 per model on binary and 167 on 4-class), it is\nmainly due to its prevalence. MT performed the\nbest on the test examples which according to the an-\nnotation require a correct understanding of the vo-\ncabulary (81 F1 on binary /54 F1 on 4-class), with\nBLSE (79/48) slightly worse. ARTETXE (70/35)\nand BARISTA (67/41) perform significantly worse.\nThis suggests that BLSE is better ARTETXE and\nBARISTA at transferring sentiment of the most im-\nportant sentiment bearing words.\nNegation: Negation is a well-studied phe-\nnomenon in sentiment analysis (Pang et al., 2002;\nWiegand et al., 2010; Zhu et al., 2014; Reitan et al.,\n2015). Therefore, we are interested in how these\nfour models perform on phrases that include the\nnegation of a key element, for example “In general,\nthis hotel isn’t bad”. We would like our models\nto recognize that the combination of two negative\nelements “isn’t” and “bad” lead to a Positive label.\nGiven the simple classification strategy, all mod-\nels perform relatively well on phrases with negation\n(all reach nearly 60 F1 in the binary setting). How-\never, while BLSE performs the best on negation in\nthe binary setting (82.9 F1), it has more problems\nwith negation in the 4-class setting (36.9 F1).\nAdverbial Modifiers: Phrases that are modified\nby an adverb, e. g., the food was incredibly good,\nare important for the four-class setup, as they often\ndifferentiate between the base and Strong labels.\nIn the binary case, all models reach more than 55\nF1. In the 4-class setup, BLSE only achieves 27.2\nF1 compared to 46.6 or 31.3 of MT and BARISTA,\nrespectively. Therefore, presumably, our model\ndoes currently not capture the semantics of the\ntarget adverbs well. This is likely due to the fact\nthat it assigns too much sentiment to functional\nwords (see Figure 6).\nExternal Knowledge Required: These errors\nare difficult for any of the models to get cor-\nrect. Many of these include numbers which imply\npositive or negative sentiment (350 meters from\nthe beach is Positive while 3 kilometers from the\nbeach is Negative). BLSE performs the best (63.5\nF1) while MT performs comparably well (62.5).\nBARISTA performs the worst (43.6).\nBinary vs. 4-class: All of the models suffer\nwhen moving from the binary to 4-class setting;\nan average of 26.8 in macro F1 for MT, 31.4 for\nARTETXE, 22.2 for BARISTA, and for 36.6 BLSE.\nThe two vector projection methods (ARTETXE and\nBLSE) suffer the most, suggesting that they are\ncurrently more apt for the binary setting.\n6.2 Effect of Bilingual Lexicon\nWe analyze how the number of translation pairs\naffects our model. We train on the 4-class Span-\nish setup using the best hyper-parameters from the\nprevious experiment.\n1.0\n0.5\n0\n-.0.5 10 20 30 40 50 60 70 10 20 30 40 50 60 70 10 20 30 40 50 60 70\nsource synonyms\nsource antonyms\ntranslation cosinetarget synonyms\ntarget antonyms\nCo\nsin\ne S\nim\nila\nrity\n(a) BLSE (b) Artetxe (c) Barista\nFigure 4: Average cosine similarity between a subsample of translation pairs of same polarity (“sentiment\nsynonyms”) and of opposing polarity (“sentiment antonyms”) in both target and source languages in each\nmodel. The x-axis shows training epochs. We see that BLSE is able to learn that sentiment synonyms\nshould be close to one another in vector space and sentiment antonyms should not.\nResearch into projection techniques for bilingual\nword embeddings (Mikolov et al., 2013; Lazaridou\net al., 2015; Artetxe et al., 2016) often uses a lex-\nicon of the most frequent 8–10 thousand words\nin English and their translations as training data.\nWe test this approach by taking the 10,000 word-\nto-word translations from the Apertium English-\nto-Spanish dictionary9. We also use the Google\nTranslate API to translate the NRC hashtag senti-\nment lexicon (Mohammad et al., 2013) and keep\nthe 22,984 word-to-word translations. We perform\nthe same experiment as above and vary the amount\nof training data from 0, 100, 300, 600, 1000, 3000,\n6000, 10,000 up to 20,000 training pairs. Finally,\nwe compile a small hand translated dictionary of\n200 pairs, which we then expand using target lan-\nguage morphological information, finally giving\nus 657 translation pairs10. The macro F1 score for\nthe Bing Liu dictionary climbs constantly with the\nincreasing translation pairs. Both the Apertium\nand NRC dictionaries perform worse than the trans-\nlated lexicon by Bing Liu, while the expanded hand\ntranslated dictionary is competitive, as shown in\nFigure 3.\nWhile for some tasks, e. g., bilingual lexicon\ninduction, using the most frequent words as trans-\nlation pairs is an effective approach, for sentiment\nanalysis, this does not seem to help. Using a trans-\nlated sentiment lexicon, even if it is small, gives\nbetter results.\n9http://www.meta-share.org\n10The translation took approximately one hour. We can\nextrapolate that hand translating a sentiment lexicon the size\nof the Bing Liu lexicon would take no more than 5 hours.\n1.0\n0.5\n0\n-.0.5 10 20 30 40 50 60 70\nCo\nsin\ne S\nim\nila\nrity\nEpochs\nBLSE\nNo M'\ntranslation\ntranslation\nsource F1\nsource F1\ntarget F1\ntarget F1\nFigure 5: BLSE model (solid lines) compared to a\nvariant without target language projection matrix\nM ′ (dashed lines). “Translation” lines show the\naverage cosine similarity between translation pairs.\nThe remaining lines show F1 scores for the source\nand target language with both varints of BLSE. The\nmodified model cannot learn to predict sentiment\nin the target language (red lines). This illustrates\nthe need for the second projection matrix M ′.\n6.3 Analysis of M ′\nThe main motivation for using two projection ma-\ntrices M and M ′ is to allow the original embed-\ndings to remain stable, while the projection ma-\ntrices have the flexibility to align translations and\nseparate these into distinct sentiment subspaces. To\njustify this design decision empirically, we perform\nan experiment to evaluate the actual need for the\ntarget language projection matrix M ′: We create a\nsimplified version of our model without M ′, using\nM to project from the source to target and then P\nto classify sentiment.\nThe results of this model are shown in Figure 5.\nThe modified model does learn to predict in the\nsource language, but not in the target language.\nThis confirms that M ′ is necessary to transfer sen-\ntiment in our model.\n7 Qualitative Analyses of Joint Bilingual\nSentiment Space\nIn order to understand how well our model trans-\nfers sentiment information to the target language,\nwe perform two qualitative analyses. First, we\ncollect two sets of 100 positive sentiment words\nand one set of 100 negative sentiment words. An\neffective cross-lingual sentiment classifier using\nembeddings should learn that two positive words\nshould be closer in the shared bilingual space than a\npositive word and a negative word. We test if BLSE\nis able to do this by training our model and after\nevery epoch observing the mean cosine similarity\nbetween the sentiment synonyms and sentiment\nantonyms after projecting to the joint space.\nWe compare BLSE with ARTETXE and BARISTA\nby replacing the Linear SVM classifiers with the\nsame multi-layer classifier used in BLSE and ob-\nserving the distances in the hidden layer. Figure 4\nshows this similarity in both source and target lan-\nguage, along with the mean cosine similarity be-\ntween a held-out set of translation pairs and the\nmacro F1 scores on the development set for both\nsource and target languages for BLSE, BARISTA,\nand ARTETXE. From this plot, it is clear that BLSE\nis able to learn that sentiment synonyms should be\nclose to one another in vector space and antonyms\nshould have a negative cosine similarity. While\nthe other models also learn this to some degree,\njointly optimizing both sentiment and projection\ngives better results.\nSecondly, we would like to know how well the\nprojected vectors compare to the original space.\nOur hypothesis is that some relatedness and simi-\nlarity information is lost during projection. There-\nfore, we visualize six categories of words in t-SNE\n(Van der Maaten and Hinton, 2008): positive senti-\nment words, negative sentiment words, functional\nwords, verbs, animals, and transport.\nThe t-SNE plots in Figure 6 show that the posi-\ntive and negative sentiment words are rather clearly\nseparated after projection in BLSE. This indicates\nthat we are able to incorporate sentiment informa-\ntion into our target language without any labeled\ndata in the target language. However, the downside\nBLSE\nOriginal\nFigure 6: t-SNE-based visualization of the Spanish\nvector space before and after projection with BLSE.\nThere is a clear separation of positive and negative\nwords after projection, despite the fact that we have\nused no labeled data in Spanish.\nof this is that functional words and transportation\nwords are highly correlated with positive sentiment.\n8 Conclusion\nWe have presented a new model, BLSE, which\nis able to leverage sentiment information from a\nresource-rich language to perform sentiment analy-\nsis on a resource-poor target language. This model\nrequires less parallel data than MT and performs\nbetter than other state-of-the-art methods with sim-\nilar data requirements, an average of 14 percentage\npoints in F1 on binary and 4 pp on 4-class cross-\nlingual sentiment analysis. We have also performed\na phenomena-driven error analysis which showed\nthat BLSE is better than ARTETXE and BARISTA\nat transferring sentiment, but assigns too much sen-\ntiment to functional words. In the future, we will\nextend our model so that it can project multi-word\nphrases, as well as single words, which could help\nwith negations and modifiers.\nAcknowledgements\nWe thank Sebastian Pado´, Sebastian Riedel, Eneko\nAgirre, and Mikel Artetxe for their conversations\nand feedback.\nReferences\nRodrigo Agerri, Josu Bermudez, and German Rigau.\n2014. Ixa pipeline: Efficient and ready to use mul-\ntilingual nlp tools. In Proceedings of the Ninth In-\nternational Conference on Language Resources and\nEvaluation (LREC’14). pages 3823–3828.\nRodrigo Agerri, Montse Cuadros, Sean Gaines, and\nGerman Rigau. 2013. OpeNER: Open polarity\nenhanced named entity recognition. Sociedad\nEspan˜ola para el Procesamiento del Lenguaje Nat-\nural 51(Septiembre):215–218.\nMariana S. C. Almeida, Claudia Pinto, Helena Figueira,\nPedro Mendes, and Andre´ F. T. Martins. 2015.\nAligning opinions: Cross-lingual opinion mining\nwith dependencies. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers). pages 408–418.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing. pages\n2289–2294.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers). pages 451–462.\nAlexandra Balahur and Marco Turchi. 2014. Compar-\native experiments using supervised learning and ma-\nchine translation for multilingual sentiment analysis.\nComputer Speech & Language 28(1):56–75.\nCarmen Banea, Rada Mihalcea, and Janyce Wiebe.\n2010. Multilingual subjectivity: Are more lan-\nguages better? In Proceedings of the 23rd Inter-\nnational Conference on Computational Linguistics\n(Coling 2010). pages 28–36.\nCarmen Banea, Rada Mihalcea, Janyce Wiebe, and\nSamer Hassan. 2008. Multilingual subjectivity anal-\nysis using machine translation. In Proceedings of\nthe 2008 Conference on Empirical Methods in Natu-\nral Language Processing. pages 127–135.\nJeremy Barnes, Patrik Lambert, and Toni Badia. 2018.\nMultibooked: A corpus of basque and catalan hotel\nreviews annotated for aspect-level sentiment classifi-\ncation. In Proceedings of 11th Language Resources\nand Evaluation Conference (LREC’18).\nSarath Chandar, Stanislas Lauly, Hugo Larochelle,\nMitesh Khapra, Balaraman Ravindran, Vikas C\nRaykar, and Amrita Saha. 2014. An autoencoder\napproach to learning bilingual word representations.\nIn Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, editors, Advances\nin Neural Information Processing Systems 27, Cur-\nran Associates, Inc., pages 1853–1861.\nXilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Q.\nWeinberger, and Claire Cardie. 2016. Adver-\nsarial deep averaging networks for cross-lingual\nsentiment classification. CoRR abs/1606.01614.\nhttp://arxiv.org/abs/1606.01614.\nErkin Demirtas and Mykola Pechenizkiy. 2013. Cross-\nlingual polarity detection with machine translation.\nProceedings of the International Workshop on Issues\nof Sentiment Discovery and Opinion Mining - WIS-\nDOM ’13 pages 9:1–9:8.\nKevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.\nIs machine translation ripe for cross-lingual senti-\nment classification? Proceedings of the 49th An-\nnual Meeting of the Association for Computational\nLinguistics: Human Language Technologies: short\npapers 2:429–433.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015. BilBOWA: Fast bilingual distributed repre-\nsentations without word alignments. Proceedings\nof The 32nd International Conference on Machine\nLearning pages 748–756.\nStephan Gouws and Anders Søgaard. 2015. Simple\ntask-specific bilingual word embeddings. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies. pages\n1386–1390.\nKarl Moritz Hermann and Phil Blunsom. 2014. Multi-\nlingual models for compositional distributed seman-\ntics. In Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers). Association for Computa-\ntional Linguistics, Baltimore, Maryland, pages 58–\n68.\nMinqing Hu and Bing Liu. 2004. Mining opinion\nfeatures in customer reviews. In Proceedings of\nthe 10th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining (KDD\n2004). pages 168–177.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daume III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classification.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers). Beijing,\nChina, pages 1681–1691.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations (ICLR) .\nAngeliki Lazaridou, Georgiana Dinu, and Marco Ba-\nroni. 2015. Hubness and pollution: delving into\ncross-space mapping for zero-shot learning. Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing pages 270–280.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. pages 142–150.\nXinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,\nGe Xu, and Houfeng Wang. 2012. Cross-lingual\nmixture model for sentiment classification. In Pro-\nceedings of the 50th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume\n1: Long Papers). Association for Computational\nLinguistics, Jeju Island, Korea, pages 572–581.\nhttp://www.aclweb.org/anthology/P12-1060.\nRada Mihalcea, Carmen Banea, and Janyce Wiebe.\n2007. Learning multilingual subjective language via\ncross-lingual projections. In Proceedings of the 45th\nAnnual Meeting of the Association of Computational\nLinguistics. pages 976–983.\nTomas Mikolov, Quoc V. Le, and Ilya Sutskever.\n2013. Exploiting similarities among languages\nfor machine translation. CoRR abs/1309.4168.\nhttp://arxiv.org/abs/1309.4168.\nSaif M. Mohammad, Svetlana Kiritchenko, and Xiao-\ndan Zhu. 2013. Nrc-canada: Building the state-of-\nthe-art in sentiment analysis of tweets. In Proceed-\nings of the seventh international workshop on Se-\nmantic Evaluation Exercises (SemEval-2013).\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up? sentiment classification using\nmachine learning techniques. In Proceedings of the\nACL-02 Conference on Empirical methods in natu-\nral language processing-Volume 10. Association for\nComputational Linguistics, pages 79–86.\nAdam Paszke, Sam Gross, Soumith Chintala, and Gre-\ngory Chanan. 2016. Pytorch deeplearning frame-\nwork. http://pytorch.org. Accessed: 2017-08-10.\nPeter Prettenhofer and Benno Stein. 2011. Cross-\nlingual adaptation using structural correspondence\nlearning. ACM Transactions on Intelligent Systems\nand Technology 3(1):1–22.\nMohammad Sadegh Rasooli, Noura Farra, Axinia\nRadeva, Tao Yu, and Kathleen McKeown. 2017.\nCross-lingual sentiment transfer with limited re-\nsources. Machine Translation .\nJohan Reitan, Jørgen Faret, Bjo¨rn Gamba¨ck, and Lars\nBungum. 2015. Negation scope detection for twitter\nsentiment analysis. In Proceedings of the 6th Work-\nshop on Computational Approaches to Subjectivity,\nSentiment and Social Media Analysis. pages 99–108.\nDuyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,\nand Bing Qin. 2014. Learning sentiment-specific\nword embedding for twitter sentiment classification.\nIn Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers). pages 1555–1565.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research 9:2579–2605.\nXiaojun Wan. 2009. Co-training for cross-lingual sen-\ntiment classification. In Proceedings of the Joint\nConference of the 47th Annual Meeting of the ACL\nand the 4th International Joint Conference on Natu-\nral Language Processing of the AFNLP. pages 235–\n243.\nMichael Wiegand, Alexandra Balahur, Benjamin Roth,\nDietrich Klakow, and Andre´s Montoyo. 2010. A sur-\nvey on the role of negation in sentiment analysis. In\nProceedings of the Workshop on Negation and Spec-\nulation in Natural Language Processing. pages 60–\n68.\nMin Xiao and Yuhong Guo. 2012. Multi-view ad-\naboost for multilingual subjectivity analysis. In Pro-\nceedings of COLING 2012. pages 2851–2866.\nAlexander Yeh. 2000. More accurate tests for the statis-\ntical significance of result differences. In Proceed-\nings of the 18th Conference on Computational lin-\nguistics (COLING). pages 947–953.\nGuangyou Zhou, Zhiyuan Zhu, Tingting He, and Xiao-\nhua Tony Hu. 2016. Cross-lingual sentiment classi-\nfication with stacked autoencoders. Knowledge and\nInformation Systems 47(1):27–44.\nHuiWei Zhou, Long Chen, Fulin Shi, and Degen\nHuang. 2015. Learning bilingual sentiment word\nembeddings for cross-language sentiment classifi-\ncation. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers).\npages 430–440.\nXiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-\nlana Kiritchenko. 2014. An empirical study on the\neffect of negation words on sentiment. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers). pages 304–313.\n",
      "id": 51504095,
      "identifiers": [
        {
          "identifier": "470620469",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.18653/v1/p18-1231",
          "type": "DOI"
        },
        {
          "identifier": "1805.09016",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:fis.uni-bamberg.de:uniba/93967",
          "type": "OAI_ID"
        },
        {
          "identifier": "618049396",
          "type": "CORE_ID"
        },
        {
          "identifier": "158378581",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1805.09016",
          "type": "OAI_ID"
        }
      ],
      "title": "Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across\n  Languages",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:fis.uni-bamberg.de:uniba/93967",
        "oai:arxiv.org:1805.09016"
      ],
      "publishedDate": "2018-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1805.09016",
        "https://fis.uni-bamberg.de/bitstreams/78d4c3df-0436-45f9-a200-2b91e1d96d03/download"
      ],
      "updatedDate": "2024-09-27T16:55:04",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/618049396.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/618049396"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/618049396/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/618049396/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/51504095"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1508.05056",
      "authors": [
        {
          "name": "Jiang Y.-G."
        },
        {
          "name": "Krizhevsky A."
        },
        {
          "name": "Lang P."
        },
        {
          "name": "LeCun Y."
        },
        {
          "name": "Plutchik R."
        },
        {
          "name": "Szegedy C."
        },
        {
          "name": "Tang Y."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Universitat Politècnica de Catalunya. GPI - Grup de Processament d'Imatge i Vídeo",
        "Universitat Politècnica de Catalunya. Departament de Teoria del Senyal i Comunicacions"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/41824045",
        "https://api.core.ac.uk/v3/outputs/644065523",
        "https://api.core.ac.uk/v3/outputs/301232165",
        "https://api.core.ac.uk/v3/outputs/474366096"
      ],
      "createdDate": "2015-09-24T01:39:35",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 1011,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1011",
          "logo": "https://api.core.ac.uk/data-providers/1011/logo"
        },
        {
          "id": 3228,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3228",
          "logo": "https://api.core.ac.uk/data-providers/3228/logo"
        },
        {
          "id": 10782,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/10782",
          "logo": "https://api.core.ac.uk/data-providers/10782/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Visual media are powerful means of expressing emotions and sentiments. The\nconstant generation of new content in social networks highlights the need of\nautomated visual sentiment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several vision problems,\ntheir application to the task of sentiment analysis is mostly unexplored and\nthere are few studies regarding how to design CNNs for this purpose. In this\nwork, we study the suitability of fine-tuning a CNN for visual sentiment\nprediction as well as explore performance boosting techniques within this deep\nlearning setting. Finally, we provide a deep-dive analysis into a benchmark,\nstate-of-the-art network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment prediction.Comment: Preprint of the paper accepted at the 1st Workshop on Affect and\n  Sentiment in Multimedia (ASM), in ACM MultiMedia 2015. Brisbane, Australi",
      "documentType": "research",
      "doi": "10.1145/2813524.2813530",
      "downloadUrl": "https://core.ac.uk/download/41824045.pdf",
      "fieldOfStudy": null,
      "fullText": "Diving Deep into Sentiment: Understanding Fine-tuned\nCNNs for Visual Sentiment Prediction\nVíctor Campos\n∗\nAmaia Salvador\n∗\nBrendan Jou\n†\nXavier Giró-i-Nieto\n∗\n∗\nUniversitat Politècnica de Catalunya (UPC), Barcelona, Catalonia/Spain\n†\nColumbia University, New York, NY USA\nvictor.campos.camunez@alu-etsetb.upc.edu, {amaia.salvador,xavier.giro}@upc.edu,\nbjou@ee.columbia.edu\nABSTRACT\nVisual media are powerful means of expressing emotions and\nsentiments. The constant generation of new content in so-\ncial networks highlights the need of automated visual senti-\nment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several\nvision problems, their application to the task of sentiment\nanalysis is mostly unexplored and there are few studies re-\ngarding how to design CNNs for this purpose. In this work,\nwe study the suitability of fine-tuning a CNN for visual sen-\ntiment prediction as well as explore performance boosting\ntechniques within this deep learning setting. Finally, we\nprovide a deep-dive analysis into a benchmark, state-of-the-\nart network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment predic-\ntion.\nCategories and Subject Descriptors\nH.1.2 [Models and Principles]: User/Machine Systems;\nI.2.10 [Artificial Intelligence]: Vision and Scene Under-\nstanding\nKeywords\nSentiment; Convolutional Neural Networks; Social Multime-\ndia; Fine-tuning Strategies\n1. INTRODUCTION\nThe recent growth of social networks has led to an ex-\nplosion in amount, throughput and variety of multimedia\ncontent generated every day. One reason for the richness\nof this social multimedia content comes from how it has be-\ncome one of the principal ways that users share their feelings\nand opinions about nearly every sphere of their lives. In par-\nticular, visual media, like images and videos, have risen as\none of the most pervasively used and shared documents in\nwhich emotions and sentiments are expressed.\nFigure 1: Overview of the presented system for vi-\nsual sentiment prediction.\nThe advantages of having machines capable of understand-\ning human feelings are numerous and would imply a revo-\nlution in fields such as robotics, medicine or entertainment.\nSome interesting preliminary applications are already begin-\nning to emerge, e.g. for emotional understanding of viewer\nresponses to advertisements using facial expressions [15].\nHowever, while machines are approaching human perfor-\nmance on several recognition tasks, such as image classi-\nfication [4], the task of automatically detecting sentiments\nand emotions from images and videos still presents many\nunsolved challenges. Numerous approaches towards bridg-\ning the affective gap, or the conceptual and computational\ndivide between low-level features and high-level affective se-\nmantics, have been presented over the years for visual multi-\nmedia [14, 5, 1, 9], but the performance has remained fairly\nconservative and related intuitions behind this have been\nlacking.\nPromising results obtained using Convolutional Neural\nNetworks (CNNs) [13] in many fundamental vision tasks\nhave led us to consider the efficacy of such machinery for\nhigher abstraction tasks like sentiment analysis, i.e. classi-\nfying the visual sentiment (either positive or negative) that\nan image provokes to a human. Recently, some works [27,\n25] explored CNNs for the task of visual sentiment analysis\nand obtained some encouraging results that outperform the\nstate of the art, but develop very little intuition and analysis\ninto the CNN architectures they used. Our work focuses on\nacquiring insight into fine-tuned layer-wise performance of\nCNNs in the visual sentiment prediction setting. We address\nthe task of assessing the contribution of individuals layers\nar\nX\niv\n:1\n50\n8.\n05\n05\n6v\n2 \n [c\ns.M\nM\n]  \n24\n A\nug\n 20\n15\nin a state-of-the-art fine-tuned CNN architecture for visual\nsentiment prediction.\nOur contributions include: (1) a visual sentiment pre-\ndiction framework that outperforms the state-of-the-art ap-\nproach on an image dataset collected from Twitter using a\nfine-tuned CNN, (2) a rigorous analysis of layer-wise perfor-\nmance in the task of visual sentiment prediction by training\nindividual classifiers on feature maps from each layer in the\nformer CNN, and (3) network architecture surgery applied\nto a fine-tuned CNN for visual sentiment prediction.\n2. RELATEDWORK\nSeveral approaches towards overcoming the gap between\nvisual features and affective semantic concepts can be found\nin the literature. In [21], the authors explore the poten-\ntial of two low-level descriptors common in object recogni-\ntion, Color Histograms (LCH, GCH) and SIFT-based Bag-\nof-Words, for the task of visual sentiment prediction. Some\nother works have considered the use of descriptors inspired\nby art and psychology to address tasks such as visual emo-\ntion classification [14] or automatic image adjustment to-\nwards a certain emotional reaction [17]. In [1] a Visual\nSentiment Ontology based on psychology theories and web\nmining consisting of 3,000 Adjective Noun Pairs (ANP) is\nbuilt. These ANPs serve as a mid-level representation that\nattempt to bridge the affective gap, but they are very de-\npendent on the data that was used to build the ontology and\nare not completely suitable for domain transfer.\nThe increase in computational power in GPUs and the\ncreation of large image datasets such as [3] have allowed\nDeep Convolutional Neural Networks (CNNs) to show out-\nstanding performance in computer vision challenges [11, 22,\n4]. And despite requiring huge amounts of training samples\nto tune their millions of parameters, CNNs have proved to\nbe very effective in domain transfer experiments [16]. This\ninteresting property of CNNs is applied to the task of vi-\nsual sentiment prediction in [25], where the winning archi-\ntecture of ILSVRC 2012 [11] (5 convolutional and 3 fully\nconnected layers) is used as a high-level attribute descrip-\ntor in order to train a sentiment classifier based on Logistic\nRegression. Although the authors do not explore the pos-\nsibility of fine-tuning, they show how the off-the-shelf CNN\ndescriptors outperform hand-crafted low-level features and\nSentiBank [1]. Given the distinct nature of visual sentiment\nanalysis and object recognition, the authors in [27] explore\nthe possibility of designing a new architecture specific for the\nformer task, training a network with 2 convolutional and 4\nfully connected layers. However, there is very little ratio-\nnale given for why they configured their network in this way\nexcept for the last two fully connected layers. Our work fo-\ncuses on fine-tuning a CNN for the task of visual sentiment\nprediction and later performing a rigorous analysis of its ar-\nchitecture, in order to shed some light on the problem of\nCNN architecture designing for visual sentiment analysis.\n3. METHODOLOGY\nThe Convolutional Neural Network architecture employed\nin our experiments is CaffeNet, a slight modification of the\nILSVRC 2012 winning architecture, AlexNet [11]. This net-\nwork, which was originally designed and trained for the task\nof object recognition, is composed by 5 convolutional layers\nand 3 fully connected layers. The two first convolutional lay-\ners are followed by pooling and normalization layers, while a\npooling layer is placed between the last convolutional layer\nand the first fully connected one. The experiments were\nperformed using Caffe [6], a publicly available deep learning\nframework.\nWe adapted CaffeNet to a sentiment prediction task us-\ning the Twitter dataset collected and published in [27]. This\ndataset contains 1,269 images labeled into positive or nega-\ntive by 5 different annotators. The choice was made based\non the fact that images in Twitter dataset are labeled by\nhuman annotators, oppositely to other annotation methods\nwhich rely on textual tags or predefined concepts. There-\nfore, the Twitter dataset is less noisy and allows the models\nto learn stronger concepts related to the sentiment that an\nimage provokes to a human. Given the subjective nature of\nsentiment, different subsets can be formed depending on the\nnumber of annotators that agreed on their decision. Only\nimages that built consensus among all the annotators (5-\nagree subset) were considered in our experiments. The re-\nsulting dataset is formed by 880 images (580 positive, 301\nnegative), which was later divided in 5 different folds to eval-\nuate experiments using cross-validation.\nEach of the following subsections is self-contained and de-\nscribes a different set of experiments. Although the training\nconditions for all the experiments were defined as similar as\npossible for the sake of comparison, there might be slight\ndifferences given each individual experimental setup. For\nthis reason, every section contains the experiment descrip-\ntion and its training conditions as well.\n3.1 Fine-tuning CaffeNet\nThe adopted CaffeNet [6] architecture contains more than\n60 million parameters, a figure too high for training the net-\nwork from scratch with the limited amount of data available\nin the Twitter dataset. Given the good results achieved by\nprevious works about transfer learning [16, 20], we decided\nto explore the possibility of fine-tuning an already exist-\ning model. Fine-tuning consists in initializing the weights\nin each layer except the last one with those values learned\nfrom another model. The last layer is replaced by a new\none, usually containing the same number of units as classes\nin the dataset, and randomly initializing their weights be-\nfore “resuming” training but with inputs from the target\ndataset. The advantage of this approach compared to fully\nre-training a network from a random initialization on all\nthe network weights is that it essentially starts the gradient\ndescent learning from a point much closer to an optimum,\nreducing both the number of iterations needed before con-\nvergence and decreasing the likelihood of overfitting when\nthe target dataset is small.\nIn our sentiment analysis task, the last layer from the orig-\ninal architecture, fc8, is replaced by a new one composed of\n2 neurons, one for positive and another for negative senti-\nment. The model of CaffeNet trained using ILSVRC 2012\ndataset is used to initialize the rest of parameters in the net-\nwork for the fine-tuning experiment. Results are evaluated\nusing 5-fold cross-validation. They are all fine-tuned during\n65 epochs (that is, every training image was seen 65 times by\nthe CNN), with an initial base learning rate of 0.001 that is\ndivided by 10 every 6 epochs. As the weights in the last layer\nare the only ones which are randomly initialized, its learning\nrate is set to be 10 times higher than the base learning rate\nin order to provide a faster convergence rate.\nFigure 2: Experimental setup for the layer analysis\nusing linear classifiers. The number between brack-\nets next to fully connected layer makes reference to\nthe amount of neurons they contain.\nA common practice when working with CNNs is data\naugmentation, consisting of generating different versions of\nan image by applying simple transformations such as flips\nand crops. Recent work has proved that this technique re-\nports a consistent improvement in accuracy [2]. We explored\nwhether data augmentation improves the spatial generaliza-\ntion capability of our analysis by feeding 10 different combi-\nnation of flips and crops of the original image to the network\nin the test stage. The classification scores obtained for each\ncombination are fused with an averaging operation.\n3.2 Layer by layer analysis\nDespite the outstanding performance of CNNs in many\nvision tasks, there is still little intuition into how to design\nthem. In order to gain some insight about the contribution\nof each individual layer to the the task of visual sentiment\nprediction, we performed an exhaustive layer-per-layer anal-\nysis of the fine-tuned network.\nThe outputs of individual layers have been previously used\nas visual descriptors [19, 20], where each neuron’s activation\nis seen as a component of the feature vector. Tradition-\nally, top layers have been selected for this purpose [25] as\nthey are thought to encode high-level information. We fur-\nther explore this possibility by using each layer as a feature\nextractor and training individual classifiers for each layer’s\nfeatures (see Figure 2). This study allows measuring the\ndifference in accuracy between layers and gives intuition not\nonly about how the overall depth of the network might affect\nits performance, but also about the role of each type of layer,\ni.e. CONV, POOL, NORM and FC, and their suitability for\nvisual sentiment prediction.\nNeural activations in fully connected layers can be rep-\nresented as d-dimensional vectors, being d the amount of\nneurons in the layer, so no further manipulation is needed.\nThis is not the case of earlier layers, i.e. CONV, NORM,\nand POOL, whose feature maps are multidimensional, e.g.\nfeature maps from conv5 are 256x13x13 dimensional. These\nfeature maps were flattened into d-dimensional vectors be-\nfore using them for classification purposes. Two different\nlinear classifiers are considered: Support Vector Machine\nwith linear kernel and Softmax. The same 5-fold cross-\nvalidation procedure followed in the previous experiment\nis employed, training independent classifiers for each layer.\nEach classifier’s regularization parameter is optimized by\ncross-validation.\n3.3 Layer ablation\nMore intuition about the individual contribution of each\nlayer can be gained by modifying the original architecture\nprior to training. This task is addressed by fine-tuning al-\ntered versions of the original CaffeNet where top layers had\nbeen successively removed.\nDifferent approaches to the layer removal problem might\nbe taken, depending on the changes made to the remaining\narchitecture. In our experiments, two different strategies are\nadopted: (1) a raw ablation by keeping the original configu-\nration and weights for the remaining layers, and (2) adding a\n2-neuron layer as a replacement to the removed one, on top\nof the remaining architecture and just before the Softmax\nlayer. A more detailed definition of the experimental setup\nfor each configuration is described in the following subsec-\ntions.\n3.3.1 Raw ablation\nIn this set of experiments, the Softmax layer is placed on\ntop of the remaining architecture, e.g. if fc8 and fc7 are\nremoved, the output of fc6 is connected to the input of the\nSoftmax layer. For the remaining layers, weights from the\noriginal model are kept as well.\nThe configurations studied in our experiments include ver-\nsions of CaffeNet where (1) fc8 has been ablated, and (2)\nboth fc8 and fc7 have been removed (architectures fc7-4096\nand fc6-4096, respectively, in Figure 3). The models are\ntrained during 65 epochs, with a base learning rate of 0.001\nthat is divided by 10 every 6 epochs. With this configuration\nall the weights are initialized using the pre-trained model, so\nrandom initialization of parameters is not necessary. Given\nthis fact, there is no need to increase the individual learning\nrate of any layer.\n3.3.2 2-neuron on top\nAs described in Section 3.1, fine-tuning consists in replac-\ning the last layer in a net by a new one and use the weights in\na pre-trained model as initialization for the rest of layers. In-\nspired by this procedure, we decided to combine the former\nmethodology with the layer removal experiments: instead of\nleaving the whole remaining architecture unmodified after\na layer is removed, its last remaining layer is replaced by a\n2-neuron layer with random initialization of the weights.\nThis set of experiments comprises the fine-tuning of mod-\nified versions of CaffeNet where (1) fc8 has been removed\nand fc7 has been replaced by a 2-neuron layer, and (2) fc8\nand fc7 have been ablated and fc6 has been replaced by a\n2-neuron layer (architectures fc7-2 and fc6-2, respectively,\nin Figure 3). The models are trained during 65 epochs, di-\nviding the base learning rate by 10 every 6 epochs and with\na learning rate 10 times higher than the base one for the 2-\nneuron layer, as its weights are being randomly initialized.\nThe base learning rate of the former configuration is 0.001,\nwhile the latter’s was set to 0.0001 to avoid divergence.\n3.4 Layer addition\nNone of the architectures that have been introduced so far\ntakes into account the information encoded in the last layer\n(fc8 ) of the original CaffeNet model. This layer contains a\nconfidence value for the image belonging to each one of the\n1,000 classes in ILSVRC 2012. In addition, fully connected\nlayers contain, by far, most of the parameters in a Deep\nConvolutional Neural Network. Therefore, from both of the\nFigure 3: Layer ablation architectures. Networks fc7-4096 and fc6-4096 keep the original configuration after\nablating the layers in the top of the architecture (Section 3.3.1), while in fc7-2 and fc6-2 the last remaining\nlayer is replaced by a 2-neuron layer (as described in Section 3.3.2). The number between brackets next to\nfully connected layer makes reference to the amount of neurons they contain.\nTable 1: 5-fold cross-validation results on 5-agree\nTwitter dataset\nModel Accuracy\nFine-tuned CNN from You et al. [27] 0.783\nFine-tuned CaffeNet 0.817 ± 0.038\nFine-tuned CaffeNet with oversampling 0.830 ± 0.034\nformer points of view, a remarkable amount of information is\nbeing lost when discarding the original fc8 layer in CaffeNet.\nSimilarly to the procedure followed in the layer removal\nexperiments, two different approaches are considered in or-\nder to take advantage of the information in the original fc8 :\n(1) the original CaffeNet architecture is fine-tuned, keep-\ning the original configuration and weights for fc8, and (2)\na 2-neuron layer (fc9 ) is added on top of the original ar-\nchitecture (architectures fc8-1000 and fc9-2, respectively, in\nFigure 4). Models are trained during 65 epochs, with a base\nlearning rate of 0.001 that is divided by 10 every 6 epochs.\nThe only layer that has a higher individual learning rate is\nthe new fc9 in configuration fc9-2, which is set to be 10 times\nhigher than the base learning rate, given that its weights are\nrandomly initialized.\n4. EXPERIMENTAL RESULTS\nThis section presents the results for the experiments pro-\nposed in the previous section, as well as intuition and con-\nclusions.\n4.1 Fine-tuning CaffeNet\nAverage accuracy results over the 5 folds for the fine-\ntuning experiment are presented in Table 1, which also in-\ncludes the results for the best fine-tuned model in [27]. This\nFigure 4: Architectures using the information con-\ntained in the original fc8 layer and weights. Config-\nuration fc8-1000 reuses the whole architecture and\nweights from CaffeNet, while fc9-2 features an addi-\ntional 2-neuron layer. The number between brackets\nnext to fully connected layer makes reference to the\namount of neurons they contain.\nTable 2: Layer analysis with linear classifiers: 5-fold\ncross-validation results on 5-agree Twitter dataset\nLayer SVM Softmax\nfc8 0.82 ± 0.055 0.821 ± 0.046\nfc7 0.814 ± 0.040 0.814 ± 0.044\nfc6 0.804 ± 0.031 0.81 ± 0.038\npool5 0.784 ± 0.020 0.786 ± 0.022\nconv5 0.776 ± 0.025 0.779 ± 0.034\nconv4 0.794 ± 0.026 0.781 ± 0.020\nconv3 0.752 ± 0.033 0.748 ± 0.029\nnorm2 0.735 ± 0.025 0.737 ± 0.021\npool2 0.732 ± 0.019 0.729 ± 0.022\nconv2 0.735 ± 0.019 0.738 ± 0.030\nnorm1 0.706 ± 0.032 0.712 ± 0.031\npool1 0.674 ± 0.045 0.68 ± 0.035\nconv1 0.667 ± 0.049 0.67 ± 0.032\nCNN, with a 2CONV-4FC architecture, was designed specif-\nically for visual sentiment prediction and trained using al-\nmost half million sentiment annotated images from Flickr\ndataset [1]. The network was finally fine-tuned on the Twit-\nter 5-agree dataset with a resulting accuracy of 0.783 which\nis, to best of our knowledge, the best result on this dataset\nso far.\nSurprisingly, fine-tuning a net that was originally trained\nfor object recognition reported higher accuracy in visual sen-\ntiment prediction than a CNN that was specifically trained\nfor that task. On one hand, this fact suggests the impor-\ntance of high-level representations such as semantics in vi-\nsual sentiment prediction, as transferring learning from ob-\nject recognition to sentiment analysis actually produces high\naccuracy rates. On the other hand, it seems that visual sen-\ntiment prediction architectures also benefit from a higher\namount of convolutional layers, as suggested by [28] for the\ntask of object recognition.\nAveraging the prediction over modified versions of the in-\nput image results in a consistent improvement in the predic-\ntion accuracy. This behavior, which was already observed by\nthe authors of [2] when addressing the task of object recog-\nnition, suggests that the former procedure also increases the\nnetwork’s generalization capability for visual sentiment anal-\nysis, as the final prediction is far less dependent on the spa-\ntial distribution of the input image.\n4.2 Layer by layer analysis\nThe results of the layer-by-layer analysis of the fine-tuned\nCaffeNet are presented in Table 2, both for the SVM and\nSoftmax classifiers.\nRecent works have studied the suitability of Support Vec-\nTable 3: Layer ablation: 5-fold cross-validation re-\nsults on 5-agree Twitter dataset.\nArchitecture Without oversampling With oversampling\nfc7-4096 0.759 ± 0.023 0.786 ± 0.019\nfc6-4096 0.657 ± 0.040 0.657 ± 0.040\nfc7-2 0.784 ± 0.024 0.797 ± 0.021\nfc6-2 0.651 ± 0.044 0.676 ± 0.029\ntor Machines for classification using deep learning descrip-\ntors [19] while others have also replaced the Softmax loss\nby a SVM cost in the network architecture [24]. Given the\nresults of our layer-wise analysis, it is not possible to claim\nthat any of the two classifiers provides a consistent gain\ncompared to the other for visual sentiment analysis, at least\nin the Twitter 5-agree dataset with the proposed network\narchitecture.\nAccuracy trends at each layer reveal that the depth of the\nnetworks contributes to the increase of performance. Not\nevery single layer produces an increase in accuracy with re-\nspect to the previous one, but even in those stages it is hard\nto claim that the architecture should be modified as higher\nlayers might be benefiting from its effect, e.g. conv5 and\npool5 report lower accuracy rates than earlier conv4 when\ntheir feature maps are used for classification, but later fully\nconnected layers might be benefiting from the effect of conv5\nand pool5 as all of them report higher accuracy than conv4.\nAn increase in performance is observed with each fully\nconnected layer, as every stage introduces some gain with\nrespect to the previous one. This fact suggests that adding\nadditional fully connected layers might report even higher\naccuracy rates, but further research is necessary to evaluate\nthis hypothesis.\n4.3 Layer ablation\nThe four ablation architectures depicted in Figure 3 are\ncompared in Table 3. These results indicate that replacing\nthe last remaining layer by a 2-neuron fully connected layer\nis a better solution than reusing the information of existing\nlayers from a much higher dimensionality. One reason for\nthis behavior might be the amount of parameters in each\narchitecture, as replacing the last layer by one with just\n2 neurons produces a huge decrease in the parameters to\noptimize and, given the reduced amount of available training\nsamples, that reduction can become beneficial.\nAccuracy is considerably reduced when ablating fc7 and\nsetting fc6 to be the last layer, independently of the method\nthat was used. Further research revealed that models learned\nfor architecture fc6-4096 always predict towards the major-\nity class, i.e. positive sentiment, which is justified by the\nreduced amount of training data. This behavior is not ob-\nserved in architecture fc6-2, where the amount of parameters\nis highly reduced in comparison to fc6-4096, but its perfor-\nmance is still very poor. Nevertheless, this result is somehow\nexpected, as the convergence from a vector dimensionality\n9,216 in pool5 to a layer with just 2 neurons might be too\nsudden. These observations suggest that a single fully con-\nnected layer might not be useful for the addressed task.\nFinally, it is important to notice that networks which are\nfine-tuned after ablating fc8, i.e. architectures fc7-4096 and\nfc7-2, provide accuracy rates which are very close to the fine-\ntuned CNN in [27] or even higher. These results, as shown by\nthe authors in [28] for the task of object recognition, suggest\nTable 4: Layer addition: 5-fold cross-validation re-\nsults on 5-agree Twitter dataset.\nArchitecture Without oversampling With oversampling\nfc8-1000 0.723 ± 0.041 0.731 ± 0.036\nfc9-2 0.795 ± 0.023 0.803 ± 0.034\nthat removing one of the fully connected layers (and with\nit, a high percentage of the parameters in the architecture)\nonly produces a slight deterioration in performance, but the\nhuge decrease in the parameters to optimize might allow the\nuse of smaller datasets without overfitting the model. This\nis a very interesting result for visual sentiment prediction\ngiven the difficulty of obtaining reliable annotated images\nfor such task.\n4.4 Layer addition\nThe architectures that keep fc8 are evaluated in Table\n4, indicating that architecture fc9-2 outperforms fc8-1000.\nThis observation, together with the previous in Section 4.3,\nstrengthens the thesis that CNNs deliver a higher perfor-\nmance in classification tasks when the last layer contains\none neuron for each class.\nThe best accuracy results when reusing information from\nthe original fc8 are obtained by adding a new layer, fc9, al-\nthough they are slightly worse than those obtained with the\nregular fine-tuning (Table 1). At first sight, this observation\nmay seem contrary to intuition gained in the layer-wise anal-\nysis, which suggested that a deeper architecture would have\na better performance. If a holistic view is taken and not only\nthe network architecture is considered, we observe that in-\ncluding information from the 1,000 classes in ILSVRC 2012\n(e.g. zebra, library, red wine) may not help in sentiment\nprediction, as they are mainly neutral or do not provide any\nsentimental cues without contextual information.\nThe reduction in performance when introducing semantic\nconcepts that are neutral with respect to sentiment, together\nwith the results in Section 4.2, highlight the importance of\nappropriate mid-level representation such as the Visual Sen-\ntiment Ontology built in [1] when addressing the task of vi-\nsual sentiment prediction. Nevertheless, they suggest that\ngeneric features such as neural codes in fc7 outperform se-\nmantic representations when the latter are not sentiment\nspecific. This intuition meets the results in [25], where the\nauthors found out that training a classifier using CaffeNet ’s\nfc7 instead of fc8 reported better performance for the task\nof visual sentiment prediction.\n5. CONCLUSIONS\nWe presented several experiments studying the suitability\nof fine-tuned CNNs for the task of visual sentiment predic-\ntion. We showed the utility of deep architectures that are\ncapable of capturing high level features when addressing the\ntask, obtaining models that outperform the best results so\nfar in the evaluation dataset. Data augmentation has been\ndemonstrated to be a useful technique for increasing visual\nsentiment prediction accuracy as well. Our study of domain\ntransfer from object recognition to sentiment analysis has re-\ninforced common good practices in the field: discarding the\nlast fully connected layer adapted to another task, and the\naddition of a new randomly initialized layer with as many\nneurons as the amount of categories to classify.\n6. ACKNOWLEDGMENTS\nThis work has been developed in the framework of the\nproject BigGraph TEC2013-43935-R, funded by the Spanish\nMinisterio de Economı´a y Competitividad and the European\nRegional Development Fund (ERDF). The Image Process-\ning Group at the UPC is a SGR14 Consolidated Research\nGroup recognized and sponsored by the Catalan Govern-\nment (Generalitat de Catalunya) through its AGAUR office.\nWe gratefully acknowledge the support of NVIDIA Corpo-\nration with the donation of the GeForce GTX Titan Z used\nin this work.\n7. REFERENCES\n[1] D. Borth, R. Ji, T. Chen, T. Breuel, and S.-F. Chang.\nLarge-scale visual sentiment ontology and detectors\nusing adjective noun pairs. In ACM MM, 2013.\n[2] K. Chatfield, K. Simonyan, A. Vedaldi, and\nA. Zisserman. Return of the devil in the details:\nDelving deep into convolutional nets. In British\nMachine Vision Conference, 2014.\n[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In Computer Vision and Pattern\nRecognition, 2009. CVPR 2009. IEEE Conference on,\npages 248–255. IEEE, 2009.\n[4] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep\ninto rectifiers: Surpassing human-level performance on\nImageNet classification. arXiv:abs/1502.01852\n[cs.CV], 2015.\n[5] J. Jia, S. Wu, X. Wang, P. Hu, L. Cai, and J. Tang.\nCan we understand van Gogh’s mood?: Learning to\ninfer affects from images in social networks. In ACM\nMM, 2012.\n[6] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,\nJ. Long, R. Girshick, S. Guadarrama, and T. Darrell.\nCaffe: Convolutional architecture for fast feature\nembedding. In ACM MM, 2014.\n[7] Y.-G. Jiang, B. Xu, and X. Xue. Predicting emotions\nin user-generated videos. In AAAI, 2014.\n[8] X. Jin, A. Gallagher, L. Cao, J. Luo, and J. Han. The\nwisdom of social multimedia: Using Flickr for\nprediction and forecast. In ACM MM, 2010.\n[9] B. Jou, S. Bhattacharya, and S.-F. Chang. Predicting\nviewer perceived emotions in animated GIFs. In ACM\nMM, 2014.\n[10] Y. Kim, H. Lee, and E. M. Provost. Deep learning for\nrobust feature generation in audiovisual emotion\nrecognition. In ICASSP, 2013.\n[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImageNet classification with deep convolutional neural\nnetworks. In NIPS, 2012.\n[12] P. Lang, M. Bradley, and B. Cuthbert. International\nAffective Picture System (IAPS): Technical manual\nand affective ratings. Technical report, NIMH CSEA,\n1997.\n[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\nGradient-based learning applied to document\nrecognition. In Proc. of the IEEE, 1998.\n[14] J. Machajdik and A. Hanbury. Affective image\nclassification using features inspired by psychology\nand art theory. In ACM MM, 2010.\n[15] D. McDuff, R. Kaliouby, J. Cohn, and R. Picard.\nPredicting ad liking and purchase intent: Large-scale\nanalysis of facial responses to ads.\n[16] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning\nand transferring mid-level image representations using\nconvolutional neural networks. In Computer Vision\nand Pattern Recognition (CVPR), 2014 IEEE\nConference on, pages 1717–1724. IEEE, 2014.\n[17] K.-C. Peng, T. Chen, A. Sadovnik, and A. Gallagher.\nA mixed bag of emotions: Model, predict, and transfer\nemotion distributions. In CVPR, 2015.\n[18] R. Plutchik. Emotion: A Psychoevolutionary\nSynthesis. Harper & Row, 1980.\n[19] A. S. Razavian, H. Azizpour, J. Sullivan, and\nS. Carlsson. CNN features off-the-shelf: An astounding\nbaseline for recognition. In Computer Vision and\nPattern Recognition Workshops (CVPRW), 2014\nIEEE Conference on, pages 512–519. IEEE, 2014.\n[20] A. Salvador, M. Zeppelzauer, D. Manchon-Vizuete,\nA. Calafell, and X. Giro-i Nieto. Cultural event\nrecognition with visual convnets and temporal models.\nIn Computer Vision and Pattern Recognition\nWorkshops (CVPRW), 2015 IEEE Conference on.\nIEEE, 2015.\n[21] S. Siersdorfer, E. Minack, F. Deng, and J. Hare.\nAnalyzing and predicting sentiment of images on the\nsocial web. In Proceedings of the international\nconference on Multimedia, pages 715–718. ACM, 2010.\n[22] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and\nA. Rabinovich. Going deeper with convolutions. arXiv\npreprint arXiv:1409.4842, 2014.\n[23] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,\nD. Erhan, I. Goodfellow, and R. Fergus. Intriguing\nproperties of neural networks. In ICLR, 2014.\n[24] Y. Tang. Deep learning using linear support vector\nmachines. In ICML Workshop on Challenges in\nRepresentation Learning, 2013.\n[25] C. Xu, S. Cetintas, K.-C. Lee, and L.-J. Li. Visual\nsentiment prediction with deep convolutional neural\nnetworks. arXiv preprint arXiv:1411.5731, 2014.\n[26] V. Yanulevskaya, J. van Gemert, K. Roth, A. Herbold,\nN. Sebe, and J. M. Geusebroek. Emotional valence\ncategorization using holistic image features. In ICIP,\n2008.\n[27] Q. You, J. Luo, H. Jin, and J. Yang. Robust image\nsentiment analysis using progressively trained and\ndomain transferred deep networks. In The\nTwenty-Ninth AAAI Conference on Artificial\nIntelligence (AAAI), 2015.\n[28] M. D. Zeiler and R. Fergus. Visualizing and\nunderstanding convolutional networks. In Computer\nVision–ECCV 2014, pages 818–833. Springer, 2014.\n[29] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and\nA. Torralba. Object detectors emerge in deep scene\ncnns. 2015.\n",
      "id": 18087668,
      "identifiers": [
        {
          "identifier": "10.1145/2813524.2813530",
          "type": "DOI"
        },
        {
          "identifier": "644065523",
          "type": "CORE_ID"
        },
        {
          "identifier": "41824045",
          "type": "CORE_ID"
        },
        {
          "identifier": "474366096",
          "type": "CORE_ID"
        },
        {
          "identifier": "301232165",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:upcommons.upc.edu:2117/82547",
          "type": "OAI_ID"
        },
        {
          "identifier": "29563947",
          "type": "CORE_ID"
        },
        {
          "identifier": "1508.05056",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1508.05056",
          "type": "OAI_ID"
        }
      ],
      "title": "Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual\n  Sentiment Prediction",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1508.05056",
        "oai:upcommons.upc.edu:2117/82547"
      ],
      "publishedDate": "2015-01-01T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1508.05056",
        "https://upcommons.upc.edu/bitstream/2117/82547/1/1508.05056v2.pdf"
      ],
      "updatedDate": "2025-02-28T17:14:26",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/41824045.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/41824045"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/41824045/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/41824045/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/18087668"
        }
      ]
    },
    {
      "acceptedDate": "2014-01-03T00:00:00",
      "arxivId": "1308.1847",
      "authors": [
        {
          "name": "Barker, Adam"
        },
        {
          "name": "Nguyen, Vu Dung"
        },
        {
          "name": "Varghese, Blesson"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/104454564",
        "https://api.core.ac.uk/v3/outputs/597095882",
        "https://api.core.ac.uk/v3/outputs/193989381",
        "https://api.core.ac.uk/v3/outputs/597425205",
        "https://api.core.ac.uk/v3/outputs/613778958"
      ],
      "createdDate": "2014-10-24T19:20:20",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 11419,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11419",
          "logo": "https://api.core.ac.uk/data-providers/11419/logo"
        },
        {
          "id": 2654,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2654",
          "logo": "https://api.core.ac.uk/data-providers/2654/logo"
        }
      ],
      "depositedDate": "2013-10-01T00:00:00",
      "abstract": "Analysis of information retrieved from microblogging services such as Twitter\ncan provide valuable insight into public sentiment in a geographic region. This\ninsight can be enriched by visualising information in its geographic context.\nTwo underlying approaches for sentiment analysis are dictionary-based and\nmachine learning. The former is popular for public sentiment analysis, and the\nlatter has found limited use for aggregating public sentiment from Twitter\ndata. The research presented in this paper aims to extend the machine learning\napproach for aggregating public sentiment. To this end, a framework for\nanalysing and visualising public sentiment from a Twitter corpus is developed.\nA dictionary-based approach and a machine learning approach are implemented\nwithin the framework and compared using one UK case study, namely the royal\nbirth of 2013. The case study validates the feasibility of the framework for\nanalysis and rapid visualisation. One observation is that there is good\ncorrelation between the results produced by the popular dictionary-based\napproach and the machine learning approach when large volumes of tweets are\nanalysed. However, for rapid analysis to be possible faster methods need to be\ndeveloped using big data techniques and parallel methods.Comment: http://www.blessonv.com/research/publicsentiment/ 9 pages. Submitted\n  to IEEE BigData 2013: Workshop on Big Humanities, October 201",
      "documentType": "research",
      "doi": "10.1109/bigdata.2013.6691669",
      "downloadUrl": "http://arxiv.org/abs/1308.1847",
      "fieldOfStudy": null,
      "fullText": "The Royal Birth of 2013:\nAnalysing and Visualising Public Sentiment in the UK Using Twitter\nVu Dung Nguyen, Blesson Varghesea and Adam Barkera\nBig Data Laboratory (http://bigdata.cs.st-andrews.ac.uk)\nSchool of Computer Science, University of St Andrews\nSt Andrews, Fife, UK KY16 9SX\nEmail: {vdn, varghese, adam.barker}@st-andrews.ac.uk\nAbstract—Analysis of information retrieved from microblog-\nging services such as Twitter can provide valuable insight into\npublic sentiment in a geographic region. This insight can be en-\nriched by visualising information in its geographic context. Two\nunderlying approaches for sentiment analysis are dictionary-\nbased and machine learning. The former is popular for public\nsentiment analysis, and the latter has found limited use for\naggregating public sentiment from Twitter data. The research\npresented in this paper aims to extend the machine learning\napproach for aggregating public sentiment. To this end, a\nframework for analysing and visualising public sentiment from\na Twitter corpus is developed. A dictionary-based approach\nand a machine learning approach are implemented within the\nframework and compared using one UK case study, namely\nthe royal birth of 2013. The case study validates the feasibility\nof the framework for analysis and rapid visualisation. One\nobservation is that there is good correlation between the results\nproduced by the popular dictionary-based approach and the\nmachine learning approach when large volumes of tweets are\nanalysed. However, for rapid analysis to be possible faster\nmethods need to be developed using big data techniques and\nparallel methods.\nKeywords-sentiment analysis; public opinion; aggregate sen-\ntiment; dictionary-based approach; machine learning; Twitter;\nroyal birth\nI. INTRODUCTION\nMicroblogging services such as Twitter have become an\nimportant platform for facilitating social interactions in mod-\nern society. As demonstrated by recent events such as the\nArab Spring and the Occupy Wall Street movements, these\nplatforms can be used to convey powerful ideas and allow\nthe general population to follow such events in real-time.\nThe information posted on these platforms is a rich resource\nfor obtaining insights into the sentiment of the general\npublic. The retrieval and analysis of such information is\noften referred to as sentiment analysis or opinion mining.\nTraditional methods for understanding public sentiment\nare questionnaires, surveys and polls which are extremely\nlimited in a number of ways. Firstly, they attract limited\nparticipation, and therefore, the sample is not a sufficient\nrepresentation of the public. Secondly, they are costly to\ndeploy and cannot be used on-the-fly without well laid out\naCorresponding authors\nlogistical plans. Thirdly, they cannot gather the sentiment as\nan event is unfolding. For example, using traditional methods\nthe sentiment of the people participating in the Occupy Wall\nStreet movement could perhaps be gathered only after the\nevent had finished.\nCurrently, Twitter with more than half a billion users is\nbeing used as a source for retrieving information. Twitter\nprovides free information through an interface in the form\nof a stream. Analysis of this information has led to a variety\nof research. Examples include prediction of elections [1],\nstock market [2], and movie sales [3], notification of events\nsuch as earthquakes [4], analysis of natural disasters [5] and\npublic health information [6], estimation of public sentiment\nduring elections [7] and recession [8]. This research along\nwith [9] are exemplars of how correlated the information\nretrieved from Twitter and the actual events are. Hence,\nmoving forward a question that arises is - ‘Why not visualise\nthe information in its geographic context in real-time?’.\nThe research reported in this paper is motivated towards\nanalysing public sentiment related to an event affecting a\ngeographic region in real-time and rapidly visualising it.\nThe most common approach employed for analysing\npublic sentiment is dictionary-based [1], [2], [3] which is\nsimple to implement. Public sentiment, for example, happy,\nsad or depressed, is understood by comparing tweets against\nlexicons from dictionaries. A second possible approach that\ncan be employed is machine learning. This approach is not\nreadily available for understanding public (or aggregate)\nsentiment [10]. However, it is used in understanding the\nsentiment of individual tweets with high accuracy [11], [12].\nThe research in this paper explores how the machine learning\napproach can be extended for public sentiment analysis.\nThe notable difference between the two approaches is that\nthe dictionary-based approach classifies individual words in\ntweets while the machine learning approach classifies an\nentire tweet. The machine learning approach is quantitatively\ncompared to the dictionary-based approach in this paper.\nThe contributions of the research presented in this paper\nare: (i) the development of a framework for analysing and\nvisualising public sentiment from a Twitter corpus, (ii) the\nimplementation and comparison of two approaches within\nar\nX\niv\n:1\n30\n8.\n18\n47\nv2\n  [\ncs\n.C\nL]\n  1\n6 A\nug\n 20\n13\nthe framework for analysing public sentiment, (iii) the\ninvestigation of visualisation techniques for public sentiment\nat multiple geographic levels, and (iv) the analysis and\nvisualisation of a Twitter corpus during the birth of Prince\nGeorge of Cambridge in 2013 as a case study.\nThe remainder of this paper is organised as follows. Sec-\ntion II presents a framework for using Twitter to understand\npublic sentiment. Section III employs the framework for\nunderstanding public sentiment in the UK at the time of\nthe royal birth of 2013. Section IV concludes this paper by\nconsidering future work.\nII. FRAMEWORK\nThe framework for analysing and visualising public sen-\ntiment presented in this paper can be used to understand\nthe shift of public sentiment seen in tweets and graphically\ndisplay the sentiment across hours or days or weeks. A score\nthat broadly captures public sentiment is estimated based on\ntwo indicators. The first indicator is a positive score to rate\nhow positive the sentiment in a geographic region is. The\nsecond indicator is a negative score to rate negative public\nsentiment in an area. The score can also be normalised with\nlower and upper bounds as zero and one respectively. The\nscore can be visualised in two geographic levels, namely\ncountry and county using a number of visualisation tech-\nniques.\nThe framework as shown in Figure 1 consists of six\nmodules, namely the Collector, the Parser, the Database, the\nAnalyser, the Estimator and the Visualiser. The Collector\nmodule gathers the Twitter corpus. The Parser ensures that\nthe obtained corpus is in a format that can be used by the\nsubsequent modules in the framework. The Database module\nis a collection of tables containing Twitter data for time\nperiods ranging from minutes to hours to days. The Analyser\nmodule mines through the tweets to analyse sentiment. The\nEstimator module estimates the scores indicating public sen-\ntiment. The visualisation of the scores is facilitated through\nthe Visualiser. The flow of data within the framework is also\nconsidered in Figure 1.\nA. Collector\nThe Collector module is responsible for gathering the\nTwitter corpus from the Web. The corpus is collected in the\nJSON format, in real-time, through the Twitter Streaming\nAPI1. This API not only provides features to select the\ngeographic region of the tweets’ origin but also provides\noptions to select parameters such as keywords and language.\nB. Parser\nThe Parser module is essential to trim the corpus offline.\nThe collection and trimming operations are performed in two\ndifferent stages since the Twitter Streaming API provides\ntweets at a fast rate. Parsing the corpus in real-time may\n1https://dev.twitter.com/docs/streaming-apis\nFigure 1: Framework for analysing and visualising public\nsentiment\ncause the tweets that are streamed to be lost if the Parser\ncannot keep up with the data flow of the Streaming API.\nThe output from the Parser makes the corpus readable for\nthe subsequent modules in the framework.\nC. Database\nThe Database module consists of three tables shown\nas T1, T2 and T3 in Figure 1. T1 is the tweet corpus\ngathered by the Collector. T1 is then parsed to produce T2,\na trimmed readable table. The Analyser retrieves data from\nT2 for analysis and the Estimator writes T3 containing the\npublic sentiment scores and associated geographic and time\ninformation.\nD. Analyser\nThis module performs sentiment analysis to extract the\nsentiment of the tweets. Two approaches are explored in\nthis paper for performing sentiment analysis, namely the\ndictionary-based and machine learning approaches. The aim\nof both the approaches is to estimate a score that captures\nthe degree of ‘positive’ or ‘negative’ public sentiment of\na geographic region in a time frame by evaluating a col-\nlection of tweets or individual tweets. The dictionary-based\napproach considers the entire collection of tweets for a given\ntime period to aggregate the public sentiment across the\ncollection. However, in the machine learning approach each\ntweet in the collection is assigned a sentiment score and then\nthe public sentiment is aggregated from individual scores.\n(a) Dictionary-based\n(b) Machine learning\nFigure 2: Sentiment analysis appraoches\nThe public sentiment score generated by both the approaches\nis independent of the number of tweets.\n1) Approach 1 - Dictionary-based: Figure 2a shows the\ndictionary-based approach. The input is a data set selected\nfor a time period from a specified geographic region (for\nexample, country or county). The tweets of the selected\ndata set are tokenised using a lexical analyser. For this the\nStanford tokeniser [13], [14] which incorporates the Penn\nTreebank 3 (PTB) tokenisation algorithm [15] is employed.\nThe tokens are then matched against a dictionary; the\nEmotional Lookup Table provided by SentiStrength [16],\n[17] is used as the dictionary. While matching, the number\nof positive sentiment and negative sentiment words in the\nentire set of tokens are counted. Then the public sentiment is\naggregated by calculating the ratio of the positive sentiment\nto negative sentiment words.\n2) Approach 2 - Machine Learning: Figure 2b shows the\nmachine learning approach. In contrast to the dictionary-\nbased approach in which ‘prior linguistic knowledge’ in\nthe form of dictionaries were used, the machine learning\napproach implemented in this paper considers a supervised\ntraining technique. The machine learning approach is pre-\nsented in three phases - firstly, the training phase, secondly,\nthe testing phase, and finally, the deployment phase.\nIn the training phase, the training data was collected using\nthe approach presented in [18] which relies on the Distant\nSupervisor technique [19]. The training data set contains\n23,000 tweets which are labelled as positive or negative. This\napproach is in contrast to the manual approach reported in\n[20], [21] which requires human intervention for labelling\ntweets. Unigram features are extracted from the training data\nset to train the classifier model; the Naive Bayes Classifier\nmodel is used.\nAfter training the model, in the testing phase, the approach\nis tested using the data set available from [22]. The test\nresults indicate over 70% accuracy in labelling tweets and a\nsimilar finding is reported in [18], [23].\nIn the deployment phase, the tweets for a geographic\nregion are selected from the table containing parsed tweets,\nT2. These tweets are labelled using the Classifier obtained\nfrom the training phase. The number of positive sentiment\nand negative sentiment tweets in the entire collection of\ntweets is counted, and public sentiment is then aggregated\nby calculating the ratio of the positive sentiment to negative\nsentiment tweets.\nE. Estimator\nThe Estimator module computes a score that captures\npublic sentiment. The estimation technique employed in\nthe dictionary-based approach is subtly different from the\nmachine learning approach and is considered in this section.\n1) Estimation in the dictionary-based approach: Con-\nsider a geographic region defined by g = 1 and 2, where\ng = 1 for a country and g = 2 for a county and time frame\nt. The public sentiment score is defined as:\nPSS(g,t) =\ncount(g,t)\n(\npositive words\n)\ncount(g,t)\n(\nnegative words\n) (1)\nThe example illustrated in Figure 3 for a geographic\nregion has one country, mycountry with two counties\nhappycounty and sadcounty. The tweets for the region are\nselected from the table containing parsed tweets, T2, for a\ntime frame denoted as t, starting at tstart and ending at tend.\nThe selected data during the time frame is represented in the\nfigure as a collection of nine tweets, five from happycounty\nand four from sadcounty. The tweets are then matched\nagainst a dictionary which results in the recognition of\npositive and negative words. In the figure, the positive words\nare represented in blue and the negative words in red. The\nnumber of positive words in the tweets is twelve (ten from\nFigure 3: Illustration of an example using dictionary-based approach\nhappycounty and two from sadcounty) and the number of\nnegative words is five (two from happycounty and three\nfrom sadcounty). Therefore, the public sentiment score for\ntime t at country level for mycountry is 2.4, and the public\nsentiment score at the county level for happycounty is 5.0\nand sadcounty is 0.66. The scores for the counties can be\nnormalised between 0 and 1, and so the normalised public\nsentiment score is 1.0 for happycounty and is 0.132 for\nsadcounty. Geographic distinctions (counties) can highlight\nthe finer level of detail which can be lost when aggregated\nto higher geographic level (country).\n2) Estimation in the Machine Learning approach: Con-\nsider a geographic region defined by g = 1 and 2, where\ng = 1 for a country and g = 2 for a county and time frame\nt. The public sentiment score is defined as:\nPSS(g,t) =\ncount(g,t)\n(\npositive tweets\n)\ncount(g,t)\n(\nnegative tweets\n) (2)\nThe example illustrated in Figure 4 for a geographic\nregion has one country, mycountry with two counties\nhappycounty and sadcounty. The tweets for the region are\nselected from the table containing parsed tweets, T2, for a\ntime frame denoted as t, starting at tstart and ending at tend.\nThe selected data during the time frame is represented in the\nfigure as a collection of nine tweets, five from happycounty\nand four from sadcounty. The classifier labels the tweets\nas positive sentiment and negative sentiment. In the figure,\nthe positive tweets are represented in blue and the nega-\ntive tweets in red. The number of positive tweets is five\n(four from happycounty and one from sadcounty) and the\nnumber of negative tweets is four (one from happycounty\nand three from sadcounty). Therefore, the public sentiment\nscore for time t at country level for mycountry is 1.25,\nand the public sentiment scores at the county levels for\nhappycounty and sadcounty are 4 and 0.33 respectively.\nThe normalised public sentiment score between 0 and 1\nfor the counties are 1.0 for happycounty and 0.0825 for\nsadcounty.\nFigure 4: Illustration of an example using machine learning\napproach\nThe PSS score from both approaches are normalised to\nNPSS to be able to compare the public sentiment trend\nestimated by the approaches.\nF. Visualiser\nThe Visualiser module facilitates the graphical display\nof public sentiment using three visualisation techniques.\nThe first technique is choropleth visualisation of public\nsentiment on a geo-browser. In the research reported in this\npaper, Google Earth2 is employed as the geo-browser. The\nThematic Mapping Engine (TME) [24] is used for generating\n.kml files [25] in which public sentiment data overlays\ngeographic data. Choropleth is useful for presenting public\nsentiment as a gradient of colours, and in this framework the\npublic sentiment of a country is presented using choropleth.\nFor example, the public sentiment of England, Scotland,\nWales and N. Ireland is represented by overlaying colours\nindicative of public sentiment in each country over the geo-\ngraphic region on Google Earth. Public sentiment of counties\nare not best represented using choropleths since it would be\nvisually difficult to distinguish between colours overlaid on\n2http://earth.google.co.uk/\nsmall geographic regions. While multiple dimensions of data\ncan be represented using distinct gradient scales it may be\nvisually challenging to distinguish between the scales.\nThe second technique using tile-maps is independent of\na geo-browser. A geographic region is represented as a\ntile and the public sentiment of the region can be visually\ndistinguished not only based on the colour of the tile but also\non its size. Google Charts API 3 is used for obtaining tile-\nmaps in the framework. For example, the public sentiments\nof all the counties in the UK are represented using tiles.\nThe third technique using line graph visualisation is again\nindependent of a geo-browser. This technique is useful to\nunderstand the relative performance of the two sentiment\nanalysis approaches over the dimension of time. For exam-\nple, the public sentiment in England in the hour following\nthe announcement of Prince George’s birth, estimated using\nthe dictionary-based approach and the machine learning ap-\nproach, can be compared and represented using line graphs.\nIII. CASE STUDY: UK ROYAL BIRTH, 2013\nThe royal birth of Prince George of Cambridge on Mon-\nday, 22 July, 2013 at 16:24 BST to the Duke and Duchess of\nCambridge is considered in the framework for analysing and\nvisualising public sentiment. The first Twitter announcement\non the day of birth that the arrival of the baby was soon\nexpected was made at 07.37 BST. This attracted a lot of\nattention from Twitter users in the UK and across the world.\nNearly 487 million users accessed tweets related to the\nbirth4. This section considers the pipeline of activities to\nanalyse the tweet corpus, followed by visualising the results\nobtained from the analysis, and finally, summarises the key\nobservations from the case study.\nA. Analysing the tweets\nThe Twitter corpus was being collected for the UK by\nthe Collector module using the Twitter Streaming API from\nSunday, July 21 2013, 00:00:01 BST until Tuesday, 23\nJuly, 2013, 23:59:59 BST. Nearly one million tweets were\ncollected from over 150,000 Twitter users. The case study is\nused to compare the dictionary-based and machine learning\napproaches. The geographic area taken into account is the\nUK.\nThe Parser module trimmed the corpus, and the fine level\nof geographic details, namely latitude and longitude, was\nused to map the tweets onto the county and the country\nof origin using the Global Administrative Areas (GADM)\nspatial database5 as shapefiles (.shp) [26]. The dictionary-\nbased and machine learning approaches were used for sen-\ntiment analysis and the aggregation of public sentiment was\n3https://developers.google.com/chart/\n4http://www.dailymail.co.uk/news/article-2374252/Royal-babys-birth-\\\nnews-sends-Twitter-meltdown-487m-congratulate-Duchess-Cambridge.\nhtml\n5http://www.gadm.org\nperformed. The results obtained at the country level for\nJuly 21, July 22 and July 23 are summarised in Table I,\nwhere PSS is the Public Sentiment Score and NPSS is the\nnormalised PSS.\nB. Visualisation\nThree techniques presented in Section II are considered\nfor visualising the public sentiment in the UK. They are\nfirstly, the choropleth visualisation technique is overlaid on\nGoogle Earth for the country level, secondly the tile-map\nvisualisation technique for the county level, and thirdly, the\nline graph visualisation technique on a hourly basis at the\ncountry level.\n1) Visualisation on geo-browser: Figure 5 shows screen-\nshots of PSS using choropleth visualisation on Google Earth\nfor July 21, July 22 and July 23 based on Table I. The highest\nvolume of tweets was obtained from England, followed by\nWales and then Scotland. The smallest number of tweets\nduring the three day period was from N. Ireland. On July\n22 and July 23 the dictionary-based approach estimates\nEngland to have had the highest PSS compared to the other\ncountries. Surprisingly, on the day after the birth, England\ndropped to the third place. On the other hand, the machine\nlearning approach places England consistently in third place.\nThe machine learning approach estimates Wales to have the\nhighest PSS on all three days.\nFurther, a correlation analysis between the PSS obtained\nfrom both the approaches was performed. The results ob-\ntained are summarised in Table II, where the correlation ratio\nindicates the closeness of the PSS scores estimated by the\ndictionary-based and machine learning approaches. Given\nthe large volume of tweets analysed for England, there is a\nlarge correlation of over 80% between the results produced\nby both the approaches. The two approaches produce least\ncorrelated results for Wales, and the correlation ratios for\nScotland and N. Ireland are not high. This is perhaps because\nthe analysis on larger volumes of tweets can produce higher\nquality of results.\n2) Visualisation using tile-maps: Figure 6 shows the tile-\nmap representation of the NPSS corresponding to all UK\ncounties using the dictionary-based approach and machine\nlearning approach. Each tile represents a county, and the size\nof each tile is relative to the volume of tweets that originated\nfrom the county. The colour of the tile is indicative of the\nnormalised PSS varying from shades of red (lowest NPSS\nscore) to green (highest NPSS score). The largest volume of\ntweets is from Manchester, West Yorkshire, West Midlands,\nLancashire, Essex all in England, and the lowest volume is\nfrom Strabane, Larne and Moyle in N. Ireland, Rhonndda\nin Wales, Orkney Islands and Shetland Islands. Using the\ndictionary-based approach the public sentiment score is\nhighest for the Greater London area that includes London,\nSutton, Westiminster, Kensington and Chelsea, Tower Ham-\nlets and Islington, and is the lowest for Shetland Islands,\n(a) Dictionary-based Approach - 21 July, 2013 (b) Dictionary-based Approach - 22 July, 2013 (c) Dictionary-based Approach - 23 July, 2013\n(d) Machine Learning Approach - 21 July, 2013 (e) Machine Learning Approach - 22 July, 2013 (f) Machine Learning Approach - 23 July, 2013\nFigure 5: Public Sentiment Score of England, Wales, Scotland and N. Ireland for case study\nCountry No. of\nTweets\nDictionary-based Approach Machine Learning Approach\nNPSS PSS No. ofPositive Words\nNo. of\nNegative Words NPSS PSS\nNo. of\nPositive Tweets\nNo. of\nNegative Tweets\n21 July 2013\nEngland 315,658 1.0000 1.6620 166,607 100,244 0.8925 1.0270 159,928 155,730\nScotland 39,233 0.8351 1.3880 20,384 14,685 0.8630 0.9930 19,548 19,685\nWales 22,322 0.8688 1.4439 11,379 7,881 1.0000 1.1507 11,943 10,379\nN. Ireland 7,864 0.9401 1.5625 4,389 2,809 0.9666 1.1123 4,141 3,723\n22 July 2013\nEngland 322,554 1.0000 1.7398 176,784 102,189 0.9648 1.0992 162,986 159,568\nScotland 10,312 0.7980 1.3884 5,247 3,779 0.8502 0.9686 5,074 5,238\nWales 22,904 0.8794 1.5301 12,522 8,184 1.0000 1.1392 12,197 10,707\nN. Ireland 8,031 0.9943 1.7299 4,755 2,733 0.8966 1.0214 4,205 3,826\n23 July 2013\nEngland 351,201 0.8801 1.5621 188,931 120,948 0.8535 0.9824 174,045 177,156\nScotland 13,816 0.7771 1.3793 7,509 5,444 0.8460 0.9734 6,815 7,001\nWales 24,233 0.8166 1.4493 13,039 8,997 1.0000 1.1510 12,967 11,266\nN. Ireland 8,222 1.0000 1.7749 4,755 2,679 0.9581 1.1028 4,312 3,910\nTable I: Summary of results from case study\n(a) Dictionary-based approach (b) Machine learning approach\nFigure 6: Tile-map representation of Public Sentiment Score in UK counties\nArmagh in N. Ireland and Rhondda in Wales. There is a\npredominance of the red shade and this is largely because\nthere are relatively few high PSS values. Therefore, when\nthe lower PSS values are normalised using the approach\npresented in Section II they diminish greatly.\nThe trends seen in the dictionary-based approach are\nquite comparable to the trends seen in the machine learning\napproach. Using the machine learning approach Strabane,\nShetland Islands and Rhondda have very high PSS scores\nwhich are notable exceptions. This is so because a very\nsmall number of tweets are analysed for these counties.\nSurprisingly, Rhondda falls under the exception though there\nis a reasonably large volume of tweets. Similar to the\ndictionary-based approach, Larne has the a low NPSS in\nthe machine learning approach. The regions that had a high\nNPSS score in the dictionary-based approach are also found\nto have a high NPSS score using machine learning.\n3) Visualisation using line graphs: Figure 7 shows the\nvisualisation of the trend of public sentiment in England,\nWales, N. Ireland and Scotland from 21 July 2013 to 23\nJuly 2013. The tweet corpus for Scotland after 10:00 BST\nwas not obtained on 22 July 2013. The number of tweets\nused to analyse the sentiment for England was nearly one\nmillion, for Wales was over 69,000, for N. Ireland was over\n24,000, and for Scotland was nearly 65,000. In general,\nboth the dictionary-based and machine learning approaches\nCountry No. of Tweets Correlation Ratio\nEngland 989,413 0.8192\nScotland 64,980 0.6110\nWales 69,459 0.3146\nN. Ireland 24,117 0.5485\nTable II: Correlation ratio between the dictionary-based and\nthe machine learning approaches\nproduce the same trend though several exceptions can be\nnoted; in the case of England, there seems to be fewer\nexceptions and is likely to be because a large number of\ntweets are analysed. For Wales the exceptions are seen for\ntwo time periods, firstly, between 00:00 and 07:00, and\nsecondly, between 17:00 to 20:00. Though the dictionary-\nbased approach estimates an increasing positive trend in the\nsentiment score after the birth of the Prince, the machine\nlearning approach fails to capture this. In the case of N.\nIreland there is a close similarity in the trend between 22\nJuly 12:00 BST and 23 July 12:00 BST when there was a\nhigh volume of tweets regarding the birth. Similarity in the\nincreasing and decreasing trends of PSS across the days are\nalso noted for Scotland.\nC. Discussion\nIn the case of England, during the announcement of the\nbirth on July 22 and for a few hours later the PSS has\n(a) England\n(b) Wales\n(c) N. Ireland\n(d) Scotland\nFigure 7: Variation of public sentiment in the UK from 21-\n23 July 2013\na steady trend at an average of 0.7. This indicates that\nthe tweets posted during this time have nearly 30% more\nnegative sentiments than positive sentiments. However, after\n20:00 BST on July 22 there is a quick spike in the PSS\nlasting a couple of hours which is again noted on July 21\nand July 23. This is perhaps due to the increase in the\nvolume of tweets posted during these hours. Interestingly,\nfor Wales and N. Ireland an increasing trend with higher\nPSS scores are noted. For example, using the dictionary-\nbased approach in Wales a steady rise of the PSS from less\nthan 0.5 to over 1.0 is noted during and after the birth. Since\nthis trend is not observed the previous day or the day after\nthe birth it can be inferred that the people of Wales were\nmore positive during the time of the birth than the people\nin England. A progressively steady decrease is noted in the\npublic sentiment of Scotland, though the PSS during and\nafter the time of the birth is higher than that of England.\nIn summary, inspite of the fact that there is strong\ncorrelation between the two approaches for England, the\ndictionary approach places England in the first place for\nJuly 21 and July 22 and then in the third place for July 23,\nand the machine learning approach places England in the\nthird place in the UK from 21-23 July for positive public\nsentiment. Therefore, ‘Does England react quickly to events\nunlike other member countries?’ This is a pointer to further\ninvestigation and is beyond the scope of this paper.\nTo conclude, the case study indicates that the public\nsentiment scores estimated by the machine learning approach\nis highly correlated to the dictionary-based approach when\nlarge volumes of tweets are analysed for a time period.\nNonetheless, several exceptions are noted and will require\na closer investigation. While the current implementation of\nthe machine learning approach is slow it is possible to\nbe employed for offline estimation, particularly when an\nanalysis of a past event is being performed. Case studies to\nvalidate the use of the framework for analysing past events\nwill be reported elsewhere.\nIV. CONCLUSIONS\nThis paper presented a framework for the analysis and\nvisualisation of public sentiment. The framework comprises\nmodules to collect, parse, analyse, estimate and visualise the\nestimated public sentiment. A Public Sentiment Score (PSS)\nand a normalised PSS based on positive and negative indices\nthat broadly capture public sentiment of geographic regions\nwas used in this research. The scores were graphically\nvisualised on a geo-browser, as tile-maps and as time graphs.\nThe two underlying approaches employed in the framework\nare dictionary-based and machine learning. While the former\napproach is commonly employed the latter is not used for\naggregating public sentiment. In this framework we explored\nhow the machine learning approach can be used like the\ndictionary-based approach for analysing public sentiment.\nOne case study, namely the Royal Birth of 2013 in the\nUK, was considered to compare the public sentiment scores\nestimated by the two approaches. Preliminary efforts indi-\ncate that there is a reasonable correlation between scores\nproduced by the two approaches and indicate the feasibil-\nity of the machine learning approach for analysing public\nsentiment.\nA key observation from the case study is that the prob-\nlem of managing and visualising tweets for events that\nspan across days cannot be maintained and analysed using\ntraditional databases and data management techniques. For\nexample, the tweet corpus for a two day period contained\nnearly one million tweets resulting in approximately five\ngigabytes of data. Such large amounts of data will require\n‘big data’ techniques, such as the use of Hadoop to address\nthe data processing challenge. Faster methods will need to\nbe developed to facilitate real-time analysis and visualisation\nof public sentiment. The machine learning approach is a\nslow method compared to the dictionary-based approach and\nin this research could not be employed for real-time visu-\nalisation as an event was unfolding. While the framework\nis capable of rapidly ingesting data, it cannot process data\nrapidly. Again fast and parallel methods for processing will\nneed to be explored.\nLooking forward, this research aims to progress in the\ndirection of employing big data techniques and parallel\nmethods to develop a framework for real-time analysis and\nvisualisation of public sentiment. Methods will be pursued\nto analyse tweets for capturing a broader spectrum of\nsentiments. Efforts will also be made towards developing\na distributed framework available for public use.\nREFERENCES\n[1] A. Tumasjan, T. O. Sprenger, P. G. Sandner and I. M. Welpe,\n“Election Forecasts With Twitter: How 140 Characters Reflect\nthe Political Landscape,” Social Science Computer Review,\nVol. 29, No. 4, 2011, pp. 402-418.\n[2] J. Bollen, H. Mao and X. Zeng, “Twitter Mood Predicts the\nStock Market,” Journal of Computational Science, Vol. 2,\nIssue 1, 2011, pp. 1-8.\n[3] S. Asur and B. A. Huberman, “Predicting the Future with\nSocial Media,” Proceedings of the International Conference\non Web Intelligence, 2010.\n[4] T. Sakaki, M. Okazaki and Y. Matsuo, “Earthquake Shakes\nTwitter Users: Real-time Event Detection by Social Sensors,”\nProceedings of the 19th international conference on World\nWide Web, 2010, pp. 851-860.\n[5] S. Doan, B. -K. H. Vo and N. Collier, “An Analysis of Twitter\nMessages in the 2011 Tohoku Earthquake,” Lecture Notes of\nthe Institute for Computer Sciences, Social Informatics and\nTelecommunications Engineering, Vol. 91, 2012, pp 58-66.\n[6] M. J. Paul and M. Dredze, “You Are What You Tweet:\nAnalysing Twitter for Public Health,” Proceedings of the\n5th International AAAI Conference on Weblogs and Social\nMedia, 2011.\n[7] B. OConnory, R. Balasubramanyan, B. R. Routledge and N.\nA. Smithy, “From Tweets to Polls: Linking Text Sentiment\nto Public Opinion Time Series,” Proceedings of the 4th In-\nternational AAAI Conference on Weblogs and Social Media,\n2010.\n[8] T. L. -Welfare, V. Lampos and N. Cristianini, “Effects of\nthe Recession on Public Mood in the UK,” Proceedings of\nthe 21st International Conference Companion on World Wide\nWeb, 2012, pp. 1221-1226.\n[9] S. Petrovic, M. Osborne, R. Mccreadie, C. Macdonald and I.\nOunis, “Can Twitter Replace Newswire for Breaking News?”\nProceedings of the 7th International AAAI Conference on\nWeblogs and Social Media, 2013.\n[10] D. J. Hopkin and G. King, “A Method of Automated Non-\nparametric Content Analysis for Social Science,” American\nJournal of Political Science, Vol. 54, Issue 1, 2010, pp. 229-\n247.\n[11] A. Agarwal, B. Xie, I. Vovsha, O. Rambow and R. Passon-\nneau, “Sentiment Analysis of Twitter Data,” Proceedings of\nthe Workshop on Language in Social Media, 2011, pp. 30-38.\n[12] H. Saif, Y. He and H. Alani, “Semantic Sentiment Analysis\nof Twitter,” Proceedings of the 11th international Semantic\nWeb Conference, 2012.\n[13] D. Klein and C. D. Manning, “Parsing with Treebank Gram-\nmars: Empirical Bounds, Theoretical Models, and the Struc-\nture of the Penn Treebank,” Proceedings of the 39th Annual\nMeeting on Association for Computational Linguistics, 2001,\npp. 338-345.\n[14] M. -C. de Marneffe, B. MacCartney and C. D. Manning,\n“Generating Typed Dependency Parses from Phrase Struc-\nture Parses,” Proceedings of the International Conference on\nLanguage Resources and Evaluation, 2006.\n[15] M. P. Marcus, M. A. Marcinkiewicz and B. Santorini, “Build-\ning a Large Annotated Corpus of English: The Penn Tree-\nbank,” Journal Computational Linguistics, Vol 19, Issue 2,\n1993, pp. 313-330.\n[16] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai and A. Kap-\npas, “Sentiment Strength Detection in Short Informal Text,”\nJournal of the American Society for Information Science and\nTechnology, Vol 61, No. 12, 2010, pp. 25442558.\n[17] M. Thelwall, K. Buckley and G. Paltoglou, “Sentiment\nStrength Detection for the Social Web,” Journal of the Amer-\nican Society for Information Science and Technology, Vol.\n63, No. 1, 2012, pp. 163-173.\n[18] A. Go, R. Bhayani and L. Huang, “Twitter Sentiment Classifi-\ncation using Distant Supervision,” Technical report, Stanford\nDigital Library, Stanford University, 2009.\n[19] M. Mintz, S. Bills, R. Snow and D. Jurafsky, “Distant\nSupervision for Relation Extraction Without Labeled Data,”\nProceedings of the Joint Conference of the 47th Annual Meet-\ning of the ACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP, Vol. 2, 2009,\npp. 1003-1011.\n[20] M. De Choudhury, M. Gamon, S. Counts and E Horvitz,\n“Predicting Depression via Social Media,” Proceedings of the\n7th International AAAI Conference on Weblogs and Social\nMedia, 2013.\n[21] M. Park, C. Cha and M. Cha, “Depressive Moods of Users\nPortrayed in Online Social Networks,” ACM SIGKDD Work-\nshop on Health Informatics, 2012.\n[22] Sentiment140 website: http://help.sentiment140.com/\nfor-students [Last accessed: 6 August 2013]\n[23] B. Pang, L. Lee and S. Vaithyanathan, “Thumbs up? Sen-\ntiment Classification Using Machine Learning Techniques,”\nProceedings of the ACL-02 Conference on Empirical Meth-\nods in Natural Language Processing, Vol. 10, 2002, pp. 79-86.\n[24] B. Sandvik, “Thematic Mapping Engine,” MSc dissertation\nin Geographical Information Science, Institute of Geography,\nSchool of Geosciences, University of Edinburgh, 2008.\n[25] J. Wernecke, “The KML Handbook: Geographic Visualisation\nfor the Web,” Addison-Wesley Professional, 1st Edition, 2008.\n[26] ESRI Shapefile Technical Description, An ESRI White Paper,\nJuly 1998, 34 pages. Available from: http://www.esri.com/\nlibrary/whitepapers/pdf/shapefile.pdf [Last accessed: 6 Au-\ngust 2013]\n",
      "id": 17141649,
      "identifiers": [
        {
          "identifier": "oai:risweb.st-andrews.ac.uk:publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:research-portal.st-andrews.ac.uk:publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1109/bigdata.2013.6691669",
          "type": "DOI"
        },
        {
          "identifier": "596461845",
          "type": "CORE_ID"
        },
        {
          "identifier": "597095882",
          "type": "CORE_ID"
        },
        {
          "identifier": "613778958",
          "type": "CORE_ID"
        },
        {
          "identifier": "104454564",
          "type": "CORE_ID"
        },
        {
          "identifier": "597425205",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.765.9124",
          "type": "OAI_ID"
        },
        {
          "identifier": "1308.1847",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1308.1847",
          "type": "OAI_ID"
        },
        {
          "identifier": "193989381",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:research-portal.st-andrews.ac.uk:openaire_cris_publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
          "type": "OAI_ID"
        },
        {
          "identifier": "147224842",
          "type": "CORE_ID"
        },
        {
          "identifier": "24954110",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:risweb.st-andrews.ac.uk:openaire_cris_publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
          "type": "OAI_ID"
        }
      ],
      "title": "The Royal Birth of 2013: Analysing and Visualising Public Sentiment in\n  the UK Using Twitter",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:risweb.st-andrews.ac.uk:openaire_cris_publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
        "oai:citeseerx.psu:10.1.1.765.9124",
        "oai:risweb.st-andrews.ac.uk:publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
        "oai:arxiv.org:1308.1847",
        "oai:research-portal.st-andrews.ac.uk:openaire_cris_publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc",
        "oai:research-portal.st-andrews.ac.uk:publications/a288e20b-18e5-49e2-82f4-21ffb29b9efc"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://doi.org/10.1109/BigData.2013.6691669",
        "http://www.adambarker.org/papers/bigdata2013.pdf",
        "http://arxiv.org/abs/1308.1847",
        "http://arxiv.org/pdf/1308.1847.pdf"
      ],
      "updatedDate": "2025-02-07T04:20:17",
      "yearPublished": 2013,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1308.1847"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17141649"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1407.0374",
      "authors": [
        {
          "name": "Borrego-Díaz, Joaquín"
        },
        {
          "name": "Galan-Paez, Juan"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Universidad de Sevilla. Departamento de Ciencias de la Computación e Inteligencia Artificial"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/158965309"
      ],
      "createdDate": "2014-10-24T19:27:36",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 1582,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1582",
          "logo": "https://api.core.ac.uk/data-providers/1582/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "A persistent challenge in Complex Systems (CS) research is the\nphenomenological reconstruction of systems from raw data. In order to face the\nproblem, the use of sound features to reason on the system from data processing\nis a key step. In the specific case of complex societal systems, sentiment\nanalysis allows to mirror (part of) the affective dimension. However it is not\nreasonable to think that individual sentiment categorization can encompass the\nnew affective phenomena in digital social networks.\n  The present papers addresses the problem of isolating sentiment concepts\nwhich emerge in social networks. In an analogy to Artificial Intelligent\nSingularity, we propose the study and analysis of these new complex sentiment\nstructures and how they are similar to or diverge from classic conceptual\nstructures associated to sentiment lexicons. The conjecture is that it is\nhighly probable that hypercomplex sentiment structures -not explained with\nhuman categorizations- emerge from high dynamic social information networks.\nRoughly speaking, new sentiment can emerge from the new global nervous systems\nas it occurs in humans",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/158965309.pdf",
      "fieldOfStudy": null,
      "fullText": "1Discovering New Sentiments from the Social Web 1\nJOAQUI´N BORREGO DI´AZ, University of Seville\nJUAN GALA´N PA´EZ, University of Seville\n1. INTRODUCTION\nA persistent challenge in Complex Systems (CS) research is the phenomenological reconstruction of\nsystems from raw data. In order to face the problem, the use of sound features to reason on the system\nfrom data processing is a key step. In the specific case of complex societal systems, sentiment analysis\nallows to mirror (part of) the affective dimension. However it is not reasonable to think that individual\nsentiment categorization can encompass the new affective phenomena in digital social networks.\nThe present papers addresses the problem of isolating sentiment concepts which emerge in social\nnetworks. In an analogy to Artificial Intelligent Singularity, we propose the study and analysis of these\nnew complex sentiment structures and how they are similar to or diverge from classic conceptual struc-\ntures associated to sentiment lexicons. The conjecture is that it is highly probable that hypercomplex\nsentiment structures -not explained with human categorizations- emerge from high dynamic social\ninformation networks. Roughly speaking, new sentiment can emerge from the new global nervous sys-\ntems [Pentland 2012] as it occurs in humans.\n1.1 Motivation of the work\nThe rise of impressive social media platforms as Twitter increases the interest in the development\nof sentiment analysis technologies and tools in order to understand how certain social digital events\noccur. Although sentiment is usually opposed to knowledge, semantic structure of human sentiments\ncan be considered as a Knowledge organization about them. Formal Concept Analysis (FCA) [Ganter\nand Wille 1999] can used to organise knowledge and extract new concepts from data. If social messages\nare considered from the point of view of sentiment, these messages may be considered as examples of\nknowledge with sentiment features. On the one hand, opinion lexicons (bag of tagged words) will data\nsource to apply FCA tools. On the other hand, structured English lexical database as WordNet provide\na robust semantic structure to conceptualize old (and new) kind of sentiment concepts. Our motivation\nis to study if FCA provides a nice framework to address these questions\nBy means of the intensive use of Formal Concept Analysis [Ganter and Wille 1999], which provides\nmathematical tools for detecting newly minted sentiments, established categorizations for sentiment\nare compared. Once selected the lexicon, we aim to estimate how such new concept structures for\ndigital societies converge to (or diverge from) single-agent based ones. Experiments to discover new\nsentiment structures about specific events from information dynamics in Twitter are shown (see in\nFig. 3 a snapshot of conceptual sentiment structure for Syria conflict).\n2. FORMAL CONCEPT ANALYSIS\nFCA mathematizes the philosophical understanding of a concept as a unit of thoughts composed of\ntwo parts: the extent and the intent. The extent covers all objects belonging to the concept, while\nthe intent comprises all common attributes valid for all the objects under consideration [Ganter and\nWille 1999]. It also allows the computation of concept hierarchies from data tables. It represents an\nautomated conceptual learning theory, which allows to detect and describes regularities and structures\nCollective Intelligence 2014.\n1:2 • J. Borrego-Dı´az and J. Gala´n-Pa´ez\nFig. 1. Formal context of fish, and its concept lattice\nof concepts. Also, it provides knowledge bases (sets of logical implications between attributes) as Stem\nBasis, Luxenburger Basis. The bass data structures in FCA are the Formal Context (O,A, I) and The\nConcept Lattice\nA formal context M = (O,A, I) consists of two sets, O (objects) and A (attributes), and a relation\nI ⊆ O × A. Finite contexts can be represented by a 1-0-table (identifying I with a boolean function on\nO ×A). Given X ⊆ O and Y ⊆ A, it defines\nX ′ = {a ∈ A | oIa for all o ∈ X} and Y ′ = {o ∈ O | oIa for all a ∈ Y }\nThe main goal of FCA is the computation of the concept lattice associated with the context. A (for-\nmal) concept is a pair (X,Y ) such that X ′ = Y and Y ′ = X. For example, the concept lattice from\nthe formal context of fishes of Fig. 1, left (attributes are understood as ”live in”) is depicted in Fig.\n1, right. Each node is a concept, and its intension (or extension) can be formed by the set of at-\ntributes (or objects) included along the path to the top (or bottom). For example, the bottom concept\n({eel}, {Coast, Sea,River}) is the concept euryhaline fish. CL contains every concept that can be ex-\ntracted from the context. As well, concepts are defined but it is possible that no specific term (word)\nexists to denote it.\nThe concept lattice represents a semantic organization of the (unstructured) information source, by\nproviding a partially ordered structure of concepts (a hierarchy of concepts). From this point of view, it\ncan be considered a complex semantic network, a network which presents different topology depending\non the nature of the data [Aranda-Corral et al. 2013a]. However, a question leads from this approach\nwhen a big amount of information about the CS is provided.\n2.1 Concept Lattice versus Knowledge Robustness\nOn the one hand, to achieve a sound (qualitative) phenomenological reconstruction of the CS requires\nof a a sound selection of the features (qualitative) to be computed/studied. Therefore, some questions\narise:\n—How is it decided if the feature selection is sound?\n—How is the soundness of the qualitative representation/reasoning model obtained from a feature\nselection analyzed?\n—Specifically: Do sound qualitative modelizations share similar structure/properties?\nThere exists a great amount of knowledge on existing real semantic networks and how the topology\nof these networks is deeply related with the soundness of the representation. In our case, semantic\nnetworks are synthesized from raw and selected data, so a kind of reverse knowledge engineering has\nto be made. That is, it seem that the concept lattice have to share topological properties with others\nsuccessful semantic network based representations. In [Aranda-Corral et al. 2013a] the authors study\nthe called Scale-Free Conceptualization Hypothesis (SFXH): Only if the attribute set selected to observe\nthe Complex System is computable, objective, and induces a Concept Lattice that provides a sound\nCollective Intelligence 2014.\nEmergent New Sentiments from the Social Web Crowd • 1:3\nanalysis of the CS (from the point of view of some type of BR), then its degree-distribution is scale-free.\n[Aranda-Corral et al. 2013a].\nThe issues involved in the use of SFCH as primary test to estimate the fit of semantic representation\nmainly are:\n—SFCH works as test of information quality\n—Monster model (that is, the concept lattice built from all data without previous selection of relevant\nfeatures) has to be useful for capturing CS dynamics\n3. A FORMAL CONTEXT FOR WORDNET\nIt is possible to apply FCA in order to study our language and relate formal concepts with those of a\nreal language. With this purpose the lexical database of English WordNet was considered. WordNet\nbasically structures the language in form of words (Lemmas) grouped in sets of cognitive synonyms\n(Synsets).\nA huge formal context was built by considering lemmas as objects and synsets as attributes and the\nassociated concept lattice was computed. It is worthy to note that the lattice shows a strong relation-\nship between formal concepts and language concepts. Also this concept lattice achieves the aforemen-\ntioned SFCH, that is, its node’s degree distribution follows a power lay distribution. Therefore we can\nclaim that, as expected, WordNet presents well structure semantics and can be a useful tool in order\nto give some structure to opinion analysis tasks.\n4. FORMAL CONCEPT ANALYSIS (FCA) FOR SENTIMENT DISCOVERING\nIn this section we succinctly describe the experiments carried out with the aim of enhancing opinion\nlexicons by providing them with a semantic structure by means of FCA and WordNet.\n4.1 Enhancing Opinion Lexicon\nOpinion lexicons basically consist on lists of words (relevant for sentiment detection) that are anno-\ntated with their polarity (how positive or negative they are). As it was mentioned, WordNet provides\nsemantic relations between words, and the concept lattice provides structure. In this way it is possible\nto enhance an opinion lexicon by means of these tools.\n4.2 Applying FCA: Sentiment Lattices\nA Sentiment Lattice is a subset of the concept lattice associated to WordNet. This subset is obtained by\nconsidering only the concepts related with the words of the opinion lexicon. The concepts of this lattice\nare called sentiment concepts, and each of them has a polarity associated. The valuation of each concept\ncomes from the aggregation of the sentiment of each word existing in the extension of the concept.\nSome experiments with three different opinion lexicon where performed in order to show the useful-\nness of FCA as tool for testing the consistency of sentiment lexicons as well as for analyzing information\nitem’s sentiment. Information items considered for testing are tweets due to the fact that are short con-\ncentrated pieces of information. As the pieces of text are quite short, no natural language processing\n(NLP) approaches (as identifying parts of speech, etc.) have been considered. Thus in order to collect\npieces of sentiment within tweets, the approach has been to look for occurrences of isolated sentiment\nwords in the information items. After that, the sentiment word set associated to each information piece\n(tweet) is allocated within the sentiment lattice in the corresponding concept.\nIn Fig. 2 the results of the analysis of the three different opinion lexicon are shown. In order to test\nthe usefulness of these lexicons to be used on twitter, some measures has been considered:\nCollective Intelligence 2014.\n1:4 • J. Borrego-Dı´az and J. Gala´n-Pa´ez\nFig. 2. Data on different opinion lexicon and their performance allocating sentiment concepts to tweets (11,500 tweets aprox.)\n—Tagged tweets: Polarised tweets, that is, tweets containing some sentiment word. The rest are not\npolarised, thus they are not considered.\n—Specific sentiment concept: Tweets allocated in an existent concept within the lattice.\n—New sentiment concept: Tweets whose sentiment words produce a new concept but respecting the\nexisting structure.\n—Ambiguous sentiment concept: Tweets which would be allocated in different parts of the lattice or\ncan not be allocated.\nAfter this analysis we can conclude that for our purposes, the most interesting sentiment lexicon is\nAFINN-111 (this does not mean is the best). Although SentiWordNet seems to be the most promising\nit showed to polarize too many words, producing some ambiguity.\n4.3 A case Study\nAfter selecting a proper sentiment lexicon, an experiment was performed with a tweet set of around\n15, 000 English tweets, containing the topic ”Syria” collected in 05/20/2013 during 6 hours. Finally, the\nwhole tweet set is allocated on the sentiment lattice in order to populate it. The graph shown in figure\n3 is a weighted sentiment lattice, where the weight (node size) shows the number of tweets allocated\nin each concept and the color indicates the polarity of the concept (green is positive and ted negative).\nOnce this populated sentiment lattice is obtained, it is possible to study the semantic relationship\nbetween tweets allocated in neighboring concepts as well as the new concepts generated which can\ngive insights of new word usage that have not been considered previously.\nAuthors have designed FCA-based tools and methods to study CS from agent interaction [Aranda-\nCorral et al. 2013c] to global semantic scale [Aranda-Corral et al. 2013a], as well as to successfully\nsimulate the predictive human behavior [Aranda-Corral et al. 2013b].\n5. CONCLUSIONS\nThe application of FCA tools to Sentiment Analysis provides structures for Knowledge Organization\nthat can be analyzed by means of both network and computational logic tools. Also, sentiment lattices\nprovide a partially ordered structure on feeling exposed in information items. This structure allows us\nto discover semantic relationships among information items that are tightly related with sentiment.\nAs discussed above, even new sentiment concepts emerge and sentiment lattice show them.\nCollective Intelligence 2014.\nEmergent New Sentiments from the Social Web Crowd • 1:5\nFig. 3. A snapshot of Sentiment Structure about Syria conflict in Twitter\nInteresting insights can be obtained by analyzing how the sentiment graph evolves over time. Using\nnetwork-based metrics can be explored how sentiment lattices evolves while the event live on the\nsocial stream. Using computational logic it is possible to conjecture how sentiment concepts evolves,\nby extracting and analyzing the sentiment lattice trend (with knowledge basis as Stem Basis).\nWith respect to the emergence of new kind of sentiment concepts, the semantic analysis of these in\nICT social systems is made by means of FCA. In fact, FCA allows extracting semantic sentiment con-\ncepts from noisy information items. The new elements are also semantically related with information\nitems and among them. Future work pass through the use NLP tools to enhance the useful information\nretrieved from an information item, and, lastly, to use Bounded Rationality techniques to forecast the\nevolution of a sentiment concept within an ICT social system (as in [Aranda-Corral et al. 2013b]).\nREFERENCES\nG. Aranda-Corral, J. Borrego-Dı´az, and J. Gala´n-Pa´ez. 2013a. On the Phenomenological Reconstruction of Complex Sys-\ntems—The Scale-Free Conceptualization Hypothesis. Systems Research and Behavioral Science 30, 6 (2013), 716–734.\nDOI:http://dx.doi.org/10.1002/sres.2240\nG. Aranda-Corral, J. Borrego-Dı´az, and J. Gala´n-Pa´ez. 2013b. Complex Concept Lattices for Simulating Human Prediction in\nSport. J. Syst. Science and Complexity 26, 1 (2013), 117–136.\nG. Aranda-Corral, J. Borrego-Dı´az, and J. Gira´ldez-Cru. 2013c. Agent-mediated shared conceptualizations in tagging services.\nMultimedia Tools and Applications 65, 1 (2013), 5–28. DOI:http://dx.doi.org/10.1007/s11042-012-1146-5\nB. Ganter and R. Wille. 1999. Formal Concept Analysis: Mathematical Foundations. Springer, Berlin/Heidelberg.\nA. Pentland. 2012. Society’s Nervous System: Building Effective Government, Energy, and Public Health Systems. Computer\n45, 1 (2012), 31–38. DOI:http://dx.doi.org/10.1109/MC.2011.299\nCollective Intelligence 2014.\n",
      "id": 17208719,
      "identifiers": [
        {
          "identifier": "1407.0374",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "158965309",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:idus.us.es:11441/74839",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1407.0374",
          "type": "OAI_ID"
        },
        {
          "identifier": "25039214",
          "type": "CORE_ID"
        }
      ],
      "title": "Discovering New Sentiments from the Social Web",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1407.0374",
        "oai:idus.us.es:11441/74839"
      ],
      "publishedDate": "2014-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://idus.us.es/xmlui/bitstream/11441/74839/1/Discovering%20New%20Sentiments.pdf",
        "http://arxiv.org/abs/1407.0374"
      ],
      "updatedDate": "2022-01-08T07:59:20",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/158965309.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/158965309"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/158965309/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/158965309/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17208719"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Desmet, Bart"
        },
        {
          "name": "Hoste, Veronique"
        },
        {
          "name": "Lefever, Els"
        },
        {
          "name": "van der Waart van Gulik, Stephan"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/55740522"
      ],
      "createdDate": "2016-11-12T12:21:57",
      "dataProviders": [
        {
          "id": 1493,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1493",
          "logo": "https://api.core.ac.uk/data-providers/1493/logo"
        },
        {
          "id": 1111,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1111",
          "logo": "https://api.core.ac.uk/data-providers/1111/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The current generation of sentiment analysis\nsystems is limited in their real-world applicability because they\ncannot detect utterances that implicitly carry positive or negative\nsentiment. We present early stage research ideas to address this\ninability with the development of a dynamic triple store of events\nassociated with their prototypical sentiment",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/55740522.pdf",
      "fieldOfStudy": null,
      "fullText": "TripleSent: a Triple Store of Events Associated with their Prototypical Sentiment  \n \nVeronique Hoste1, Els Lefever1,Stephan van der Waart van Gulik2 and Bart Desmet1 \n \n1LT3 Language and Translation Technology Team \n2Centre for Logic and Philosophy of Science \nGhent University, Belgium \nemail: firstname.lastname@ugent.be \n \nAbstract—The current generation of sentiment analysis \nsystems is limited in their real-world applicability because they \ncannot detect utterances that implicitly carry positive or negative \nsentiment. We present early stage research ideas to address this \ninability with the development of a dynamic triple store of events \nassociated with their prototypical sentiment. \nKeywords—sentiment detection; triple store; implicit sentiment; \nnatural language processing. \nI.  INTRODUCTION \nIn the last decades, state-of-the-art research in natural \nlanguage processing (NLP) has made a shift from rule-based \nto statistical corpus-based approaches, which require high-\nquality electronic text corpora. Supervised and unsupervised \nstatistical approaches to structure and interpret patterns in text \nand speech have been successfully developed on such corpora. \nExamples include part-of-speech taggers, parsers, named \nentity recognition, machine translation, speech recognition, \ntext classification and summarization, sentiment analysis, etc. \nSome of these tasks can be performed with near-human \naccuracy (e.g., part-of-speech tagging), whereas for more \ncomplex tasks, such as sentiment analysis, performance is \nlimited by the amount of available knowledge. \nIn sentiment analysis, the objective is to automatically \ndetermine the sentiment (positive, neutral or negative) \nexpressed in an utterance, e.g., (a) “I love to go shopping”, (b) \n“Coke tastes great”, (c) “I bought the mattress a week ago, \nand a valley has formed”. Most state-of-the-art sentiment \nanalysis systems combine a statistical approach with lists of \nsubjective words (“love”, “great”), such as the MPQA \n(Multi-Perspective Question Answering) [1] lexicon. As a \nresult, they are capable of detecting expressions of sentiment \nonly if they can learn them from annotated corpora or \nsentiment lexicons. While current sentiment analyzers can deal \nwith expressions that address sentiments explicitly, as in \nexamples (a) and (b), they struggle with sentiments that are \nonly implicitly present in so-called polar facts, as is the case \nin example (c) [2]. Current systems fail to detect polar facts, \nwhich implicitly carry positive or negative sentiment. This is \nproblematic, because implicit sentiment has been shown to \naccount for more than half of the sentiment in certain domains \n(e.g., product reviews, “Web surfing drains the battery”, or \nfinancial reporting, “Fed lowers interest rates”) [3]. Progress \nin the automatic detection of ironic utterances such as “Going \nto the dentist tomorrow yippee”, in which the expressed \nsentiment is not to be understood in its literal sense, also \nsuffers from the lack of common sense knowledge [4][5].  \nAs this severely limits the real-world applicability of the \ncurrent generation of sentiment analyzers, we aim to \ninvestigate the feasibility of developing a dynamic triple store \nof events associated with their prototypical sentiment. Such \ncommon-sense knowledge could then complement other \nknowledge sources (e.g., sentiment lexicons) and other types \nof features derived from training data in a classification-based \napproach to sentiment analysis or irony detection.  \nKnowledge bases, such as WordNet, DBPedia, Freebase, \nOpenCyc, SUMO and Open Mind Common Sense, which \nstore and structure lexical and factual knowledge in machine-\nreadable formats, have been instrumental for the success of \ncomplex language understanding applications, such as the \nIBM Watson question answering system [6]. They are an \nessential resource for tasks that involve factual analysis, such \nas summarization, wikification, question answering and \ntextual entailment. For sentiment analysis, however, there is \nan additional need for knowledge about the prototypical \nsentiments people hold towards entities and events. As \n“prototypical” sentiment, we consider sentiments that are \ncommonly associated with a certain event, an event being the \ncombination of a verb and a direct, indirect or prepositional \nobject. Certain events may entail multiple prototypical \nsentiments, depending on perspective. As an example, the \nsentence “Fed lowers interest rates” will be considered \nprototypically positive for people who want to take out a loan, \nbut it can also be considered negative in that it may cause \ninflation. \nThe remainder of this ideas paper is organized as follows. \nIn Section 2, we propose the methodology we intend to use to \nbuild a knowledge base of events and their prototypical \nsentiment.  In the last section, we present some prospects for \nfuture work beyond the construction of the knowledge base.  \nII. RESEARCH OBJECTIVES \nWe conceive TripleSent as consisting of two interacting \nlayers: a knowledge base and a reasoner. The knowledge \nbase contains events for which the prototypical sentiment is \nknown with a high certainty. This information is stored in the \nform of sentiment triples. For example, the negative sentiment \ncommonly associated with “going to the dentist” can be \nformally captured by the sentiment triple <visit-dentist, has-\nsentiment, negative> (note that there is some notational abuse \nhere to facilitate the reader). The reasoner, on the other hand, \nis capable of inferring sentiment for events that are not stored \nin the database. When a user asks for the prototypical \n91Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-472-5\neKNOW 2016 : The Eighth International Conference on Information, Process, and Knowledge Management\n                         100 / 140\nsentiment for “visit the oncologist”, the reasoner combines \ninformation from factual knowledge bases like WordNet [7] \n(which knows that oncologists, like dentists, are a kind of \ndoctor) with the sentiment information from the triple store, to \n(conditionally) infer the expected sentiment triple <visit-\noncologist, has-sentiment, negative>. Some of the inferences \ncan be truly ‘conditional’ because whenever new, more \nreliable information contradicting the inferred triple is added \nor generated, the reasoner will need to revoke the inference \n(and all other inferences that rely on it). Like human \nreasoning, this requires a non-monotonic logic approach (see \nObjective 2). \n  \nObjective 1: Event extraction and enrichment \nTo kick-start the knowledge base, events will be collected \nfor which the sentiment is known. These events will be \nobtained by extracting patterns for highly explicit sentiment \nexpressions (e.g., “I hate” or “I love”) or from large web data \ncrawls (e.g., commoncrawl.org), which will subsequently be \nsyntactically and semantically parsed to extract events and \nsentiment triples. In the same vein, we will investigate \nleveraging existing large parsed datasets to extract high-\nconfidence sentiment triples with minimal human intervention, \nusing pattern-based and supervised sentiment analysis \ntechniques [8]. Events for which both polarities are found \nfrequently in the data will initially not be considered for \nfurther processing and will be investigated in more detail to \nunderstand the nature of this ambiguity.   Given the linguistic \ndiversity with which events can be expressed, the usefulness \nof the resulting triple store will also heavily depend on the \nability to automatically handle orthographic variation (as for \nexample in “pediatrician”, “paediatrician” or “pediatrist”), \nand syntactic and semantic synonymous structures (e.g., \n“visit”, “going to”, “seeing”, etc. “a pediatrician”). \nIn order to allow for the creation of new sentiment triples, \nexplicit sentiment triples present in the knowledge base will be \nlinked to ontological information provided by lexical \nresources and factual knowledge bases such as WordNet and \nDBPedia, respectively.   \n \nObjective 2: Opinion inferencing \nThe reasoner can infer all kinds of new sentiment triples \nfrom already known triples using (decidable) fragments of \nfirst-order predicate logic. However, in order to enable \nTripleSent to also deal with the expected sentiment for events \nthat are not yet stored in the database, the reasoner should \nallow dynamic, conditional inferences of unseen triples. For \nexample, starting from the explicit sentiment triple <visit-\noncologist, has-sentiment, negative>, the reasoner relies on \nWordNet information like <oncologist, is-a, medical \nspecialist> to (provisionally) derive <visit-medical-specialist, \nhas-sentiment, negative>, and, again by relying on WordNet \ninformation, to (provisionally) derive <visit-dentist, has-\nsentiment, negative> and <visit-podologist, has-sentiment, \nnegative>. Note that the last sentiment attribution is debatable, \nand can be revoked in the (future) presence of other, more \nreliable triples (stating explicitly, for example, that \nprototypical visits to podologists are not negative). For the \nimplementation of this type of reasoning, we will evaluate \ndifferent non-monotonic logic approaches, such as default \nlogic [9], adaptive logics [10] or answer set programming \n[11]. \nIn order to evaluate the event extraction, event enrichment \nand opinion inferencing, we will manually annotate test \ncorpora by relying both on expert annotators and \ncrowdsourcing. For the evaluation of the event extraction, we \nwill assess precision both for the event extraction and the \nsentiment attached to these events. In order to also enable the \nmeasuring of recall, we will furthermore rely on an existing \ncorpus for irony detection annotated with event-sentiment \nannotations [12]. As in previous annotation efforts, it was \nshown that crowdsourcing is a reliable and very cost-effective \nmeans of collecting human knowledge, we will also \ninvestigate the use of a crowdsourcing methodology to \nvalidate and enrich the output of the platform. Inferred \nsentiment triples will be presented to a crowd of human \nannotators who indicate what they consider to be the \nprototypical sentiment for the given event. This could provide \nadditional high-confidence triples to be stored, contradicting \nevidence to inform non-monotonic decisions (e.g., exceptions \nsuch as <visit-podologist, has-sentiment, neutral>), and \ngrounding that can be used in a feedback loop to improve the \ninference engine.  \nIII. CONCLUSION AND FUTURE WORK \nTo date, there is a complete lack of reusable and \ndynamically growing knowledge bases linking events to \nimplicit sentiment, which can be used for research and \ndevelopment in opinion inferencing. The TripleSent platform \nincluding the knowledge base and the automatic reasoner will \nopen new perspectives in NLP research and can push the state-\nof-the-art in semantic text processing and inferencing, and \nmore specifically in NLP applications such as sentiment \nanalysis and irony detection.   \nREFERENCES \n[1] J. Wiebe, T. Wilson, and C. Cardie, “Annotating expressions of \nopinions and emotions in language”, Language Resources and \nEvaluation, vol. 39, issue 2-3, 2005, pp. 165-210.  \n[2] B. Liu, “Sentiment Analysis and Opinion Mining”, Morgan & \nClaypool Publishers, May 2012. \n[3] M. Van de Kauter, B. Desmet, and V. Hoste, “The good, the bad \nand the implicit: a comprehensive approach to annotating \nexplicit and implicit sentiment”, Language Resources and \nEvaluation, Springer Netherlands, vol. 49, 2015, pp. 685-720. \n[4] E. Riloff, et al., “Sarcasm as Contrast between a Positive \nSentiment and Negative Situation”, Proceedings of the 2013 \nConference on Empirical Methods in Natural Language \nProcessing (EMNLP 2013), 2013, pp. 704-714. \n[5] C. Van Hee, E. Lefever, and V. Hoste, “LT3: Sentiment \nAnalysis of Figurative Tweets: piece of cake #NotReally”, \nProceedings of the 9th International Workshop on Semantic \nEvaluation (SemEval 2015), 2015, pp. 684-688. \n[6] D. Ferrucci, et al., “Building Watson: An overview of the \nDeepQA project”, AI Magazine, vol. 31, no. 3, 2010, pp. 59-79. \n[7] C. Fellbaum, “WordNet: An Electronic Lexical Database”, \nCambridge, MA: MIT Press, 1998. \n92Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-472-5\neKNOW 2016 : The Eighth International Conference on Information, Process, and Knowledge Management\n                         101 / 140\n[8] C. Van Hee, M. Van de Kauter, O. De Clercq, E. Lefever and V. \nHoste, “LT3: Sentiment Classification in User-Generated \nContent Using a Rich Feature Set”, Proceedings of the 8th \nInternational Workshop on Semantic Evaluation (SemEval \n2014), 2014, pp. 406-410. \n[9] R. Reiter,  “A logic for default reasoning”, Artificial \nIntelligence, vol. 13, no. 1, 1980, pp. 81-132. \n[10] D. Batens, “A General Characterization of Adaptive Logics”, \nLogique et Analyse, vol. 44, no. 173-175, 2003, pp. 45-68. \n[11] M. Blondeel, S. Schockaert, D. Vermeir, and M. De Cock, \n“Fuzzy Answer Set Programming: An Introduction”, Soft \nComputing: State of the Art Theory, vol. 291, 2013, pp. 209-\n222.  \n[12] C. Van Hee, E. Lefever, and V. Hoste, “Exploring the \nRealization of Irony in Twitter Data”, Proceedings of the Tenth \nInternational Conference on Language Resources and \nEvaluation (LREC’16). European Language Resources \nAssociation (ELRA), accepted for publication.  \n[13] O. De Clercq, et al., “Using the Crowd for Readability \nPrediction”. Natural Language Engineering, vol. 20, no. 3, 2014, \npp. 293-235. \n \n \n93Copyright (c) IARIA, 2016.     ISBN:  978-1-61208-472-5\neKNOW 2016 : The Eighth International Conference on Information, Process, and Knowledge Management\n                         102 / 140\n",
      "id": 31254680,
      "identifiers": [
        {
          "identifier": "55740522",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:archive.ugent.be:8071695",
          "type": "OAI_ID"
        },
        {
          "identifier": "570371899",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:search.ugent.be:pug01:8071695",
          "type": "OAI_ID"
        }
      ],
      "title": "TripleSent: a triple store of events associated with their prototypical sentiment",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:archive.ugent.be:8071695",
        "oai:search.ugent.be:pug01:8071695"
      ],
      "publishedDate": "2016-01-01T00:00:00",
      "publisher": "IARIA",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://biblio.ugent.be/publication/8071695/file/8071708"
      ],
      "updatedDate": "2023-10-27T16:53:50",
      "yearPublished": 2016,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/55740522.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/55740522"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/55740522/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/55740522/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/31254680"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Algaba, Andres"
        },
        {
          "name": "Ardia, David"
        },
        {
          "name": "Bluteau, Keven"
        },
        {
          "name": "Borms, Samuel"
        },
        {
          "name": "Boudt, Kris"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Econometrics and Data Science"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/387932563",
        "https://api.core.ac.uk/v3/outputs/323232776",
        "https://api.core.ac.uk/v3/outputs/412124068",
        "https://api.core.ac.uk/v3/outputs/429095338"
      ],
      "createdDate": "2020-05-27T16:13:15",
      "dataProviders": [
        {
          "id": 11074,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11074",
          "logo": "https://api.core.ac.uk/data-providers/11074/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 708,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/708",
          "logo": "https://api.core.ac.uk/data-providers/708/logo"
        },
        {
          "id": 1493,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1493",
          "logo": "https://api.core.ac.uk/data-providers/1493/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The advent of massive amounts of textual, audio, and visual data has spurred the development of econometric methodology to transform qualitative sentiment data into quantitative sentiment variables, and to use those variables in an econometric analysis of the relationships between sentiment and other variables. We survey this emerging research field and refer to it as sentometrics, which is a portmanteau of sentiment and econometrics. We provide a synthesis of the relevant methodological approaches, illustrate with empirical results, and discuss useful software",
      "documentType": "research",
      "doi": "10.1111/joes.12370",
      "downloadUrl": "https://core.ac.uk/download/323232776.pdf",
      "fieldOfStudy": null,
      "fullText": "doi: 10.1111/joes.12370\nECONOMETRICS MEETS SENTIMENT: AN\nOVERVIEW OF METHODOLOGY AND\nAPPLICATIONS\nAndres Algaba\nSocial Sciences and Solvay Business School\nVrije Universiteit Brussel\nand Department of Economics\nUniversiteit Gent\nDavid Ardia\nDepartment of Decision Sciences\nHEC Montre´al\nKeven Bluteau\nDepartment of Economics\nUniversiteit Gent\nand Department of Decision Sciences\nHEC Montre´al\nSamuel Borms*\nSocial Sciences and Solvay Business School\nVrije Universiteit Brussel\nand Institute of Financial Analysis\nUniversity of Neuchaˆtel\nKris Boudt\nSocial Sciences and Solvay Business School\nVrije Universiteit Brussel\nDepartment of Economics\nUniversiteit Gent\nand School of Business and Economics\nVrije Universiteit Amsterdam\n∗Corresponding author contact email: samuel.borms@unine.ch; Tel: +32 496 95 53 22.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use,\ndistribution and reproduction in any medium, provided the original work is properly cited.\n2 ALGABA ET AL.\nAbstract. The advent of massive amounts of textual, audio, and visual data has spurred the\ndevelopment of econometric methodology to transform qualitative sentiment data into quantitative\nsentiment variables, and to use those variables in an econometric analysis of the relationships between\nsentiment and other variables. We survey this emerging research field and refer to it as sentometrics,\nwhich is a portmanteau of sentiment and econometrics. We provide a synthesis of the relevant\nmethodological approaches, illustrate with empirical results, and discuss useful software.\nKeywords. Qualitative data; Sentiment analysis; Sentometrics; Survey; Textual analysis\n1. Introduction\nThere is a long-standing tradition of using sentiment as either a parameter or a variable in econometric\nmodeling. Historically, the use of questionnaires and proxies to quantify sentiment variables has been\npredominant. In recent years, it has become popular to analyze the sentiment embedded in textual, audio,\nand visual data. Such data are becoming increasingly available in large amounts due to the digitization of\ncommunication media. These media are carriers of potentially interesting information useful for economic\nanalysis. This has spurred a new strand of econometric research that investigates the transformation of\nlarge volumes of qualitative sentiment data into quantitative sentiment variables, and their subsequent\napplication in an econometric analysis of the relationships between sentiment and other variables. We\nrefer to this emerging field as sentometrics, which is a portmanteau of sentiment and econometrics.\nIn this survey, we overview the methodology and applications related to an econometric analysis of\nsentiment extracted from qualitative data. We first define sentiment as the disposition of an entity toward an\nentity, expressed via a certain medium. Examples of entities include individuals, news media, companies,\ngovernment associations, industries, and markets. This disposition can be conveyed numerically but\nis primarily expressed qualitatively through text, audio, and visual media. Sentometrics studies the\ncomputation of sentiment from any type of qualitative data, the evolution of sentiment, and the application\nof sentiment in an economic analysis using econometric methods. Many approaches already exist for\nusing econometrics with textual data, as recently overviewed by Gentzkow et al. (2019a), but our focus\non qualitative sentiment data is unique. The overview by Lewis and Young (2019) is limited to the most\nimportant analytical approaches for analyzing textual content in accounting and finance.\nThe goal of our survey is to provide a synthesis of the relevant work that serves as a gateway for\nresearchers in econom(etr)ics, finance, and machine learning interested in the analysis of qualitative\nsentiment data. The survey takes a hands-on approach by synthesizing the research around the common\nchallenges. The first critical step is the clarification of the problem that one is trying to solve. In function\nof the question, one collects, prepares, and selects the different data. The filtered qualitative data are then\ntransformed into numbers using domain-specific sentiment quantification techniques. These numbers are\nnext aggregated into meaningful sentiment variables. The different intermediate aggregation steps involve\ncombinations of sentiment calculation methods, as well as the use of various within-text, across-text, and\nacross-time aggregation methods. These variables are used as main input in an econometric model that\nis set up to solve the question at hand. An important part of the econometric analysis of qualitative\nsentiment data is a continuous validation activity. For each of the above topics, we discuss the relevant\nmethodological approaches and illustrate with empirical results. We also have a section that sums up\nsome of the available software to perform each step.\n2. Definition of Sentiment\nThe term “sentiment” is used in many different contexts and research areas, but there is no established\ndefinition. We propose a working definition that encapsulates the most important characteristics of\nsentiment from the perspective of a researcher wishing to transform textual, audio, and visual data into\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 3\nsentiment variables and to apply them in an economic analysis. We also summarize the literature that\nhighlights other characteristics and alternative definitions of sentiment.\n2.1 Working Definition\nWe propose the following generic definition of sentiment:\nSentiment is the disposition of an entity toward an entity, expressed via a certain medium.\nThis working definition of sentiment embeds three components. First, the expression by an entity of\na disposition in the form of verbal or nonverbal communication. To observe the private state of mind of\nany entity, one has to look at their subjective expressions through both qualitative sentiment data such as\ntextual, audio, and visual data, as well as numeric data such as quantitative survey data and stock market\ndata. The combined use of different sources of qualitative sentiment data is called multimodal sentiment,\ncompared to the use of one data source, which is referred to as unimodal sentiment. Not many studies\nhave assessed the added value of multimodal sentiment, but, in general, findings confirm an increased\nlevel of accuracy over unimodal sentiment (see, e.g., Soleymani et al., 2017). Despite the fact that there\nare differences between, for example, the expression of emotion and sentiment (or other human subjective\nterms), it is not necessarily in the interest of a researcher interested in sentiment to make this distinction\nexplicit. Second, the disposition has a measurable polarity or semantic orientation that shows through the\nmedium of expression. It reveals the direction and intensity of the subjective expression, on a discrete or\na continuous scale. Many definitions simply use positive or negative to indicate semantic orientation, but\napplication-specific terminologies, such as bullish and bearish (Antweiler and Frank, 2004), dovish and\nhawkish (Picault and Renault, 2017), or Democrat and Republican (Gentzkow and Shapiro, 2010), can be\nhelpful. Sentiment is usually asserted at different levels of granularity (e.g., a sentence, an entire article,\na sequence of sounds, or an image). Third, the sentiment is oriented toward (an aspect of) another entity,\nor exceptionally the expressing entity itself.\nThis general synthesis encompasses a broad range of different sentiment definitions that are currently\nused in the field of economics. Casey and Owen (2013) describe consumer confidence as the consumer\nexpectations about the future state of the economy. Ludvigson (2004) notes that surveys are most often\nused to measure consumer confidence. De Long et al. (1990) define investor sentiment as a belief about\nfuture cash flows and investment risks that is not justified by the fundamentals. Baker and Wurgler (2007)\nlist several mediums through which this investor disposition is expressed. Notable examples include the\nuse of investor surveys, proxies for investor mood changing variables such as the number of hours of\ndaylight, and the analysis of market data such as trading volume, implied volatility, mutual fund flows,\nthe premium on dividend-paying firms, and the closed-fund discount. In a survey on sentiment in finance,\nKearney and Liu (2014) argue that there are two types of sentiment—namely, investor sentiment, which\nincludes only the subjective judgments and behavioral characteristics of investors, and text-based or\ntextual sentiment, which may also contain a more objective reflection of the conditions of a certain entity.\nKra¨ussl and Mirgorodskaya (2017) hypothesize that media sentiment translates into investor sentiment.\nMoreover, Chang et al. (2015) say that sentiment affects the formation of investors’ beliefs and thereby\ntheir reactions to information shocks.\nIn the remainder of the paper, we focus on the use of qualitative sentiment data as the medium through\nwhich the sentiment is expressed.\n2.2 Other Definitions\nFrom a psychological viewpoint, Munezero et al. (2014) state that sentiment is one of the so-called\nhuman subjectivity terms that reflects a person’s desires, beliefs, and feelings. These human subjectivity\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n4 ALGABA ET AL.\nterms are features of a person’s private state of mind that can only be observed through textual, audio,\nor visual communication. For instance, the tonality of one’s voice, or the frequency and length of pauses\nare informative about underlying sentiment. The same holds for pitch, facial and bodily gestures, the\nword use in a written article, and the colors present in a picture. Other human subjectivity terms include\naffect, feeling, emotion, and opinion. While Munezero et al. (2014) argue that there are some notable\ndifferences, these terms are mostly used interchangeably in different strands of the literature. For instance,\nthe distinction that sentiment involves enduring emotional dispositions toward an object, whereas emotions\nare briefer, is not of direct interest for most economic applications. Based on social theory, Evans and\nAceves (2016) classify sentiment as a human state that reflects its condition at a given time and place.\nOther states include preference, uncertainty, and ideology, among others.\nTaboada (2016) details linguistic sentiment as the expression of subjectivity either as a positive or as\na negative opinion through language. Soleymani et al. (2017) define sentiment as a long-term disposition\nwith a certain polarity toward an entity. From a text mining perspective, Taboada et al. (2011) treat\nsentiment as equivalent to semantic orientation, containing an evaluative factor (i.e., positive or negative)\nand a corresponding strength. In a survey on sentiment analysis, Liu (2015) makes no distinction between\nsentiment and opinion and defines an opinion as a quintuple of (1) the expressed sentiment, (2) the entity\ntoward which it is expressed, (3) the particular (aspect of the) entity that is mentioned, (4) the opinion\nholder, and (5) a time stamp. This definition is closest to ours. Van de Kauter et al. (2015) distinguish\nbetween explicit sentiment (conveying subjective private states) and implicit sentiment (conveying factual\ninformation). Shapiro et al. (2018) deploy a characterization of emotions along the two dimensions valence\n(how positive) and arousal (how charged). The valence of word-of-mouth in marketing research is referred\nto by Gelper et al. (2018) as a discrete or continuous metric that captures the attitudes toward a brand.\nAdditionally, sentiment can be looked at from the perspective of the sender (e.g., the sentiment attached\nby the author of a text), and from the perspective of the receiver (e.g., the sentiment perceived by the\naverage reader).\nIn their analysis of political sentiment, Grimmer and Stewart (2013) use both the terms “sentiment”\nand “tone.” They determine tone based on whether information is conveyed positively or negatively.\nTone is more often used in the accounting and finance literature. Bajo and Raimondo (2017) characterize\ntone of news as a combination of the degree of positiveness, negativeness, and uncertainty. Feldman\net al. (2010) define tone as the optimism or pessimism of the information embedded in qualitative verbal\ndisclosures. Henry (2008) defines tone in earnings press releases as the effect of a communication. In\nthe construction of a news-based coincident index of business cycles, Thorsrud (2018) uses tone as a\nsynonym for sentiment and identifies it by determining whether news articles are positive or negative. In\nthis survey, we also treat tone as a synonym for sentiment.\n3. Problem Definition\nSentiment data have the potential to help solve or understand many problems involving the use of\neconometrics, across the fields of economics, finance, accounting, marketing, psychology, and computer\nscience, among others. The adequate choice of methods to analyze sentiment data depends on the goal of\nthe analysis. A common ground for econometrics applied to sentiment analysis is that one first needs to\nmeasure sentiment. Below, we discuss the use of qualitative sentiment data in applied economic theory\nand as an information source in nowcasting and forecasting economic variables.\n3.1 The Role of Qualitative Sentiment Data in Applied Economic Theory\nAt least since the work of Keynes (1936), economists have been wondering what role sentiment plays\nin influencing economic decision-making. Understanding the relationship between sentiment data and\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 5\ndecision-making at the micro and macro level is still important in economic theory. Sentiment can be\nconsidered either to contain fundamental information in the news sense or to capture irrationality up\nto “animal spirits” in the noise sense. Both types of shocks move economic expectations and market\noutcomes at different horizons (Angeletos et al., 2018). The challenge according to Angeletos and La’O\n(2013) is that economists first have to model and then to quantify the sentiment forces behind the formation\nof market expectations. They develop an economic theory that does not depart from rationality but rather\nconnects market expectations with market outcomes through external shocks they call sentiments. Barsky\nand Sims (2012) create a dynamic stochastic general equilibrium (DSGE) model that accommodates\nboth the information and animal spirits view of confidence. They find most empirical evidence for\nthe perspective that innovations in confidence reflect information about future economic prospects. One\nexplanation for the importance of economic sentiment is that it acts frequently as a self-fulfilling prophecy\n(see Petropoulos Petalas et al., 2017, and the references therein). When there is consumer or business\npessimism about economic growth, actual negative growth can be a direct consequence of it. A more\nspecific example is bank runs. When too many depositors’ sentiment about other depositors is negative,\nthe unwanted outcome, a bank run, is more likely to materialize (Diamond and Dybvig, 1983).\nIn this regard, sentiment indices based on qualitative data can provide a more direct data-driven\ninstrument to assess various types of economic shocks, or proxy for matters such as confidence or\nexpectations. Larsen and Thorsrud (2019) use structural vector autoregression (VAR) to identify news\nand noise shocks in a panel of text-based measures and other economic variables.\nSentiment proxies provide a way to test behavioral hypotheses on the aggregate level or on the individual\nlevel. In general, the key questions pondered in a behavioral analysis are “What drives sentiment?” and\n“What is the behavioral impact of the sentiment transmitted?” An example of a behavioral hypothesis is\nwhether entities inflate the tone in their written communication to influence market reactions. Both Picault\nand Renault (2017) (for the European Central Bank) and Arslan-Ayaydin et al. (2016) (for firms) validate\nthis hypothesis. Sentiment in texts can be argued to be driven by a self-interest to generate particular\nexternal outcomes. In Garz (2014), the evidence shows a strong bias in terms of the number of negative and\npositive reports related to unemployment that is not the consequence of an asymmetric interpretation of the\nofficial numbers but rather associated with noneconomic information and the process of news production\nitself. Along these lines, the degree of sentiment involved in images appended to advertisements can also\nhave clear intentions to impact customer behavior. Kalogeropoulos (2018) studies the impact of various\nmedia outlets on individual economic expectations, not finding tone to be a good predictor. In the finance\nliterature, behavioral theory predicts that short-horizon returns are reversed in the long run (Tetlock, 2007).\nThere is a growing concern about the severity and impact of media bias. Closely related, Flaxman et al.\n(2016) explain that there are two strands of thought about the impact of improved production, distribution,\nand discovery of news articles (or generally, any multimedia). Some defend it increases exposure to diverse\nperspectives; others argue that it increases ideological segregation. They find empirical support for both\ncamps, and thus, a further investigation of the impact of a biased media production and consumption\nbehavior would be worthwhile.\nBoudt and Thewissen (2019) base their analysis of CEO letters on psychological phenomena such as\nframing (Tversky and Kahneman, 1981) and the serial position effect (Glanzer and Cunitz, 1966). These\nand similar phenomena can also be used to better understand the sentiment conclusions. Purely as an\nillustration, stronger weights of negative sentiment words in comparison to those of positive sentiment\nwords could be supported by the negativity bias, claiming that negative things have a greater impact. It\ncould also be related to the fact that news media tend to emphasize negative news (e.g., Lowry, 2008).\nThe priming, agenda setting, and framing communication theories described in Scheufele and Tewksbury\n(2007) can also be subject to more precise testing using multimodal sentiment measures.\nMany different entities can have sentiment attributed from different data sources. Sentiment across these\ndifferent entities tends to interact in particular ways, possibly in a contagious manner. The assessment of\nthese sentiment flows, the feedback effects, and the associated information dispersion over time concern\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n6 ALGABA ET AL.\na network analysis of sentiment. The Global Database of Events, Language, and Tone (GDELT) project1\nis the most comprehensive effort to date of a global network analysis of events and related sentiment.\nThese data and social media data are useful to analyze what network structures would mitigate behavioral\nmisperceptions, a question brought up by Teoh (2018).\nLarsen and Thorsrud (2018) use a graphical Granger causality modeling framework to gain insights into\nthe network of economically relevant news topics. Every node in the graph represents a sentiment/topic\ntime series. The graph can be used to detect which narratives dominate and what the degree of news\nspillover is – that is, what news stories from which countries Granger cause the occurrence of any other\nnews stories.\nEshbaugh-Soha (2010) emphasizes the role news coverage and tone can have on government trust and\nhow it is central to explaining effective leadership. News provides country leaders a means to communicate\ntheir messages, but the local perception can differ significantly. Therefore, an interesting study would be\nto assess the spread between the sentiment of news reported in one region and the sentiment of similar\nnews reported in another region.\n3.2 Measuring, Nowcasting, and Forecasting of and with Sentiment\nSentiment time series indices aim to reflect the evolution of sentiment over time. A well-known text-based\nexample is the economic policy uncertainty (EPU) index of Baker et al. (2016).2 This index measures\nuncertainty, a specific type of sentiment. Manela and Moreira (2017) create a news-based measure of\noption-implied uncertainty, arguing that it incorporates disaster concerns expressed via the media. In\nmany other applications, sentiment is also considered an explicit or implicit proxy for a certain desired\noutput, such as for the visualization of company reputation (Saleiro et al., 2017).\nWe refer to quantifying already observed sentiment as sentiment measurement, while the prediction of\nthe unobserved current and future sentiment is called sentiment nowcasting and forecasting, respectively.\nSentiment is a latent variable, meaning that it is not readily observable. Measuring sentiment is a key task\nin any sentiment-based analysis.\nIn the now and forecasting literature, sentiment measures are considered a timely driver of other\nvariables. There are three approaches to the type of sentiment that is used. Sentiment is proxied using\navailable (questionnaire-based) indices, sentiment is constructed itself from a qualitative data source\ncommonly using relatively simple methods, or sentiment is bought from a data provider such as Reuters\n(e.g., Thomson Reuters MarketPsych Indices) who in general uses a more complex methodology for the\ncomputation. The obtained sentiment is then transformed for usage in prediction models, to obtain the\nbest possible prediction at any time. Sentiment variables are rarely used alone as explanatory variables\nbut are usually added to a set of standard explanatory variables to see whether its integration improves or\ndeteriorates forecasting performance.\nThe integration of sentiment has indeed already shown its capacity to improve forecasting performance.\nA significant impact of sentiment expressed through diverse media on stock returns and trading volume\nis found by Heston and Sinha (2017), Jegadeesh and Wu (2013), Tetlock et al. (2008), Tetlock\n(2007), and Antweiler and Frank (2004). Ardia et al. (2019b) incorporate textual sentiment time series\ninto the long-term forecasting of the U.S. industrial production growth rate using sparse regression\ntechniques.\nBeyond improved predictions, using sentiment data is very flexible and timely, especially compared to\ntraditional sentiment extraction methods such as surveys. Changes in sentiment methodology can easily be\nbacktested using the available data. Modifying the structure of a survey, however, necessitates the survey\nto be sent out again to obtain new results. Information derived from sentiment data hardly suffers from\nrelease lags, making timely sentiment an ideal variable to enhance nowcasting models and consequently\nto craft timelier policy responses.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 7\nAs in Hamilton et al. (2016), sentiment analysis on word level can be used to measure the time-varying\nperception surrounding certain words. For instance, “terrific” had a negative connotation up to 1960, but\nthen became more positive. Lukesˇ and Søgaard (2018) find that words predictive of sentiment at one\npoint in time remain not necessarily equally predictive at a later point, and that models trained on old data\nperform worse than models trained on recent data. They suggest a predictive feature selection approach to\ndeal with temporal polarity shifts. The implication of changes in language over time is that the methods\nof sentiment quantification should evolve with it.\nIt is becoming well established in economics and finance that adding soft (qualitative) information on\ntop of hard (quantitative) information results in predictive information gains. However, the soft information\nis usually explored through textual content. Audio and visual content have been explored less so but may\ndeliver additional information value, according to Mayew and Venkatachalam (2012), who find that vocal\ncues of managers during conference calls predict a firm’s future performance.\n4. Qualitative Sentiment Data\nThe various ways in which economic agents express their sentiment leads to textual, audio, and visual\nsentiment data. Sentiment data are short for “sentiment-bearing” data. Most of the examples and methods\nin the remainder of this survey focus on textual data, because audio, and visual sentiment analysis is still in\nits infancy (Soleymani et al., 2017). Teoh (2018) does so similarly but acknowledges the rising relevance\nof audio and visual data. Currently, the main focus of current research in sentometrics effectively lies\nwith textual data due to their wide availability in the digital form of news media articles, company filings,\nor social media posts (see, e.g., Loughran and McDonald, 2016).\nThe choice for textual data thus comes from the fact that most research and applications have been\ndeveloped for this type of data. An advantage of focusing on texts is that many other forms of unstructured\ndata can be transformed into textual data and then analyzed as if they were textual. For instance, audio\ndata are often transcribed into textual data and can thus be analyzed using tools from this domain.\nMultimodal sentiment analysis techniques are expected to gain importance due to the Internet, which has\nbecome more of a widespread multimedia platform. Where possible, we highlight close relations between\nthe analysis of textual data and the analysis of audio and visual data, covering potential similarities\nfrom one data-type approach to the other. Doing so, we outline a uniform high-level framework that\nis applicable to all these data sources. The concepts of feature extraction, quantification, aggregation,\nmodeling, and validation are very much transferable, though almost never presented as such.\n4.1 Information Sources\nThe information sources for sentiment analysis in econometrics can be grouped in two ways. First,\nit can represent where the data were published. This includes news outlets (a journal, a social media\nchannel, YouTube, a vlog, or a blog), companies and governments (regarding the publication of an official\npress release or an official report), or publication venues (an academic journal or a book publisher),\namong others. The source in this context should not be confused with the actual expresser of the\nsentiment; for instance, the source can be a journal, and the expresser a company or one of its top\nmanagers.\nSecond, it can represent from where the data are retrieved. The largest worldwide textual data providers\nare LexisNexis, Dow Jones’ Factiva, and Reuters. Access to these databases is paid. A cheaper alternative,\nif allowed, is to scrape textual data from the web. A specific scraping procedure needs to be set up, which\nis a cumbersome activity, and in general goes with a considerable degree of hit-and-miss in terms of texts\nsuccessfully collected. There also circulate some freely available data sets – for instance, the eight text\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n8 ALGABA ET AL.\ndata sets analyzed by Zhang et al. (2015) or the list of freely available text data sets provided by Ravi and\nRavi (2015).3\nThe acquisition of the data requires a good data management system, able to structurally store many\ngigabytes, such asMySQL. The database should also have fast query functionalities, for example, delivered\nby technologies such as Solr or Elasticsearch.\n4.2 Alternative Sentiment Variables\nInstead of the algorithmic extraction of sentiment from data, sentiment is also often proxied by asking\npeople through surveys. The U.S. Consumer Confidence Index or the European Economic Sentiment\nIndicator is actively monitored examples of indices based on surveys (see, e.g., the analyses of Ludvigson,\n2004, and Gelper and Croux, 2010). However, surveys have the downside of being costly, are hard to\nreplicate, have a publication lag, and cannot be backtested. Both survey-based measures and data-based\nmeasures have their value, and are in many cases complementary. Ardia et al. (2019b) show that the\nspecification that includes both time series measures generates the best out-of-sample predictive power.\nBaker and Wurgler (2006) derive a sentiment index through a principal component procedure from\nsix sentiment proxies proposed in the literature, without going through any sentiment quantification\nprocess themselves.\nSimilar to textual data providers, there exist a number of textual sentiment data providers. Two\noften-used solutions are the series from RavenPack, and the Refinitiv (formerly Thomson Reuters)\nMarketPsych Indices.\n4.3 Data Limitations\nA first limitation concerns data availability and the disagreement between textual databases. Ridout et al.\n(2012) find preliminary evidence that there are stories (in their study mostly international coverage) from\nprinted newspapers that are systematically missing in electronic databases. Thus, not only do texts need to\nbe collected, but the content from multiple sources also needs to be aggregated neatly. Chiou and Tucker\n(2017) cover some of the likely issues of content aggregation. The problem of data availability and data\ndisagreement is small for open government databases, such as accounting textual data (e.g., EDGAR),\ncourt decisions (e.g., PACER), or patents (e.g., Espacenet). Much in the same way Riffe et al. (2019)\nnote that the universe of online posts is “unlimited and unknowable and inherently unstable over time,”\nthe problem becomes more persistent for data coming from corporate resources, news media, or social\nmedia. Any sample drawn from that data might not be representative due to nonrandom sampling; true\nprobability sampling is hard in the context of big data. Lacy et al. (2015) mention convenience sampling\n(a sample primarily defined by availability) and purposive sampling (a sample primarily defined by the\nnature of a research undertaking) as common practices.\nAn important aspect in data collection is the notion of data vintages. In a real-time setting, a researcher\nuses the data available at a given time, called a vintage or a snapshot. Yet, many data are subject to\nrevisions. For instance, most data used in macroeconomics are updated one or more times until final\nnumbers are reached (Croushore and Stark, 2003). The compilation of the FRED-MD historical vintage\ndatabase of macroeconomic indicators was a response to this problematic (McCracken and Ng, 2016).\nThis same difficulty persists in textual data, particularly with online publication and social media, with\nthe data frequently updated, revised, or even removed from the information outlets. Saltzis (2012) reveals\nin a sample of breaking news stories on six major U.K. online news sites that the stories were updated on\naverage 5.7 times. As such, the traditional process of scraping websites for historical news may lead to a\nforward-looking bias since the retrieved news will typically be the latest version of the news articles and\nnot the one at the time of first publication. This phenomenon is crucial to deal with in intraday studies.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 9\nThe problems described above lead to issues of reproducibility and limitations to generalizability\nof results.\n5. Preprocessing, Enrichment, and Selection of Qualitative Sentiment Data\nTextual, audio, or visual data rarely arrive in a format that is ready for input into an algorithm.\nThe data typically start off being very unstructured, and through a sequence of steps, structure\nis imposed to make the data ready for further analysis. We define restructuring as doing two\nthings: preprocessing and enriching the data. Both the preprocessing and metadata enrichment ideally\ncome before the actual data selection to have the most information available to do an optimal\nfiltering.\n5.1 Restructuring Textual Data\nIn this subsection, we describe the preprocessing and metadata generation concerning textual data. Bholat\net al. (2015) provide a useful summary of many relevant text mining techniques for preprocessing and\ndata enhancement.\n5.1.1 Preprocessing\nRaw textual data often come in a JSON or an XML file from which the actual text needs to be\nextracted first. This process is called parsing. Depending on the type of data available, this can\nbe a relatively straightforward or tedious task. As part of this process, remaining garbage such as\nHTML tags, addresses, or other formatting is removed, or simply not selected through the parsing\nalgorithm.\nFurthermore, textual data are inherently (ultra)high-dimensional (Kelly et al., 2019). Gentzkow et al.\n(2019a) highlight that to structure a text with a length of w words, each of which is drawn from a\nvocabulary of q possible words, the unique representations of this text has dimension qw. Moreover, all\ncharacters in the text are probably not equally informative in assessing the sentiment of a particular\ndocument. For example, stop words such as “the” are seldom indicative and are usually removed\nto reduce the noise and the dimensionality. Some type of further cleaning is often required to deal\nwith issues such as spelling mistakes or (nonstandard) abbreviations (Nowak and Smith, 2017). Denny\nand Spirling (2018) outline several common preprocessing steps. The output is a corpus of cleaned\ntexts.\nTextual data come in various granularities: words, sentences, paragraphs, and whole articles. Sentiment\nis the output of a function applied to specific components extracted from texts, also called terms. The\nmost common kind of components are n-grams, a sequence of n words. Breaking up text into n-grams\nis called tokenization. If n = 1, tokens are referred to as unigrams. A bag-of-words approach presumes\nthat the relative order of unigrams is irrelevant, but words are not necessarily independent of each other.\nMore generally, a bag-of-words can be denoted by bag-of-tokens, where tokens can be any sequence of\nwords. Further cleaning is needed to drop, for instance, punctuation marks, or transform all terms into\nlowercase, stemmed, or lemmatized form.\nTerms are summarized into a document-term matrix, with the rows as the documents, the columns\nas the terms, and the cells as the values that measure the (weighted) frequency of occurrence of\nthe terms. A document-term matrix is usually of high dimension and consequently very sparse,\nmeaning, with a lot of zero entries. In a document-term matrix, the sparsest features are typically\nremoved.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n10 ALGABA ET AL.\n5.1.2 Metadata Enrichment\nA corpus as is consisting of only documents can be enriched by adding all sorts of metadata. Metadata\neither already exist or are objective, such as a time stamp, the author, the news outlet, the language, or\nthe geography. A good case for having metadata is that textual information is expressed across many\ndifferent venues, including newspaper articles, newswires, and social media, all with a possibly differing\ndegree of information value. Heston and Sinha (2017) emphasize the importance of studying news types\nto understand how financial markets process information and when underreaction and overreaction in\nreturns occur. Aggregation across a metadata marker gives information about the sentiment concerning\nthat particular metadata – for example, the sentiment about a given economic topic.\nThe available qualitative metadata need to be quantified for further use in the analysis. This can be\ndone using binary or relevance variables. In the first case, one enumerates all qualitative metadata across\nthe corpus for a given article and assigns a value of 1 if the metadata are of importance to that article,\nand 0 if not. A relevance variable follows the same principle but assigns a continuous score based on\nthe connectedness of the metadata to the article. Some metadata lend better to be modeled as a dummy\nvariable (e.g., language or geography) and others as a relevance variable (e.g., predefined topics). If there\nare too many individual instances of the metadata, one can consider to group them. Other metadata can\nbe generated using text mining models. The first type of metadata that can be generated are entities,\nusing named entity recognition extraction techniques. The second type of useful metadata are topics and\nrelated keywords based on a supervised or an unsupervised topic model. The features can be valued as\nthe probability score coming out of the topic model. Readability of a text (e.g., Loughran and McDonald,\n2014) or the tense of a text are two other potentially useful metadata indicators.\n5.2 Restructuring Audio and Visual Data\nThe underlying raw format of audio and visual data is less comprehensible and vaster than textual data.\nOne second of a video is size-wise equivalent to at least hundreds of pages of text; the maxim “An\nimage is worth a thousand words” is no exaggeration. We emphasize some important aspects about the\nrestructuring of audio and visual data.\n5.2.1 Preprocessing\nFor sentiment classification, visual and audio data are processed into emotional clues handy to discriminate\nbetween different sentiment categories. A major focus of sentiment extraction in visual data is on facial\nexpressions. Secondary are other nonverbal expressions (e.g., hand gestures) and environmental factors\nsuch as what is happening in the background. There are seven basic emotion classes (danger, sadness,\nsurprise, fear, disgust, joy, and contempt) that can be inferred using a facial expression coding system\noriginally proposed by Ekman and Friesen (1976). One can then construct variables that express the\ndistance between several of these positional facial characteristics.\nVisual data can be boiled down to image data. A video in that respect is a collection of segments, and\nevery segment is a collection of images. Audio data can be boiled down to textual data using speech-to-text\ntechnology complemented with specific audio features (such as pause duration).\n5.2.2 Metadata Enrichment\nThe principles of metadata enhancement for textual, audio, and visual data are similar, but the content\nof the metadata is different. Qualitative metadata such as author or time of publication are the same.\nExamples of useful audio features are pitch, pause, laughter, overlaps, and voice intensity; examples of\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 11\nvisual features are color and motion. Videos have both visual and audio features. Wang et al. (2003)\ndescribe features and extraction techniques across four categories (spatial visual, motion, coding, and\naudio).\nA downside of features retrieved from nontextual data is that many of them require a large-dimensional\nrepresentation. For a video, a manner to construct a feature called “smiling” could be to take the number\nof seconds a person smiles in the video. The decision of whether a person smiles is a function of various\nfacial characteristic points that should be mapped deterministically to the binary outcome “smiling” or\n“not smiling.”\n5.3 Selection of the Relevant Data\nFollowing a general-to-specific approach, the original and vast corpus of documents needs to be trimmed\nto a subselection of relevant texts. If the selection procedure is too restrictive, important data may be\nomitted; however, if irrelevant data are included, it may drastically lower the signal-to-noise ratio. This\ncan be considered as “querying” the corpus database to extract the right selection of texts. Querying can\nbe based on a search of keywords in the database using, for example, a regular expression. It should\nbe made clear beforehand which texts are necessary to include in the analysis, or the selection can be\napproached as an optimization problem itself. The latter strategy would require defining different sets of\nkeywords and finding out which keywords give the best outcome in terms of an objective (e.g., forecast\naccuracy).\nTo model an outcome variable, it is not always needed to focus exclusively on (sentiment) measures\ndirectly related to that variable. On the contrary, Larsen et al. (2020) argue that many news topics\nare, in their case, of interest to form inflation expectations, and thus, it would be limiting to only\ntarget media mentioning terms related to inflation. Kelly et al. (2019) tackle the problem of selection\nsimultaneously with modeling a set of observed covariates. They propose a method that only includes\nphrases of interest when useful, conditioning on the observed covariates, building on the model of Taddy\n(2015a).\n6. Quantification of Sentiment\nAny sentiment measure is a proxy for the actual prevailing sentiment and needs to be estimated. This\ncan be done by human annotators or by a statistical function. A wide variety of techniques exist to\ninfer the sentiment embedded in qualitative data, but measuring sentiment is inherently application- and\ndata-specific. Therefore, it is neither possible nor recommended to consider sentiment computation in a\nsingle manner.\nSentiment is quantified for a given observational data unit – for instance, a text or a video. Quantification\nof sentiment is either on a discrete scale (classification into two or more classes, such as negative, positive,\nand neutral) or on a continuous scale. Based on decision rules, one can go from continuous to discrete\noutput. Some methods produce a tuple of a positive and a negative sentiment (probability) score. Multiple\nsentiment scores from one computation method can be considered as separate methods and can turn\nout to be more informative.4 Sentiment scores might benefit from a normalization for interpretation\npurposes and possible outlier elimination. Sentiment analysis can also take up a more fine-grained\nexternalization, called aspect-based sentiment analysis (De Clercq et al., 2017). This type of sentiment\nanalysis separately measures the sentiment for different aspects and entities mentioned in the data unit.\nThis is a combined problem that requires the extraction of entities and their aspect terms, classifying\nthe aspect terms, before doing the sentiment calculation for each of the extracted combinations. One\ncould draw an analogy with “feature-based opinion summarization” (Hu and Liu, 2004), which is less\nspecific.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n12 ALGABA ET AL.\n6.1 Textual Data\nTextual sentiment quantification uses tools from the broad field of natural language processing (NLP) to\nquantify the sentiment of a given text. It consists of many NLP-related subtasks, such as identifying entities\nand extracting relevant features. We briefly discuss the lexicon-based and machine learning approaches\nas the two main types of methods for sentiment computation. The data unit is usually a document, a\nparagraph, or a sentence. Some fields prefer one over the other. Sentiment can be detected more precisely\nat sentence level, but in political science, for instance, most often the analysis remains at document\nlevel since it requires less heavy NLP (Grimmer and Stewart, 2013). For a broader treatment of textual\nsentiment computation and associated subtasks, we refer to Liu (2015) and Ravi and Ravi (2015).\nThe classification accuracy of the various sentiment approaches varies. Typically, machine learning\nalgorithms outperform lexicon-based methods out-of-sample at the expense of computational efficiency\nand model transparency. The difference in performance is a function of the type of texts and the domain\nspecificity of the lexicon employed. Ribeiro et al. (2016) provide an extensive overview of the accuracy\nof both lexicon-based and machine learning–based sentence-level sentiment analysis. They compare 24\npopular sentiment methods over 18 labeled data sets. Their experiments convey first of all a rather low\naverage level of accuracy. More importantly, there are large differences in the accuracy across sentiment\nmethods and across data sets. Their results also reveal no outstanding method at the sentence level.\nThe conclusion is that a sentiment quantification method needs to be selected carefully depending on\nthe purpose.\n6.1.1 Lexicon-Based Approaches\nA lexicon-based computation of sentiment is the most straightforward, efficient, and parsimonious method.\nTurney (2002) defines lexicon-based sentiment analysis as “calculating sentiment for a document from the\nsentiment of words or phrases in the document.” Mechanically, this requires the use of a sentiment lexicon\nwith sentiment information about important (combinations of) words, which is then matched with a text.\nA lexicon is thus a collection of pairs of words (or a sequence of words) and associated sentiment scores.\nIn most cases, lexicons stick to unigrams, but for some applications, it is more effective to use n-grams.\nPicault and Renault (2017) construct a lexicon specific to European Central Bank communication and\nexplicitly consider n-grams, such as the positive bigram “lower unemployment.” The size of a lexicon\nranges on average from in the hundreds to in the thousands. There is no preferred lexicon size; too large\ncan mean inaccuracy due to noise, and too small might mean not enough coverage or a lack of important\nwords. Comparing lexicons is not always easy, given the often varying sizes but also because there is no\nuniversal polarity grading system (Ravi and Ravi, 2015).\nThere is a distinction between general lexicons and domain-specific lexicons. Both the Henry lexicon\n(Henry, 2008) and the Loughran and McDonald lexicon (Loughran and McDonald, 2011) were developed\nas a response to the suboptimal applicability of generic lexicons to texts in the finance domain – for\nexample, earnings press releases. The Lexicoder Sentiment Dictionary (Young and Soroka, 2012) is\ntailored to news content about politics. Lexicons are simple and the least black-box solution, and usable\nat any text level. However, lexicons can be brittle when facing domain shift and complex syntactic\nconstructions (Ta¨ckstro¨m and McDonald, 2011). Very few lexicons are domain-portable, meaning\napplicable across several domains and text structures. It is difficult to achieve, if it is at all, and therefore\nhardly desirable.\nLiu (2015) sees three broad ways of generating lexicons – namely, manually, dictionary based, and\ncorpus based. An additional approach to building lexicons involves a combination of manual labor\nand a statistical methodology, which may arise from the machine learning literature. It is important\nto differentiate between machine learning algorithms for lexicon construction and those algorithms to\nmeasure sentiment but with no explicit intention to obtain a sentiment lexicon. We cover the latter\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 13\nin the next subsection. Apart from the manual approach, all methods entail automatic processes to\nvarying degrees.\nThe manual approach to building lexicons has annotators assigning a sentiment score to selected words.\nNotable fully hand-curated lexicons are the Stone et al. (1963) General Inquirer and the Bradley and Lang\n(1999) ANEW word lists. Crowdsourcing platforms such as Amazon Mechanical Turk have made the\ntask of developing high-quality manual lexicons more accessible nowadays. To our knowledge, the NRC\nlexicon of Mohammad and Turney (2013) was the first to be built using crowdsourcing services.\nA dictionary-based approach allows producing lexicons more cheaply while keeping a good level\nof accuracy. This method starts from a list of seed sentiment words with known polarity (often found\nusing the manual approach) and then expands this list by using synonyms and antonyms coming from a\nlarge base dictionary. A suitable base dictionary is the WordNet database (Miller, 1995). This lexicon in\nconjunction with sentiment seed words was used to produce WordNet-Affect (Strapparava and Valitutti,\n2004) and SentiWordNet Baccianella et al. (2010).\nThe corpus-based method adapts an existing lexicon using information from a domain-specific corpus.\nThe researcher first needs to adjust the sentiment orientation of the words to the new domain. Second,\nit may use linguistic rules to include new words in the lexicon. In this regard, Hatzivassiloglou and\nMcKeown (1997) introduce the notion of sentiment consistency. For instance, adjectives with a similar\nsentiment orientation are often used in groups. Kanayama and Nasukawa (2006) propose the idea of\nsentiment coherency; the same sentiment orientation tends to be expressed in consecutive sentences,\nwhile sentiment change is expressed by an adversarial expression (e.g., “but” or “however”).\nStatistical methodologies are the fastest and cheapest but the most prone to error. They typically start\nfrom a set of words from a previously built lexicon or a corpus, and then, a statistical methodology is used\nto find the sentiment orientation of those words. Jegadeesh and Wu (2013) use a regression framework\nto measure the sensitivity of words (“word power”) to stock returns; this could then be used to form a\nfinance-specific sentiment lexicon. Lexicons can also be derived from (Bayesian) regularized methods,\nsuch as the Ridge, the LASSO, or the elastic net regression (see, e.g., Pro¨llochs et al., 2015; Nowak and\nSmith, 2017). Pro¨llochs et al. (2015) argue that a shrinkage approach (Ridge regression) is superior over\na variable selection approach (LASSO regression) because multicollinearity among the token predictors\ntends to be strong. In the corpus-based category, Engle et al. (2020) create a climate change vocabulary\nbased on a collection of climate change of white papers and glossaries. Their final lexicon is composed\nof the unique stemmed unigrams and bigrams, weighted by their respective term frequency–inverse\ndocument frequency (tf-idf) scores. To create a daily climate change index, instead of term matching,\nthey use the cosine similarity between the tf-idf scores of a given article and the scores in the lexicon.\nLexicons do not cope with the linguistic context around which the sentiment words appear. To this\nend, advanced lexicon-based methods integrate so-called valence shifters in the sentiment computation.\nCommon types of valence shifters are amplifiers (e.g., very), downtoners (e.g., barely), negators (e.g.,\nnot), and adversative conjunctions (e.g., but). These valence shifters act on polarized words in the lexicon\nin particular ways depending on how close they appear to these polarized words. Taking the example of\nnegation, one way to apply it to lexical entries is termed shift negation (Taboada et al., 2011) as opposed\nto switch negation.5 Having lexicons consisting of n-grams would also allow disambiguating of word use\nin different contexts. According to Young and Soroka (2012), even a modest integration of contextual\n(preprocessing) routines is fruitful. Taboada (2016) enumerates multiple linguistic insights to account for\nin sentiment analysis.\nNot only domain specificity, but also language specificity is important. Most resources are still in\nEnglish (Ravi and Ravi, 2015). In practice, one often sticks to translation. Either one translates the\nfocused text from a resource-poor language into a resource-rich language (usually English) for which a\nrobust sentiment method (e.g., lexicon) is available, or one translates an existing word list into the focused\nlanguage. A third option is to translate annotated corpus resources from a resource-rich language to the\nfocused language and use these to develop (or improve) another sentiment method. In many circumstances,\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n14 ALGABA ET AL.\nhowever, the performance of translation results in a loss of accuracy. Mohammad et al. (2016) surprisingly\nfind that, with Arabic social media as the focused texts, sentiment analysis of automatic English translations\nis competitive to existing Arabic sentiment analysis systems. On the other hand, translation made the\nhuman annotations become worse than sentiment analysis, and adding Arabic translations of sentiment-\nlabeled English tweets data to Arabic training data resulted in a drop in accuracy, due to bad translations.\nTranslation invariably comes with additional problems to solve. Bannier et al. (2019) start from the\nEnglish Loughran and McDonald lexicon by doing word-by-word translation to German. On top of that,\nthey deal with distinct grammatical features of the German language related to inflectional and lexical\nmorphology, as well as compound wording. They claim to have described a comprehensive framework\nfor future adaptations of dictionaries into other languages. To test the equivalence between their lexicon\nand the Loughran and McDonald one across positive, negative, and neutral categories, they rely on the\ntwo-sided equivalence test of Blair and Cole (2002). The test checks for accordance in terms of the mean\nnumber of detected polarity categories, given a confidence interval.\n6.1.2 Machine Learning Approaches\nThe extraction of sentiment as a stand-alone problem is studied by machine learning and computational\nlinguistics scientists. The purpose is to optimize the measurement of sentiment based on a learning\nalgorithm typically benchmarked against an annotated data set of text with corresponding sentiment\nvalues. The objective, in this case, is well defined and dependent on the type of data source (e.g.,\nproduct reviews or images) and the type of sentiment output (e.g., classification into positive or negative).\nThe learning algorithm identifies the characteristics among the preprocessed smaller pieces of textual\ncharacteristics (i.e., words, n-grams, phrases, counts, and other information) that are most important in\nmeasuring sentiment. A survey of different machine learning algorithms applicable to text is given in\nEvans and Aceves (2016). Machine learning can be branched into supervised and unsupervised learning,\nboth used on many occasions for sentiment analysis.\nSupervised machine learning requires an annotated data set – meaning, a set of documents with,\nfor every document, a sentiment value, leading to what is often called the gold standard. Annotation can\nalready exist from the data (e.g., product rating stars), but, in most cases, is constructed manually. Building\nsuch a data set from scratch can be expensive and time-consuming while also prone to bias. Especially\nfor domain corpora, annotation can be hard due to possibly complex specific sociolinguistic contexts\n(Hamilton et al., 2016). The annotation cost also depends on the type of text. Van de Kauter et al. (2015)\nreview some of the complexities of doing annotation. Taddy (2013a) outlines a procedure to select from\na large corpus the texts that are most useful to annotate. Determining the best data examples to be labeled\nis referred to as (pool-based) active learning. Once the tagged data set is obtained, a specific machine\nlearning algorithm is trained with it. Pang et al. (2002) clarify the sentiment classification problem,\nand experiment with the Naive Bayes, maximum entropy classification, and support vector machines\n(SVMs) learning techniques. Naive Bayes and SVM are essentially text regressions of the sentiment\ntarget variable on a large-dimensional space of textual elements, such as words, which get assigned a\nweight. More recently, neural networks, primarily due to the emergence of deep learning, have become\nmore prominent. One can also combine several learning algorithms. For instance, Das and Chen (2007)\nemploy a majority voting scheme across five classifiers, claiming that it minimizes false positives.\nAn unsupervised learning approach lets the data decide the categories or representation by themselves.\nAny unsupervised method is typically hybrid or semisupervised, as there is need for specific minimal\ninputs from the modeler. A classic example is the suggested approach by Turney (2002), which ranks\nphrases based on their pointwise mutual information (PMI) with respect to two seed words, one negative\n(“poor”) and one positive (“excellent”). It infers the semantic orientation from the semantic association\nwith respect to a manual set of seed words. Remus et al. (2010) develop the German SentiWortschatz\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 15\ndictionary using the PMI approach. A vector space model (VSM) is a more complex undertaking.\nThese models generate word embeddings, which are latent quantitative vector representations of textual\ninformation, such as documents, paragraphs, words, phrases, and even letters. A VSM learns distributed\nvector representations that capture many precise syntactic and semantic word relationships. Words closer\nto each other in terms of linguistic context receive a more similar quantitative representation because they\nare assumed to share the same semantic meaning.6 Global matrix factorization methods (co-occurrence\ncounts based) and local context window methods (prediction based) are the two main families for learning\nword vectors.\nLatent semantic analysis (LSA) is a notable example of a global matrix factorization method. It reduces\nhigh-dimensional count vectors to a lower dimensional latent semantic vector space. Hofmann (2001)\nintroduces a probabilistic version of LSA, defining the semantic space over a set of latent variables\nreferred to as “aspects” based on a generative model for word-to-document co-occurrences. His model\nallows figuring out, for instance, which latent aspects are most likely to generate a word, or what the latent\nclass posterior probabilities are given a certain document and word. Liu et al. (2009) refactor the model\nto capture a multidimensional measure of blog sentiment, considering sentiment as a joint contribution of\na few hidden factors. They call their work sentiment probabilistic LSA (S-PLSA). In a subsequent time\nseries regression, they form sentiment variables as the average sentiment mass attributed to each of the\nhidden sentiment factors.\nMost of the recent research on word embeddings has gravitated toward the prediction-based method\nusing neural network architectures. The Word2Vec approach of Mikolov et al. (2013) is one of the earliest\nand best-known techniques in this category. Word2Vec uses the continuous bag-of-words (CBOW) or\nthe continuous skip-gram model architecture. In CBOW, one tries to predict the current word in a text\nfrom a window of surrounding context words. In contrast, in the skip-gram model, one tries to predict\nthe surrounding context words using the current word. Mikolov et al. (2013) also formalized the idea of\nusing vector operation, such as vec(“Madrid”) − vec(“Spain”) + vec(“France”) \u0002 vec(“Paris”). GloVe\n(Pennington et al., 2014) aims at taking the best of the count-based and prediction-based methods, with\na first attempt to integrate both global and local statistics. Pennington et al. (2014) find that the quality of\nGloVe’s learned representations is slightly better than Word2Vec’s vectors, but it depends on the task at\nhand. A more recent method is fastText (Bojanowski et al., 2017). It incorporates subword information\ninto the learning process such that words not observed in the training corpus (out-of-vocabulary) can still\nbe assigned a word vector. The current state of the art in word embeddings is the deep neural network\nBidirectional Encoder Representations from Transformers (BERT) models and its variants (Devlin et al.,\n2018). These models most explicitly integrate global and local context. For example, the word vector for\n“right” in “I am right” and “I take a right turn” will be different.\nEstimated word embeddings are used as an input to more traditional sentiment classification methods\n(e.g., logistic regression), or to probabilistic methods such as the one proposed by Taddy (2015b).\nAlternatively, by selecting several known positive and negative seed words, the vector space can be\nused to pinpoint words adjacent to those seed words and consider them as carrying the same polarity.\nThe SENTPROP method from Hamilton et al. (2016) first constructs a lexical graph from a VSM with\nthe words connected according to their embedding using cosine similarity, and then, performs label\npropagation to define the polarity. The sentiment score of a word is proportional to the probability of a\nrandom walk hitting that word, as propagated starting from a seed set. To obtain confidence bands around\nthe scores, they bootstrap over random subsets of seed words.\nIn the same vein as for lexicons, learning algorithms are ideally adapted for specific domains and\nlanguages to optimize the sentiment quantification. Thus, for optimal accuracy, the analysis for a specific\ndomain needs a separate annotated data set, as opposed to using an annotated broad corpus and the\nresulting generic trained algorithm. Transfer learning is the strand that investigates the optimal conversion\nof methods in one domain or one language to another. Good transfer learning minimizes the burden on the\nresearcher to acquire equally informative domain-specific annotated corpora for all domains of interest.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n16 ALGABA ET AL.\nAn application of transfer learning is to deduce sentence-level sentiment from document-level sentiment\nlabels. Ta¨ckstro¨m and McDonald (2011) use hidden conditional random fields as a latent variable structure\nmodel to deduce the latent sentence-level sentiment.\n6.2 Audio and Visual Data\nSome of the tools discussed for textual sentiment computation are also of value for the extraction of\nsentiment from audio and visual data. A lexicon can be constructed with entries such as “light smile,”\n“big smile,” “eye contact,” “crying,” “shouting,” “high pitch,” or “low pitch,” all with a certain calibrated\npolarity, and the number of seconds the action is held as a measure of polarity strength.\nDomain specificity can be thought of as speaker specificity in the context of audio data. Speaker-\ndependent approaches give (much) better results than speaker-independent approaches (Poria et al.,\n2016). The number of possible speakers is almost always larger than the number of possible languages\nor domains, making it infeasible to develop a specific algorithm for every individual speaker. However,\nmaking algorithms for types of speakers (e.g., political speakers) makes sense and is achievable.\nRousseeuw et al. (2018) define a measure of directional outlyingness that is applied on image data to\ndetect (sudden) changes in how a video frame appears relative to another frame. A transformed aggregation\nof the various outlyingness measures would make a good candidate as a proxy for sentiment.\n7. Aggregation of Sentiment Variables\nMost researchers are not interested in an entity’s or a data unit’s sentiment at one specific point in time but\nin the average value on several moments, or across many entities, methods, and data sources. Therefore,\nappropriate aggregation is required.\n7.1 Within-Unit\nAn essential aspect of the sentiment quantification as discussed in Section 6 is within-unit aggregation. For\ntextual data, this becomes within-document or intratextual aggregation. Within-document aggregation is\nthe weighting of the document-level sentiment information (e.g., the sentiment of a word or of a sentence)\ninto a score that represents sentiment for that document. For visual data, this becomes, for instance,\nwithin-video aggregation, which consists of the aggregation of sentiment of the different segments of the\nvideo into a whole video sentiment score.\nA widely used weighting scheme, in preprocessing and for text aggregation, is the tf-idf statistic.\nThis scheme weighs terms based on their frequency of occurrence (“tf”), but revalues upward the words\nappearing across few documents (“idf”), under the idea that less frequent terms can be of greater value to\ndetect the specificity of a document. This weighting approach makes document specificity a function of\nterm use rather than term meaning. Another option is to weight based on reader’s attention, which could be\nassumed higher in the beginning and end of a text. Allee and DeAngelis (2015) find an important degree\nof dispersion of sentiment in financial disclosures. Documents have typically one dominant sentiment\nclass but no uniform sentiment across paragraphs or sentences. Boudt and Thewissen (2019), for example,\nshow a clearly U-shaped pattern of sentiment within CEO letters.\nPoria et al. (2016) outline two approaches to aggregating, or fusing, textual, audio, and visual signals,\nwhich happens when dealing with video material. A first strategy is to combine characteristics from every\ntype of data into a joint vector and use this vector as input in a classification algorithm. The second\nstrategy is to model sentiment individually per data stream, and then combine the unimodal results based\non suitable metrics and weighting. The dynamic weighting of the unimodal results is an interesting\nresearch issue to explore. Pham et al. (2018) propose a third strategy, closest related to the first strategy,\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 17\naiming at a joint multimodal representation. They use an unsupervised encoder-decoder framework but\nadmit that a unimodal textual approach led to the best overall results in their empirical video analysis.\n7.2 Cross-Sectional\nCross-sectional aggregation can occur at multiple levels. A first level is across documents at a given\nfrequency, which results in a time series. This across-document aggregation is the natural next step after\nwithin-document aggregation. For example, to obtain a weekly time series, all sentiment scores need to\nbe aggregated at a weekly frequency. An interesting possibility for the aggregation is to let the weights\ndepend on the articles’ reach (e.g., the number of reads). One can then decide to further adjust the weights\nbased on some empirical knowledge – for example, to cope with the underrepresentation of far-right\nvoters on social media, as suggested in Ceron et al. (2014).\nA second level is across documents for a given metadata marker. For instance, one could aggregate\nsentiment scores for all documents coming from a given source, or discussing a certain entity. Only\nconsidering a limited number of sources to measure sentiment for a given period risks to give a biased\nestimate due to an unrepresentative sample. Typically, the first and the second levels are combined to\nobtain a time series for a given metadata occurrence. Many of such combinations capture different\ndynamics of the corpus and its metadata. Borovkova et al. (2017) obtain weekly sentiment values by\na weighting that takes into account the relevance and novelty scores supplied by the Thomson Reuters\nNews Analytics database.\nA third possible level of cross-sectional aggregation is across sentiment methods. The order of when\nto do this aggregation depends on the goal. In the simplest scenario, only one method is used, or multiple\nmethods are kept side by side – meaning no across-method aggregation at all. Another simple scenario\nis to average the sentiment scores from any given number of methods to obtain an averaged sentiment\nscore. Boudt et al. (2018) take the centered average of the scores coming from the lexicons they apply.\nA more statistical approach is commonality extraction, using principal component analysis or latent\nfactor modeling. Rogers et al. (2011) define sentiment as the first principal component over a range of\nsentiment measures. Lastly, an objective-based approach optimally weighs different methods based on\ntheir relationship with a target variable or based on another quantifiable objective. We further develop the\ntechniques, problems, and open questions regarding the last two approaches in Section 8.\n7.3 Across-Time\nAcross-time aggregation aims to smooth obtained sentiment time series or, more generally, to infuse a\ncertain time dependency pattern. There are various valid reasons for smoothing. One of those is to remove\noutliers. This especially holds for short-term sentiment series, for example, at a daily frequency. Thorsrud\n(2018) applies a 60-day moving average to his daily tone-adjusted textual topic time series to filter out the\nnoise. Another motivation for smoothing is related to the belief that sentiment at a certain time usually\nalso partly reflects earlier sentiment. Sentiment needs to be updated when new information arrives but\nremains affected by previous information. Ardia et al. (2019b), for example, use beta weighting schemes\ncovering a large number of possible time dynamics. They use a data-driven calibration to deal with the\nproblem of not knowing in advance which time pattern has the most value for forecasting. The Kalman\nfilter is also an appropriate technique to smooth out sentiment time series. It can be used to retrieve\nthe unobserved sentiment state from the observed (already aggregated) sentiment variable. Borovkova\net al. (2017) employ a simple local-level state-space model, leading to significantly less noisy sentiment\nvariables. Shapiro et al. (2018) use a monthly fixed effect as time series sentiment indicator, controlling\nfor newspaper and article-type fixed effects.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n18 ALGABA ET AL.\n7.4 Across Variables or Proxies\nThe combination of the likely heterogeneity in the input data, the number of variables that can be\nassociated with the data, and the number of sentiment implementations and aggregations may give rise to\nmany constructed sentiment time series. For instance, Gelper and Croux (2010) use a one-factor model,\nestimated either as the first principal component or using partial least squares, to form an aggregate\nsentiment indicator from 160 sentiment proxies. In Ardia et al. (2019b), the different sentiment variables\nare weighted and assembled into a sentiment-based index using the elastic net regression. The obtained\nsentiment index is specific to the dependent variable used in the regression. Aggregation here is thus\nacross metadata as well, which is usually not done at the across-document level. For example, to measure\nthe sentiment around the economy, one may want to obtain this sentiment as a weighted average of several\ncomponents such as employment, production, and the business cycle. Borovkova et al. (2017) obtain a\nfinal aggregated weekly sentiment index as an average of sentiment indices about important financial\ninstitutions, weighted by a bank-related measure such as net debt.\nIn a multivariate setting, one can repeat this process of creating separate sentiment indices for a series\nof proxies and then aggregate across these sentiment time series to obtain a final sentiment measure. That\nmeasure ought to be the optimized representation of the latent variable that is assumed to be represented\nby the collection of proxies. An example of a latent variable is the reputation of a company, which depends\non observable variables such as profitability, market share, stock returns, and sustainability. Simplicity in\nweighting might be desired (e.g., equal weighting), but more complex (aggregation) schemes deserve to\nbe studied. Larsen and Thorsrud (2019) use the marginal likelihoods across predictive regression models\nto form weights aggregating text-based time series into an index that best captures the variable to predict.\nGoing forward, the idea of forecast combination could be useful for across-proxy aggregation.\nNimark and Pitschner (2019) define two interesting aggregated measures based on topic probabilities\ncoming from a probabilistic topic model. The first is topic-specific deviation of a certain news topic\n(“specialization”); the second measures the news homogeneity in terms of agreement which topic is\ndeemed most important. Empirically, they use the measures to show that different news sources emphasize\ndifferent topics, but major events make news coverage more homogeneous. Similar measures could be\nconstructed to test for the sentiment agreement across various sources.\nCreating interactions of sentiment time series with other variables allows testing their interplay\nin explaining a dependent variable. The joint assessment of sentiment and topics is most prevalent\nin the literature (see, e.g., the sentiment-adjusted topic measures of Thorsrud, 2018, or the context-\nspecific sentiment time series in Calomiris and Mamaysky, 2019). Calomiris and Mamaysky (2019) and\nGlasserman and Mamaysky (2019) use an entropy-based measure to characterize a collection of news\nduring a given time frame in terms of “unusualness” and create simple interaction terms with sentiment\nvariables aggregated at the same frequency. These interaction terms add information, allowing one, for\nexample, to uncover that negative unusual news leads to an increase in U.S. stock market volatility\n(Glasserman and Mamaysky, 2019). Boudt et al. (2018) assess the interaction of sentiment with various\ncompany variables (finding that the informativeness of sentiment depends on the level of information\nasymmetry), while Arslan-Ayaydin et al. (2016) interact sentiment with managerial compensation (finding\nthat the informativeness of sentiment depends on the incentives to manipulate the sentiment). Garcı´a (2013)\ninteracts a measure based on the New York Times news with a dummy variable to indicate a recession and\nconcludes that daily stock returns are better predicted during recessions.\n8. Modeling\nThis section is mainly approached as the problem of modeling an outcome variable Y as a function of the\nsentiment variables stored in a matrix S, and possibly a number of control variables in another matrix X.\nIt can very generally be seen as modeling the joint density function f (Y, S, X).\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 19\n8.1 Time Series Models\nA very simple setup exists in modeling the output variable with a small number of sentiment variables\nand possibly other explanatory variables through a linear regression. Simple means it can be solved with\nordinary least squares (OLS) regression. Penalized, or regularized, regression is required when OLS\nregression cannot be applied – that is, when the number of explanatory variables is too high relative to the\nsample size, or when there is a severe problem of multicollinearity. Regularization of a high-dimensional\nvariables set shrinks the coefficients of the least informative variables toward zero. The Ridge (Hoerl and\nKennard, 1970) and the LASSO (Tibshirani, 1996) approaches are the most common ways to specify the\npenalized regression. The elastic net regularization of Zou and Hastie (2005) embeds both the Ridge and\nthe LASSO.\nFactor models extract one or more latent common patterns among a set of time series. Thorsrud\n(2018) develops a mixed-frequency time-varying dynamic factor model from which he extracts a daily-\nnews-based coincident index of business cycles. Both the mixed-frequency and (dynamic) factor aspects\nare useful approaches. For the first, for example, sentiment aggregated at both weekly and quarterly\nfrequency could be fed through a mixed-frequency factor model to obtain a short term, a long term, and\nan overall trend. Similarly, grouped data settings can be used to extract common sentiment in groups of\ntime series – for example, a common factor for every industry group consisting of all firms’ sentiment\nmeasures. Andreou et al. (2019) derive asymptotics to identify common and group-specific factors in\nsuch a setting. Specifically, they introduce a test to assess which factors are common across a set of\ngroup-specific vectors.\nThe news-based measure from Manela and Moreira (2017) is an estimate from an SVM regression\nusing the VIX index as dependent variable and normalized n-gram counts from texts as independent\nvariables. This is a valid way to create a final optimized index – that is, to let an index be constructed from\nhow well it captures a target variable. However, using such sentiment proxies in a second-stage regression\nusually has an impact on the uncertainty surrounding the then estimated coefficients. Manela and Moreira\n(2017) adjust the standard errors around the eventual point estimates to account for the uncertainty that is\nintroduced by the first-stage regression.\nMany target variables of interest could be discrete – for instance, an indicator variable whether a month\nlies in a recession period or not. Regularization is also perfectly applicable in a nonlinear context. Pure\nmachine learning algorithms, such as SVM, neural networks, or Random Forest, are more relevant in a\nnonlinear setup, also applicable in case of time series variables.\nMultiple sentiment variables and target variables can be jointly modeled in a multivariate regression\nframework, such as VAR models (see Qin, 2011, for a historical development of VAR models, and\nLu¨tkepohl, 2017, for a survey on structural VAR models). These frameworks are in general less prone\nto identification issues, since the variables are treated as endogenous, unless when explicitly considered\nexogenous or not modeled.\n8.2 Generative Models\nOne can distinguish between two key econometric approaches to measuring sentiment (Gentzkow et al.,\n2019a). Sentiment is either seen as a function of the written text (sentiment = f (text)), or the written\ntext is seen as a function of the underlying sentiment (text = f (sentiment)). In the latter case, sentiment\ncan be considered as a parameter of a stochastic process that generates texts as realizations. A seminal\nresearch paper in this is field is by Blei et al. (2003) proposing the latent Dirichlet allocation (LDA)\nmodel. Under this model, documents are assumed to be random mixtures over a predefined number of\nlatent topics, where each topic is characterized by a distribution over words. Fitting such a model on a\ncorpus of texts allows studying topic prevalence (the proportion of a document devoted to a topic) and\ntopic content (the words used to discuss a topic).\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n20 ALGABA ET AL.\nBlei and Lafferty (2006) come up with a dynamic topic model that allows the content of the topics to\nchange over time. Blei and Lafferty (2007) extend the LDA model by making correlation across topic\nproportions possible. Roberts et al. (2016) develop a structural topic model that lets the discovery of\ntopics be a function of both word counts and observable covariates. These covariates can consist of\nsentiment variables, or metadata such as author and time of publication. The generative paradigm in a\nsentiment context thus starts from a statistical model that should be viewed as the source of all statements\ngenerated. For example, a model can be set up in which tokens are hypothesized to follow a generative\nmodel conditioned on a sentiment variable.\nTaddy (2013b) introduces a framework to obtain low-dimensional document representations rich\nin sentiment information, called multinomial inverse regression (MNIR). He defines sentiment\nas the observable variables (e.g., product rating or whether a text is positive or not) impact-\ning the composition of text data. Hence, his approach clearly follows the “text = f (sentiment)”\nassumption. The most probable sentiment output can be associated with any unseen text using\nforward regression. Taddy (2015a) extends the MNIR framework to also account for potentially\nlarger dimensions of the sentiment variables, referred to as distributed multinomial regression\n(DMR).\n8.3 Combining Time Series Models and Joint Generative Models\nGiven the natural role that topics play as metadata features, the joint generative modeling of topics and\nsentiment is very useful, especially when a time series perspective is included. The dynamic topic model\nframework of Blei and Lafferty (2006) can be deemed a time series generalization of the topic models\nproposed earlier. Eguchi and Lavrenko (2006) address both the topic and sentiment of a text unit using\nprobabilistic generative modeling. Every statement is considered to have a set of topic-bearing and a set\nof sentiment-bearing words, each coming from respectively an underlying topic and sentiment language\nmodel. The dependence between both models is explicitly taken into account, under the assumption\nthat sentiment depends on the topic. This assumption is, for example, supported by the importance of\ndomain-specific sentiment lexicons.\nLin and He (2009) jointly extract document-level sentiment and the mixture of topics using\nan unsupervised procedure. They go from the three-layered LDA (topics associated with doc-\numents, and words associated with topics) to their joint sentiment/topic (JST) model, having\nfour layers (sentiment labels associated to documents, topics associated with sentiment labels,\nand words associated with sentiment labels and topics). The joint sentiment and topic modeling\nanswers to the need for domain specificity of sentiment analysis. It generally is approached\nas a two-stage process: first the detection of topics, and then the assignment of sentiment\nlabels.\nHe et al. (2013) and Fu et al. (2015) further develop two related joint sentiment-topic models that allow\nselected dynamic parameters to account for the time variation in topics and sentiment. The inclusion\nof external variables makes it easier to interpret the driving processes behind discourse and content\nof qualitative material. In the approach of Gentzkow et al. (2019b) to measure trends in the degree\nof polarization in political speech, one can, for instance, include observed and unobserved speaker-\nspecific characteristics.\nThere does not seem to be any longitudinal approach that uses the current state of a set of external\nvariables (e.g., representing the economic and financial markets) as drivers for the time variation\nof the used sentiment and topics in written media articles. Such a holistic parametric model has,\nhowever, clear advantages in terms of econometric inference about the relationship between the\nobserved news coverage, the features of the news sources, and the time variation in the variables\nsystem.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 21\n8.4 Normal and Abnormal Sentiment\nThere are several modeling approaches to decomposing sentiment into a normal and an abnormal\ncomponent. Huang et al. (2014) distinguish between normal tone and abnormal tone, defining abnormal\ntone as the residual of a regression of tone on firm-specific characteristics. Ardia et al. (2019a) make\nthe same distinction. They similarly consider a regression approach but use a static observable factor\nmodel, more precisely a market-cap-weighted sentiment index, with abnormal tone also the residual.\nOther alternatives could be to use the residuals of a simple mean model or of a latent factor model.\nHubert and Labondance (2018) identify sentiment as the unpredictable component of lexicon-based\ntextual tone, orthogonal to a series of variables representing economic fundamentals. In other words,\nthey define sentiment as the soft information conveyed through the tone of a communication beyond\ntraditional quantitative and qualitative information conveyed through the content. Sentiment is obtained\nas the residual, with its first-order autoregressive component removed, from a regression on the variables\nrepresenting the fundamental content.\n8.5 Attribution Analysis for Model Interpretation\nInterpretation is strongly tied to the problem definition and generally qualitative. On the statistical side,\nwe point out attribution analysis to interpret measured, nowcasted, and forecasted sentiment.\nSentiment aggregation and modeling condenses a lot of information into a few quantitative sentiment\nrepresentations of interest. A natural question is then how much of the final value is explained by the input\ndata. Obtaining such a decomposition of the final value into the contributions of the component input data is\nthe purpose of a top-down attribution analysis. These constituents are weighted based on their relationship\nwith a target variable, and thus, allows studying the relative importance of every constituent or of groups\nof constituents. This, in fact, is a more fine-grained approach to doing sentiment decomposition, though\ntypically not model-based. Aggregation based on the metadata features allows obtaining a predefined\ndecomposition of the relevant sentiment and may help with identifying the underlying sentiment drivers\nin relation to a target variable. Because of the linearity of the aggregation performed in Ardia et al.\n(2019b), the attribution to any of the aggregation dimensions could be easily obtained. For example,\nthey attribute the full sentiment-based forecast of the U.S. industrial production growth to six clusters of\nseparate economic topics. The aggregate news index from Thorsrud (2018) can also be decomposed in\nterms of topic contribution. An interesting avenue to explore is to do the same attribution to various news\nsources and bring this into relation to how readers are exposed to these sources and their potential media\nbiases. Larsen et al. (2020) analyze the variation in attribution by looking at the proportion of attribution\nthat is unchanged for model updates up to 60 months in the future. During the global financial crisis,\nthe predictive attribution relationship turned out to be much less stable, with only a small proportion\nof the explanatory news variables remaining important. This speaks in favor of doing regular model\nreestimations when times are troubling to incorporate the relevant news. Calomiris and Mamaysky (2019)\nalso detect strongly time-varying coefficient estimates for news measures when forecasting the stock\nmarket. This is due to both the changing mix of the news sources as well as the actual impact of the\nnews. Interestingly, Larsen and Thorsrud (2018) find that narratives mostly go viral during downs in the\nbusiness cycle, albeit for a duration of only a few months.\nIn case of multivariate economic systems, impulse response functions in the VAR framework are\nusually used for interpretation. An impulse response function describes a variable’s evolution along a\nspecified time horizon after a shock in the regression system. When a meaningful sentiment shock is\ninfused, its impact on all other variables can be quantified and understood across time. Borovkova et al.\n(2017) analyze the impact of a one-standard-deviation change in sentiment on various macroeconomic\nvariables and find it to last significantly up to two months later.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n22 ALGABA ET AL.\n9. Validation\nThe entire workflow is about extracting sentiment variables from qualitative data and using those variables\nin an economic analysis. Validation takes place at the end of every step but can be broken down into\nfour categories: (1) evaluation of the quality and selection of the data, (2) evaluation of the sentiment\nquantification and aggregation, (3) model estimation and hypothesis testing, and (4) evaluation of the\nout-of-sample statistical and economic performance of the model-based predictions.\nMany choices in the econometric analysis of textual, audio, and visual sentiment remain ad hoc. To\nadequately gauge the presence and impact of sentiment, the entire analysis should be frequently validated\nin a problem-specific way, both quantitatively and qualitatively. Comprehensive validation combines\ntools from econometrics with tools from machine learning. Machine learning is mostly about accuracy\nof prediction; econometrics is about uncovering (causal) relationships between economic variables.7\nValidation essentially jointly tests the current step and all previous steps as to whether they satisfy the\nassumptions for correct further (econometric) analysis.\nWhen a sentiment variable does not seem to have a significant effect on the variable of interest, it\nmay be due to two things. Either there is no significant effect of sentiment, or there is a significant\neffect, but the sentiment variables used are a weak proxy for real sentiment and do not capture the\nsignificant relationship. This can be conceived as a “joint hypothesis” problem. To mitigate this problem,\nthe validation in the field of sentometrics is largely twofold. First, one should validate the sentiment\nvariables created and then the model. When a model is deemed adequate in a statistical sense, further\nvalidation includes the interpretation of the results. A sentiment-based model that cannot be interpreted\nis not useful to convincingly answer the question outlined.\n9.1 Data Quality and Data Selection\nSince textual, audio, and visual data arrive in raw formats, the quality can vary substantially. Chances\nthat are not all data units are fully cleaned even after preprocessing. Data quality checking is an iterative\nprocess. It is natural to go back to the cleaning and selection when some errors are found a few steps\nfurther in the workflow.\nA basic quality check asks whether everything necessary for analysis is present. For instance, to be\nable to do a time series analysis, time stamps are inevitable. Any preprocessing of data involves a clear\ntrade-off between simplifying the data and information loss. Denny and Spirling (2018) document the\nsensitivity of textual preprocessing choices on the outcome of an unsupervised analysis. They devise a\nscoring and regression approach to quantify this sensitivity.\nValidation of the data quality and its selection exists in minimizing the exposure to the limitations\ndescribed in Section 4.3 or acknowledging them going forward. Ideally, the selected data are maximally\nspread out across relevant data sources. If there are several major broadcasters but data for only one\nare available, there is a severe risk of bias when generalizing any obtained results from this restricted\ndata set, as opposed to being only interested in and sticking with the conclusions of the particular data\nsource studied.\nDirecting the analysis of audio data via speech-to-text to a textual analysis brings up the question of\nhow trustworthy the conversion was. It is important to treat every transformation step and its possible\nerrors as such, not confusing the textual data for the actual source audio data.\nThe data should be controlled for duplicates or near duplicates. If the duplicated data entries come\nfrom a different source, the content has likely been consumed more widely. A way to omit duplication but\nstill maintain the implications it has is to add a metadata component that counts the number of duplicated\noccurrences. Wang et al. (2014) provide a (technical) overview with different techniques useful for\nduplicate detection.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 23\n9.2 Sentiment Quantification and Aggregation\nThe quantification of sentiment is highly important because it provides the numbers that any further step\nand interpretation is based on.\nRelying on machine learning to train sentiment classifiers works under the assumption that the annotated\ndata set is a faithful representation of the actual sentiment. Not every annotation procedure leads to a\nreliable annotation set. The quality of the gold standard can be measured by the level of interannotator\nagreement using, for instance, Cohen’s kappa. To measure the effectiveness of a sentiment classifier\nor a lexicon, one has to compare the model-generated scores with the gold standard. More precisely,\nthe trade-off between precision (the proportion of positives that is correct) and recall (the proportion of\npositives that is found) is at stake.8 Recall and precision extend easily from a two-class problem (e.g.,\npositive sentiment versus negative sentiment) to a multiclass setting doing micro or macro averaging (see,\ne.g., Zhang and Zhou, 2014).\nEvery lexicon tends to undergo one or more rounds of expert-based checks, to explicitly classify words\ninto positive or negative, delete irrelevant words, and correct obvious mistakes. The validity of individual\nentries of lexicons are thus still mainly evaluated by humans. Overall, lexicons should undergo the same\nlevel of scrutiny as any other sentiment computation method in terms of validation. It should be tested if\nthe accuracy of domain-specific lexicons is higher than generic lexicons. Loughran and McDonald (2011)\nuse careful inspection of frequently occurring words as the only basis to create their alternative word lists.\nTo validate this procedure, they relate tone computed from their negative lexicon to filing period excess\nstock returns, finding this sentiment measure to be in general more significant than tone based on the\ngeneric Harvard dictionary negative lexicon. The approach of Labille et al. (2017) compares a set domain\nlexicons on other domain texts. If the domain-specific lexicon is well constructed, it should rank first in\nterms of accuracy for the domain it is designed for. Apart from accuracy levels, another simple comparison\nprocedure is an ANOVA analysis to see which lexicon’s score variability is best captured by human coders.\nWhen the lexicon is generated with a regression, one looks at fit or information criteria statistics to validate\nthe overall power of a lexicon (e.g., Pro¨llochs et al., 2015). An imbalance between positive and negative\nentries might make sense from a domain-specific perspective but should be defendable, since bias in the\nsentiment quantification algorithm can also be due to a biased training set. The Loughran and McDonald\ndictionary, for instance, is left with the large proportion of 78% negative words as a result of the domain\nadaptation to financial disclosures.\nWhen creating sentiment measures, a first and simple analysis is to determine the correlation with\nexisting related sentiment time series. All EPU indices of Baker et al. (2016) were validated using a\nvery diligent human audit process, showing that the computer-generated indices are highly correlated\nwith the human-generated ones. Soo’s (2018) media sentiment housing index correlates strongly with the\nUniversity of Michigan Survey of Consumers, albeit lagging. He further validates his index by confirming\na reasonably strong lagged correlation with a multifactor index that combines multiple proxies, constructed\nbased on the methodology of Baker and Wurgler (2006). The most difficult task can end up to find related\nproxies, as sometimes, 0 they are rare or do not exist at all. Another simple time series validation procedure\nis what is referred to as event validation. This entails visualizing a sentiment measure and confirming\nwhether sharp increases or drops coincide with the incidence of important events that intuitively would\nresult in a strong increase or decrease in sentiment, respectively.\n9.3 Econometric Modeling and Interpretation\nMany models are evaluated by measuring the accuracy in an out-of-sample prediction exercise. However,\nprediction is not always of interest; measuring which words and how they convey sentiment can be a more\nimportant objective that is not always related to prediction accuracy. As mentioned in Justin Grimmer’s\ncomment on Taddy (2013b), how to do trustworthy task-specific sentiment evaluation still needs to be\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n24 ALGABA ET AL.\nformalized. How is one to know with a high degree of confidence whether a token can be attributed to a\nparticular sentiment feature? This problem can be particularly apparent when doing a large-dimensional\nregression of a sentiment variable on unigrams, for instance, with the resulting coefficients of the unigrams\nnot always easy to interpret and sensitive to change across different specifications.\nThe problem of extensive and problem-specific validation is brought up in detail in Grimmer and\nStewart (2013). For supervised methods, validation is fairly straightforward; it boils down to minimizing\nthe prediction error in replicating a set of annotated outputs or maximizing the classification accuracy\n(typically making use of confusion matrices). A data set is best divided into a training, a validation, and\na testing set to avoid a biased view on accuracy due to overfitting (Varian, 2014). Alternatively, one can\ndo k-fold cross-validation or rolling forecasting origin cross-validation when dealing with time series.\nUnsupervised methods require combining experimental, substantive, and statistical evidence to show the\nconceptual validity of a model output. Proper validation of unsupervised models is especially important\nwhen used for inference or measurement rather than prediction or exploration (Roberts et al., 2016).\n9.3.1 Model Estimation and Hypothesis Testing\nIt is common to evaluate the in-sample goodness of fit of a sentiment-based regression model with the\n(adjusted) R2 statistic. When adding their word flow measures, Calomiris and Mamaysky (2019) find a\nsubstantial increase in the R2 for predicting returns, volatility, and drawdown risk. The main concerned\nparameters are those associated with the sentiment variables. Their significance should be assessed\nstatistically and economically. Statistical significance shows whether an effect exists, but its applicability\nis mainly limited to low-dimensional models. Gandomi and Haider (2015) review various issues of\ndoing econometrics in a big data environment, pointing out the “irrelevance of statistical significance.”\nEconomic significance inspects the sign and strength of the association. Economic meaning can be given\nthrough, for instance, an attribution analysis.\nIn general, textual, audio, and visual data bring the known endogeneity challenges to econometricians.\nThe creation and publication of texts, videos, and speeches is correlated with many factors, so positing\na cause and effect remains dangerous when no further insights into the (many) underlying factors of the\ndata are available. Is it the sentiment of the alternative data set that is at the heart of a certain correlation\nor causality, or is the sentiment a reflection of associated underlying factors? Does the sentiment impact\nthe outcome variable directly or indirectly, and through what mechanism? Larsen and Thorsrud (2018)\npartition their network of sentiment/topic variables into more and less exogenous variables. Variables\nare considered exogenous if they have predictive power for other topics but are not (often) predicted\nthemselves. The most exogenous variables seem to be associated with economic fundamentals. Hubert\nand Labondance (2018) correct for endogeneity in their central bank tone measure by stripping away\nfundamentals, expectations of future fundamentals, standard monetary shocks, investor sentiment, and\npast sentiment shocks. Benhabib and Spiegel (2019) deal with endogeneity and, more specifically, reverse\ncausality using instrumental variables. They use political data to instrument for differences in (survey-\nbased) sentiment levels by state. When testing the effect of sentiment on the target variable and finding\nsignificant results, it is also recommended to test the effect of the target variable on sentiment via a\nreverse (lagged) regression specification. Few research papers on sentiment have carried forward this\nrobustness step.\nModel uncertainty is assessed through analyzing the impact of sentiment parameter estimates across\nvarious model specifications. This has to do with both a good and exhaustive definition of the control\nvariables X and with testing for enough different model structures. Soo (2018) creates robustness\nvariables from the qualitative data themselves. He computes indices from those news articles that convey\nfundamental market information rather than sentiment, adds those to his regression specifications, and\nfinds that his major sentiment index remains significant. Varian (2014) states that it is important “to be\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 25\nexplicit about examining how parameter estimates vary with respect to choices of control variables and\ninstruments.” Validation is rarely a black-and-white matter. The researcher should identify when and how\nsentiment is informative and when it is not.\n9.3.2 Out-of-Sample Evaluation\nAn out-of-sample version of the R2 statistic can be used to measure the relative reduction or increase in\nthe mean square out-of-sample prediction error of a sentiment-based forecasting strategy with respect to\na baseline strategy. Using the out-of-sample R2, Ada¨mmer and Schu¨ssler (2019) document statistically\nsignificant increased predictive power of the monthly U.S. equity premium when using news-aggregated\nvariables combined with a model-switching strategy. Caporin and Poli (2017) use five metrics (mean\nabsolute error, mean square error, heteroskedasticity adjusted mean square error, QLIKE loss function,\nand R2 of Mincer–Zarnowitz forecasting regressions) to compare the forecasting performance of a news-\nbased realized volatility model versus a baseline.\nThe magnitude of the impact of sentiment variables on economic and financial variables is highly\nsubject to time variation. Stability needs to be tested by performing the analysis, and measuring the\nperformance, on various subsamples, or by doing rolling forward regressions.\nA simple way to sidestep the issue of endogeneity is by comparing existing linear models to\nmodels enhanced with the quantified alternative data sources and to simply focus on whether predictive\npower improves or existing (significant) relationships hold. This could be formulated as testing models\n“controlling for sentiment.” The model confidence set procedure from Hansen et al. (2011) allows testing\nwhether different model specifications are truly different according to some significance level.\n10. Software\nThis section points to a selection of useful software tools to carry out a detailed sentometrics analysis\nfrom textual data.9 The selection is by no means exhaustive – meaning, there exist plenty of other\nsoftware tools equally of use to perform (parts of) a sentometrics analysis. We limit ourselves to the open-\nsource R and Python programming environments, due to their large popularity, strong communities of\ndevelopers, and relatively gradual learning curves. For instance, MATLAB rarely comes to mind for doing\ntextual analysis, but its Text Analytics Toolbox has many capabilities for doing powerful preprocessing,\nvectorization, sentiment analysis, and topic modeling. One does not necessarily have to choose one\nprogramming environment. Like Glasserman and Mamaysky (2019), a common workflow includes doing\nthe handling of textual, audio, and visual data in Python, and the statistical analysis in R. The available\nsoftware is linked to specific tasks involved in an econometric analysis of qualitative sentiment data and\nis summarized in Table 1.\nThe quanteda package (Benoit et al., 2018) is a general text mining toolkit in R. Its development has\nbeen actively supported by the European Commission. The package tidytext (Silge and Robinson, 2016)\ncan also be used to do many text processing tasks, following “tidy” data principles. The tm package\n(Feinerer et al., 2008) is an older textual analysis framework, but is still used as a backend in many\ntext-related R packages.10\nThe NLTK library (Bird et al., 2009), short for Natural Language Toolkit, is the text mining toolkit\ncounterpart in Python, albeit even more exhaustive.11 In Python, the spaCy library (Honnibal and\nMontani, 2017) is the most complete alternative. It is faster, but more black box. The TextBlob library\n(Loria, 2019) is built on the NLTK library and is therefore more specialized as to what concerns several\ntextual extractions, such as sentiment analysis through machine learning classification.\nThe R package sentometrics (Ardia et al., 2020) provides a collection of functions to do sentiment\ncomputation, sentiment aggregation, and (high-dimensional) sentiment-based regression. Wischnewsky\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n26 ALGABA ET AL.\nTa\nbl\ne\n1.\nN\non\nex\nha\nus\ntiv\ne\nO\nve\nrv\nie\nw\no\nfT\nex\ntu\nal\nD\nat\na\nA\nn\nal\nys\nis\nTo\no\nls\nin\nR\nan\nd\nP\nyt\nh\no\nn\n.\nTa\nsk\ns\nR\nes\ntru\nct\nur\nin\ng\nSe\nn\ntim\nen\ntq\nua\nnt\nifi\nca\ntio\nn\nTi\nm\ne\nse\nrie\ns\nEc\no\nn\no\nm\net\nric\nan\nal\nys\nis\nSo\nftw\nare\nCl\nea\nni\nng\nM\net\nad\nat\na\nTo\nke\nn\ns\nLe\nx\nic\non\n-B\nas\ned\nM\nL\nA\ngg\nre\nga\ntio\nn\nVi\nsu\nal\niz\nat\nio\nn\nR\neg\nre\nss\nio\nn\nVa\nlid\nat\nio\nn\nR\nca\nre\nt\n√\n√\n√\ngl\nm\nne\nt\n√\n√\n√\nqu\nan\nte\nda\n√\n√\n√\n√\n√\nrJ\nST\n√\n√\nSe\nnt\nim\nen\ntA\nna\nly\nsis\n√\n√\nse\nn\nto\nm\net\nri\ncs\n√\n√\n√\n√\n√\n√\nte\nxt\nir\n√\n√\n√\ntid\nyt\nex\nt\n√\n√\n√\n√\ntm\n√\n√\n√\n√\nP\nyt\nh\no\nn\nN\nLT\nK\n√\n√\n√\n√\n√\n√\nsc\nik\nit-\nle\nar\nn\n√\n√\n√\n√\n√\n√\nsp\naC\ny\n√\n√\n√\nTe\nn\nso\nrF\nlo\nw\n√\n√\n√\nTe\nx\ntB\nlo\nb\n√\n√\n√\n√\nNo\nte\n:\nTh\ne\nab\nbr\nev\nia\ntio\nn\nM\nL\nst\nan\nds\nfo\nrm\nac\nhi\nne\nle\nar\nn\nin\ng.\nA\ntic\nk\nin\ndi\nca\nte\nst\nha\ntt\nhe\nso\nftw\nar\ne\nca\nn\nbe\ndi\nre\nct\nly\no\nr\nin\ndi\nre\nct\nly\n(i.\ne.,\nby\nm\nin\nim\nal\nly\nch\nai\nni\nng\nw\nith\no\nth\ner\nav\nai\nla\nbl\ne\nto\no\nls)\nu\nse\nd\nto\npe\nrfo\nrm\na\npa\nrt\nic\nul\nar\nw\no\nrk\nflo\nw\nst\nep\n.T\nhe\npa\nck\nag\nes\nin\ncl\nud\ned\nfo\nr\nR\nar\ne\nca\nre\nt(\nKu\nhn\n,2\n01\n8),\ngl\nm\nne\nt(\nFr\nied\nma\nne\nta\nl.,\n20\n10\n),\nqu\nan\nte\nda\n(B\nen\noit\net\na\nl.,\n20\n18\n),\nrJ\nST\n(B\noit\nen\n,2\n01\n9),\nSe\nnt\nim\nen\ntA\nna\nly\nsis\n(F\neu\nerr\nieg\nel\nan\nd\nPr\no¨\nllo\nch\ns,\n20\n19\n),\nse\nn\nto\nm\net\nri\ncs\n(A\nrdi\nae\nt\na\nl.,\n20\n20\n),\nte\nxt\nir\n(T\nad\ndy\n,\n20\n18\n),\ntid\nyt\nex\nt(\nSil\nge\nan\nd\nR\no\nbi\nns\non\n,2\n01\n6),\nan\nd\ntm\n(F\nein\nere\nre\nta\nl.,\n20\n08\n).F\no\nr\nP\nyt\nh\no\nn\n,\nth\ne\nlib\nra\nrie\nst\nab\nu\nla\nte\nd\nar\ne\nN\nLT\nK\n(B\nird\net\na\nl.,\n20\n09\n),s\nci\nki\nt-l\nea\nrn\n(P\ned\nreg\nos\na\net\na\nl.,\n20\n11\n),s\npa\nC\ny\n(H\non\nnib\nal\nan\nd\nM\non\nta\nni\n,2\n01\n7),\nTe\nn\nso\nrF\nlo\nw\n(A\nba\ndi\net\na\nl.,\n20\n16\n),a\nn\nd\nTe\nx\ntB\nlo\nb\n(L\nori\na,\n20\n19\n).\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 27\net al. (2019) use the package to create a “Sentoindex” that represents financial stability sentiment as\nexpressed during testimonies at U.S. Congressional hearings. The sentiment computation in sentometrics\nis lexicon-based, but other sentiment scores can be used as input for further aggregation. The\nsentometrics package also provides a simple keywords-based approach to generating metadata features.\nThe SentimentAnalysis package (Feuerriegel and Pro¨llochs, 2019) can be used to create lexicons and\ncompute sentiment according to the method of Pro¨llochs et al. (2015).\nThe regression framework in sentometrics relies on both the caret (Kuhn, 2018) and glmnet (Friedman\net al., 2010) packages but is specific to the sentiment time series generated within the package. The\nglmnet package implements various penalized regressions; the caret package provides more generic\nclassification and regression modeling. The inverse text regression methods developed by Taddy (2013b)\nand Taddy (2015a) are available in the R package textir (Taddy, 2018).12 The rJST package (Boiten,\n2019) implements the joint sentiment/topic model of Lin and He (2009).\nPython’s scikit-learn (Pedregosa et al., 2011) is one if its most established machine learning libraries.\nIt supports the majority of the common learning algorithms used in sentiment analysis and is easy to\nuse with respect to feature engineering. To do the same, but also particularly deep learning, Google’s\nTensorFlow library (Abadi et al., 2016) is the standard, albeit imposing more on the user in terms of\nsetting up the individual components of a chosen model. Since recently, there also exists a comprehensive\nR interface to the TensorFlow framework.\nThese days, research papers also go increasingly accompanied with standalone open-source replication\ncode (see, e.g., the MATLAB code used in Thorsrud, 2018).13 Another example, to do sentiment analysis\nbenchmarking, is the online tool iFeel 2.0 (Arau´jo et al., 2016) based on Ribeiro et al. (2016).\nA shortcoming of the current software landscape is that there are no libraries that propose a full and easy\nintegration of the required data handling, machine learning, and econometric tools. The preprocessing\nand sentiment quantification packages have very little in common with the packages used for modeling.\nHaving to combine too many packages or even multiple programming languages is prone to error, for\ninstance, due to the usage of different types of object classes that need to be converted.\n11. Concluding Remarks\nSentiment analysis allows us to accurately and automatically map alternative data into quantitative\nstatistics as a support for decision-making across many business applications. Economists and investors,\nand also politicians and journalists, have started to embrace the utilization of econometric methods in the\nanalysis and application of textual, audio, and visual data, to understand historical evolutions and better\nforecast future evolutions.\nWe overview the emerging field of sentometrics that investigates the transformation of qualitative data\ninto quantitative sentiment variables, and their subsequent application in an econometric analysis of the\nrelationships between sentiment and other variables. This survey is organized around the different steps\nof a typicalanalysis. The most important terminology is collected in the appendix.\nTextual, audio, and visual data will continue to become more cheaply and widely available, together\nwith becoming more easily accessible. The interest of public and private institutions to monetize these\ndata and their proprietary data will grow as well. We recommend further research on multimodal sentiment\nanalysis in econometrics. The future will be exceedingly multimedia in terms of content generated, hence\nthe analysis indispensably multimodal. A major challenge is the development of appropriate technology\nfor unified multimodal sentiment analysis systems.\nProgress toward better integrated and more reproducible sentiment data research will require\ncollaborative cross-disciplinary efforts. We end this paper with a call for more efforts toward\nreproducibility in the econometric study of sentiment from qualitative data. It would benefit greatly\nfrom reference data and associated state-of-the-art performance, for different sentiment quantification\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n28 ALGABA ET AL.\ntechniques, data, and econometric approaches. In the field of computer science, such practices are more\nwidespread. Other researchers can evaluate any new approach on the reference data and as such provide a\nconsistent picture of reproducibility or improved performance. Even though the sharing of code and data\nhas gained adoption, there are yet no standard practices on how to do so. The reference data and results\nshould be made available through an open database with easy access and well-documented formats. This\nmatches with the proposition of Lacy et al. (2015) to set up a standard scholarly repository to share\nresearch-related materials. As a companion to this survey paper, we have therefore set up a collaborative\neconometrics and sentiment GitHub project to gather such resources.14\nAcknowledgments\nWe thank the Associate Editors (Les Oxley and SteliosBekiros) and two anonymous Referees,seminar\nparticipants at Ca’ Foscari University of Venice, the European Commission JRC Ispra “Big Data and Forecasting\nWorkshop” (Ispra, 2019), Ghent University, HEC Montral, the International Conference on Computational\nand Financial Econometrics (London, 2019), Skema Business School, University of Delaware, and Vrije\nUniversiteit Brussel for their useful comments. We are also grateful to Francesco Audrino, Leopoldo Catania,\nMaxime De Bruyn, William Doehler, Nitish Sinha, and Leif Anders Thorsrud for stimulating discussions\nand feedback. This project benefited from financial support from Innoviris (https://innoviris.brussels),\nIVADO (https://ivado.ca), swissuniversities (https://www.swissuniversities.ch), and the Swiss National Science\nFoundation (http://www.snf.ch, grants #179281 and #191730).\nNotes\n1. See https://www.gdeltproject.org.\n2. The EPU index for various countries can be retrieved from: http://www.policyuncertainty.com.\nThe online publication of text-based indices is becoming prevalent, see also: https://www.retriever-\ninfo.com/fni.\n3. A collection of open-source textual, audio, and visual data can be found at https://pathmind.com/\nwiki/open-datasets.\n4. For instance, a net textual sentiment value of two can be obtained from both two positive and zero\nnegative words, or 20 positive and 18 negative words. The number of polarized words can be retained\nas separate sentiment scores, else its information can be used for within-unit aggregation.\n5. The importance and application of valence shifters is also a function of the document type. Hutto\nand Gilbert (2014) created the VADER sentiment analysis system for social media texts, letting word\nshape (e.g., capitalization), slang (e.g., “kinda”), and emoticons, among others, act as valence shifters.\n6. Word embeddings are an advanced way of doing text vectorization, compared to, for instance, the\nsimpler construction of a document-term matrix.\n7. Advancements in machine learning and econometrics have been going more hand in hand. An\ninteresting example is “double” or “orthogonal” machine learning, a development that aims to deal\nwith the invalidity of inference infused by many machine learning methods (see mainly Chernozhukov\net al., 2017 and related work).\n8. The precision and recall metrics can be combined in the Fβ -score, with Fβ ≡ (1 +\nβ2) precision×recall\nβ2×precision+recall . The β factor defines the relative level of importance put on recall. If β = 1,\nboth metrics are weighted equally in a harmonic mean sense.\n9. A well-known open-source software tool for audio data processing is openSMILE (Eyben et al.,\n2013). The LVA software (https://lva650.com) can be used for preprocessing, deconstruction,\nand immediate emotion analysis of audio data (see Mayew and Venkatachalam, 2012, for an\napplication in finance). For visual data processing, alternatives are the commercial softwares OKAO\nVision System from OMRON (https://plus-sensing.omron.com/technology) or Luxand FaceSDK\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 29\n(https://www.luxand.com/facesdk), both mainly for facial features extraction. A good commercial\nspeech-to-text technology is Vocapia (https://www.vocapia.com). In the open-source sphere, the\nDeepSpeech project (Hannun et al., 2014) and associated software packages are very useful.\nGenerally, the outputs returned by the above tools can be easily loaded into any programming\nenvironment to perform the remaining steps in the analysis.\n10. A helpful starting point to explore the plethora of textual analysis tools in R is CRAN’s Task View\n“NaturalLanguageProcessing” (https://CRAN.R-project.org/view=NaturalLanguageProcessing).\n11. The quanteda package website gives an overview of the actual functions across the packages referred\nto perform several specific tasks (see https://quanteda.io/articles/pkgdown/comparison.html).\n12. The DMR from Taddy (2015a) is also implemented with the programming language Julia, available\nat https://github.com/AsafManela/HurdleDMR.jl, which mainly includes the Hurdle Distributed\nMultiple Regression algorithm from Kelly et al. (2019).\n13. The code is available at https://github.com/leifandersthorsrud/NCI.\n14. See https://sborms.github.io/econometrics-meets-sentiment.\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M.,\nKudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D.G., Steiner, B., Tucker, P., Vasudevan, V.,\nWarden, P., Wicke, M., Yu, Y. and Zheng, X. (2016) TensorFlow: a system for large-scale machine learning.\nIn Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation,\n(pp. 265–283). USENIX Association.\nAda¨mmer, P. and Schu¨ssler, R.A. (2019) Forecasting the equity premium: mind the news! Forthcoming in\nReview of Finance.\nAllee, K.D. and DeAngelis, M.D. (2015) The structure of voluntary disclosure narratives: evidence from tone\ndispersion. Journal of Accounting Research 53(2): 241–274.\nAndreou, E., Gagliardini, P., Ghysels, E. and Rubin, M. (2019) Inference in group factor models with an\napplication to mixed frequency data. Econometrica 87(4): 1267–1305.\nAngeletos, G.-M., Collard, F. and Dellas, H. (2018) Quantifying confidence. Econometrica 86(5): 1689–1726.\nAngeletos, G.-M. and La’O, J. (2013) Sentiments. Econometrica 81(2): 739–779.\nAntweiler, W. and Frank, M.Z. (2004) Is all that talk just noise? The information content of internet stock\nmessage boards. Journal of Finance 59(3): 1259–1294.\nArau´jo, M., Diniz, J.P., Bastos, L., Soares, E., Ju´nior, M., Ferreira, M., Riberio, F. and Benevenuto, F. (2016)\niFeel 2.0: a multilingual benchmarking system for sentence-level sentiment analysis. In Proceedings of\nthe 10th International AAAI Conference on Web and Social Media (pp. 758–759).\nArdia, D., Bluteau, K., Borms, S. and Boudt, K. (2020) The R package sentometrics to compute, aggregate and\npredict with textual sentiment. Forthcoming in Journal of Statistical Software.\nArdia, D., Bluteau, K. and Boudt, K. (2019a) Media and the stock market: their relationship and abnormal\ndynamics around earnings announcements. Working Paper.\nArdia, D., Bluteau, K. and Boudt, K. (2019b) Questioning the news about economic growth: sparse forecasting\nusing thousands of news-based sentiment values. International Journal of Forecasting 35(4): 1370–1386.\nArslan-Ayaydin, ¨O., Boudt, K. and Thewissen, J. (2016) Managers set the tone: equity incentives and the tone\nof earnings press releases. Journal of Banking and Finance 72: 132–147.\nBaccianella, S., Esuli, A. and Sebastiani, F. (2010) SentiWordNet 3.0: an enhanced lexical resource for sentiment\nanalysis and opinion mining. In Proceedings of the 7th Conference on International Language Resources\nand Evaluation.\nBajo, E. and Raimondo, C. (2017) Media sentiment and IPO underpricing. Journal of Corporate Finance 46:\n139–153.\nBaker, M. and Wurgler, J. (2006) Investor sentiment and the cross-section of stock returns. Journal of Finance\n61(4): 1645–1680.\nBaker, M. and Wurgler, J. (2007) Investor sentiment in the stock market. Journal of Economic Perspectives\n21(2): 129–152.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n30 ALGABA ET AL.\nBaker, S.R., Bloom, N. and Davis, S.J. (2016) Measuring economic policy uncertainty. The Quarterly Journal\nof Economics 131(4): 1593–1636.\nBannier, C., Pauls, T. and Walter, A. (2019) Content analysis of business communication: introducing a German\ndictionary. Journal of Business Economics 89(1): 79–123.\nBarsky, R.B. and Sims, E.R. (2012) Information, animal spirits, and the meaning of innovations in consumer\nconfidence. American Economic Review 102(4): 1343–1377.\nBenhabib, J. and Spiegel, M.M. (2019) Sentiments and economic activity: evidence from US states. Economic\nJournal 129(618): 715–733.\nBenoit, K., Watanabe, K., Wang, H., Nulty, P., Obeng, A., Mu¨ller, S. and Matsuo, A. (2018) quanteda: An R\npackage for the quantitative analysis of textual data. Journal of Open Source Software 3(30): 774.\nBholat, D., Hans, S., Santos, P. and Schonhardt-Bailey, C. (2015) Text mining for central banks. Technical\nreport, Centre for Central Banking Studies, Bank of England.\nBird, S., Klein, E. and Loper, E. (2009) Natural Language Processing with Python. Beijing: O’Reilly Media.\nBlair, R.C. and Cole, S.R. (2002) Two-sided equivalence testing of the difference between two means. Journal\nof Modern Applied Statistical Methods 1(1): 139–142.\nBlei, D.M. and Lafferty, J.D. (2006) Dynamic topic models. In Proceedings of the 23rd International Conference\non Machine Learning (ICML ’06), pp. 113–120. ACM.\nBlei, D.M. and Lafferty, J.D. (2007) A correlated topic model of Science. Annals of Applied Statistics 1(1):\n17–35.\nBlei, D.M., Ng, A.Y. and Jordan, M.I. (2003) Latent Dirichlet allocation. Journal of Machine Learning Research\n3: 993–1022.\nBoiten, M. (2019) rJST: Joint Sentiment Topic Modelling, R package.\nBojanowski, P., Grave, E., Joulin, A. and Mikolov, T. (2017) Enriching word vectors with subword information.\nTransactions of the Association for Computational Linguistics 5: 135–146.\nBorovkova, S., Garmaev, E., Lammers, P. and Rustige, J. (2017) SenSR: a sentiment-based systemic risk\nindicator. Technical Report 553, De Nederlandsche Bank.\nBoudt, K. and Thewissen, J. (2019) Jockeying for position in CEO letters: impression management and sentiment\nanalytics. Financial Management 48(1): 77–115.\nBoudt, K., Thewissen, J. and Torsin, W. (2018) When does the tone of earnings press releases mat-\nter?International Review of Financial Analysis 57: 231–245.\nBradley, M.M. and Lang, P.J. (1999) Affective norms for English words (ANEW): instruction manual and\naffective ratings. Technical report.\nCalomiris, C.W. and Mamaysky, H. (2019) How news and its context drive risk and returns around the world.\nJournal of Financial Economics 133(2): 299–336.\nCaporin, M. and Poli, F. (2017) Building news measures from textual data and an application to volatility\nforecasting. Econometrics 5(3): 1–46.\nCasey, G.P. and Owen, A.L. (2013) Good news, bad news, and consumer confidence. Social Science Quarterly\n94(1): 292–315.\nCeron, A., Curini, L., Iacus, S.M. and Porro, G. (2014) Every tweet counts? How sentiment analysis of social\nmedia can improve our knowledge of citizens’ political preferences with an application to Italy and France.\nNew Media & Society 16(2): 340–358.\nChang, C.-C., Hsieh, P.-F. and Wang, Y.-H. (2015) Sophistication, sentiment, and misreaction. Journal of\nFinancial and Quantitative Analysis 50(4): 903–928.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C. and Newey, W. (2017) Dou-\nble/debiased/Neyman machine learning of treatment effects. American Economic Review 107(5): 261–265.\nChiou, L. and Tucker, C. (2017) Content aggregation by platforms: the case of the news media. Journal of\nEconomics & Management Strategy 26(4): 782–805.\nCroushore, D. and Stark, T. (2003) A real-time data set for macroeconomists: does the data vintage\nmatter?Review of Economics and Statistics 85(3): 605–617.\nDas, S.R. and Chen, M.Y. (2007) Yahoo! for Amazon: sentiment extraction from small talk on the web.\nManagement Science 53(9): 1375–1388.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 31\nDe Clercq, O., Lefever, E., Jacobs, G., Carpels, T. and Hoste, V. (2017) Towards an integrated pipeline for\naspect-based sentiment analysis in various domains. In Proceedings of the 8th Workshop on Computational\nApproaches to Subjectivity, Sentiment and Social Media Analysis, pp. 136–142. ACM.\nDe Long, J.B., Shleifer, A., Summers, L.H. and Waldmann, R.J. (1990) Noise trader risk in financial markets.\nJournal of Political Economy 98(4): 703–738.\nDenny, M.J. and Spirling, A. (2018) Text preprocessing for unsupervised learning: why it matters, when it\nmisleads, and what to do about it. Political Analysis 26(2): 168–189.\nDevlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2018) BERT: pre-training of deep bidirectional\ntransformers for language understanding. Working Paper.\nDiamond, D.W. and Dybvig, P.H. (1983) Bank runs, deposit insurance, and liquidity. Journal of Political\nEconomy 91(3): 401–419.\nEguchi, K. and Lavrenko, V. (2006) Sentiment retrieval using generative models. In Proceedings of the 2006\nConference on Empirical Methods in Natural Language Processing, pp. 345–354. ACM.\nEkman, P. and Friesen, W.V. (1976) Measuring facial movement. Environmental Psychology and Nonverbal\nBehavior 1(1): 56–75.\nEngle, R., Giglio, S., Kelly, B., Lee, H. and Stroebel, J. (2020) Hedging climate change news. Review of\nFinancial Studies 33(3): 1184–1216.\nEshbaugh-Soha, M. (2010) The tone of local presidential news coverage. Political Communication 27(2):\n121–140.\nEvans, J.A. and Aceves, P. (2016) Machine translation: mining text for social theory. Annual Review of Sociology\n42: 21–50.\nEyben, F., Weninger, F., Gross, F. and Schuller, B. (2013) Recent developments in openSMILE, the Munich\nopen-source multimedia feature extractor. In Proceedings of the 21st ACM International Conference on\nMultimedia, MM ’13, pp. 835–838. ACM.\nFeinerer, I., Hornik, K. and Meyer, D. (2008) Text mining infrastructure in R. Journal of Statistical Software\n25(5): 1–54.\nFeldman, R., Govindaraj, S., Livnat, J. and Segal, B. (2010) Management’s tone change, post earnings\nannouncement drift and accruals. Review of Accounting Studies 15(4): 915–953.\nFeuerriegel, S. and Pro¨llochs, N. (2019) SentimentAnalysis: Dictionary-Based Sentiment Analysis, R package.\nFlaxman, S., Goel, S. and Rao, J. (2016) Filter bubbles, echo chambers, and online news consumption. Public\nOpinion Quarterly 80: 298–320.\nFriedman, J., Hastie, T. and Tibshirani, R. (2010) Regularization paths for generalized linear models via\ncoordinate descent. Journal of Statistical Software 33(1): 1–22.\nFu, X., Yang, K., Huang, J.Z. and Cui, L. (2015) Dynamic non-parametric joint sentiment topic mixture model.\nKnowledge-Based Systems 82: 102–114.\nGandomi, A. and Haider, M. (2015) Beyond the hype: big data concepts, methods, and analytics. International\nJournal of Information Management 35(2): 137–144.\nGarcı´a, D. (2013) Sentiment during recessions. Journal of Finance 68(3): 1267–1300.\nGarz, M. (2014) Good news and bad news: evidence of media bias in unemployment reports. Public Choice\n161(3–4): 499–515.\nGelper, S. and Croux, C. (2010) On the construction of the European economic sentiment indicator. Oxford\nBulletin of Economics and Statistics 72(1): 47–62.\nGelper, S., Peres, R. and Eliashberg, J. (2018) Talk bursts: the role of spikes in pre-release word-of-mouth\ndynamics. Journal of Marketing Research 55(6): 801–817.\nGentzkow, M., Kelly, B. and Taddy, M. (2019a) Text as data. Journal of Economic Literature 57(3): 535–574.\nGentzkow, M. and Shapiro, J.M. (2010) What drives media slant? Evidence from U.S. daily newspapers.\nEconometrica 78(1): 35–71.\nGentzkow, M., Shapiro, J.M. and Taddy, M. (2019b) Measuring group differences in high-dimensional choices:\nmethod and application to congressional speech. Econometrica 87: 1307–1340.\nGlanzer, M. and Cunitz, A.R. (1966) Two storage mechanisms in free recall. Journal of Verbal Learning and\nVerbal Behavior 5(4): 351–360.\nGlasserman, P. and Mamaysky, H. (2019) Does unusual news forecast market stress?Journal of Financial and\nQuantitative Analysis 54(5): 1937–1974.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n32 ALGABA ET AL.\nGrimmer, J. and Stewart, B.M. (2013) Text as data: the promise and pitfalls of automatic content analysis\nmethods for political texts. Political Analysis 21(3): 267–297.\nHamilton, W.L., Clark, K., Leskovec, J. and Jurafsky, D. (2016) Inducing domain-specific sentiment lexicons\nfrom unlabeled corpora. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language\nProcessing, pp. 595–605. ACM.\nHannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., Prenger, R., Satheesh, S., Sengupta, S.,\nCoates, A. and Ng, A.Y. (2014) Deep speech: scaling up end-to-end speech recognition. Working Paper.\nHansen, P., Lunde, A. and Nason, J. (2011) The model confidence set. Econometrica 79: 453–497.\nHatzivassiloglou, V. and McKeown, K.R. (1997) Predicting the semantic orientation of adjectives. In\nProceedings of the 35th Annual Meeting of the Association of Computational Linguistics and 8th\nConference of the European Chapter of the Association for Computational Linguistics, pp. 174–181.\nHe, Y., Lin, C., Gao, W. and Wong, K.-F. (2013) Dynamic joint sentiment-topic model. ACM Transactions on\nIntelligent Systems and Technology 5(1): 1–21.\nHenry, E. (2008) Are investors influenced by how earnings press releases are written?Journal of Business\nCommunication 45(4): 363–407.\nHeston, S. and Sinha, N. (2017) News vs. sentiment: predicting stock returns from news stories. Financial\nAnalysts Journal 73(3): 67–83.\nHoerl, A. and Kennard, R. (1970) Ridge regression: biased estimation for nonorthogonal problems.\nTechnometrics 12(1): 55–67.\nHofmann, T. (2001) Unsupervised learning by probabilistic latent semantic analysis. Machine Learning 42(1–2):\n177–196.\nHonnibal, M. and Montani, I. (2017) spaCy 2: Natural Language Understanding with Bloom Embeddings,\nConvolutional Neural Networks and Incremental Parsing, Python library.\nHu, M. and Liu, B. (2004) Mining opinion features in customer reviews. In Proceedings of the 19th National\nConference on Artificial Intelligence, AAAI’04, pp. 755–760. AAAI Press.\nHuang, X., Teoh, S.H. and Zhang, Y. (2014) Tone management. Accounting Review 89(3): 1083–1113.\nHubert, P. and Labondance, F. (2018) Central bank sentiment. Working Paper.\nHutto, C.J. and Gilbert, E. (2014) VADER: a parsimonious rule-based model for sentiment analysis of social\nmedia text. In Proceedings of the 8th International AAAI Conference on Weblogs and Social Media, Vol.\n23, pp. 216–225.\nJegadeesh, N. and Wu, D. (2013) Word power: a new approach for content analysis. Journal of Financial\nEconomics 110(3): 712–729.\nKalogeropoulos, A. (2018) Economic news and personal economic expectations. Mass Communication and\nSociety 21(2): 248–265.\nKanayama, H. and Nasukawa, T. (2006) Fully automatic lexicon expansion for domain-oriented sentiment\nanalysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,\npp. 355–363.\nKearney, C. and Liu, S. (2014) Textual sentiment in finance: a survey of methods and models. International\nReview of Financial Analysis 33: 171–185.\nKelly, B.T., Manela, A. and Moreira, A. (2019) Text selection. Working Paper.\nKeynes, J.M. (1936) The General Theory of Employment, Interest, and Money. London, UK: Palgrave\nMacmillan.\nKra¨ussl, R. and Mirgorodskaya, E. (2017) Media, sentiment and market performance in the long run. European\nJournal of Finance 23(11): 1059–1082.\nKuhn, M. (2018) caret: Classification and Regression Training, R package.\nLabille, K., Gauch, S. and Alfarhood, S. (2017) Creating domain-specific sentiment lexicons via text mining.\nIn Proceedings of the 6th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining, pp. 1–9.\nLacy, S., Watson, B.R., Riffe, D. and Lovejoy, J. (2015) Issues and best practices in content analysis. Journalism\n& Mass Communication Quarterly 92(4): 791–811.\nLarsen, V.H. and Thorsrud, L.A. (2018) Business cycle narratives. Technical Report 7468, CESifo.\nLarsen, V.H. and Thorsrud, L.A. (2019) The value of news for economic developments. Journal of Econometrics\n210(1): 203–218.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 33\nLarsen, V.H., Thorsrud, L.A. and Zhulanova, J. (2020) News-driven inflation expectations and information\nrigidities. Forthcoming in Journal of Monetary Economics.\nLewis, C. and Young, S. (2019) Fad or future? Automated analysis of financial text and its implications for\ncorporate reporting. Accounting and Business Research 49(5): 587–615.\nLin, C. and He, Y. (2009) Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM\nConference on Information and Knowledge Management, CIKM ’09, pp. 375–384. ACM.\nLiu, B. (2015) Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge: Cambridge\nUniversity Press.\nLiu, Y., Yu, X., Huang, X. and An, A. (2009) Blog data mining: the predictive power of sentiments. In P.S. Yu\nand C. Zhang (eds ), Data Mining for Business Applications (pp. 183–195). Dordrecht: Springer.\nLoria, S. (2019) TextBlob: Simplified Text Processing, Python library.\nLoughran, T. and McDonald, B. (2011) When is a liability not a liability? Textual analysis, dictionaries, and\n10-Ks. Journal of Finance 66(1): 35–65.\nLoughran, T. and McDonald, B. (2014) Measuring readability in financial disclosures. Journal of Finance\n69(4): 1643–1671.\nLoughran, T. and McDonald, B. (2016) Textual analysis in accounting and finance: a survey. Journal of\nAccounting Research 54(4): 1187–1230.\nLowry, D. (2008) Network TV news framing of good vs. bad economic news under Democrat and Republican\npresidents: a lexical analysis of political bias. Journalism & Mass Communication Quarterly 85(3): 483–\n498.\nLudvigson, S.C. (2004) Consumer confidence and consumer spending. Journal of Economic Perspectives 18(2):\n29–50.\nLukesˇ, J. and Søgaard, A. (2018) Sentiment analysis under temporal shift. In Proceedings of the 9th Workshop\non Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pp. 65–71. ACM.\nLu¨tkepohl, H. (2017) Estimation of structural vector autoregressive models. Communications for Statistical\nApplications and Methods 24: 421–441.\nManela, A. and Moreira, A. (2017) News implied volatility and disaster concerns. Journal of Financial\nEconomics 123(1): 137–162.\nMayew, W.J. and Venkatachalam, M. (2012) The power of voice: managerial affective states and future firm\nperformance. Journal of Finance 67(1): 1–43.\nMcCracken, M.W. and Ng, S. (2016) FRED-MD: a monthly database for macroeconomic research. Journal of\nBusiness & Economic Statistics 34(4): 574–589.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. and Dean, J. (2013) Distributed representations of words\nand phrases and their compositionality. In Proceedings of the 26th International Conference on Neural\nInformation Processing Systems, Vol. 2 (pp. 3111–3119).\nMiller, G.A. (1995) WordNet: a lexical database for English. Communications of the ACM 38(11): 39–41.\nMohammad, S., Salameh, M. and Kiritchenko, S. (2016) How translation alters sentiment. Journal of Artificial\nIntelligence Research 55: 95–130.\nMohammad, S.M. and Turney, P.D. (2013) Crowdsourcing a word-emotion association lexicon. Computational\nIntelligence 29(3): 436–465.\nMunezero, M.D., Montero, C.S., Sutinen, E. and Pajunen, J. (2014) Are they different? Affect, feeling, emotion,\nsentiment, and opinion detection in text. IEEE Transactions on Affective Computing 5(2): 101–111.\nNimark, K.P. and Pitschner, S. (2019) News media and delegated information choice. Journal of Economic\nTheory 181: 160–196.\nNowak, A. and Smith, P. (2017) Textual analysis in real estate. Journal of Applied Econometrics 32(4): 896–918.\nPang, B., Lee, L. and Vaithyanathan, S. (2002) Thumbs up? Sentiment classification using machine learning\ntechniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,\nVolume 10 of EMNLP ’02, pp. 79–86.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer,\nP., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and\nDuchesnay, E. (2011) Scikit-learn: machine learning in Python. Journal of Machine Learning Research\n12: 2825–2830.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n34 ALGABA ET AL.\nPennington, J., Socher, R. and Manning, C. (2014) GloVe: global vectors for word representation. In Proceedings\nof the 2014 Conference on Empirical Methods in Natural Language Processing, pp. 1532–1543. ACM.\nPetalas, D.P., vanSchie, H. and Vettehen, P.H. (2017) Forecasted economic change and the self-fulfilling\nprophecy in economic decision-making. PLoS One 12(3): e0174353.\nPham, H., Manzini, T., Liang, P.P. and Poczos, B. (2018) Seq2Seq2Sentiment: multimodal sequence to\nsequence models for sentiment analysis. In Proceedings of the Grand Challenge and Workshop on Human\nMultimodal Language, pp. 53–63. ACM.\nPicault, M. and Renault, T. (2017) Words are not all created equal: a new measure of ECB communication.\nJournal of International Money and Finance 79: 136–156.\nPoria, S., Cambria, E., Howard, N., Huang, G.-B. and Hussain, A. (2016) Fusing audio, visual and textual clues\nfor sentiment analysis from multimodal content. Neurocomputing 174(A): 50–59.\nPro¨llochs, N., Feuerriegel, S. and Neumann, D. (2015) Generating domain-specific dictionaries using Bayesian\nlearning. In Proceedings of the European Conference on Information Systems, pp. 1–14.\nQin, D. (2011) Rise of VAR modelling approach. Journal of Economic Surveys 25(1): 156–174.\nRavi, K. and Ravi, V. (2015) A survey on opinion mining and sentiment analysis: tasks, approaches and\napplications. Knowledge-Based Systems 89: 14–46.\nRemus, R., Quasthoff, U. and Heyer, G. (2010) SentiWS - a publicly available German-language resource\nfor sentiment analysis. In Proceedings of the 7th Conference on International Language Resources and\nEvaluation, pp. 1168–1171. European Languages Resources Association (ELRA).\nRibeiro, F.N., Arau´jo, M., Gonc¸alves, P., Gonc¸alves, M.A. and Benevenuto, F. (2016) SentiBench - a benchmark\ncomparison of state-of-the-practice sentiment analysis methods. EPJ Data Science 5: 1–23.\nRidout, T.N., Fowler, E.F. and Searles, K. (2012) Exploring the validity of electronic newspaper databases.\nInternational Journal of Social Research Methodology 15(6): 451–466.\nRiffe, D., Lacy, S., Watson, B.R. and Fico, F. (2019) Analyzing Media Messages: Using Quantitative Content\nAnalysis in Research. New York: Routledge.\nRoberts, M.E., Stewart, B.M. and Airoldi, E.M. (2016) A model of text for experimentation in the social\nsciences. Journal of the American Statistical Association 111(515): 988–1003.\nRogers, J.L., Van Buskirk, A. and Zechman, S.L.C. (2011) Disclosure tone and shareholder litigation.\nAccounting Review 86(6): 2155–2183.\nRousseeuw, P., Raymaekers, J. and Hubert, M. (2018) A measure of directional outlyingness with applications\nto image data and video. Journal of Computational and Graphical Statistics 27: 345–359.\nSaleiro, P., Rodrigues, E., Soares, C. and Oliveira, E. (2017) TexRep: a text mining framework for online\nreputation monitoring. New Generation Computation 35(4): 365–389.\nSaltzis, K. (2012) Breaking news online. Journalism Practice 6(5–6): 702–710.\nScheufele, D.A. and Tewksbury, D. (2007) Framing, agenda setting, and priming: the evolution of three media\neffects models. Journal of Communication 57(1): 9–20.\nShapiro, A.H., Su¨dhof, M. and Wilson, D. (2018) Measuring news sentiment. Technical Report 2017-01, Federal\nReserve Bank of San Francisco.\nSilge, J. and Robinson, D. (2016) tidytext: text mining and analysis using tidy data principles in R. Journal of\nOpen Source Software 1(3): 37.\nSoleymani, M., Garcia, D., Jou, B., Schuller, B., Chang, S.-F. and Pantic, M. (2017) A survey of multimodal\nsentiment analysis. Image and Vision Computing 65: 3–14.\nSoo, C.K. (2018) Quantifying sentiment with news media across local housing markets. The Review of Financial\nStudies 31(10): 3689–3719.\nStone, P.J., Dunphy, D.C. and Smith, M.S. (1963) The general inquirer: a computer approach to content\nanalysis. In Proceedings of the American Federation of Information Processing Societies spring joint\ncomputer conference, pp. 241–256.\nStrapparava, C. and Valitutti, A. (2004) WordNet-Affect: An affective extension of WordNet. In Proceedings\nof the 4th International Conference on Language Resources and Evaluation.\nTaboada, M. (2016) Sentiment analysis: An overview from linguistics. Annual Review of Linguistics 2: 325–347.\nTaboada, M., Brooke, J., Tofiloski, M., Voll, K. and Stede, M. (2011) Lexicon-based methods for sentiment\nanalysis. Computational Linguistics 37(2): 267–307.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\nECONOMETRICS MEETS SENTIMENT: AN OVERVIEW OF METHODOLOGY AND APPLICATIONS 35\nTa¨ckstro¨m, O. and McDonald, R. (2011) Discovering fine-grained sentiment with latent variable structured\nprediction models. In P. Clough, C. Foley, C. Gurrin, G.J.F. Jones, W. Kraaij, H. Lee and V. Mudoch (eds),\nProceedings of the Advances in Information Retrieval, pp. 368–374. Springer.\nTaddy, M. (2013a) Measuring political sentiment on Twitter: Factor optimal design for multinomial inverse\nregression. Technometrics 55(4): 415–425.\nTaddy, M. (2013b) Multinomial inverse regression for text analysis. Journal of the American Statistical\nAssociation 108(503): 755–770.\nTaddy, M. (2015a) Distributed multinomial regression. Annals of Applied Statistics 9(3): 1394–1414.\nTaddy, M. (2015b) Document classification by inversion of distributed language representations. In Proceedings\nof the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing, pp. 45–49.\nTaddy, M. (2018) textir: Inverse Regression for Text Analysis, R package.\nTeoh, S.H. (2018) The promise and challenges of new datasets for accounting research. Accounting,\nOrganizations and Society 68–69: 109–117.\nTetlock, P.C. (2007) Giving content to investor sentiment: the role of media in the stock market. Journal of\nFinance 62(3): 1139–1168.\nTetlock, P.C., Saar-Tsechansky, M. and Macskassy, S. (2008) More than words: quantifying language to\nmeasure firms’ fundamentals. Journal of Finance 63(3): 1437–1467.\nThorsrud, L.A. (2018) Words are the new numbers: a newsy coincident index of the business cycle. Journal of\nBusiness & Economic Statistics 38(2): 393–409.\nTibshirani, R.J. (1996) Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical\nSociety: Series B 58(1): 267–288.\nTurney, P. (2002) Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification\nof reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,\nACL ’02, pp. 417–424. ACM.\nTversky, A. and Kahneman, D. (1981) The framing of decisions and the psychology of choice. Science\n211(4481): 453–458.\nVan de Kauter, M., Desmet, B. and Hoste, V. (2015) The good, the bad and the implicit: a comprehensive\napproach to annotating explicit and implicit sentiment. Language Resources & Evaluation 49(3): 685–\n720.\nVarian, H.R. (2014) Big data: new tricks for econometrics. Journal of Economic Perspectives 28(2): 3–28.\nWang, H., Divakaran, A., Vetro, A., Chang, S.-F. and Sun, H. (2003) Survey of compressed-domain features\nused in audio-visual indexing and analysis. Journal of Visual Communication and Image Representation\n14(2): 150–183.\nWang, J., Shen, H.T., Song, J. and Ji, J. (2014) Hashing for similarity search: a survey. Working Paper.\nWischnewsky, A., Jansen, D.-J. and Neuenkirch, M. (2019) Financial stability and the Fed: evidence from\ncongressional hearings. Technical Report 633, De Nederlandsche Bank.\nYoung, L. and Soroka, S. (2012) Affective news: the automated coding of sentiment in political texts. Political\nCommunication 29(2): 205–231.\nZhang, M.-L. and Zhou, Z.-H. (2014) A review on multi-label learning algorithms. IEEE Transactions on\nKnowledge and Data Engineering 26(8): 1819–1837.\nZhang, X., Zhao, J. and LeCun, Y. (2015) Character-level convolutional networks for text classification. In C.\nCortes, N.D. Lawrence, D.D. Lee, M. Sugiyama and R. Garnett (eds ), Advances in Neural Information\nProcessing Systems (pp. 649–657). Red Hook, NY: Curran Associates, Inc.\nZou, H. and Hastie, T. (2005) Regularization and variable selection via the elastic net. Journal of the Royal\nStatistical Society: Series B 67(2): 301–320.\nAppendix: Glossary\nCorpus. A corpus in linguistics jargon designates the collection of textual data units (e.g., documents)\nto be analyzed. It can be generalized to indicate the collection of data units from textual, audio, or visual\ndata.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n36 ALGABA ET AL.\nFeatures. A feature is a broad term to represent any type of metadata attached to the original textual,\naudio, or visual data as stored in a corpus. Examples are source, expresser, entity, location, topic, and\nso on. This definition is slightly different but in line with how features are used in a machine learning\ncontext, where they refer to the set of explanatory variables. In video and audio data, (low-level) features\nare compact, mathematical representations of the physical properties of the data (Wang et al., 2003).\nLexicon. A lexicon is a list of tokens (e.g., words, a sequence of words, a facial expression, or a sound)\nwith, for each token, an associated score that represents its average sentiment. Also interchangeably called\na sentiment lexicon, a sentiment word list, or a sentiment dictionary.\nNatural language processing (NLP). The broad subfield within artificial intelligence occupied with\nthe understanding, interpretation, and manipulation of human language. It draws from computer science,\ncomputational linguistics, and machine learning.\nPolarity. The polarity (or semantic orientation) of an expression (whether it is a text, a sound, or\nsomething else) represents its degree of positivity. Polarity categories go from very positive to very\nnegative, discrete, or continuous.\nSentiment. Sentiment equals the disposition of an entity toward an entity, expressed via a certain\nmedium. This working definition consists of (1) the expression by an entity of its disposition, in the\nform of verbal or non-verbal communication, (2) the expression has a polarity or a semantic orientation\nmeasurable on a discrete or a continuous scale, and (3) the expression is oriented toward (an aspect of)\nan entity.\nSentiment analysis. Sentiment analysis is about the extraction of sentiment from the medium it is\nexpressed through. Multimodal sentiment analysis covers textual, audio, and visual media.\nSentometrics. The term “sentometrics” is a portmanteau of sentiment and econometrics. It deals with\nthe computation of sentiment from any type of qualitative data, the evolution of sentiment, and the\napplication of sentiment in an economic analysis using econometric methods.\nSupervised learning. Supervised learning is a branch of machine learning that requires an annotated\ndata set (i.e., a set of input data with associated output values) to train a model.\nUnsupervised learning. Unsupervised learning is a branch of machine learning where the input data\ndecide the output categories or representation by themselves. Any unsupervised method is typically hybrid\nor semisupervised, as there is often need for certain minimal inputs from the modeler.\nJournal of Economic Surveys (2020) Vol. 00, No. 0, pp. 1–36\nC© 2020 The Authors. Journal of Economic Surveys published by John Wiley & Sons Ltd\n",
      "id": 85952649,
      "identifiers": [
        {
          "identifier": "10.1111/joes.12370",
          "type": "DOI"
        },
        {
          "identifier": "429095338",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:archive.ugent.be:8662858",
          "type": "OAI_ID"
        },
        {
          "identifier": "388086531",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:research.vu.nl:publications/6c3ae9d8-05ed-483e-a2b1-d482d5ff99c5",
          "type": "OAI_ID"
        },
        {
          "identifier": "323232776",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:research.vu.nl:openaire_cris_publications/6c3ae9d8-05ed-483e-a2b1-d482d5ff99c5",
          "type": "OAI_ID"
        },
        {
          "identifier": "387932563",
          "type": "CORE_ID"
        },
        {
          "identifier": "412124068",
          "type": "CORE_ID"
        }
      ],
      "title": "Econometrics meets sentiment : an overview of methodology and applications",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:archive.ugent.be:8662858",
        "oai:research.vu.nl:publications/6c3ae9d8-05ed-483e-a2b1-d482d5ff99c5",
        "oai:research.vu.nl:openaire_cris_publications/6c3ae9d8-05ed-483e-a2b1-d482d5ff99c5"
      ],
      "publishedDate": "2020-01-01T00:00:00",
      "publisher": "'Wiley'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://research.vu.nl/en/publications/6c3ae9d8-05ed-483e-a2b1-d482d5ff99c5",
        "https://biblio.ugent.be/publication/8662858/file/8667072.pdf"
      ],
      "updatedDate": "2022-06-20T19:27:06",
      "yearPublished": 2020,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/323232776.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/323232776"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/323232776/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/323232776/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/85952649"
        }
      ]
    },
    {
      "acceptedDate": "2017-01-31T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Bollen"
        },
        {
          "name": "Feng"
        },
        {
          "name": "Lin"
        },
        {
          "name": "Shaffer"
        },
        {
          "name": "Thelwall"
        },
        {
          "name": "Turney"
        },
        {
          "name": "Turney"
        },
        {
          "name": "Weaver"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Hassan"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/131317080",
        "https://api.core.ac.uk/v3/outputs/208697534"
      ],
      "createdDate": "2017-11-21T08:40:48",
      "dataProviders": [
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 86,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/86",
          "logo": "https://api.core.ac.uk/data-providers/86/logo"
        }
      ],
      "depositedDate": "2017-04-06T00:00:00",
      "abstract": "Sentiment analysis over social streams offers governments and organisations a fast and effective way to monitor the publics' feelings towards policies, brands, business, etc. General purpose sentiment lexicons have been used to compute sentiment from social streams, since they are simple and effective. They calculate the overall sentiment of texts by using a general collection of words, with predetermined sentiment orientation and strength. However, words' sentiment often vary with the contexts in which they appear, and new words might be encountered that are not covered by the lexicon, particularly in social media environments where content emerges and changes rapidly and constantly. In this paper, we propose a lexicon adaptation approach that uses contextual as well as semantic information extracted from DBPedia to update the words' weighted sentiment orientations and to add new words to the lexicon. We evaluate our approach on three different Twitter datasets, and show that enriching the lexicon with contextual and semantic information improves sentiment computation by 3.4% in average accuracy, and by 2.8% in average F1 measure",
      "documentType": "research",
      "doi": "10.3233/sw-170265",
      "downloadUrl": "https://core.ac.uk/download/131317080.pdf",
      "fieldOfStudy": null,
      "fullText": "Open Research OnlineThe Open University’s repository of research publicationsand other research outputsSentiment Lexicon Adaptation with Context andSemantics for the Social WebJournal ItemHow to cite:Saif, Hassan; Ferna´ndez, Miriam; Kastler, Leon and Alani, Harith (2017). Sentiment Lexicon Adaptation withContext and Semantics for the Social Web. Semantic Web Journal, 8(5) pp. 643–665.For guidance on citations see FAQs.c© [not recorded]Version: Accepted ManuscriptLink(s) to article on publisher’s website:http://dx.doi.org/doi:10.3233/SW-170265Copyright and Moral Rights for the articles on this site are retained by the individual authors and/or other copyrightowners. For more information on Open Research Online’s data policy on reuse of materials please consult the policiespage.oro.open.ac.ukSemantic Web 0 (0) 1–22 1IOS PressSentiment Lexicon Adaptation with Contextand Semantics for the Social WebEditor(s): Andreas Hotho, Julius-Maximilians-Universität Würzburg, Germany; Robert Jäschke, University of Sheffield, UK; Kristina Lerman,University of Southern California, USASolicited review(s): Three anonymous reviewersHassan Saif a,∗, Miriam Fernandez a, Leon Kastler b and Harith Alani aa Knowledge Media Institute, Open University, MK76AA, Milton Keynes, UKE-mail: {h.saif, m.fernandez, h.alani}@open.ac.ukb Institute for Web Science and Technology, University of Koblenz-Landau, 56070 Koblenz, GermanyE-mail: lkastler@uni-koblenz.deAbstract. Sentiment analysis over social streams offers governments and organisations a fast and effective way to monitor thepublics’ feelings towards policies, brands, business, etc. General purpose sentiment lexicons have been used to compute sentimentfrom social streams, since they are simple and effective. They calculate the overall sentiment of texts by using a general collectionof words, with predetermined sentiment orientation and strength. However, words’ sentiment often vary with the contexts in whichthey appear, and new words might be encountered that are not covered by the lexicon, particularly in social media environmentswhere content emerges and changes rapidly and constantly. In this paper, we propose a lexicon adaptation approach that usescontextual as well as semantic information extracted from DBPedia to update the words’ weighted sentiment orientations and toadd new words to the lexicon. We evaluate our approach on three different Twitter datasets, and show that enriching the lexiconwith contextual and semantic information improves sentiment computation by 3.4% in average accuracy, and by 2.8% in averageF1 measure.Keywords: Sentiment Lexicon Adaptation, Semantics, Twitter1. IntroductionSentiment analysis on social media, and particularlyon Twitter, has gained much attention in recent years.Twitter offers a platform where users often express theiropinions and attitudes towards a great variety of top-ics, offering governments and organisations a fast andeffective way to monitor the publics’ feelings towardstheir brand, business, policies, etc.However, sentiment analysis over social media dataposes new challenges due to the typical ill-formed syn-tactical and grammatical structures of such content [30].Although different type of approaches have been pro-posed in the last few years to extract sentiment over thistype of data, Lexicon-based approaches have gained*Corresponding author. E-mail: hassan.saif@open.ac.uk.popularity because, as opposed to Machine Learningapproaches, they do not require the use of training data,which is often expensive and/or impractical to obtain.These approaches use general-purpose sentiment lexi-cons (sets of words with associated sentiment scores) tocompute the sentiment of a text regardless of its domainor context [4,21,35,14]. However, a word’s sentimentmay vary according to the context in which the wordis used [36]. For example, the word great conveys dif-ferent sentiment when associated with the word prob-lem than with the word smile. Therefore, the perfor-mance of these lexicons may drop when used to analysesentiment over specific domains or contexts.Some works have attempted to address this problemby generating domain-specific lexicons from scratch[2,9,19,17], which tends to be costly, especially whenapplied to dynamic and generic microblog data (e.g.,1570-0844/0-1900/$27.50 c© 0 – IOS Press and the authors. All rights reserved2 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web[7,10]). Others opted for extending popular lexiconsto fit new domains [8,16,31,15]. Automatic adaptationof existing lexicons not only reduces the burden ofcreating a new lexicon, but also ensures that the words’sentiment and weights, generated and tested duringthe construction of existing lexicons, are taken intoconsideration as basis for adaptation [8,24].In addition, while some approaches have made useof semantic information to generate general purposesentiment lexicons [6], little attention has been givento the use of semantic information as a resource to per-form sentiment lexicon adaptation. Our hypothesis isthat semantics can help to better capture the domainor context for which the lexicon is being adapted, thusaiming to contribute towards a more informed calcula-tion of words’ sentiment weights. For example, the con-text of the word “Ebola” in “Ebola continuesspreading in Africa!” does not indicate aclear sentiment for the word. However, “Ebola”is associated with the semantic type (concept)“Virus/Disease”, which suggests that the senti-ment of “Ebola” is likely to be negative.In this paper, we propose a general method to adaptsentiment lexicons to any given domain or context,where context is defined by a collection of microblogposts (Tweets). A key novelty of our method is that itdoes not only captures the domain context (contextualor distributional semantics), it also introduces the useof conceptual semantics, i.e., semantics extracted frombackground ontologies such as DBpedia. In performingour study we make the following contributions:1. We introduce a generic, unsupervised, methodfor adapting existing sentiment lexicons to givendomains and contexts, defined by a collection ofmicroblog posts (Tweets)2. We propose two methods for semantically-enriching the lexicon adaptation method: (i) en-richment with the semantic concepts of words,and (ii) enrichment based on the semantic rela-tions between words in tweets3. We study three lexicon adaptation techniques: up-dating the words’ sentiment weights, expandingthe lexicon with new words, and the combinationof both4. We evaluate our context-based lexicon adaptationmethod over three Twitter datasets, and show anaverage, statistically significant, improvement of3.4% in accuracy, and 2.8% in F1, against thebaseline methods5. We investigate the impact of the proposedsemantic-enrichment approaches on the lexiconadaptation performance. We show that enrichmentwith semantic concepts, when used for updat-ing the words’ sentiment weights in the lexicon,increases performance slightly over the context-based method by 0.27% and 0.25%, in accuracyand F1 respectively. Enrichment based on the se-mantic relations between entities in tweets brings4.12% and 3.12% gain in accuracy and F1 whenused for expanding the lexicon with new opinion-ated words, in comparison with lexicon expandingwithout semantic enrichment6. We investigate the impact of dataset imbalancewhen using lexicons for calculating tweet-levelsentiment and show that our adapted lexiconshave higher tolerance to imbalanced datasetsThe remainder of this paper is structured as follows.Related work is discussed in Section 2. Our method forsentiment lexicon adaptation and its semantic enrich-ment is presented in Sections 3 and 4 respectively. Ex-perimental setup and results are presented in Sections5 and 6 respectively. Section 7 covers discussion andfuture work. Conclusions are reported in Section 8.2. Related WorkGeneral purpose sentiment lexicons (MPQA[21],SentiWordNet[1], Thelwall-lexicon[34], Nielsen-Lexicon[21]) have been traditionally used in theliterature to determine the overall sentiment of texts.These lexicons capture a selection of popular wordsand their associated weighted sentiment orientations,without considering the domain, topic, or contextwhere the lexicons are being used. However, a word’sassociated sentiment may vary according to the contextin which the word is used [36]. To address this problemmultiple works have emerged in recent years to: (i)create domain-specific lexicons or, (ii) adapt existinglexicons to specific domains.Most of existing works belong to the first cate-gory, where approaches have been proposed to de-velop sentiment lexicons tailored for specific domains[2,9,16,19,31,17]. Works like [2,17,31] propose theuse of bootstrapping methods for building sentimentlexicons. These methods use seed sets of subjectivewords [2,31], dictionaries [2,19], domain-specific cor-pora [2,16,9,19], training data from related domains[17], ratings [19] and graph-based formalisms [31] toidentify words and to induce sentiment weights.Recently, several works were focused on the develop-ment of domain-specific sentiment lexicons for socialH. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 3Tweets General-Purpose Sentiment Lexicon Extracting Word’s Contextual Sentiment Adapted Lexicon (Context) Rule-based Lexicon Adaptation (a) Context-based AdaptationConceptual Semantics Enrichment Tweets General-Purpose Sentiment Lexicon Extracting Word’s Contextual Sentiment Adapted Lexicon (Context) Rule-based Lexicon Adaptation Conceptual Semantics Extraction Enriched Tweets (b) Conceptually-enriched AdaptationSemantic Relations Enrichment Tweets General-Purpose Sentiment Lexicon Extracting Word’s Contextual Sentiment Adapted Lexicon (Context) Rule-based Lexicon Adaptation Semantic Relations Extraction (c) Adaptation with Semantic Relation AdjustmentFig. 1. Pipelines for (a) Context-based Adaptation Model, (b) Context-based Adaptation Model with Semantic Concepts Enrichment, and (c)Context-based Adaptation Model with Semantic Relations Adjustmentmedia [7,10]. To infer words and sentiment weightsthese approaches make use of linguistic and statisticalfeatures from tweets [10], including emoticons [15]. Itis important to highlight that the informality of the lan-guage used in this type of data makes building domain-specific sentiment lexicons a more difficult task.Rather than creating domain-specific sentiment lex-icons, several approaches have proposed methods foradapting existing, well-known lexicons, to specificdomains [8,24,28]. As previously mentioned, lexiconadaptation not only reduces the burden of creating lex-icons from scratch, but also supplements the processwith a collection of pre-existing words and their sen-timent orientations and weights. Note that the lexiconadaptation problem is different in nature to the problemof domain adaptation for sentiment classifiers [11,22].Creating sentiment classifiers requires the use of train-ing data (labelled, in the case of supervised classifica-tion [22], or unlabelled, in the case of unsupervisedlearning [11]). Classifiers generated with training datafrom one domain tend to lower their performance whentested on data from a different domain. In this scenario,domain adaptation is usually needed and classifiers areadapted by adding, discarding or modifying featuresbased on new training data. As opposed to these works,we focus on the problem of sentiment lexicon adapta-tion.While the majority of work on lexicon adaptationfocuses on conventional text, lexicon adaptation for so-cial media data is still in its infancy. One very recentwork in this line [15] has focused on updating the sen-timent of neutral words in SentiWordNet. In additionto this work, we not only adapt sentiment weights, butalso study the extraction and addition of new terms notprovided in the original lexicon [28]. This is potentiallyuseful in the case of social media data, where new termsand abbreviations constantly emerge. Note that, in-linewith the work of Lu and colleagues [19], our proposedlexicon adaptation method is not restricted to domain-adaptation, but rather considers a more fine-grainedcontext adaptation, where the context is defined by acollection of posts. Moreover, our approach does notmake use of training data to adapt the lexicon.Another novelty of our approach with respect to pre-vious works, is the use of conceptual semantics, i.e.semantics extracted from ontologies such as DBPedia,to adapt sentiment lexicons. Our hypothesis is that con-ceptual semantics can help to better capture the domainfor which the lexicon is being adapted, by enabling thediscovery of relevant concepts and semantic relationsbetween terms. The extraction of these concepts and re-lations (e.g., knowing that “Ebola” is an entity appear-ing within the tweet collection, and that it is associatedwith the semantic type (concept) “Virus/Disease”)facilitates a better enrichment of the context and pro-vides a higher term relationship coverage for the calcu-lation of sentiment weights. Capturing the relationshipsamong terms helps inferring the sentiment influencethat terms have on one another within the context.4 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web3. Context-based Lexicon AdaptationThe main principle behind lexicon adaptation is thatthe sentiment of a term is not as static as given ingeneral-purpose sentiment lexicons, but it rather de-pends on the context in which the term is used [29]. Inthis section we present our method for adapting senti-ment lexicons based on words’ context in tweets.The pipeline of our proposed context-based lexiconadaptation method consists of two main steps, as de-picted in Figure 1(a). First, given a tweet collectionand a general-purpose sentiment lexicon, our approachdetects the context of each word in the tweet collectionand uses it to extract the word’s contextual sentiment.Secondly, a set of rules are applied to amend the priorsentiment of terms in the lexicon based on their corre-sponding contextual sentiment. Both steps are furtherdetailed in Sections 3.1 and 3.2. The semantic enrich-ment of this pipeline is described in Section 4. Concep-tual semantics are used to enrich the context or domainin which the words are used with the aim of enabling abetter interpretation of this context.3.1. Word’s Contextual SentimentThe first step in our pipeline is to extract the contex-tual sentiment of terms (i.e., sentiment extracted basedon a word’s context) in a given tweet collection. Thisstep consists of: (i) capturing the context in which theword occurs, and (ii) computing the word’s contextualsentiment. A common method for capturing the word’scontext is by looking at its co-occurrence patterns withother terms in the text. The underlying principle behindthis method comes from the distributional semantic hy-pothesis:1 words that are used and occur in the samecontexts tend to purport similar meanings [13,38]. Forexample, the word “great”, when occurs in the con-text “smile”, denotes a different meaning than whenit occurs within the context “pain” and “loss”. Suchcontext variations of the word often affect its sentiment:“great” with “smile” indicates a positive sentiment,while “great” with “pain” indicates a negative one.Several approaches have been built and used forextracting the words’ contextual sentiment followingthe above principle [37,18]. In this paper, we use theSentiCircle approach [27], which similarly to otherfrequency-based approaches, it detects the context of aterm from its co-occurrence patterns with other termsin tweets. In particular, the context for each term tin a tweet collection T is represented as a vector1Also known as Statistical Semantics [39]~c = (c1, c2, ..., cn) of terms that occur with t in anytweet in T . The contextual sentiment of t is then ex-tracted by first transforming the term vector c into a 2dcircle representation, and then extracting the geomet-ric median of the points (context terms) within the cir-cle. The position of the median within the circle repre-sents the overall contextual sentiment of t. This simpletechnique has proven effective in calculating contextualsentiment [27].Figure 2 depicts the representation and extractionof the contextual sentiment of the term “great” bythe SentiCircle approach. First, given a tweet collec-tion T , the target term mgreat is represented as a vec-tor ~cgreat = (c1, c2, ..., cn) of terms co-occurring withterm m in any tweet in T (e.g., “pain”, “loss”, ...,“death”). Secondly, the context vector ~cgreat is trans-formed into a 2d circle representation. The center ofthe circle represents the target term mgreat and pointswithin the circle denote the context terms of mgreat.The position (xci , yci) of each context term ci ∈ ~cgreatis defined as:xci = ri cos θi yci = ri sin θi (1)Where the angle θi represents the prior sentiment ofthe context term ci multiplied by pi, and it is obtainedfrom the lexicon to be adapted. The radius ri repre-sents co-occurrence frequency between ci and the targetterm mgreat and it is computed based on the TF-IDFweighting scheme as follows:ri = corr(mgreat, ci) = f(ci,mgreat)×log NNci(2)where f(ci,mgreat) is the number of times ci occurswith mgreat in tweets, N is the total number of terms,and Nci is the total number of terms that occur with ci.Based on the SentiCircle representation, terms withpositive prior sentiment are positioned on the upperhalf of the circle (e.g., please) and terms with neg-ative prior sentiment are positioned in the lower half(e.g., pain, loss). Term co-concurrence determinesthe distance (i.e., radius) of these terms with respect tothe origin. Thirdly, the geometric median G of the Sen-tiCircle of “Great” is computed (-4 in our example),which constitutes the contextual sentiment of the term.22We refer the reader to the body of [27] for more details about theSentiCircle approach.H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 5Feeling a great pain hearing that 4 doctors died from Ebola #please #help #Africa A great tragedy begets another: Ebola outbreak impacted Malaria services #sad Ebola continues spreading in Africa!  Thelwall-Lexicon (i) Context Vector (Great) pain\t   …. died\t   loss\t  C1 C2 Cn (ii) SentiCircle(Great) died sad pain help  loss (iii)\t  Calculate\t  Contextual\t  Sen3ment\t  (Great)\t  G Great (-4)  (iv)\t  Adapt\t  Lexicon\t  Great(+3) Loss(-3) death(-3) pain(-4) Please(+2) please Ci ri θi xi yi Fig. 2. Illustrative example of extracting the contextual sentiment of the word Great from a tweet collection and adapting Thelwall-Lexicon withthe new extracted sentiment respectively. Red dot in the SentiCircle represents the geometric median G of the context points in the circle.In the following subsection we describe how to adaptthe sentiment lexicon using the contextual sentimentextracted in this step.The reasons for using SentiCircle for extractingterms’ contextual sentiment are threefold. First, unlikeother approaches, SentiCircle is built for social mediadata, and specifically for Twitter data [27]. Secondly,it enables detecting not only the contextual sentimentorientation of words (i.e., positive, negative, neutral),but also the words’ contextual sentiment strength (e.g.,negative(-3), positive(+4)). This in turn allows for bet-ter fine-tuning and adaptation of the sentiment of wordsin the lexicon. Thirdly, SentiCircle, as explained above,relies on simple, yet effective frequency-based repre-sentation of words’ context. This representation is easyto extend and enrich with the conceptual semantics ofwords, as will be explained in Section 4.3.2. Rules for Lexicon AdaptationThe second step in our pipeline is to adapt the priorsentiment of terms in a given sentiment lexicon basedon the terms’ contextual sentiment information ex-tracted in the previous step. To this end, we propose ageneral rule-based method to decide on the new senti-ment of terms in the lexicon. In the following we give aformal definition of the general purpose sentiment lexi-con and its properties, and explain how our proposedmethod functions on it accordingly.General-purpose sentiment lexicon: is a set ofterms L = {t1, t2, ..., tn} of fixed size n. Each termt ∈ L is coupled with a prior sentiment score that isoften a numerical value priorm ∈ [−λ,−δ, δ, λ], de-noting the sentiment orientation and strength of t. Inparticular, t is positive if priort ∈ (δ, λ], negative ifpriort ∈ [−λ,−δ), and neutral if priort ∈ [−δ, δ]. |λ|is the maximum sentiment strength that a term can have.The closer the priort is to λ the higher the sentimentstrength is. |δ| defines the boundaries of the neutralsentiment range. The values of both, λ and δ dependon the specifications of the studied sentiment lexiconand are defined at the design/construction phase of thelexicon (see section 5.1).Lexicon adaptation rules: our proposed methoduses a set of 4 antecedent-consequent rules (Table 1)to decide how to update the prior sentiment of a term(priort) in a given lexicon with respect to its contextualsentiment (contextualt). As noted in Table 1, theserules are divided into:– Updating rules: for updating only the existingterms in the lexicon. These rules are further di-vided into rules that deal with terms with the sameprior and contextual sentiment orientations (e.g.,both, priort and contextualt are positive or neg-ative), and rules that deal with terms that havedifferent prior and contextual sentiment orienta-tions (i.e., priort is negative and contextualt ispositive or vice-versa).– Expanding rules: rules for expanding the lexiconwith new opinionated terms.The notion behind the proposed rules is rather sim-ple: For a given term t ∈ L, check how strong/weakthe contextual sentiment (contextualt) is and howstrong/weak the prior sentiment (priort) is→ update6 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social WebUpdating Rules (Same Sentiment Orientations)Id Antecedents Consequent1 (|contextualt| > |priort|) ∧ (|contextualt| > θ) priort =[priort + α; priort > 0priort − α; priort < 0]Updating Rules (Different Sentiment Orientations)2 (|contextualt| > θ) ∧ (|priort| 6 θ) priort =[α; priort < 0−α; priort > 0]3 (|contextualt| > θ) ∧ (|priort| > θ) priort =[priort − α; priort > 0priort + α; priort < 0]Expanding Rule4 term(t) /∈ lexicon(L) (priort = contextualt) ∧AddTerm(t,L)Table 1Adaptation rules for sentiment lexicons, whereAddTerm(t,mathcalL): add term t to lexicon L.priort in the lexicon accordingly. As mentioned ear-lier, contextualt is obtained as described in Section3.1 and its value range [−λ, λ]. The threshold θ is com-puted as θ = |λ|/2 and it is used to determine howstrong/weak the sentiment of the term is. If the termdoes not exist in the lexicon, we add it to the lexiconwith its corresponding contextual sentiment.In Thelwall-Lexicon [35], as will be explained inSection 5.1, |λ| = 5 and |δ| = 1, i.e., the prior senti-ment for the terms in this lexicon is between [−5,+5],and the neutral sentiment range is between [−1, 1]. Thevalue of θ is set up to 3.3We use the same example depicted in Figure 2 toshow how these rules are applied for lexicon adaptation.The word “Great” in Thelwall-Lexicon has a weakpositive sentiment (priorgreat = +3; |priorgreat| =3 6 θ), while its contextual sentiment, as previouslyexplained, is strongly negative (contextualgreat =−4; |contextualgreat| = 4 > θ). Therefore, rule num-ber 2 is applied, since the prior and contextual sen-timent have different sentiment orientation (i.e., theprior is positive and the contextual is negative). Thenew prior for the word “Great” will therefore be setup to -α. In the case of Thelwall-lexicon α is 1 (i.e.,adaptedpriorgreat = −1). In the same example inFigure 2, the word “tragedy” is not covered by theThelwall-Lexicon, and therefore, it has no prior sen-timent. However, its contextual sentiment, extractedusing the process described in the previous section, isnegative (i.e., contextualtragedy = −3). In this caserule number 4 is applied and the term is added to thelexicon with a negative sentiment strength of -3.3Since Thelwall-Lexicon uses discrete and not continuous valuesfor priors, θ is rounded up to the nearest integer value to match theannotation format of Thelwall-Lexicon4. Semantic Enrichment for Context-basedLexicon AdaptationIn this section we propose enriching our originalcontext-based adaptation model, described in the pre-vious section, with the conceptual semantics of wordsin tweets. To this end, we follow two different method-ologies: (1) Enriching the adaptation model with thesemantic concepts of named-entities extracted from agiven tweet collection, Conceptually-enriched Model.(2) Adjusting the contextual correlation between twoco-occurring named-entities in tweets based on the se-mantic relations between them, Semantically-adjustedRelations Model. In the following subsections we de-scribe both enrichment models and the motivation be-hind them.4.1. Conceptually-enriched Adaptation ModelIn Section 3 we showed our proposed methodto adapt sentiment lexicons based on the contex-tual sentiment of terms in a given collection oftweets. However, relying on the context only for de-tecting terms’ sentiment might be insufficient. Thisis because the sentiment of a term may be con-veyed via its conceptual semantics rather than byits context [5]. In the example in Figure 2, the con-text of the word “Ebola” in “Ebola continuesspreading in Africa!” does not indicate aclear sentiment for the word. However, “Ebola”is associated with the semantic type (concept)“Virus/Disease”, which suggests that the senti-ment of “Ebola” is likely to be negative.In order to address to above issue, we propose enrich-ing the context-based lexicon adaptation model with theconceptual semantics of words in tweets. To this end,we add two additional steps to our original pipeline (Fig-H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 7ure 1:b): conceptual semantic extraction, and concep-tual semantic enrichment. These two steps are executedprior to the extraction of words’ contextual sentiment,as follows:1. Conceptual semantic extraction: This stepextracts the named entities that appear in atweet collection (e.g., “Obama”, “Illinois”,“NBC”) along with their associated semantic types(“Person”, “City”, “Company”) and theirsemantic subtypes (e.g., “Politician”, “USCounty”, “TV Network”). To this end, we usethe semantic extraction tool AlchemyAPI4 due toits accuracy and high coverage of semantic typesand subtypes [25].2. Conceptual semantic enrichment: This step in-corporates the conceptual semantics extractedfrom the previous step into the extraction processof the terms’ contextual sentiment. To this end, theentities’ semantic subtypes are first added as addi-tional unigrams to the tweets in which the entitiesoccur. After that, the enriched Twitter dataset ispassed to the contextual sentiment extraction step,as depicted in Figure 1:b. As mentioned in Sec-tion 3.1, the context of a term t in the latter step,is represented as a vector ~c = (c1, c2, ..., cn) ofterms that occur with t in a given tweet collection.Using the semantically enriched Twitter dataset toconstruct the context vector ~c results in extending~c with the semantic subtypes ~s = (s1, s2, ..., sm)of named entities ~e = (e1, e2, ..., em) that occurwith t in the tweet collection as:~cs = ~c+~s = (c1, c2, ..., cn, s1, s2, ..., sm) (3)where ~cs is the new semantically-enriched con-textual vector of t, which will be subsequentlyused instead of ~c to extract the overall contextualsentiment of t.Note that we currently rely only on the entities’ se-mantic subtypes for the semantic enrichment phase, ex-cluding the semantic types. Unlike semantic types, se-mantic subtypes capture more fine-grained knowledgeabout the entity (e.g., “Obama” > “Politician”).4.2. Semantically-adjusted Relations ModelUsing the distributional semantic hypothesis, ourcontext-based approach assigns a stronger relation towords that tend to co-occur more frequently in same4www.alchemyapi.comcontext. However the document collection may rep-resent only a partial view of the contexts in whichtwo words my co-occur together. For example, in theGASP Twitter dataset around the dialogue for earth gasprices[33], the entities Barack Obama and Texas tend toappear together and therefore have a strong contextualrelation. However, these two entities are related withina high number of different contexts. Figure 3 showsa small sample of the different semantic contexts thatlink the two previous entities. These contexts includeBarack Obama’s birth place, his candidatures and hisduties as president.To capture the variety of contexts in which two termscan potentially appear together we compute the num-ber of relations between these two terms in DBPediaby using the approach proposed by Pirro [23]. Our as-sumption is that the strength of the contextual relationbetween two terms, captured by their co-occurrencewithin the document collection, should be modified ac-cording to the number of contexts in which these termscan potentially appear together. The smaller the numberof contexts, the stronger the contextual relation shouldbe.Based on the above assumption we propose adjustingthe strength of the contextual relations between terms,captured by the context-based model, by using the se-mantic relations between them. To this end, we addtwo additional steps to the original pipeline (see Figure1:c): semantic relation extraction and semantic rela-tion adjustment. These two steps are further describedbelow.1. Semantic relation extraction: This step extractsthe sets of semantic relations for every pair ofnamed entities co-occurring together in the tweets.For the purpose of our study we extract semanticrelations using the approach proposed by Pirro[23] over DBPedia, since DBPedia is a largegeneric knowledge graph which captures a highvariety of relations between terms. To extract theset of relations between two name entities this ap-proach takes as input the identifiers (i.e., URIs) ofthe source entity es, the target entity et and an in-teger value K that determines the maximum pathlength of the relations between the two namedentities. The output is a set of SPARQL queriesthat enable the retrieval of paths of length at mostK connecting es and et. Note that in order to ex-tract all the paths, all the combinations of ingo-ing/outgoing edges must be considered. Follow-ing our previous example, if we were interested8 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social WebBarack_Obama Texas United_States leader country birthPlace Mitt_Romney Category:United_States_presidential_candidates,_2012 candidate subject subject Gulf_of_Mexico lowestpoint countries Yvonne_Gonzalez_Rogers placeOfBirth appointer Fig. 3. Example for sentiment relations between the entities Barack Obama and Texas with a path length of ≤ 3in finding paths of length K <= 2 connectinges = Obama and et = Texas our approach willconsider the following set of SPARQL queries:SELECT * WHERE {:Obama ?p1 :Texas}SELECT * WHERE {:Texas ?p1 :Obama}SELECT * WHERE {:Obama ?p1 ?n1. ?n1 ?p2 :Texas}SELECT * WHERE {:Obama ?p1 ?n1. :Texas ?p2 ?n1}SELECT * WHERE {?n1 ?p1 :Obama. :Texas ?p2 ?n1}SELECT * WHERE {?n1 ?p1 :Obama. ?n1 ?p2 :Texas}As it can be observed, the first two queries con-sider paths of length one. Since a path may existin two directions, two queries are required. Theretrieval of paths of length 2 requires 4 queries.In general, given a value K, to retrieve paths oflength K, 2k queries are required.2. Semantic relation adjustment: Now we havefor every pair of named entities (es, et) co-occurring together in the tweets, a setR(es,et) ={p1, p2, ..., pN} of paths of size N , representingthe semantic relations between es and et.As mentioned earlier, our goal behind enrichingthe context-based model with semantic relationsis to adjust the strength of the contextual relationbetween es and et based the number of semanticrelations (paths) between them. To this end, weconstruct the SentiCircle Ses of the source entityes, as depicted in Figure 4. Since both entitiesco-occur together in tweets, the target entity etis positioned in the SentiCircle Ses with a radiusrt representing the strength of the contextual re-lation between es and et, as described in Section3.1. Therefore, the task of adjusting the contex-tual relations between es and et breaks down intoaltering the value of rt as follows:r′t = rt +[NM(1− rt)](4)Where N is the number of the semantic pathsbetween es and et extracted in the previous step,M is the maximum number of paths extracted fora pair of entities in the Twitter dataset, and r′t isthe new radius of entity et after adjustment.et rt θt es Fig. 4. SentiCircle of entity es showing the strength of the contextualrelation between es and et, represented by the radius rtAs can be noted, the above equation modifies thevalue of rt based on the number of paths betweenes and et. The smaller the number of paths is, thestronger the contextual relation should be, andthereby the higher the value of r′t is.Note that the enrichment by semantic relations inthis model is done in two iterations of the adap-tation process. Specifically, in the first iterationthe sentiment lexicon is adapted using the origi-nal context-based model (Figure 1:a). In the sec-ond iteration the semantically-adjusted relationmodel is applied on the adapted lexicon, where thesemantic-relation adjustment takes place. Adapta-H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 9tion in the first iteration allows us to capture thecontextual relations of entities within tweets andassign them a sentiment value. Note that senti-ment lexicons are generic and most of the tweetentities (e.g., Obama, Texas) will not appearin these lexicons. By relying on one iteration ofadaptation only, an entity will have little impacton the contextual sentiment of other entities sinceentities don’t generally have any initial sentimentscore within the lexicon to be adapted. Hence, asecond iteration of adaptation is required in orderto detect the sentiment of entities that do not oc-cur in the lexicon, and maximise the impact of thesemantic relation adjustment in our models.5. Experimental SetupIn this section we present the experimental set upused to assess our proposed lexicon adaptation mod-els, the context-based adaptation model (Section 3) andthe semantic adaptation models (Section 4). This setuprequires the selection of: (i) the sentiment lexicon tobe adapted, (ii) the context (Twitter datasets) for whichthe lexicon will be adapted, (iii) the baseline modelsfor cross-comparison, (iv) the different configurationsfor adapting the lexicon and, (iv) the semantic infor-mation used for the semantic adaptation models. Allthese elements will be explained in the following sub-sections. We evaluate the effectiveness of our methodby using the adapted lexicons to perform tweet-levelsentiment detection, i.e., detect the overall sentimentpolarity (positive, negative) of tweets messages.5.1. Sentiment LexiconFor the evaluation we choose to adapt the state-of-the-art sentiment lexicon for social media; Thelwall-Lexicon [34,35]. Thelwall-Lexicon is a general pur-pose sentiment lexicon specifically designed to func-tion on social media data. It consists of 2546 termscoupled with values between -5 (very negative) and +5(very positive), defining their sentiment orientation andstrength. Terms in the lexicon are grouped into threesubsets of 1919 negative terms (priort ∈[-2,-5]), 398positive terms (priort ∈[2,5]) and 229 neutral terms(priort ∈{-1,1}). Based on the aforementioned spec-ifications, the parameters in our proposed adaptationmethod (Section 3.2) are set as: |λ| = 5, |δ| = 1,|θ| = 3 and |α| = 1. Thelwall-lexicon was selected forthis evaluation because, to the best of our knowledge,and according to existing literature [21][12][34][35],it is currently one of the best performing lexicons forcomputing sentiment in social media data.5.2. Evaluation DatasetsTo assess the performance of our lexicon adapta-tion method we require the use of datasets annotatedwith sentiment labels. for this work we selected threeevaluation datasets often used in the literature of senti-ment analysis (SemEval, WAB and GASP) [26]. Thesedatasets differ in their sizes and topical focus. Numbersof positive and negative tweets within these datasets aresummarised in Table 2.5.3. Evaluation BaselineAs discussed in Section 2, several methods have beenproposed for context-based sentiment lexicon bootstrap-ping and/or adaptation. In this paper we compare ouradaptation models against the semantic orientation byassociation approach (SO) [37], due to its effectivenessand simple implementation. To generate a sentimentlexicon, this approach starts with a balanced set of 14positive and negative paradigm words (e.g., good, nice,nasty, poor). After that, it bootstraps this set by addingwords in a given corpus that are statistically correlatedwith any of the seed words. The new words added to thelexicon have positive orientation if they have a strongerdegree of association to positive words in the initial setthan to negative ones, and vice-versa. Statistical corre-lation between words is measured using the pointwisemutual information (PMI). From now on we refer tothis approach shortly as the SO-PMI method.We apply the SO-PMI method to adapt Thelwall-Lexicon to each of the three datasets in our study in thesame manner as described above. Specifically, given atwitter dataset we compute the pointwise mutual infor-mation between each opinionated term t in Thelwall-Lexicon and each word w that co-occur with t in thetwitter dataset as follows:PMI(t, w) = log(p(t, w)p(t)p(w))(5)After that, we assign w the sentiment orientation(SO) of the term in Thelwall-Lexicon (L) that have thehighest PMI with w as:SO(w) = SO(argmaxt∈LPMI(t, w))(6)Note that in addition to the lexicons adapted by theSO-PMI method, we also compare our proposed ap-proaches against the original Thelwall-Lexicon withoutany adaptation.10 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social WebDataset Tweets #Negative #Positive #UnigramsSemeval Dataset (SemEval) [20] 7520 2178 5342 22340The Dialogue Earth Weather Dataset (WAB) [33] 5482 2577 2905 10917The Dialogue Earth Gas Prices Dataset (GASP) [33] 6032 4999 1033 12868Table 2Twitter datasets used for evaluation. Details on how these datasets were constructed and annotated are provided in [26].5.4. Configurations of the Lexicon AdaptationModelsWe test our context-based adaptation model and thetwo semantic adaptations model under three differentconfigurations. We use terms from the running examplein Figure 2 to illustrate the impact of each adaptationmodel:1. Lexicon Update (LU): The lexicon is adaptedonly by updating the prior sentiment of existingterms. In our running example, the prior sentimentof the pre-existing word “Great” in Thelwall-Lexicon (i.e., priorgreat=+3) will be updatedbased on the words’ contextual sentiment (i.e.,contextualgreat = −4) to -1.2. Lexicon Expand (LE): The lexicon is adaptedonly by adding new opinionated terms. Here newwords, such as “Tragedy” and “Ebola”, alongwith their contextual sentiment, will be added tothe lexicon.3. Lexicon Update and Expand (LUE): The lexi-con is adapted by adding new opinionated terms(“Tragedy” and “Ebola”) and by updating theprior sentiment of existing terms (“Great”).5.5. Extracted SemanticsWe use AlchemyAPI to extract the conceptual se-mantics of named entities from the three evaluationdatasets (Section 4.1). Table 3 lists the total numberof entities extracted and the number of semantic typesand subtypes mapped against them for each dataset.Table 4 shows the top 10 frequent semantic subtypesunder each dataset. As mentioned in Section 4.1, weonly use the entities’ semantic subtypes for our seman-tic enrichment, mainly due to their stronger representa-tion and distinguishing power than general higher leveltypes (e.g., “Person”). Table 5 shows the number of se-mantic relations extracted between the entities of eachdataset. This table also includes the minimum, maxi-mum and average path length among all the extractedrelations. A maximum path length of 3 was considerfor our experiments.SemEval WAB GASPNo. of Entities 2824 685 750No. of Semantic Types (Concepts) 31 25 23No. of Semantic Subtypes 230 93 109Table 3Unique Entity/Types/Subtypes for SemEval, WAB, and GASP datasets6. Evaluation ResultsIn this section, we report the results obtained fromusing the different adaptations of Thelwall-Lexiconto compute tweet-level sentiment detection. To com-pute sentiment, we use the approach proposed by Thel-wall [35], where a tweet is considered positive if itsaggregated positive sentiment strength (i.e., the senti-ment strength obtained by considering the sentimentweights of all words in the tweet) is 1.5 times higherthan the aggregated negative one, and vice versa. Ourbaselines for comparison are the original version ofThelwall-Lexicon and the version adapted by the SO-PMI method.Results in all experiments are computed using 10-fold cross validation over 30 runs of different randomsplits of the data to test their significance. The null hy-pothesis to be tested is that for a given dataset, the base-line lexicons and the lexicons adapted by our modelswill have the same performance. We test this hypothesisusing the Paired T-Test since it determines the meanof the changes in performance, and reports whetherthis mean of the differences is statistically significant.Specifically, we perform a pair-wise comparison of thedistributions of Precision, Recall, and F1-Measure re-sulted from each baseline lexicon against the distribu-tions obtained from the lexicons adapted by each ofour three adaptation models. Additionally, since we usethe three proposed adaptation models under three adap-tation settings (Update, Expand, Update & Expand),we also test the aforementioned hypothesis for everymodel-setting pair. P values are corrected for multi-ple hypothesis testing by using the Bonferroni correc-tion [32]. This correction sets the significance cut-offat α/n, where n is the number of tests and α is set to0.05. Considering that we have three models and threeH. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 11SemEval WAB GASPSubtype Frequency Subtype Frequency Subtype FrequencyTVActor 505 AdministrativeDivision 93 AwardWinner 350AwardWinner 351 GovernmentalJurisdiction 91 Politician 328MusicalArtist 344 Location 66 Celebrity 321Filmactor 324 Placewithneighborhoods 49 Location 104Athlete 316 PoliticalDistrict 45 AdministrativeDivision 103Location 263 Sportsteam 12 GovernmentalJurisdiction 102GovernmentalJurisdiction 263 FieldofStudy 11 PlaceWithNeighborhoods 15Footballplayer 238 Invention 10 Musicalartist 14Celebrity 230 MusicalArtist 10 AutomobileCompany 13AwardNominee 225 VentureFundedCompany 9 BroadcastArtist 11Table 4Top 10 frequent semantic subtypes of entities extracted from the threedatasetsDataset numRelations minPath maxPath AveragePathSemEval 1,011,422 1 3 2.82GASP 811,741 1 3 2.95WAB 796,021 1 3 2.92Table 5Amount of relations and path lengths extracted for each datasetadaptation settings we have a total of 9 tests. Thereforethe corrected α = 0.0055.The evaluation presented in the subsequent sectionsconsists of 5 main phases:1. Measure the performance of our context-basedadaptation model using the three evaluationdatasets and the three adaptation settings (Section6.1).2. Evaluate the performance of the conceptually-enriched adaptation model and report the evalu-ation results averaged across the three datasets(Section 6.2).3. Test the performance of the semantically-adjustedrelations model on the three evaluation datasets(Section 6.3).4. Conduct a statistical analysis on the impact of ouradaptation models on Thelwall-Lexicon (Section6.5).5. Study the effect of the sentiment class distribu-tion on the performance of our adaptation models(Section 6.6).6.1. Results of Context-based Lexicon AdaptationThe first task in our evaluation is to assess the ef-fectiveness of our context-based adaptation model. Ta-ble 6 shows the results of binary sentiment detectionof tweets performed on the three evaluation datasetsusing (i) the original Thelwall-Lexicon (Original), (ii)Thelwall-Lexicon adapted by the PMI method (SO-PMI), (iii) Thelwall-Lexicon adapted under the updatesetting (LU), (iv) Thelwall-Lexicon adapted under theexpand setting (LE), and (v) Thelwall-Lexicon adaptedunder the update and expand setting (LUE). The tablereports accuracy and three sets of precision (P), recall(R), and F1-measure (F1), one for positive sentimentidentification, one for negative sentiment identification,and the third showing the average of the two.Statistical significance over the results reported inTable 6 is computed by comparing the P, R, and F1distributions obtained by: (i) the Original Lexicon and,(ii) the SO-PMI lexicon against the distributions thatresulted from using the LU, LE, and LUE adapted lex-icons. Statistical significance is reported as well perdataset. As shown in Tables 7 and 8. It can be notedthat all results are statistically significant (ρ < 0.0055).From Table 6 we notice that in the case of the Se-mEval and GASP datasets the LU and LUE lexiconsoutperform the original lexicon in all the average mea-sures by up to 6.5%. For example, LU and LUE on Se-mEval improve the performance upon the original lexi-con by at least 6.3% in accuracy and 5.1% in averageF1 (ρ < 0.0055). Similarly, the improvement of LUand LUE on GASP reaches 5% and 4.6% in accuracyand F1 comparing to the original lexicon. Adaptinglexicons by expanding terms only (LE) does not havemuch impact on the sentiment detection performance.Compared to the state-of-the-art SO-PMI method,we notice a similar performance trend for all the lex-icons. In particular, the SO-PMI lexicon outperformsthe original lexicon on both, the SemEval and GASPdatasets by up to 2.8% in accuracy and 2.5% in averageF1. However, both the LU and LUE lexicons outrunthe SO-PMI lexicon by 3.9% in accuracy and 3.3% inaverage F1. The LE lexicon, on the other hand, gives onaverage 1.7% and 1.5% lower performance in accuracyand F1 than the SO-PMI, respectively.12 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social WebDataset Lexicon Accuracy Negative Sentiment Positive Sentiment AverageP R F1 P R F1 P R F1SemEvalOriginal 71.85 50.84 85.17 63.67 91.66 66.42 77.02 71.25 75.79 70.35SO-PMI 73.9 53.19 82.37 64.64 90.74 70.44 79.31 71.96 76.41 71.97LU 76.9 57.35 79.02 66.46 89.89 76.04 82.39 73.62 77.53 74.42LE 72.14 51.16 83.79 63.53 91.07 67.39 77.46 71.12 75.59 70.5LUE 76.66 57.07 78.42 66.06 89.62 75.95 82.22 73.34 77.18 74.14WABOriginal 79.24 77.96 77.84 77.9 80.37 80.48 80.43 79.17 79.16 79.16SO-PMI 79.04 80.49 73.15 76.64 77.96 84.27 80.99 79.22 78.71 78.82LU 79.11 81.05 72.53 76.55 77.71 84.96 81.17 79.38 78.74 78.86LE 79.15 78.03 77.45 77.74 80.13 80.65 80.39 79.08 79.05 79.07LUE 79 80.87 72.49 76.45 77.65 84.78 81.06 79.26 78.64 78.75GASPOriginal 69.38 86.89 74.25 80.08 26.88 45.79 33.87 56.88 60.02 56.97SO-PMI 69.69 86.73 74.89 80.38 26.82 44.53 33.48 56.77 59.71 56.93LU 73.01 87.46 78.72 82.86 30.59 45.4 36.55 59.03 62.06 59.71LE 69.21 86.9 74.01 79.94 26.78 45.98 33.84 56.84 60 56.89LUE 72.99 87.44 78.72 82.85 30.55 45.3 36.49 59 62.01 59.67AverageOriginal 73.49 71.90 79.09 73.88 66.30 64.23 63.77 69.10 71.66 68.83S0-PMI 74.21 73.47 76.80 73.89 65.17 66.41 64.59 69.32 71.61 69.24LU 76.34 75.29 76.76 75.29 66.06 68.80 66.70 70.68 72.78 71.00LE 73.50 72.03 78.42 73.74 65.99 64.67 63.90 69.01 71.55 68.82LUE 76.22 75.13 76.54 75.12 65.94 68.68 66.59 70.53 72.61 70.85Table 6Results obtained from adapting Thelwall-Lexicon on three datasetsusing the context-based adaptation model. Bold=highest performance.LU=Lexicon Update, LE=Lexicon Expand, and LUE=Lexicon Up-date and Expand.Dataset Contextual Model Original LexiconP R F1SemEvalLU ρ < 0.001 ρ < 0.001 ρ < 0.001LE ρ < 0.001 ρ < 0.001 ρ < 0.001LUE ρ < 0.001 ρ < 0.001 ρ < 0.001WABLU ρ < 0.001 ρ < 0.001 ρ < 0.001LE ρ < 0.001 ρ < 0.001 ρ < 0.001LUE ρ = 0.051 ρ < 0.001 ρ < 0.001GASPLU ρ < 0.001 ρ < 0.001 ρ < 0.001LE ρ < 0.001 ρ < 0.001 ρ < 0.001LUE ρ < 0.001 ρ < 0.001 ρ < 0.001Table 7ρ-values of the contextually-adapted lexicons vs the Original lexiconIn the case of the WAB dataset, the highest senti-ment detection performance (79.24% in accuracy and79.16% in average F1) is obtained using the original lex-icon. In this case, the context-based adaptation modelhas a modest impact. Only the LU lexicon on WABgives a 0.2% better precision than the original lexicon.Compared to SO-PMI, our adaptation model gives acomparable performance, except for the case of the LElexicon, where the performance improves by 0.3% inaverage F1.Overall, the average performance across the threedatasets shows that the improvement of the adapted LUand LUE lexicons over the original lexicon and the SO-PMI lexicon reaches 3.9% and 2.9% in accuracy, andDataset Contextual Model SO-PMI LexiconP R F1SemEvalLU ρ < 0.001 ρ < 0.001 ρ < 0.001LE ρ < 0.001 ρ < 0.001 ρ < 0.005LUE ρ < 0.001 ρ < 0.001 ρ < 0.001WABLU ρ < 0.001 ρ < 0.001 ρ < 0.001LE ρ < 0.001 ρ < 0.001 ρ < 0.001LUE ρ = 0.051 ρ < 0.001 ρ < 0.001GASPLU ρ < 0.001 ρ < 0.001 ρ < 0.001LE ρ < 0.001 ρ < 0.001 ρ < 0.001LUE ρ < 0.001 ρ < 0.001 ρ < 0.001Table 8ρ-values of contextually-adapted lexicons vs the SO-PMI lexicon.Bold=insignificant ρ-values3.2% and 2.4% in F1 respectively. On the other hand,the LE lexicon gives negligible performance improve-ments over the original lexicon, and a slightly lowerperformance than SO-PMI by 0.9% in accuracy and0.6% in F1.The variation in the performance of our adapted lex-icons through the three datasets might be due to theirdifferent sentiment class distribution. According to Ta-ble 2 the class distribution in SemEval and GASP ishighly skewed towards the positive and negative classesrespectively. On the other hand, the WAB dataset is themost balanced dataset amongst the three. The impactof such skewness on sentiment detection is investigatedfurther in Section 6.6.H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 136.2. Results of the Conceptually-enriched Adapta-tion ModelThe second evaluation task in this paper is to assessthe effectiveness of our conceptually-enriched modelin adapting sentiment lexicons (Section 4.1). Table 9shows the average results across the three datasets con-sidering the three different settings of lexicon adapta-tion: update, expand, and update and expand. We re-fer to adapted lexicons by the conceptually-enrichedmodel under these settings as SLU, SLE, and SLUEto differentiate them from the lexicons adapted by thecontext-based model. Note that here we do not discussthe results of the semantic model on each dataset toavoid repetition, as the performance trend of the seman-tic model on each of the datasets is very similar to theone reported for the context-based model. For the com-plete list of results we refer the reader to Table 16 in Ap-pendix A. All results in average P, R and F1 measuresare statistically significance with ρ-values < 0.0055.ρ-values are reported in Table 10. Note that, for simplic-ity, the ρ-values displayed in this table are computed bycombining the significance scores across three datasetsusing the Sum of Logs method (aka Fisher’s method)[3].As we can see in the Table 9, the original lexicongives the lowest performance in all measures in com-parison with the SO-PMI lexicon and the conceptu-ally adapted lexicons, SLU, SLE and SLUE. In particu-lar, the SLU lexicon achieves the highest performanceamong all other lexicons, outperforming the originallexicon by 4.1% in accuracy and 3.3% in average F1.The SLUE lexicon comes next with quite close per-formance to the SLU lexicon; SLUE produces 4.0%and 3.2% higher accuracy and F1 than the original lex-icon respectively. The SLE lexicon comes third withmarginal impact on sentiment detection performance.Compared to the SO-PMI lexicon, a similar perfor-mance trend of our conceptually-adapted lexicons canbe observed. Specifically, both, SLU and SLE outper-form SO-PMI by up to 2.96% in accuracy and 2.6% inaverage F1. On the other hand, the SLE lexicon pro-duces lower performance than the SO-PMI lexicon by0.56% and 0.43% in accuracy and average F1 respec-tively.6.3. Results of the Semantically-adjusted RelationsModelThe third step in our evaluation is to test the per-formance of the adapted lexicons by the semantically-adjusted relations model (Section 4.2). The lower partof Table 9 lists the average results across the threedatasets for the adapted lexicon under the update setting(SRU), the expand setting (SRE), and the update andexpand setting (SRUE). For the complete list of resultswe refer the reader to Table 17 in Appendix A. Resultsreported in this section are statistically significance withρ < 0.0055 as shown in Table 11.According to these results in Table 9, we notice thatthe three semantically adapted lexicons SRU, SRE andSRUE outperforms both, the original lexicon and theSO-PMI lexicon by a large margin. In particular, thelexicon adapted under the expand setting, SRE outper-form both baseline lexicons by up to 4.3% in accuracyand 3.2% in average F1. The SRU and the SRUE lexi-cons come next by a performance that is 3.6% and 2.2%higher in accuracy and F1 than the baselines.6.4. Context-based Adaptation vs. Semantic-basedAdaptationIn the previous sections we showed that lexiconsadapted by our context-based model, as well as bothsemantically-enriched models, outperform both, theoriginal lexicon and the SO-PMI lexicon in most evalu-ation scenarios.In this section we investigate how the conceptually-enriched model and the semantically-adjusted relationsmodel perform in comparison with the original context-based adaptation model. Such comparison allows us tounderstand and highlight the added value of using wordsemantics for sentiment lexicon adaptation. To this end,we compute the win/loss in accuracy, P, R and averageF1 when using both semantic models for lexicon adap-tation compared to the context-based model across thethree datasets, as depicted in Figure 5.The results show that the impact of the two seman-tic models varies across the three lexicon adaptationsettings. ρ-values of the statistical significance of theseresults are reported in Table 12. All results in average P,R and F1 measures are statistically significance.From Figure 5, we notice that under both, the lexiconupdate setting and the lexicon update & expand setting(Figures 5:a and 5:c) the conceptually-enriched modelimproves performance upon the context-based model inaccuracy, P, and F1 by up to 0.27%, but only gives sim-ilar recall. On the other hand, the semantically-adjustedrelations model always gives, under these settings, alower performance on all measures compared to thecontext-based model.A different performance trend can be noted for theexpand setting (Figure 5:b). While the conceptually-enriched model does not show significant improve-ment over the context-based model, the semantically-14 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social WebModel Lexicon Accuracy Negative Sentiment Positive Sentiment AverageP R F1 P R F1 P R F1Baselines Original 73.49 71.90 79.09 73.88 66.30 64.23 63.77 69.10 71.66 68.83PMI 74.21 73.47 76.80 73.89 65.17 66.41 64.59 69.32 71.61 69.24Conceptually-enriched ModelSLU 76.47 75.54 76.31 75.29 65.93 69.20 66.87 70.74 72.75 71.08SLE 73.78 72.40 77.02 73.58 65.44 65.66 64.31 68.92 71.34 68.94SLUE 76.42 75.47 76.31 75.24 65.90 69.10 66.81 70.69 72.71 71.03Semantically-adjusted Relations ModelSRU 76.14 75.99 74.74 74.50 64.78 68.95 66.14 70.39 71.84 70.31SRE 76.66 77.38 73.62 74.76 64.84 71.42 67.31 71.11 72.52 71.03SRUE 76.13 76.03 74.75 74.52 64.78 69.01 66.16 70.41 71.88 70.34Table 9Average results across the three datasets of Thelwall-Lexicon adaptedby the semantic model. Bold=highest performance.Semantically-enriched Model Original Lexicon SO-PMI LexiconP R F1 P R F1SLU ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001SLE ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001SLUE ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001Table 10Combined ρ-values across the three datasets of semantically-enriched lexicons vs (i) the Original lexicon and (ii) the lexicon adapted by theSO-PMI methodSemantically-adjusted Relations Model Original Lexicon SO-PMI LexiconP R F1 P R F1SRU ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001SRE ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001SRUE ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001Table 11Combined ρ-values across the three datasets of lexicons adapted by the Semantically-adjusted Relations Model vs (i) the Original lexicon and (ii)the lexicon adapted by the SO-PMI methodContext-based Model Semantically-enriched model Semantically-adjusted Relations modelP R F1 P R F1SLU ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001SLE ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001SLUE ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001 ρ < 0.001Table 12Combined ρ-values across the three datasets of lexicons adapted by the Context-based model vs lexicons adapted by: (i) the Semantically-enrichedmodel and (ii) the Semantically-adjusted Relations modeladjusted relation model boosts the performance sub-stantially, with 4.12% and 3.12% gain in accuracy andF1 respectively.Hence, we can notice that while both semantic en-richment models have a noticeable impact on the lexi-con adaptation performance, the conceptually-enrichedmodel has a higher impact on tuning the sentiment ofexisting words in the lexicon (i.e., the update setting).On the other hand, the semantically-adjusted relationsmodel is more useful in expanding the lexicon withnew opinionated words (i.e., the expand setting). Thisis probably due to the mechanism in which each modelfunctions. As described in Section 4, the enrichmentwith semantic concepts is done at the dataset level (Fig-ure 1:b) in the first iteration of the lexicon adaptationprocess. On the other hand, the enrichment with seman-tic relations is performed during the contextual-relationextraction phase in the second iteration of the lexiconadaptation process. This will be further discussed inSection 7.6.5. Adaptation Impact on Thelwall-LexiconApplying our adaptation models to Thelwall-Lexiconresults in substantial changes to the lexicon. Table 13shows the average percentage of words across the threedatasets that, either changed their sentiment polarityand strength, or were added to the lexicon, by both,H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 15-­‐1.50\t  -­‐1.00\t  -­‐0.50\t  0.00\t  0.50\t  Accuracy\t   P\t   R\t   F1\t  Lexicon\t  Update\t  Conceptually-­‐enriched\t  Model\t  (SLU)\t  SemanAcally-­‐adjusted\t  RelaAons\t  Model\t  (SRU)\t  (a) Lexicon Update-­‐1.50\t  0.50\t  2.50\t  4.50\t  Accuracy\t   P\t   R\t   F1\t  Lexicon\t  Expand\t  Conceptually-­‐enriched\t  Model\t  (SLE)\t  SemanCcally-­‐adjusted\t  RelaCons\t  Model\t  (SRE)\t  (b) Lexicon Expand-­‐1.50\t  -­‐1.00\t  -­‐0.50\t  0.00\t  0.50\t  Accuracy\t   P\t   R\t   F1\t  Lexicon\t  Update\t  and\t  Expand\t  Conceptually-­‐enriched\t  Model\t  (SLUE)\t  SemanBcally-­‐adjusted\t  RelaBons\t  Model\t  (SRUE)\t  (b) Lexicon Update and ExpandFig. 5. Win/Loss in Accuracy, P, R and F1 measures of adapting sentiment lexicons by the conceptually-enriched model and semantically-adjustedrelations model in comparison with the context-based model.Context-based Adaptation Semantic Adaptation AverageWords found in the lexicon 4.96 5.01 5.00Words flipped their sentiment orientation 38.40 38.32 38.36Words changed their sentiment strength 60.60 60.62 60.61New positive words 10.77 11.03 10.90New negative words 4.50 4.68 4.59Table 13Average percentage of words across the three datasets that had their sentiment orientation or strength updated by the context-based and semanticadaptation models.context-based adaptation model and the conceptually-enriched model.5On average only 5% of the words in the datasets werefound in the original lexicon. However, adapting thelexicon by either model resulted in 38% of these wordsflipping their sentiment orientation and 60% changingtheir sentiment strength while keeping their prior sen-timent orientation. Only 1% of the words that werefound in Thelwall-Lexicon remained untouched. Also,10% and 4% of previously unseen (hidden) words inthe original lexicon were assigned positive and nega-tive sentiment, and were added to the adapted lexiconsaccordingly. Adding semantic information helped de-tecting more words in the original lexicon as well asadding more positive and negative terms to the adaptedlexicon. Table 14 shows an example of 10 semantic sub-types added to the Thelwall-Lexicon by our adaptationmodel.6.6. Impact of Sentiment Class DistributionIn this section we analyse the impact of sentimentclass distribution in the datasets on the performanceof our adaptation models. To this end, we first bal-5Note that we do not report statistics for the semantically-adjustedrelations model in Table 13, since they are similar to the once of theconceptually-enriched model.Semantic SubtypesFilmActor PositiveGovernmentalBody NegativeComposer PositiveAirline NeutralInventor PositiveLocation NeutralPhysician NegativeOrganizationSector NegativeFootballplayer NeutralUniversity NeutralTable 14Example of 10 subtypes of entities added to the lexicon after adaptationance the number of positive and negative tweets inthe three datasets by mapping the size of the domi-nant sentiment class to the size of the minor sentimentclass, as shown in Table 15. Once we have a balanceddataset with the same number of elements in the pos-itive and negative classes we imbalance this datasetby fixing one class and reducing the number of ele-ments in the other class by 10% in each step (e.g., main-taining all elements of the positive class and reducingthe elements of the negative class by 10%, 20%, 30%,...etc.). By performing this process we obtain 20 ver-sions (folds) of the same dataset, from completely bal-16 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web53\t  58\t  63\t  68\t  73\t  100%\t   90%\t   80%\t   70%\t   60%\t   50%\t   40%\t   30%\t   20%\t   10%\t  Percentage\t  of\t  Nega-ve\t  Tweets\t  F1-­‐Original\t   F1-­‐Context\t   F1-­‐Seman=c\t  (a) Positively-skewed Class Distribution59\t  61\t  63\t  65\t  67\t  69\t  71\t  73\t  100%\t   90%\t   80%\t   70%\t   60%\t   50%\t   40%\t   30%\t   20%\t   10%\t  Percentage\t  of\t  Posi.ve\t  Tweets\t  F1-­‐Original\t   F1-­‐Context\t   F1-­‐Seman=c\t  (b) Negatively-skewed Class DistributionFig. 6. Average F1 of applying the original and the adapted lexicons on 10 folds of tweets of (a) positively-skewed class distribution and (b)negatively-skewed class distribution.Dataset Tweets #Negative #PositiveSemEval [20] 4356 2178 2178WAB [33] 5154 2577 2577GASP [33] 2066 1033 1033Table 15Number of positive and negative tweets in the balanced SemEval,WAB and GASP datasets.anced, to completely skewed towards the positive class,to completely skewed towards the negative class. Figure6 shows the average F1 of binary sentiment detection ofapplying the original Thelwall-Lexicon (F1-Original),the context-based adapted lexicon (F1-Context) andthe semantically adapted lexicon by the conceptually-enriched model (F1-Semantic) on the 20 imbalancedfolds of tweets. Note that the results here are averagedover the three datasets and the three adaptation settings.Figure 6:(a) depicts the performance over the 10positive-skewed tweet folds. Here we can see that theperformance of all lexicons decreases by graduallylowering the number of negative tweets in the data(i.e., increasing the degree positive-skewness). How-ever, we notice that lexicons adapted by both our pro-posed context-based and semantic adaptation modelsconsistently outperform the original lexicon by 3% inaverage F1 in all degrees of positive class skewness.Figure 6:(b) shows the performance over the 10negatively-skewed folds. We notice that both adaptedlexicons keep a 1% higher F1 on average than the origi-nal lexicon up to level where number of positive tweetsis less than 40% (equals to 60% negative-skewness de-gree). After that level, all the three lexicons just givesimilar performance.It is worth noting that all lexicons, including the orig-inal one, are more affected by the positive-skewness inthe data than the negative-skewness. Lexicons appliedon positively-skewed data give a 2.6% lower F1 on av-erage than lexicons applied on negatively-skewed data.This might be due to imbalanced number of the opin-ionated words in Thelwall-Lexicon. As mentioned inSection 5.1, Thelwall-Lexicon has 79% more negativewords than positive ones.Overall, one can conclude that the sentiment classdistribution clearly impacts the performance of the orig-inal lexicon as well as the adapted ones. The moreskewed the distribution is (in either direction), the lowerthe performance is. Nevertheless, results show that lexi-cons adapted by our models are more tolerant to imbal-anced sentiment distributions in the data than the origi-nal lexicon. In real life scenarios, imbalanced distribu-tions of tweets’ sentiment are perhaps more likely to oc-cur, and lexicon adaptation methods can therefore helpenriching sentiment identification in such scenarios.7. Discussion and Future WorkOne of the most fascinating dimensions of socialmedia is the way in which new topics and themes con-stantly emerge and dissipate. The ability of accuratelyidentifying opinions and sentiment in this dynamic en-H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 17GASP SemEval WAB00.030.040.230.730.960.97Negative Positive Neutral Negative Positive Neutral Negative Positive NeutralSentimentCount (%)Fig. 7. Percentage of positive, negative and neutral semantic subtypes extracted from the three datasets and added to the lexicon after adaptationvironment is crucial to governments, organisations andbusiness who want to profit from the users’ opinionsexpressed within this medium.To this end, this paper proposed an approach foradapting general-purpose sentiment lexicons to particu-lar domains or contexts. While our proposed approachis generic, this study focuses on Twitter. However, theuse of contextual and semantic information may affectdifferently the adaptation of sentiment lexicons in dif-ferent social media platforms (e.g., Facebook, Tumblr),as well as conventional data sources (e.g., online fo-rums, product reviews websites). Further work is there-fore needed to study variances across these differenttypes of social and conventional data.Our selection of the approach to extract the words’contextual sentiment, SentiCircles [27], is inspired bythe scope of the provided information, since it does notonly capture the words’ contextual sentiment orienta-tion but also the words’ contextual sentiment strength,which enables a more fine-grained adaptation of thelexicons.For our experiments we selected to use Thelwall-Lexicon[34] since it is one of the most popular lexiconsfor sentiment analysis for the social web. However, amore extensive experimentation is needed to assesswhether sentiment lexicons of different characteristicsmay require different types of adaptation.In our work we have used a third-party commercialtool (AlchemyAPI) to extract the semantic concepts andsubtypes of words from tweets. As future work, we planto experiment with other entity extraction tools, suchas DBpedia Spotlight6 or TexRazor,7 since we haveobserved that these tools provide more fine-grained6https://github.com/dbpedia-spotlight/dbpedia-spotlight7https://www.textrazor.com/subtypes for certain domains. Combining some of thesetools may help us to achieve better coverage of semantictypes and subtypes.Similarly, the approach of Pirro [23] was used toextract semantic relations between every pair of entitieswithin our datasets. Our assumption is that differentrelations reflect different contexts in which the twowords can appear together. However, a better filteringand/or clustering of semantic relations may be neededto provide a more fine-grained identification of thesecontexts.In our experiments, we have observed that the useof semantic information helps to improve lexicon adap-tation performance (Section 6.4). However, resultsshowed that enriching the adaptation process with se-mantic subtypes (i.e., the conceptually-enriched adap-tation model) did not have much impact on the perfor-mance when expanding the lexicon with new opinion-ated terms. This is probably due to the type of senti-ment assigned to the semantic subtypes during the en-richment process. Figure 7 shows the sentiment distri-bution of the semantic subtypes for the three evalua-tion datasets. According to this figure, we notice that,on average, 90% of the subtypes added to the lexiconwere assigned neutral sentiment after adaptation, whileonly 9% and 1% of the added subtypes were assignedpositive and negative sentiment respectively.Unlike enrichment with semantic subtypes, the en-richment with semantically-adjusted relations was per-formed in two iterations of the lexicon adaptation pro-cess. The semantic relations between two entities wereused to tune the strength of the entities’ contextual re-lations computed in the second iteration (Section 4.2).Such enrichment strategy has proven to enhance thelexicon adaptation performance, especially when ex-panding lexicons with new opinionated terms (Figure5:b). As future work, we plan to investigate the case18 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Webof running the lexicon adaptation process for highernumber of iterations, and study the impact of doing soon the lexicon’s performance as well as on the run-timecomplexity of our models.Extracting semantic relations between a high numberof entities via a SPARQL endpoint is a high-cost pro-cess. Specific details of the cost of extracting these re-lations are discussed in [23]. Our implementation usesmultithreading, so that queries are sent in parallel toenhance the performance of the retrieval of relations.However, with an increase in maximum path length,the likelyhood of a path existing between two entitiesincreases, as well as the amount of existing paths. In ourimplementation, we consider a maximum path lenghtof 3. Note that higher values of maximum path lengthcome close to the diameter of the DBPedia graph itselfand may lead to an explosion in the number of extractedrelationships8. Despite the cost of this step, it is impor-tant to consider that this process is computed once perdataset and that relations extracted between entities canbe stored and reused when adapting lexicons to Twittercollections of similar topics.For our evaluation we chose to compare lexiconsadapted by our proposed models against the originalThelwall-Lexicon as well as the lexicon adapted by thestate-of-the-art SO-PMI method. In our future work wealso aim to investigate how our adapted lexicons per-form when compared against lexicons generated fromscratch by other existing methods. This will provideus with better insights on whether adapting lexicons ispreferable, not only in terms of efficiency but also interms of performance, on on the situations for whichone method may be better than the other one.For a more qualitative and fine-grained evaluationwe also plan a manual assessment of the updates andexpansions generated by our methods, so that poten-tial adaptation mistakes for individual terms can be de-tected and considered for a further enhancement of ourapproach. In addition, an detailed error analysis of thesentiment classification is also planned, so that poten-tial error types can be identified, categorised and usedto provide further improvements.In summary, while there is still extensive room forfuture work, our experiments and results show howcontextual and semantic information can be combinedto successfully adapt generic-purpose sentiment lexi-cons to specific contexts, helping therefore to correctly8The effective estimated diameter of DBPedia is 6.5082edges. See http://konect.uni-koblenz.de/networks/dbpedia-allidentify the sentiment expressed by social media users.We hope that the presented study will serve as basesfor future work within the community and enable fur-ther research into the semantic adaptation of sentimentlexicons for microblogs.8. ConclusionsAlthough much research has been done on creatingdomain-specific sentiment lexicons, very little attentionhas been giving to the problem of lexicon adaptation insocial media, and to the use of semantic information asa resource to perform such adaptations.This paper proposed a general method to adapt senti-ment lexicons based on contextual information, wherethe domain or context of adaptation is defined by a col-lection of posts. A semantic enrichment of this methodis also proposed where conceptual semantics are usedto better capture the context for which the lexicon isbeing adapted.An evaluation of our proposed method was per-formed by adapting the state-of-the-art sentiment lexi-con for the social web [34] to three different contexts(Twitter datasets) using various configurations of ourproposed approach. Results showed that the adaptedsentiment lexicons outperformed the baseline methodsin average by 3.4% in accuracy and 2.8% in F1 mea-sure, when used to compute tweet-level polarity detec-tion with context-based adaptation. While enrichingthe adaptation process with words’ semantic subtypeshas modest impact on the lexicons’ performance, En-richment based on the semantic relations between enti-ties in tweets, yields in 4.12% and 3.12% gain in accu-racy and F1 measure in comparison with context-basedadaptation. Our results also showed that the lexiconsadapted using our proposed method are more robust toimbalanced datasets.AcknowledgmentThis work was supported by the EU-FP7 projectSENSE4US (grant no. 611242).Appendix ATables 16 and 17 lists the complete results of usingthe conceptually-enriched model and the semantically-adjusted relations models on the three evaluationdatasets respectively.H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 19Dataset Lexicon Accuracy Negative Sentiment Positive Sentiment AverageP R F1 P R F1 P R F1SemEvalOriginal 71.85 50.84 85.17 63.67 91.66 66.42 77.02 71.25 75.79 70.35SO-PMI 73.9 53.19 82.37 64.64 90.74 70.44 79.31 71.96 76.41 71.97SLU 76.12 56.2 79.24 65.72 89.84 74.84 81.65 73.02 77.04 73.68SLE 72.95 52.17 79.71 63.03 89.45 70.2 78.65 70.81 74.96 70.84SLUE 76.18 56.31 79.35 65.83 89.89 74.9 81.7 73.1 77.12 73.76WABOriginal 79.24 77.96 77.84 77.9 80.37 80.48 80.43 79.17 79.16 79.16SO-PMI 79.04 80.49 73.15 76.64 77.96 84.27 80.99 79.22 78.71 78.82SLU 79.13 81.13 72.45 76.52 77.68 85.06 81.18 79.41 78.76 78.85SLE 79.17 78.13 77.34 77.71 80.07 80.78 80.4 79.1 79.06 79.05SLUE 79.04 81.05 72.34 76.41 77.59 84.99 81.1 79.32 78.66 78.75GASPOriginal 69.38 86.89 74.25 80.08 26.88 45.79 33.87 56.88 60.02 56.97SO-PMI 69.69 86.73 74.89 80.38 26.82 44.53 33.48 56.77 59.71 56.93SLU 73.04 87.48 78.73 82.87 30.68 45.54 36.57 59.08 62.14 59.72SLE 69.21 86.9 74.02 79.93 26.78 46 33.77 56.84 60.01 56.85SLUE 73.03 87.47 78.74 82.86 30.62 45.41 36.5 59.05 62.07 59.68AverageOriginal 73.49 71.90 79.09 73.88 66.30 64.23 63.77 69.10 71.66 68.83S0-PMI 74.21 73.47 76.80 73.89 65.17 66.41 64.59 69.32 71.61 69.24SLU 76.47 75.54 76.31 75.29 65.93 69.20 66.87 70.74 72.75 71.08SLE 73.78 72.40 77.02 73.58 65.44 65.66 64.31 68.92 71.34 68.94SLUE 76.42 75.47 76.31 75.24 65.90 69.10 66.81 70.69 72.71 71.03Table 16Results obtained from adapting Thelwall-Lexicon on three datasetsusing the conceptually-enriched adaptation model. Bold=highestperformance.Dataset Lexicon Accuracy Negative Sentiment Positive Sentiment AverageP R F1 P R F1 P R F1SemEvalOriginal 71.85 50.84 85.17 63.67 91.66 66.42 77.02 71.25 75.79 70.35SO-PMI 73.9 53.19 82.37 64.64 90.74 70.44 79.31 71.96 76.41 71.97SRU 75.66 55.8 76.76 64.59 88.81 75.22 81.44 72.31 75.99 73.01SRE 77.23 58.27 75.34 65.67 88.58 78 82.94 73.43 76.67 74.31SRUE 75.72 55.88 76.8 64.66 88.85 75.27 81.48 72.36 76.04 73.07WABOriginal 79.24 77.96 77.84 77.9 80.37 80.48 80.43 79.17 79.16 79.16SO-PMI 79.04 80.49 73.15 76.64 77.96 84.27 80.99 79.22 78.71 78.82SRU 78.97 84.17 68.06 75.23 75.78 88.64 81.68 79.97 78.35 78.46SRE 78.95 84.8 67.31 75.01 75.47 89.29 81.78 80.14 78.3 78.39SRUE 78.89 84.11 67.98 75.15 75.72 88.56 81.62 79.91 78.27 78.39GASPOriginal 69.38 86.89 74.25 80.08 26.88 45.79 33.87 56.88 60.02 56.97SO-PMI 69.69 86.73 74.89 80.38 26.82 44.53 33.48 56.77 59.71 56.93SRU 72.96 86.81 79.44 82.95 29.51 41.58 34.43 58.16 60.51 58.69SRE 72.8 87.41 78.47 82.69 30.25 45.3 36.22 58.83 61.89 59.46SRUE 72.91 86.82 79.35 82.9 29.43 41.68 34.42 58.12 60.52 58.66AverageOriginal 73.49 71.90 79.09 73.88 66.30 64.23 63.77 69.10 71.66 68.83S0-PMI 74.21 73.47 76.80 73.89 65.17 66.41 64.59 69.32 71.61 69.24SRU 76.14 75.99 74.74 74.50 64.78 68.95 66.14 70.39 71.84 70.31SRE 76.66 77.38 73.62 74.76 64.84 71.42 67.31 71.11 72.52 71.03SRUE 76.13 76.03 74.75 74.52 64.78 69.01 66.16 70.41 71.88 70.34Table 17Results obtained from adapting Thelwall-Lexicon on three datasets using the semantically-adjusted relations model. Bold=highest performance.20 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social WebReferences[1] Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani.SentiWordNet 3.0: An enhanced lexical resource for sen-timent analysis and opinion mining. In Nicoletta Calzo-lari, Khalid Choukri, Bente Maegaard, Joseph Mariani, JanOdijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, ed-itors, Proceedings of the International Conference on Lan-guage Resources and Evaluation, LREC 2010, 17-23 May2010, Valletta, Malta. European Language Resources As-sociation, 2010. URL http://www.lrec-conf.org/proceedings/lrec2010/summaries/769.html.[2] Carmen Banea, Rada Mihalcea, and Janyce Wiebe. A boot-strapping method for building subjectivity lexicons for lan-guages with scarce resources. In Proceedings of the In-ternational Conference on Language Resources and Eval-uation, LREC 2008, 26 May - 1 June 2008, Marrakech,Morocco. European Language Resources Association, 2008.URL http://www.lrec-conf.org/proceedings/lrec2008/summaries/700.html.[3] Betsy Jane Becker. Combining significance levels. In Har-ris Cooper and Larry V. Hedges, editors, The Handbook ofResearch Synthesis, chapter 15, pages 215–230. Russell SageFoundation, 1994.[4] Johan Bollen, Huina Mao, and Xiao-Jun Zeng. Twitter moodpredicts the stock market. Journal of Computational Science, 2(1):1–8, 2011. 10.1016/j.jocs.2010.12.007.[5] Erik Cambria. An introduction to concept-level sentiment anal-ysis. In Félix Castro-Espinoza, Alexander F. Gelbukh, andMiguel González-Mendoza, editors, Advances in Soft Comput-ing and Its Applications - 12th Mexican International Confer-ence on Artificial Intelligence, MICAI 2013, Mexico City, Mex-ico, November 24-30, 2013, Proceedings, Part II, volume 8266of Lecture Notes in Computer Science, pages 478–483. Springer,2013. 10.1007/978-3-642-45111-9_41.[6] Erik Cambria, Catherine Havasi, and Amir Hussain. SenticNet2: A semantic and affective resource for opinion mining andsentiment analysis. In G. Michael Youngblood and Philip M.McCarthy, editors, Proceedings of the Twenty-Fifth Interna-tional Florida Artificial Intelligence Research Society Confer-ence, Marco Island, Florida. May 23-25, 2012. AAAI Press,2012. URL http://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS12/paper/view/4411.[7] Ilia Chetviorkin and Natalia V. Loukachevitch. Two-step modelfor sentiment lexicon extraction from Twitter streams. InAlexandra Balahur, Erik Van der Goot, Ralf Steinberger, andAndrés Montoyo, editors, Proceedings of the 5th Workshop onComputational Approaches to Subjectivity, Sentiment and So-cial Media Analysis, WASSA@ACL 2014, June 27, 2014, Bal-timore, Maryland, USA, pages 67–72. Association for Com-putational Linguistics, 2014. URL http://aclweb.org/anthology/W/W14/W14-2612.pdf.[8] Yejin Choi and Claire Cardie. Adapting a polarity lexicon us-ing integer linear programming for domain-specific sentimentclassification. In Proceedings of the 2009 Conference on Empir-ical Methods in Natural Language Processing, EMNLP 2009,6-7 August 2009, Singapore, A meeting of SIGDAT, a SpecialInterest Group of the ACL, pages 590–598. ACL, 2009. URLhttp://www.aclweb.org/anthology/D09-1062.[9] Weifu Du, Songbo Tan, Xueqi Cheng, and Xiao-chun Yun.Adapting information bottleneck method for automatic construc-tion of domain-oriented sentiment lexicon. In Brian D. Davison,Torsten Suel, Nick Craswell, and Bing Liu, editors, Proceedingsof the Third International Conference on Web Search and WebData Mining, WSDM 2010, New York, NY, USA, February 4-6,2010, pages 111–120. ACM, 2010. 10.1145/1718487.1718502.[10] Shi Feng, Kaisong Song, Daling Wang, and Ge Yu. A word-emoticon mutual reinforcement ranking model for buildingsentiment lexicon from massive collection of microblogs. WorldWide Web, 18(4):949–967, 2015. 10.1007/s11280-014-0289-x.[11] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domainadaptation for large-scale sentiment classification: A deep learn-ing approach. In Lise Getoor and Tobias Scheffer, editors,Proceedings of the 28th International Conference on MachineLearning, ICML 2011, Bellevue, Washington, USA, June 28 -July 2, 2011, pages 513–520. Omnipress, 2011.[12] Pollyanna Gonçalves, Matheus Araújo, Fabrício Benevenuto,and Meeyoung Cha. Comparing and combining sentiment anal-ysis methods. In S. Muthu Muthukrishnan, Amr El Abbadi,and Balachander Krishnamurthy, editors, Conference on OnlineSocial Networks, COSN’13, Boston, MA, USA, October 7-8,2013, pages 27–38. ACM, 2013. 10.1145/2512938.2512951.[13] Zellig S Harris. Distributional structure. Word, 1954.[14] Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. Unsupervisedsentiment analysis with emotional signals. In Daniel Schwabe,Virgílio A. F. Almeida, Hartmut Glaser, Ricardo A. Baeza-Yates, and Sue B. Moon, editors, 22nd International WorldWide Web Conference, WWW ’13, Rio de Janeiro, Brazil, May13-17, 2013, pages 607–618. International World Wide WebConferences Steering Committee / ACM, 2013. URL http://dl.acm.org/citation.cfm?id=2488442.[15] Yongyos Kaewpitakkun, Kiyoaki Shirai, and Masnizah Mohd.Sentiment lexicon interpolation and polarity estimation of ob-jective and out-of-vocabulary words to improve sentiment clas-sification on microblogging. In Wirote Aroonmanakun, PrachyaBoonkwan, and Thepchai Supnithi, editors, Proceedings of the28th Pacific Asia Conference on Language, Information andComputation, PACLIC 28, Cape Panwa Hotel, Phuket, Thailand,December 12-14, 2014, pages 204–213. The PACLIC 28 Or-ganizing Committee and PACLIC Steering Committee / ACL /Department of Linguistics, Faculty of Arts, Chulalongkorn Uni-versity, 2014. URL http://aclweb.org/anthology/Y/Y14/Y14-1026.pdf.[16] Hiroshi Kanayama and Tetsuya Nasukawa. Fully automatic lex-icon expansion for domain-oriented sentiment analysis. In DanJurafsky and Éric Gaussier, editors, EMNLP 2007, Proceedingsof the 2006 Conference on Empirical Methods in Natural Lan-guage Processing, 22-23 July 2006, Sydney, Australia, pages355–363. ACL, 2006. URL http://www.aclweb.org/anthology/W06-1642.[17] Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and XiaoyanZhu. Cross-domain co-extraction of sentiment and topic lexi-cons. In The 50th Annual Meeting of the Association for Compu-tational Linguistics, Proceedings of the Conference, July 8-14,2012, Jeju Island, Korea - Volume 1: Long Papers, pages 410–419. Association for Computational Linguistics, 2012. URLhttp://www.aclweb.org/anthology/P12-1043.[18] Chenghua Lin, Yulan He, Richard Everson, and Stefan M. Rüger.Weakly supervised joint sentiment-topic detection from text.IEEE Transactions on Knowledge and Data Engingneering, 24(6):1134–1145, 2012. 10.1109/TKDE.2011.48.H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web 21[19] Yue Lu, Malú Castellanos, Umeshwar Dayal, and ChengXiangZhai. Automatic construction of a context-aware sentiment lexi-con: an optimization approach. In Sadagopan Srinivasan, KrithiRamamritham, Arun Kumar, M. P. Ravindra, Elisa Bertino,and Ravi Kumar, editors, Proceedings of the 20th InternationalConference on World Wide Web, WWW 2011, Hyderabad, In-dia, March 28 - April 1, 2011, pages 347–356. ACM, 2011.10.1145/1963405.1963456.[20] Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, VeselinStoyanov, Alan Ritter, and Theresa Wilson. SemEval-2013Task 2: Sentiment analysis in Twitter. In Mona T. Diab,Timothy Baldwin, and Marco Baroni, editors, Proceedingsof the 7th International Workshop on Semantic Evaluation,SemEval@NAACL-HLT 2013, Atlanta, Georgia, USA, June 14-15, 2013, pages 312–320. Association for Computational Lin-guistics, 2013. URL http://aclweb.org/anthology/S/S13/S13-2052.pdf.[21] Finn Årup Nielsen. A new ANEW: evaluation of a word listfor sentiment analysis in microblogs. In Matthew Rowe, Mi-lan Stankovic, Aba-Sah Dadzie, and Mariann Hardey, editors,Proceedings of the ESWC2011 Workshop on ’Making Senseof Microposts’: Big things come in small packages, Herak-lion, Crete, Greece, May 30, 2011, volume 718 of CEUR Work-shop Proceedings, pages 93–98. CEUR-WS.org, 2011. URLhttp://ceur-ws.org/Vol-718/paper_16.pdf.[22] Viswa Mani Kiran Peddinti and Prakriti Chintalapoodi. Domainadaptation in sentiment analysis of Twitter. In Analyzing Mi-crotext, Papers from the 2011 AAAI Workshop, San Francisco,California, USA, August 8, 2011, volume WS-11-05 of AAAIWorkshops. AAAI, 2011. URL http://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/view/3992.[23] Giuseppe Pirrò. Explaining and suggesting relatedness in knowl-edge graphs. In Marcelo Arenas, Óscar Corcho, Elena Sim-perl, Markus Strohmaier, Mathieu d’Aquin, Kavitha Srinivas,Paul T. Groth, Michel Dumontier, Jeff Heflin, KrishnaprasadThirunarayan, and Steffen Staab, editors, The Semantic Web- ISWC 2015 - 14th International Semantic Web Conference,Bethlehem, PA, USA, October 11-15, 2015, Proceedings, PartI, volume 9366 of Lecture Notes in Computer Science, pages622–639. Springer, 2015. 10.1007/978-3-319-25007-6_36.[24] Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. Expand-ing domain sentiment lexicon through double propagation.In Craig Boutilier, editor, IJCAI 2009, Proceedings of the21st International Joint Conference on Artificial Intelligence,Pasadena, California, USA, July 11-17, 2009, pages 1199–1204, 2009. URL http://ijcai.org/Proceedings/09/Papers/202.pdf.[25] Hassan Saif, Yulan He, and Harith Alani. Semantic senti-ment analysis of Twitter. In Philippe Cudré-Mauroux, JeffHeflin, Evren Sirin, Tania Tudorache, Jérôme Euzenat, Man-fred Hauswirth, Josiane Xavier Parreira, Jim Hendler, GuusSchreiber, Abraham Bernstein, and Eva Blomqvist, editors, TheSemantic Web - ISWC 2012 - 11th International Semantic WebConference, Boston, MA, USA, November 11-15, 2012, Pro-ceedings, Part I, volume 7649 of Lecture Notes in ComputerScience, pages 508–524. Springer, 2012. 10.1007/978-3-642-35176-1_32.[26] Hassan Saif, Miriam Fernández, Yulan He, and Harith Alani.Evaluation datasets for Twitter sentiment analysis: A survey anda new dataset, the STS-Gold. In Cristina Battaglino, CristinaBosco, Erik Cambria, Rossana Damiano, Viviana Patti, andPaolo Rosso, editors, Proceedings of the First InternationalWorkshop on Emotion and Sentiment in Social and Expres-sive Media: approaches and perspectives from AI (ESSEM2013) A workshop of the XIII International Conference ofthe Italian Association for Artificial Intelligence (AI*IA 2013),Turin, Italy, December 3, 2013., volume 1096 of CEUR Work-shop Proceedings, pages 9–21. CEUR-WS.org, 2013. URLhttp://ceur-ws.org/Vol-1096/paper1.pdf.[27] Hassan Saif, Miriam Fernández, Yulan He, and Harith Alani.SentiCircles for contextual and conceptual semantic sentimentanalysis of Twitter. In Valentina Presutti, Claudia d’Amato,Fabien Gandon, Mathieu d’Aquin, Steffen Staab, and AnnaTordai, editors, The Semantic Web: Trends and Challenges -11th International Conference, ESWC 2014, Anissaras, Crete,Greece, May 25-29, 2014. Proceedings, volume 8465 of Lec-ture Notes in Computer Science, pages 83–98. Springer, 2014.10.1007/978-3-319-07443-6_7.[28] Hassan Saif, Yulan He, Miriam Fernández, and Harith Alani.Adapting sentiment lexicons using contextual semantics for sen-timent analysis of Twitter. In Valentina Presutti, Eva Blomqvist,Raphaël Troncy, Harald Sack, Ioannis Papadakis, and AnnaTordai, editors, The Semantic Web: ESWC 2014 Satellite Events- ESWC 2014 Satellite Events, Anissaras, Crete, Greece, May25-29, 2014, Revised Selected Papers, volume 8798 of Lec-ture Notes in Computer Science, pages 54–63. Springer, 2014.10.1007/978-3-319-11955-7_5.[29] Hassan Saif, Yulan He, Miriam Fernandez, and Harith Alani.Contextual semantics for sentiment analysis of Twitter. Infor-mation Processing & Management, 2015.[30] Hassan Saif, F Javier Ortega, Miriam Fernández, and IvánCantador. Sentiment analysis in social streams. In MarkoTkalcˇicˇ, Berardina De Carolis, Marco de Gemmis, Ante Odic´,and Andrej Košir, editors, Emotions and Personality in Person-alized Services: Models, Evaluation and Applications, Human-Computer Interaction Series, pages 119–140. Springer, 2016.10.1007/978-3-319-31413-6_7.[31] Christian Scheible and Hinrich Schütze. Bootstrapping senti-ment labels for unannotated documents with polarity PageR-ank. In Nicoletta Calzolari, Khalid Choukri, Thierry De-clerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani,Jan Odijk, and Stelios Piperidis, editors, Proceedings of theEighth International Conference on Language Resources andEvaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012,pages 1230–1234. European Language Resources Associa-tion (ELRA), 2012. URL http://www.lrec-conf.org/proceedings/lrec2012/summaries/124.html.[32] Juliet Popper Shaffer. Multiple hypothesis testing. An-nual review of psychology, 46:561–584, 1995. 10.1146/an-nurev.ps.46.020195.003021.[33] Amir Asiaee Taheri, Mariano Tepper, Arindam Banerjee, andGuillermo Sapiro. If you are happy and you know it... tweet. InXue-wen Chen, Guy Lebanon, Haixun Wang, and Mohammed J.Zaki, editors, 21st ACM International Conference on Informa-tion and Knowledge Management, CIKM’12, Maui, HI, USA,October 29 - November 02, 2012, pages 1602–1606. ACM,2012. 10.1145/2396761.2398481.[34] Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, andArvid Kappas. Sentiment strength detection in short informaltext. Journal of the American Society for Information Scienceand Technology, 61(12):2544–2558, 2010. 10.1002/asi.21416.22 H. Saif et al. / Sentiment Lexicon Adaptation with Context and Semantics for the Social Web[35] Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. Sen-timent strength detection for the social web. Journal of theAmerican Society for Information Science and Technology, 63(1):163–173, 2012. 10.1002/asi.21662.[36] Peter D. Turney. Thumbs up or thumbs down? semanticorientation applied to unsupervised classification of reviews.In Proceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics, July 6-12, 2002, Philadel-phia, PA, USA., pages 417–424. Association for Computa-tional Linguistics, 2002. URL http://www.aclweb.org/anthology/P02-1053.pdf.[37] Peter D. Turney and Michael L. Littman. Measuring praiseand criticism: Inference of semantic orientation from associa-tion. ACM Transactions on Information Systems, 21(4):315–346, 2003. 10.1145/944012.944013.[38] Peter D. Turney and Patrick Pantel. From frequency to mean-ing: Vector space models of semantics. Journal of ArtificialIntelligence Research, 37:141–188, 2010. 10.1613/jair.2934.[39] Warren Weaver. Translation. Machine translation of languages,14:15–23, 1955.",
      "id": 8059043,
      "identifiers": [
        {
          "identifier": "131317080",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:oro.open.ac.uk:51171",
          "type": "OAI_ID"
        },
        {
          "identifier": "208697534",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.3233/sw-170265",
          "type": "DOI"
        }
      ],
      "title": "Sentiment Lexicon Adaptation with Context and Semantics for the Social Web",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:oro.open.ac.uk:51171"
      ],
      "publishedDate": "2017-04-06T00:00:00",
      "publisher": "'IOS Press'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://oro.open.ac.uk/51171/1/swj1437.pdf"
      ],
      "updatedDate": "2022-04-18T06:15:53",
      "yearPublished": 2017,
      "journals": [
        {
          "title": null,
          "identifiers": [
            "2210-4968"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/131317080.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/131317080"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/131317080/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/131317080/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/8059043"
        }
      ]
    },
    {
      "acceptedDate": "2016-10-08T00:00:00",
      "arxivId": "1709.06311",
      "authors": [
        {
          "name": "AP Aprosio"
        },
        {
          "name": "C Fellbaum"
        },
        {
          "name": "JK-C Chung"
        },
        {
          "name": "K Schouten"
        },
        {
          "name": "M Dragoni"
        },
        {
          "name": "M Schuster"
        },
        {
          "name": "Q Le"
        },
        {
          "name": "R Collobert"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Soufian",
        "Harald"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/255164657"
      ],
      "createdDate": "2017-10-17T02:45:37",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2016-01-01T00:00:00",
      "abstract": "The World Wide Web holds a wealth of information in the form of unstructured\ntexts such as customer reviews for products, events and more. By extracting and\nanalyzing the expressed opinions in customer reviews in a fine-grained way,\nvaluable opportunities and insights for customers and businesses can be gained.\nWe propose a neural network based system to address the task of Aspect-Based\nSentiment Analysis to compete in Task 2 of the ESWC-2016 Challenge on Semantic\nSentiment Analysis. Our proposed architecture divides the task in two subtasks:\naspect term extraction and aspect-specific sentiment extraction. This approach\nis flexible in that it allows to address each subtask independently. As a first\nstep, a recurrent neural network is used to extract aspects from a text by\nframing the problem as a sequence labeling task. In a second step, a recurrent\nnetwork processes each extracted aspect with respect to its context and\npredicts a sentiment label. The system uses pretrained semantic word embedding\nfeatures which we experimentally enhance with semantic knowledge extracted from\nWordNet. Further features extracted from SenticNet prove to be beneficial for\nthe extraction of sentiment labels. As the best performing system in its\ncategory, our proposed system proves to be an effective approach for the\nAspect-Based Sentiment Analysis",
      "documentType": "research",
      "doi": "10.1007/978-3-319-46565-4_12",
      "downloadUrl": "http://arxiv.org/abs/1709.06311",
      "fieldOfStudy": null,
      "fullText": "Aspect-Based Sentiment Analysis Using a\nTwo-Step Neural Network Architecture\nSoufian Jebbara and Philipp Cimiano\nSemantic Computing Group\nCognitive Interaction Technology – Center of Excellence (CITEC)\nBielefeld University, Germany\nAbstract. The World Wide Web holds a wealth of information in the\nform of unstructured texts such as customer reviews for products, events\nand more. By extracting and analyzing the expressed opinions in cus-\ntomer reviews in a fine-grained way, valuable opportunities and insights\nfor customers and businesses can be gained.\nWe propose a neural network based system to address the task of Aspect-\nBased Sentiment Analysis to compete in Task 2 of the ESWC-2016 Chal-\nlenge on Semantic Sentiment Analysis. Our proposed architecture divides\nthe task in two subtasks: aspect term extraction and aspect-specific sen-\ntiment extraction. This approach is flexible in that it allows to address\neach subtask independently. As a first step, a recurrent neural network is\nused to extract aspects from a text by framing the problem as a sequence\nlabeling task. In a second step, a recurrent network processes each ex-\ntracted aspect with respect to its context and predicts a sentiment label.\nThe system uses pretrained semantic word embedding features which we\nexperimentally enhance with semantic knowledge extracted from Word-\nNet. Further features extracted from SenticNet prove to be beneficial for\nthe extraction of sentiment labels. As the best performing system in its\ncategory, our proposed system proves to be an effective approach for the\nAspect-Based Sentiment Analysis.\n1 Introduction\nThe World Wide Web contains customer reviews for all kinds of topics and en-\ntities such as products, movies, events, restaurants and more. The wealth of\ninformation that is expressed in these reviews in the form of the writer’s opin-\nion offers valuable opportunities and insights for customers and businesses alto-\ngether. However, due to the vast amounts of customer reviews that are available\nin the Web, the manual extraction and analysis of these opinions is infeasible and\nthus requires automated tools. First attempts to extract opinions automatically\nhave focused on extracting an overall polarity on a document or sentence level.\nThis, however, is a too coarse-grained approach as it neglects huge amounts of\ninformation in these reviews.\nIn a more fine-grained way, Sentiment analysis can be regarded as a relation\nextraction problem in which the sentiment of some opinion holder towards a\nar\nX\niv\n:1\n70\n9.\n06\n31\n1v\n1 \n [c\ns.C\nL]\n  1\n9 S\nep\n 20\n17\ncertain aspect of a product needs to be extracted. The following example clearly\nshows that the mere extraction of an overall polarity for a sentence is not suffi-\ncient:\nThe serrated portion of the blade is sharp\npos\nbut the straight edge is\nmarginal at best\nneg\n.\nwhere aspect terms are outlined with solid boxes, opinion phrases with dashed\nones, opinion polarities are displayed as superscripts, and aspect-opinion depen-\ndencies are depicted as arrows. Sentiment analysis needs to be regarded thus on\na more fine-grained level that allows to assign sentiments to individual aspects\nin order to extract complex opinions more accurately.\nIn this work, we present a system that competes in the ESWC 2016 Challenge\non Semantic Sentiment Analysis addressing the task of Aspect-Based Sentiment\nAnalysis. The goal of this task is to extract a set of aspect terms with their\nrespective binary polarities (positive and negative) from a given sentence. The\nsentences in the overall dataset are extracted from online reviews from different\ndomains (restaurants, laptops and hotels). We approach the problem in two\nsteps: i) the extraction of aspect terms and ii) the assignment of a polarity label\nto each extracted aspect term. Following this approach, we design a modular,\nneural network based architecture that is easy to extend.\nIn the following, we give a brief overview of related work in the field of\naspect-based sentiment analysis. Afterwards, we present our overall system and\ndescribe its two main components and the features we employ. We further analyse\nthe performance of our architecture on both subtasks and give insights into its\npredictive performance. Lastly, we conclude the paper and give suggestions for\nfurther improvements.\n2 Related Work\nOur work is inspired by different related approaches for sentiment analysis. Over-\nall, our work is in line with the growing interest of providing more fine-grained,\naspect-based sentiment analysis [15,14,23], going beyond a mere text classifica-\ntion or regression problem that aims at predicting an overall sentiment for a\ntext.\nSan Vicente et al. [24] present a system that addresses opinion target extrac-\ntion as a sequence labeling problem based on a perceptron algorithm with local\nfeatures. The extraction of a sentiment polarity for an extracted opinion target is\nperformed using an SVM. The approach uses a window of words around a given\nopinion target and classifies it based on a set of features such as word clusters,\nPart-of-Speech tags and polarity lexicon features.\nToh and Wang [32] propose a Conditional Random Field (CRF) as a sequence\nlabeler that includes a variety of features such as POS tags and dependencies,\nword clusters and WordNet taxonomies. Additionally, the authors employ a lo-\ngistic regression classifier to address aspect term polarity classification.\nJakob and Gurevych [11] follow a very similar approach that addresses opin-\nion target extraction as a sequence labeling problem using CRFs. Their approach\nincludes features derived from words, POS tags and dependency paths, and per-\nforms well in a single and cross-domain setting.\nKlinger and Cimiano [13,14] have modeled the task of joint aspect and opinion\nterm extraction using probabilistic graphical models and rely on Markov Chain\nMonte Carlo methods for inference. They have demonstrated the impact of a\njoint architecture on the task with a strong impact on the extraction of aspect\nterms, but less so for the extraction of opinion terms.\nLakkaraju et al. [15] present a recursive neural network architecture that is\ncapable of extracting multiple aspect categories1 and their respective sentiments\njointly in one model or separately using two softmax classifiers. They show that\nthe joint modeling of aspect categories and sentiments is beneficial for the pre-\ndictive performance of their system.\nAnother way to address opinion extraction is the summarization of reviews.\nHu and Liu [10] present an approach that summarizes reviews based on the\nproduct features for which an opinion is expressed using data mining and natural\nlanguage processing techniques. Similarly, Titov and McDonald [30] describe a\nstatistical model for joint aspect and sentiment modeling for the summarization\nof reviews. The method is based on Multi-Grain Latent Dirichlet Allocation\nwhich models global and local topics extended by a Multi-Aspect Sentiment\nModel.\nLastly, the general idea expressed in this paper to incorporate semantic web\ntechnologies in a machine learning framework for sentiment analysis is rooted in\nprevious contributions of ESWC Challenges [27,1,7,4].\n3 Aspect-Based Sentiment Analysis\nWe follow a two-step approach in designing a system that is capable of extract-\ning a writer’s sentiment towards certain aspects of an entity (such as a product\nor restaurant). As a first step, given a text, the system extracts explicitly ex-\npressed aspects2 in this text. Secondly, each extracted aspect term is processed\nindividually and a sentiment value is assigned given the context of the aspect\nterm.\nThis two-step approach allows us to extract an arbitrary amount of aspects\nfrom a text. Additionally, by decoupling the aspect extraction from the senti-\nment extraction, the system is also applicable to settings where aspect terms are\nalready given and only the individual sentiments towards these aspects need to\n1 Here, we distinguish between the terminologies of aspect category extraction\nand aspect term extraction: The set of possible aspect categories is predefined\nand rather small (e.g. Price, Battery, Accessories, Display, Portability,\nCamera), while aspect terms can take many shapes (e.g. “sake menu”, “wine se-\nlection” or “French Onion soup”).\n2 Parts of a sentence that refer to an aspect of the product, event, entity, etc.\nbe extracted. The following sections elaborate on our design and feature choices\nfor our aspect and sentiment extraction components.\n3.1 Features\nIn this section, we describe the features we use to address aspect term extrac-\ntion and aspect-specific sentiment extraction. For both sub tasks, we lowercase\neach input sentence and tokenize it using a simple regular expression in a pre-\nprocessing step. We do not remove punctuations or stopwords, but keep them\nintact.\nWord Embeddings The most important features that we use are pretrained\nword embeddings which have been successfully employed in numerous NLP tasks\n[6,25,16,20,22]. We use the skip-gram model [20] with negative sampling on a\nhuge corpus of ≈ 83 million Amazon reviews [18,19] to compute 100 dimensional\nword embeddings. In total, our computation of word embeddings yields vectors\nfor ≈ 1 million words. For this work, however, we reduce this vocabulary to only\ncontain the 100,000 most frequent words. The resulting vocabulary is denoted\nas V .\nIn a pre-processing step, we replace rare words that appear less than 10 times\nin our dataset with a special token <UNK> and learn a placeholder vector for this\ntoken. At test time, we use this token as a replacement for Out-of-Vocabulary\nwords. The sequence of word embedding vectors for a sentence with words3\n1 . . . N is denoted as:\n[w]N1 = {w1, . . . , wN} with wi ∈ R100.\nBy using this domain-specific dataset we expect to obtain embeddings that\ncapture the semantics of each word for our targeted domain more closely than\nembeddings trained on domain-independent data. A welcomed side effect of us-\ning this huge dataset of reviews is that we also obtain word embeddings for\nmisspelled forms of a word that appear commonly in reviews. As shown in Table\n1, the learned representation of a misspelled word is in many cases very close4\nto its correctly spelled counterpart.\nAlthough our approach technically works without any features apart from\nword embeddings, we are interested in improving its performance by means of\nsemantic web technology. For that, we employ features derived from two graph-\nbased semantic resources: WordNet and SenticNet.\nRetrofitting Word Embeddings to WordNet Although word embeddings\nhave been shown to encode semantic and syntactic features of their respective\nwords well [21,25,20], we try to enhance their encoded semantics by using a\n3 For a more convenient notation, we use words and their respective indices inter-\nchangeably.\n4 We use the euclidean vector distance as a distance measure.\nWord speed quality display\nNearest Neighbors\nspped qualtiy displays\nspeeds qualilty diplay\nspeeed qulaity dislay\nTable 1. Three commonly used words in product reviews and their 3 nearest neighbors\nin the embedding space. Often, misspelled versions (italic) of the original word are\namong its closest neighbors.\nlexical resource. For this, we employ a technique called retrofitting [8]. The idea\nbehind retrofitting is to iteratively adapt precomputed word vectors to better\nfit the (lexical) relations modeled in a given lexical resource. The graph-based\nalgorithm gradually “moves” each word vector towards the word vectors of its\nneighboring nodes while still staying close to its original position.\nFormally, following the notation by Faruqui et al. [8], let V = {v1, . . . , vNV }\nbe the considered vocabulary of size NV and Wˆ = (wˆ1, . . . , wˆ|V |) with wi ∈\nRd are their respective precomputed word vectors. G = (V,E) is the graph of\nsemantic relationships to which we want to fit the word vectors with (vi, vj) ∈\nE ⊆ V × V denoting the edges between words. With W = (w1, . . . , w|V |) being\nthe fitted word vectors, the algorithm tries to minimize the following objective\nfunction:\nΨ(Wˆ ,W ) =\n|V |∑\ni=1\n[\nαi||wi − wˆi||2 +\n∑\n(i,j)∈E\nβij ||wi − wj ||2\n]\n(1)\nThe online update rule for each wi is then:\nwi =\n∑\nj:(i,j)∈E βijwj + αiwˆi∑\nj:(i,j)∈E βij + αi\n(2)\nwhere α and β are parameters of the retrofitting procedure.\nIn this work, we chose WordNet [9] as our lexico-semantic resource. We con-\nstruct a subgraph of the WordNet relations that links each word in our vocabu-\nlary to all its synonyms (lemma names) in the WordNet graph. We set all αi = 1\nand all βij = 1/degree(i) and run the retrofitting algorithm for 10 iterations.\nThe resulting embeddings are still very similar to their original embeddings, yet\nincorporate part of the semantics of WordNet. We investigate the benefit of using\nthese retrofitted word embeddings in comparison to their original counterparts\nin section 4.\nSenticNet SenticNet 3 [2] is a graph-based, concept-level resource for seman-\ntic and affective information. For each of the 30,000 concepts that are part\nof the knowledge graph, SenticNet 3 provides real-valued scores for 5 sentics:\npleasantness, attention, sensitivity, aptitude, polarity.\nWe experimentally include the provided scores in our system as an addi-\ntional input source that our networks can draw information from. Since these\nsentics encode information about the semantics and polarity of a concept, the\naspect-specific sentiment extraction component is expected to benefit from the\nadditional information in particular. For that, we construct a 5-dimensional fea-\nture vector sc for each concept c that is represented in SenticNet 3. We refer to\nthese vectors as sentic vectors.\nUnfortunately, our system is not designed to process text on a concept level\nbut only on a word level. Therefore, we omit all multi-word concepts (e.g.\nnotice problem or beautiful music) in SenticNet 3 and only keep single-word\nconcepts (e.g. experience or improvement) that are part of our vocabulary V .\nDoing that, we can treat the sentic vector si as an additional word vector for the\nword i. To account for Out-of-Vocabulary words during test time, we provide\na default vector sunk = 0. The sequence of sentic vectors for a sentence with\nwords 1 . . . N is denoted as:\n[s]N1 = {s1, . . . , sN} with si ∈ R5.\nPart-of-Speech Tags Apart from these word embeddings and sentic vectors,\nour system can incorporate other features as well. For each word in a text, Part-\nof-Speech (POS) tags can be provided that might aid both the aspect extraction\nand aspect-specific sentiment extraction components. When including POS tags,\nwe employ a 1-of-K coding scheme that transforms each tag into a K-dimensional\nvector that represents this specific tag. Specifically, we use the Stanford POS\nTagger [17] with a tag set of 45 tags. These vectors are then concatenated with\ntheir respective word vectors before being fed to the extraction components. The\nsequence of POS tag vectors for a sentence with words 1 . . . N is denoted as:\n[p]N1 = {p1, . . . , pN} with pi ∈ R45.\n3.2 Aspect Term Extraction\nOur first step in extracting aspect-based sentiment from a text is the extraction\nof mentioned aspect terms. We propose a system to extract an arbitrary number\nof aspect terms from a given text by framing the extraction as a sequence labeling\nproblem. For this, we encode expressed aspect terms using the IOB2 tagging\nscheme [31]. According to this scheme, each word in our text receives one of 3\ntags, namely I, O or B that indicate if the word is at the Beginning, Inside or\nOutside of an annotation:\nThe sake menu should not be overlooked !\nO B I O O O O O\nThis tagging scheme allows us to encode multiple non-overlapping aspect terms\nat once. Ultimately, each tag is represented as a 1-of-K vector:\nI =\n10\n0\n , O =\n01\n0\n , B =\n00\n1\n .\nWe design a neural network based sequence tagger that reads in a sequence\nof words and predicts a sequence of corresponding IOB2 tags that encode the\ndetected aspect terms. Figure 1 depicts the neural network component.\nFig. 1. The aspect term extraction component. The network processes the input sen-\ntence as a sequence of word vectors wi, sentic vectors si and POS tags pi using a\nbidirectional GRU layer and regular feed-forward layers. The output of the network is\na predicted tag sequence in the IOB2 format. The aspect term that is to be predicted\nis outlined in the input sentence.\nNeural Network Sequence Tagger The procedure to generate a tag sequence\nfor a given word sequence can be described as follows: First, the sequence of words\nis mapped to a sequence of word embedding vectors [w]N1 = {w1, . . . , wN}, sentic\nvectors [s]N1 = {s1, . . . , sN} and POS tag vectors [p]N1 = {p1, . . . , pN} using the\nresources described in Section 3.1. We concatenate each word vector with its\ncorresponding sentic vector and POS tag vector to receive the sequence:\n[u]N1 = {u1, . . . , uN} = {(w1, s1, p1)T , . . . , (wN , sN , pN )T } with ui ∈ R100+5+45.\nThe resulting sequence is passed to a bidirectional layer [28] of Gated Recurrent\nUnits (GRU, [3]) that produces an output sequence of recurrent states:\n[g]N1 = BiGRU([u]\nN\n1 ) = {g1, . . . , gN} with gi ∈ R50,\nusing a combination of update and reset gates in each recurrent hidden unit.\nDespite its simpler architecture and less demanding computations, the GRU\nis shown to be a competitive alternative to the well-known Long Short-Term\nMemory [5]. In practice, we implement the bidirectional GRU layer as two sep-\narate GRU layers. One layer processes the input sequence in a forward direction\n(left-to-right) while the other processes it in reversed order (right-to-left). The\nsequences of hidden states of each GRU layer are concatenated element wise in\norder to yield a single sequence of hidden states:\n[g]N1 = {(−→g 1,←−g 1)T , . . . , (−→g N ,←−g N )T } with −→g i,←−g i ∈ R25,\nwhere −→g i and ←−g i are the hidden states for the forward and backward GRU\nlayer, respectively. Each hidden state gi is passed to a regular feed-forward layer\nthat produces a further hidden representation h′i ∈ R50 for that state. Lastly, a\nfinal layer in the network projects each h′i of the previous layer to a probability\ndistribution qi over all possible output tags, namely I, O or B, using a softmax\nactivation function:\n[q]N1 = {q1, . . . qN} with qi ∈ R3.\nFor each word, we choose the tag with the highest probability as its predicted\nIOB2 tag.\nSince the prediction of each tag can be interpreted as a classification, the\nnetwork is trained to minimize the categorical cross-entropy between expected\ntag distribution pi and predicted tag distribution qi of each word i:\nH(pi, qi) = −\n∑\nt∈T\npi(t) log(qi(t)),\nwhere T = {I,O,B} is the set of IOB2 tags, pi(t) ∈ {0, 1} is the expected\nprobability of tag t and q(t) ∈ [0, 1] the predicted probability. The network’s\nparameters are optimized using the stochastic optimization technique Adam [12].\nFor further processing, a predicted tag sequence can be decoded into aspect\nterm annotations using the IOB2 scheme in reverse. Note that we do not enforce\nthe syntactic correctness of the predicted IOB2 scheme on a network-level. It is\npossible that the network produces a tag sequence that is not correct in terms of\nthe employed IOB2 scheme. Thus, we post process each predicted tag sequence\nsuch that it constitutes a valid IOB2 tag sequence. Specifically, we replace each\nI tag that follows an O tag with a B in order to properly mark the beginning\nof an aspect term.\n3.3 Aspect-Specific Sentiment Extraction\nThe second step in our two-step architecture for aspect-based sentiment extrac-\ntion is the prediction of a polarity label given a previously detected aspect term.\nWe address this aspect-specific sentiment extraction using a recurrent neural net-\nwork that is, in parts, very similar to the architecture for aspect term extraction\nin Section 3.2.\nIn order to predict a polarity label for a specific aspect term in a sentence, we\nneed to mark the aspect term in question. For this, we apply a similar technique\nas has been done for relation extraction [33] and Semantic Role Labeling [6]. We\ntag each word in the input sentence with its relative distance to the aspect term,\nas follows:\nFig. 2. The aspect-specific sentiment extraction component. The network processes\nthe input sentence as a sequence of word vectors wi, sentic vectors si, POS tags pi\nand distance embeddings di using a bidirectional GRU layer and regular feed-forward\nlayers. The output of the network is a single predicted polarity label for the aspect term\nof interest. The aspect term for which a polarity label is to be predicted is outlined in\nthe input sentence.\nGreat service , great food .\n-1 0 1 2 3 4\nwhere the bold word “service” is the aspect term for which we want to extract\nthe polarity. The italic word “food” marks another aspect term. The relative\ndistance to the selected aspect term is shown below each word. This sequence of\nrelative distances implicitly encodes the position of the aspect term in question\nin the sentence. In theory, this strategy permits to incorporate long range infor-\nmation in the prediction process in contrast to cutting a fixed-sized (and usually\nsmall) window of words around the aspect term in the sentence. In practice,\nwe do not use the raw distance values directly but represent them as 10 di-\nmensional distance embedding vectors similar as in [26,33,29] and treat them as\nlearnable parameters in our network. We further denote the sequence of distance\nembedding vectors for a sentence of N words as:\n[d]N1 = {d1, . . . , dN} with di ∈ R10.\nFigure 2 depicts the neural network component.\nNeural Network Polarity Extraction The procedure for predicting a po-\nlarity label for an aspect term can be described as follows: Assume we have a\nsentence and an already extracted aspect term. We concatenate each word vec-\ntor with its corresponding sentic vector, its POS tag vector and distance vector\nto receive the sequence:\n[u]N1 = {(w1, s1, p1, d1)T , . . . , (wN , sN , pN , dN )T } with ui ∈ R100+5+45+10.\nThe resulting sequence is passed to a bidirectional GRU layer that produces an\noutput sequence of recurrent states:\n[g]N1 = BiGRU([u]\nN\n1 ) = {(−→g 1,←−g 1)T , . . . , (−→g N ,←−g N )T } with −→g i,←−g i ∈ R25.\nWe take the final hidden state −→g N of the forward GRU and the final hidden\nstate ←−g 1 of the backward GRU5 and concatenate them to receive a fixed sized\nrepresentation h = (−→g N ,←−g 1)T ∈ R50 of the aspect term in the whole input\nsentence. Next, the network passes the hidden representation h of the aspect\nterm through a densely connected feed-forward layer producing another hidden\nrepresentation h′ ∈ R50. As a last step, a final densely connected layer with a\nsoftmax activation function projects h′ to a 2-dimensional vector q ∈ R2 rep-\nresenting a probability distribution over the two polarity labels positive and\nnegative. We consider the label with the highest estimated probability to be\nthe predicted polarity label for the given aspect term.\nAgain, we train the network to minimize the categorical cross-entropy be-\ntween expected polarity label distribution p and predicted polarity label distri-\nbution q of each aspect term:\nH(p, q) = −\n∑\nl∈L\np(l) log(q(l)),\nwhere L = {positive, negative} is the set of polarity labels and p(l) and q(l)\nthe expected and predicted probability, respectively, for label l. As before, we\napply the Adam technique to update network parameters.\n4 Experiments and Evaluation\nIn order to see the performance of the overall system and the impact of the\nindividual features, we perform an evaluation on the provided training data for\nthe aspect-based sentiment analysis task. Based on that we select a final model\nconfiguration that is used in the actual challenge evaluation on additional test\ndata.\nEvaluation on Training Data All experiments on the training data are per-\nformed as a 5-fold cross-validation. We evaluate the two steps of our approach\nseparately to better see the individual performances of the two components.\nSince we do not have access to official evaluation scripts, we evaluate as-\npect term extraction using Precision, Recall and F1 score. We only take explic-\nitly mentioned aspect terms into account6 that have a polarity label of either\npositive or negative. Identical annotations i.e. annotations that target the\nsame aspect term (in terms of character offsets) with the same polarity, are con-\nsidered as one. Table 2 shows the results for aspect term extraction for different\nfeature combinations. Here, WE denotes the usage of amazon review word em-\n5 Since this GRU processes the sequence in a reversed direction, the final hidden state\nis the hidden state for the first word.\n6 We exclude annotations with aspect=“NULL”.\nFeatures F1 Precision Recall\nWE+POS 0.684 0.659 0.710\nWE+POS+Sentics 0.679 0.663 0.697\nWE-Retro+POS 0.678 0.651 0.708\nWE-Retro+POS+Sentics 0.679 0.655 0.706\nTable 2. Results of 5-fold cross-validation for aspect term extraction using different\nfeature combinations.\nbeddings, WE-Retro denotes the retrofitted embeddings, POS specifies additional\nPOS tag features and Sentics indicates the usage of sentic vectors.\nComparing the models in Table 2, we can see that using the retrofitted em-\nbeddings seems to downgrade the performance of our system. Also, employing\nthe sentic vectors for aspect term extraction degrades the networks performance.\nThis is not completely unexpected, though, since the sentic vectors mainly en-\ncode sentiment information and aspect term extraction on its own is rather\ndecoupled from the actual sentiment extraction. A more positive effect would be\nexpected for the second step in our system, the prediction of polarity labels.\nTo evaluate the aspect-specific sentiment extraction, we extract polarity la-\nbels for all aspect terms of the ground truth annotations. By separating aspect\nterm extraction and sentiment extraction, we can better evaluate the sentiment\nextraction in isolation. Again, we only consider unique aspect terms that are ei-\nther labeled with a positive or negative polarity. We report the performance\nof our sentiment extraction in terms of the accuracy of the system for different\nfeature combinations. Table 3 shows the results for the 5-fold cross-validation on\nthe training data. WE, WE-Retro, POS and Sentics are defined as before, while\nDist denotes the obligatory distance embedding features.\nWhile the retrofitted embeddings do not contribute positively to the per-\nformance for sentiment extraction either, a notable gain is achieved using the\nsentic vectors in our component for aspect-specific sentiment extraction. Here,\nwe observe a gain of 3.5% points accuracy compared to using only word em-\nbeddings, distance embeddings and POS tags. Apart from that, the usage of\nsentic vectors drastically reduces the training time needed to achieve these re-\nsults. The best results for the WE+POS+Dist and WE-Retro+POS+Dist model\nwere achieved with 102 iterations over the training portion of the data, while\nthe WE+POS+Dist+Sentic and WE-Retro+POS+Dist+Sentic model reached their\nbest performances for only 12 and 9 iterations, respectively. See Figure 3 for a\nvisualization of the system’s accuracy with respect to the employed features and\nthe iteration over the training data.\nEvaluation on Test Data Apart from our custom evaluation, each partici-\npating system is evaluated on a separate test set of customer reviews as part of\nthe Sentiment Analysis Challenge. While the annotated training data covers the\nFeatures Accuracy\nWE+POS+Dist 0.776\nWE+POS+Dist+Sentics 0.811\nWE-Retro+POS+Dist 0.776\nWE-Retro+POS+Dist+Sentics 0.809\nTable 3. Results of 5-fold cross-\nvalidation for aspect-specific sentiment\nextraction using different feature combi-\nnations.\n0 20 40 60 80 100\n#iterations\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAc\ncu\nra\ncy\nWE+POS+Dist\nWE+POS+Dist+Sentics\nFig. 3. Visualization of the performance\ngain of using sentic vectors with respect to\nthe number of iterations over the training\ndata. By using additional sentic vectors\nwe achieve better results with less train-\ning needed.\ndomains laptops and restaurants, the data for the test set is obtained from the\ndomains restaurants and hotels in order to test the systems on a previously un-\nseen review domain. For comparability, the predicted results for each system are\nevaluated by the organizers. Aspect term extraction is evaluated with precision,\nrecall and F1 score regarding exact matches. Polarity extraction is evaluated\nwith the accuracy of the predicted polarity label with respect to the subset of\ncorrectly extracted aspect terms from the previous step.\nFor this evaluation, we train final models for our two architectural compo-\nnents using knowledge gained from our preliminary results on the training data.\nThe aspect term extract model WE+POS is trained on all training samples for 5\nepochs and the polarity extraction model WE+POS+Dist+Sentics for 10 epochs.\nThe official evaluation on the test data shows an F1 score of 0.433 with a preci-\nsion of 0.415 and a recall of 0.452 for the aspect term extraction in separation.\nThe extraction of aspect-specific polarity labels for correctly identified aspect\nterms results in an accuracy of 0.874. With these results, the proposed system\nachieves the highest scores of the 2016th ESWC fine-grained sentiment analysis\nchallenge.\n5 Conclusion\nWith this work we propose a two-step approach for aspect-based sentiment anal-\nysis. We decouple the extraction of aspects and sentiment labels in order to\nobtain a flexibly applicable system. By using a recurrent neural network, we\npresent a novel neural network based approach to tackle aspect extraction as a\nsequence labeling task. Furthermore, we present a novel way to address aspect-\nspecific sentiment extraction using a recurrent neural network architecture with\ndistance embedding features. This model is able to extract sentiments expressed\ntowards a specific aspect that is mentioned in the text and is thus able to detect\nmultiple opinions in a single sentence.\nBoth components of our overall sentiment analysis system incorporate addi-\ntional semantic knowledge by using pretrained word vectors that are retrofitted\nto a semantic lexicon as well as semantic and sentiment-related features obtained\nfrom SenticNet. Although our first experiments could not show a benefit in us-\ning the retrofitted embeddings, the sentics obtained from SenticNet proved to\nbe a valuable feature for extracting aspect-based polarity labels that increased\naccuracy and shortened training time considerably.\nFor this work, we could only incorporate single-word concepts from SenticNet\nas additional features. For the future, we plan to modify our architecture to\npermit incorporation of all concepts from SenticNet, thus moving the system to\nconcept-level sentiment analysis even further.\nAcknowledgements\nThis work was supported by the Cluster of Excellence Cognitive Interaction\nTechnology ’CITEC’ (EXC 277) at Bielefeld University, which is funded by the\nGerman Research Foundation (DFG).\nReferences\n1. Aprosio, A.P., Corcoglioniti, F., Dragoni, M., Rospocher, M.: Supervised opinion\nframes detection with RAID. In: Gandon, F., Cabrio, E., Stankovic, M., Zim-\nmermann, A. (eds.) Semantic Web Evaluation Challenges - Second SemWebEval\nChallenge at ESWC 2015, Portorozˇ, Slovenia. Communications in Computer and\nInformation Science, vol. 548, pp. 251–263. Springer (2015)\n2. Cambria, E., Olsher, D., Rajagopal, D.: SenticNet 3: a common and common-\nsense knowledge base for cognition-driven sentiment analysis. In: Proceedings of the\nTwenty-Eighth AAAI Conference on Artificial Intelligence. pp. 1515–1521 (2014)\n3. Cho, K., Van Merrie¨nboer, B., Gu¨lc¸ehre, C¸., Bahdanau, D., Bougares, F., Schwenk,\nH., Bengio, Y.: Learning phrase representations using rnn encoder–decoder for sta-\ntistical machine translation. In: Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). pp. 1724–1734. Association\nfor Computational Linguistics, Doha, Qatar (Oct 2014)\n4. Chung, J.K., Wu, C., Tsai, R.T.: Polarity detection of online reviews using senti-\nment concepts: NCU IISR team at ESWC-14 challenge on concept-level sentiment\nanalysis. In: Presutti, V., Stankovic, M., Cambria, E., Cantador, I., Iorio, A.D.,\nNoia, T.D., Lange, C., Recupero, D.R., Tordai, A. (eds.) Semantic Web Evaluation\nChallenge - SemWebEval 2014 at ESWC 2014, Anissaras, Crete, Greece. Communi-\ncations in Computer and Information Science, vol. 475, pp. 53–58. Springer (2014)\n5. Chung, J., Gu¨lc¸ehre, C¸., Cho, K., Bengio, Y.: Empirical evaluation of gated re-\ncurrent neural networks on sequence modeling. In: NIPS Deep Learning Workshop\n(2014)\n6. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P.:\nNatural language processing (almost) from scratch. Journal of Machine Learning\nResearch 12, 2493–2537 (2011)\n7. Dragoni, M., Tettamanzi, A.G.B., da Costa Pereira, C.: A fuzzy system for concept-\nlevel sentiment analysis. In: Presutti, V., Stankovic, M., Cambria, E., Cantador,\nI., Iorio, A.D., Noia, T.D., Lange, C., Recupero, D.R., Tordai, A. (eds.) Semantic\nWeb Evaluation Challenge - SemWebEval 2014 at ESWC 2014, Anissaras, Crete,\nGreece. Communications in Computer and Information Science, vol. 475, pp. 21–\n27. Springer (2014)\n8. Faruqui, M., Dodge, J., Jauhar, S.K., Dyer, C., Hovy, E., Smith, N.A.: Retrofitting\nWord Vectors to Semantic Lexicons. Proceedings of Human Language Technolo-\ngies: The 2015 Annual Conference of the North American Chapter of the ACL pp.\n1606–1615 (2015)\n9. Fellbaum, C.: Wordnet and wordnets. In: Brown, K. (ed.) Encyclopedia of Lan-\nguage and Linguistics. pp. 665–670. Elsevier, Oxford (2005)\n10. Hu, M., Liu, B.: Mining and summarizing customer reviews. In: Proceedings of the\nTenth ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining (KDD ’04). pp. 168–177. ACM, New York, NY, USA (2004)\n11. Jakob, N., Gurevych, I.: Extracting opinion targets in a single-and cross-domain\nsetting with conditional random fields. In: Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing. pp. 1035–1045 (October 2010)\n12. Kingma, D., Ba, J.: Adam: A Method for Stochastic Optimization. International\nConference on Learning Representations (2015)\n13. Klinger, R., Cimiano, P.: Bi-directional inter-dependencies of subjective expres-\nsions and targets and their value for a joint model. In: Proceedings of the 51st\nAnnual Meeting of the Association for Computational Linguistics (ACL), Volume\n2: Short Papers. pp. 848–854 (August 2013)\n14. Klinger, R., Cimiano, P.: Joint and pipeline probabilistic models for fine-grained\nsentiment analysis: Extracting aspects, subjective phrases and their relations. In:\nProceedings of the 13th IEEE International Conference on Data Mining Workshops\n(ICDM). pp. 937–944 (December 2013)\n15. Lakkaraju, H., Socher, R., Manning, C.: Aspect Specific Sentiment Analysis using\nHierarchical Deep Learning. Proceedings of the NIPS Workshop on Deep Learning\nand Representation Learning (2014)\n16. Le, Q., Mikolov, T.: Distributed Representations of Sentences and Documents.\nICML 32, 1188–1196 (2014)\n17. Manning, C., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S., McClosky, D.: The\nStanford CoreNLP Natural Language Processing Toolkit. In: Proceedings of 52nd\nAnnual Meeting of the Association for Computational Linguistics: System Demon-\nstrations. pp. 55–60 (2014)\n18. McAuley, J., Pandey, R., Leskovec, J.: Inferring networks of substitutable and\ncomplementary products. In: Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (KDD ’15). pp. 785–794.\nACM, New York, NY, USA (2015)\n19. McAuley, J., Targett, C., Shi, Q., van den Hengel, A.: Image-based recommen-\ndations on styles and substitutes. In: Proceedings of the 38th International ACM\nSIGIR Conference on Research and Development in Information Retrieval. pp.\n43–52. ACM (2015)\n20. Mikolov, T., Corrado, G., Chen, K., Dean, J.: Efficient Estimation of Word Rep-\nresentations in Vector Space. In: Proceedings of the International Conference on\nLearning Representations (2013)\n21. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-\nsentations of words and phrases and their compositionality. In: Advances in Neural\nInformation Processing Systems. pp. 3111–3119 (2013)\n22. Pennington, J., Socher, R., Manning, C.: Glove: Global Vectors for Word Represen-\ntation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP). pp. 1532–1543. Association for Computational\nLinguistics, Doha, Qatar (2014)\n23. Pontiki, M., Galanis, D., Papageorgiou, H., Manandhar, S., Androutsopoulos, I.:\nSemeval-2015 task 12: Aspect based sentiment analysis. In: Proceedings of the\n9th International Workshop on Semantic Evaluation. pp. 486–495. Association for\nComputational Linguistics, Denver, Colorado (June 2015)\n24. San Vicente, I.n., Saralegi, X., Agerri, R.: Elixa: A modular and flexible ABSA\nplatform. In: Proceedings of the 9th International Workshop on Semantic Evalu-\nation. pp. 748–752. Association for Computational Linguistics, Denver, Colorado\n(June 2015)\n25. dos Santos, C., Zadrozny, B.: Learning character-level representations for part-of-\nspeech tagging. In: Proceedings of the 31st International Conference on Machine\nLearning. pp. 1818–1826 (2014)\n26. dos Santos, C.N., Xiang, B., Zhou, B.: Classifying relations by ranking with con-\nvolutional neural networks. In: Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing. vol. 1, pp. 626–634 (2015)\n27. Schouten, K., Frasincar, F.: The benefit of concept-based features for sentiment\nanalysis. In: Gandon, F., Cabrio, E., Stankovic, M., Zimmermann, A. (eds.) Se-\nmantic Web Evaluation Challenges - Second SemWebEval Challenge at ESWC\n2015, Portorozˇ, Slovenia. Communications in Computer and Information Science,\nvol. 548, pp. 223–233. Springer (2015)\n28. Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE Trans-\nactions on Signal Processing 45(11), 2673–2681 (1997)\n29. Sun, Y., Lin, L., Tang, D., Yang, N., Ji, Z., Wang, X.: Modeling mention, context\nand entity with neural networks for entity disambiguation. In: Proceedings of the\n24th International Conference on Artificial Intelligence (IJCAI). pp. 1333–1339.\nAAAI Press (2015)\n30. Titov, I., Mcdonald, R.: A Joint Model of Text and Aspect Ratings for Senti-\nment Summarization. In: Proceedings of Annual Meeting of the Association for\nComputational Linguistics (ACL). pp. 308–316 (2008)\n31. Tjong Kim Sang, E.F., Veenstra, J.: Representing text chunks. In: Proceedings of\nEuropean Chapter of the ACL (EACL). pp. 173–179. Bergen, Norway (1999)\n32. Toh, Z., Wang, W.: DLIREC: Aspect Term Extraction and Term Polarity Classi-\nfication System. In: Proceedings of the 8th International Workshop on Semantic\nEvaluation. pp. 235–240 (2014)\n33. Zeng, D., Liu, K., Lai, S., Zhou, G., Zhao, J.: Relation Classification via Convolu-\ntional Deep Neural Network. In: Proceedings of the 25th International Conference\non Computational Linguistics (COLING). pp. 2335–2344 (2014)\n",
      "id": 44608631,
      "identifiers": [
        {
          "identifier": "1709.06311",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "255164657",
          "type": "CORE_ID"
        },
        {
          "identifier": "2529471236",
          "type": "MAG_ID"
        },
        {
          "identifier": "93947612",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1709.06311",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1007/978-3-319-46565-4_12",
          "type": "DOI"
        }
      ],
      "title": "Aspect-Based Sentiment Analysis Using a Two-Step Neural Network\n  Architecture",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1709.06311"
      ],
      "publishedDate": "2017-09-19T01:00:00",
      "publisher": "'Springer Science and Business Media LLC'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1709.06311"
      ],
      "updatedDate": "2021-08-04T17:01:17",
      "yearPublished": 2017,
      "journals": [
        {
          "title": null,
          "identifiers": [
            "1865-0929"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1709.06311"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/44608631"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Lai, Chapmann C.L."
        },
        {
          "name": "Lau, Raymond Y.K."
        },
        {
          "name": "Li, Yuefeng"
        },
        {
          "name": "Ma, Jian"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/301346171"
      ],
      "createdDate": "2013-07-02T14:31:28",
      "dataProviders": [
        {
          "id": 310,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/310",
          "logo": "https://api.core.ac.uk/data-providers/310/logo"
        },
        {
          "id": 10830,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/10830",
          "logo": "https://api.core.ac.uk/data-providers/10830/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Automated analysis of the sentiments presented in online consumer feedbacks can facilitate both organizations’ business strategy development and individual consumers’ comparison shopping. Nevertheless, existing opinion mining methods either adopt a context-free sentiment classification approach or rely on a large number of manually annotated training examples to perform context sensitive sentiment classification. Guided by the design science research methodology, we illustrate the design, development, and evaluation of a novel fuzzy domain ontology based contextsensitive opinion mining system. Our novel ontology extraction mechanism underpinned by a variant of Kullback-Leibler divergence can automatically acquire contextual sentiment knowledge across various product domains to improve the sentiment analysis processes. Evaluated based on a benchmark dataset and real consumer reviews collected from Amazon.com, our system shows remarkable performance improvement over the context-free baseline",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/10904076.pdf",
      "fieldOfStudy": null,
      "fullText": "This is the author’s version of a work that was submitted/accepted for pub-\nlication in the following source:\nLau, Raymond Y.K., Lai, Chapmann C.L., Ma, Jian, & Li, Yuefeng (2009)\nAutomatic domain ontology extraction for context-sensitive opinion mining.\nIn ICIS 2009 Proceedings, AIS Electronic Library, Phoenix, Arizona, pp.\n35-53.\nThis file was downloaded from: http://eprints.qut.edu.au/42065/\nc© Copyright 2009 [please consult the authors]\nNotice: Changes introduced as a result of publishing processes such as\ncopy-editing and formatting may not be reflected in this document. For a\ndefinitive version of this work, please refer to the published source:\nAssociation for Information Systems\nAIS Electronic Library (AISeL)\nICIS 2009 Proceedings International Conference on Information Systems(ICIS)\n1-1-2009\nAutomatic Domain Ontology Extraction for\nContext-Sensitive Opinion Mining\nRaymond Y.K. Lau\nCity University of Hong Kong, raylau@cityu.edu.hk\nChapmann C.L. Lai\nCity University of Hong Kong, chunllai@cityu.edu.hk\nJian Ma\nCity University of Hong Kong, isjian@cityu.edu.hk\nYuefeng Li\nQueensland University of Technology, Y2@qut.edu.au\nThis material is brought to you by the International Conference on Information Systems (ICIS) at AIS Electronic Library (AISeL). It has been\naccepted for inclusion in ICIS 2009 Proceedings by an authorized administrator of AIS Electronic Library (AISeL). For more information, please\ncontact elibrary@aisnet.org.\nRecommended Citation\nLau, Raymond Y.K.; Lai, Chapmann C.L.; Ma, Jian; and Li, Yuefeng. \"Automatic Domain Ontology Extraction for Context-Sensitive\nOpinion Mining\" (2009). ICIS 2009 Proceedings. Paper 35.\nhttp://aisel.aisnet.org/icis2009/35\n Thirtieth International Conference on Information Systems, Phoenix 2009 1 \nAUTOMATIC DOMAIN ONTOLOGY EXTRACTION FOR \nCONTEXT-SENSITIVE OPINION MINING \nCompleted Research Paper \nRaymond Y.K. Lau \nDepartment of Information Systems \nCity University of Hong Kong \nTat Chee Avenue, Kowloon,  \nHong Kong SAR \nraylau@cityu.edu.hk  \nChapmann C.L. Lai \nDepartment of Information Systems \nCity University of Hong Kong \nTat Chee Avenue, Kowloon,  \nHong Kong SAR \nchunllai@cityu.edu.hk \nJian Ma \nDepartment of Information Systems \nCity University of Hong Kong \nTat Chee Avenue, Kowloon,  \nHong Kong SAR \nisjian@cityu.edu.hk \nYuefeng Li \nSchool of Information Technology \nQueensland University of Technology \nGPO Box 2434, Brisbane, Qld 4001 \nAustralia  \nY2@qut.edu.au \nAbstract \nAutomated analysis of the sentiments presented in online consumer feedbacks can facilitate both \norganizations’ business strategy development and individual consumers’ comparison shopping. \nNevertheless, existing opinion mining methods either adopt a context-free sentiment classification \napproach or rely on a large number of manually annotated training examples to perform context-\nsensitive sentiment classification. Guided by the design science research methodology, we \nillustrate the design, development, and evaluation of a novel fuzzy domain ontology based context-\nsensitive opinion mining system. Our novel ontology extraction mechanism underpinned by a \nvariant of Kullback-Leibler divergence can automatically acquire contextual sentiment knowledge \nacross various product domains to improve the sentiment analysis processes. Evaluated based on \na benchmark dataset and real consumer reviews collected from Amazon.com, our system shows \nremarkable performance improvement over the context-free baseline.   \nKeywords:  Opinion Mining, Sentiment Analysis, Kullback-Leibler divergence, Fuzzy Sets, \nDomain Ontology, Ontology Extraction, Sentiment Context.  \n \nIntroduction \nWith the norm of users contributed data in the era of Web 2.0, increasingly more people have submitted or retrieved \nindividual viewpoints about products, organizations, or political issues via a variety of Web-based channels such as \nBlogs, forums, social networks, and e-Commerce sites. Due to the problem of information overload (Lau et. al. \n2008; Lau and Lai 2008), manually browsing a large number of consumer reviews posted to the Web may not be \nfeasible, if not totally impossible. The huge volume of documents (e.g., consumer reviews) archived on the Web has \ntriggered the development of intelligent tools to automatically extract, analyze, and summarize their contents. \nOpinion mining is also referred to as opinion analysis, sentiment analysis, or subjectivity analysis (Abbasi et. al. \n2008; Turney and Littman 2003; Wright 2009). Opinion mining differs from Information Retrieval (IR) in that it \naims at extracting the viewpoints about some entities rather than simply identifying the topical information about the \nentities (Macdonald and Ounis 2007; Wilson et. al. 2004). Analyzing the sentiment of consumer feedbacks posted to \nBlogs, forums, or e-Commerce sites can generate huge business values for organizations (Archak et. al. 2007; \nDanescu-Niculescu-Mizil et. al. 2009). Although consumer reviews are subjective in nature, these reviews are often \nData and Web Mining Track \n \n2 Thirtieth International Conference on Information Systems, Phoenix 2009  \nconsidered more creditable and trustworthy than other traditional information sources from the perspectives of \ncustomers (Bickart and Schindler 2001; Wright 2009). In this paper, we will illustrate a novel opinion mining \nmethodology which can automatically extract, analyze, and summarize consumers’ reviews about various products \nwith reference to the specific product contexts. \nThe Research Challenges and Our Contributions \nThough traditional sentiment analysis or opinion mining was performed at the document level (Dave et. al. 2003; \nTurney and Littman 2003), increasingly more research has examined opinion mining at the more fine-grained \nsentence or phrase level in recent years (Agarwal et. al. 2009; Xu et. al. 2008; Wilson et. al. 2005). Even if a review \n(i.e., document) is rated as positive, negative sentiments could appear in the same review. Therefore, opinion mining \nagainst consumers’ reviews is often performed at the product feature level to provide deep analytics for the target \nproduct (Archak et. al. 2007; Hu and Liu 2004; Popescu and Etzioni 2005). The quest for a more fine-grained \nopinion mining method is driven by the fact that sentiment words are often context-dependent (Agarwal et. al. \n2009). For instance, while the token “small” in the expression “the hotel room is so small” implies a negative \nsentiment, the same token may have a positive meaning in another situation such as “it’s so convenient to bring a \nsmall notebook for a business trip”. Another example is that “unpredictable” has a negative orientation in the \ncontext of “automotive”. However, the same sentiment has a positive orientation such as “unpredictable plot” in the \ncontext of “movie”. In fact, the token “unpredictable” has a strong negative orientation defined in sentiment lexicons \nsuch as OpinionFinder (Wilson et. al. 2005) and SentiWordNet (Esuli and Sebastiani 2005). Therefore, using \nlexicon-based approach alone may not provide an effective solution for context-sensitive opinion mining.  \nLinguistic or inference-based method can deal with sentiment analysis for some general cases, but there are many \nsituations (particularly down to the phrase level) that the general rules or inference process could not be applied. For \nexample, no general linguistic rule can be applied to detect the polarity of the sentiment “small” in the expression \n“The camera is good in general; the viewer panel is small”. On the other hand, machine learning methods usually \nrequire a large number of manually labeled training examples to build an accurate classifier. Nevertheless, manually \nannotating a large number of review messages at the phrase level is extremely labor intensive and expensive. Even \nthough attempts are made to mine consumers’ reviews at the product feature level, the polarities of sentiments are \nassumed the same across product domains (i.e., context-free) (Archak et. al. 2007; Hu and Liu 2004; Popescu and \nEtzioni 2005). For instance, “small” is often assumed negative no matter it is referring to a hotel room or the \nphysical size of a Netbook computer.  Indeed, it has been pointed out that developing an automated technique for \nbuilding sentiment lexicon is an important topic for research and practices in opinion mining (Macdonald and Ounis \n2007), and contextual domain knowledge is essential to improve the performance of opinion mining systems (Bao \net. al. 2008)    \nThe main contributions of our research are: (1) the design of a novel context-sensitive opinion mining methodology \nto improve the effectiveness of sentiment analysis; (2) the extension of a fuzzy domain ontology extraction method \n(i.e., learning the non-taxonomic fuzzy relations) for the automatic construction of sentiment lexicons; (3) the \ndevelopment of a novel computational method to predict the context-sensitive polarities of sentiments; (4) the design \nand development of a prototype system for context-sensitive sentiment analysis. The practical implication of our \nwork is that an effective opinion mining methodology is developed to enhance both organizations’ business strategy \ndevelopment and individuals’ comparison shopping processes.    \nResearch Methodology \nOur research work is driven by the “Design Science” research methodology (Hevner et al. 2004).  The design \nscience research methodology emphasizes on the discovery of novel knowledge of a problem domain by the \nconstruction and application of “designed artifacts”. Such artifacts should also be rigorously evaluated, and they \nshould contribute to address relevant business problems. For our research, the designed artifacts include a \nmethodology for context-sensitive opinion extraction and prediction, a fuzzy domain ontology based computational \nmodel for the representation of context-sensitive sentiment lexicon, and an instantiation of the design by the \nconstruction of a Web-based context-sensitive opinion mining prototype system. Our design is based on sound \ntheories with rigorous theoretical foundations. For example, the fuzzy domain ontology extraction method is \ndeveloped based on fuzzy sets and fuzzy relations (Zadeh 1965) which offer the expressive power to capture the \nuncertainty presented in opinion mining. Our approach of predicting the polarities of sentiments is based on a well-\nknown statistical learning technique, a variant of Kullback-Leibler divergence (Kullback and Leibler 1951). \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 3 \nMoreover, our designed artifacts are rigorously evaluated based on a benchmark e-Commerce dataset and real \nconsumer reviews retrieved from a popular e-Commerce Website. Above all, our designed artifacts make significant \ncontributions to improve both organizations and individuals’ capabilities of analyzing the sheer volume of consumer \nfeedbacks posted to the Web these days. As a result, organizations can develop appropriate marketing and product \ndesign strategies quickly and individuals can conduct comparison shopping easily.  As a whole, our research is \ndriven by the processes of designing and developing the artifacts (e.g., a fuzzy domain ontology based \ncomputational model for context-sensitive opinion mining) to improve business strategy development and \nindividuals’ online shopping experience. The main research questions can be summarized as follows: \nHow can we apply a fuzzy domain ontology extraction method to automatically build domain specific sentiment \nlexicons to facilitate context-sensitive opinion mining?   \nCan we develop an effective sentiment polarity classification method which does not rely on extra human effort to \nannotate training examples? \nIs the proposed ontology-based context-sensitive opinion mining approach more effective than a context-free \nopinion mining approach? \nOutline of the Paper \nThe remainder of the paper is organized as follows. The next section highlights previous research related to opinion \nmining and ontology learning, which is followed by the architectural design of an ontology-based context-sensitive \nopinion mining system. The computational models for fuzzy domain ontology extraction and context-sensitive \nopinion mining are then illustrated. The quantitative evaluation of our prototype system is reported afterwards. \nFinally, we offer concluding remarks and describe future direction of our research work. \nRelated Research \nA light weight fuzzy domain ontology extraction method has been developed to automatically generate concept \nhierarchies based on textual contents extracted from online message boards (Lau et. al. 2009).  The algorithm of \nfuzzy domain ontology extraction includes concept extraction, concept pruning, dimensionality reduction, and fuzzy \nrelation extraction. Fuzzy relation extraction involves the generation of taxonomic relations using the structural \nsimilarity (SSIM) metric developed in the field of image analysis.  Formal concept analysis (Cimiano et. al. 2005) \nand fuzzy formal concept analysis (Tho et. al. 2006) have also been applied to build domain ontology automatically. \nFormal concept analysis is a systematic method for deriving implicit relationships among concepts described by a \nset of attributes (Wille 2005). For the research work reported in this paper, we utilize a simplified version of the \nfuzzy domain ontology model (Lau et. al. 2009) for sentiment knowledge representation. In particular, we develop \neffective computational methods to learn the non-taxonomic relations among concepts (e.g., products, product \nfeatures, and sentiments) to support context-sensitive opinion mining. \nAn econometric opinion mining method has been proposed to analyze product feature evaluations expressed in \nonline consumer reviews (Archak et. al. 2007). Each product feature is represented by a noun which frequently \nappears in the consumer reviews. A manual procedure is then involved to filter the candidate nouns to identify \ncorrect product features. The adjectives collocated with product features are taken as the sentiment words. A pair of \nproduct feature and sentiment (also called an opinion phrase) is formally represented by a vector in the tensor \nproduct space. Hedonic regressions are applied to estimate the relative weights of product features and the strength \nof the sentiments associated with those features. OPINE employs the “relaxation labeling” classification method \ndeveloped by the computer visioning research community to detect sentiment polarity (Popescu and Etzioni 2005). \nSimilarly, Feature-Based Summarization (FBS) system has been developed to extract explicit product features and \nsentiments at the sentence level (Hu and Liu 2004).  The Apriori association rule mining algorithm is applied to \nextract the product features (i.e., noun phrases) frequently occurring in product reviews. A similar product feature \nextraction method is also applied to a product review mining system (Miao et. al. 2008). The ReviewSeer system \nadopts an n-gram approach for feature extraction and a machine learning approach for sentiment polarity \nclassification (Dave et. al. 2003). For the aforementioned opinion mining systems, polarity detection of sentiments is \nnot conducted with respect to a particular product domain. Our proposed opinion mining approach supports context-\nsensitive polarity detection rather than assuming that the polarity of a sentiment is the same across different product \ndomains.  \nData and Web Mining Track \n \n4 Thirtieth International Conference on Information Systems, Phoenix 2009  \nA hybrid lexicon and machine learning based approach has been applied to extract the sentiments from online stock \nmessage boards and then classify the discussions as bullish, bearish, or neutral (Das and Chen 2007). Sentiment \nidentification is conducted based on the General Inquirer sentiment lexicon (Stone et. al. 1966); five statistical or \nmachine learning classifiers coupled with a voting scheme are applied to classify the polarity of each message.  Our \napproach differs in the sense that we focus on applying a statistical learning method to automatically build a context-\nsensitive sentiment lexicon rather than relying on the manually crafted sentiment lexicons for polarity detection. \nEntropy Weighted Genetic Algorithm (EWGA) has been developed to select the best syntactic (e.g., POS pattern) \nand stylistic features (e.g., number of special characters used in a document) for multilingual (e.g., English and \nArabic) sentiment classification against various extremist online forums (Abbasi et. al. 2008). The EWGA algorithm \nselects the most informative features (e.g., n-gram1) according to information gain and passing those features to a \nSVM classifier for polarity classification (e.g., positive or negative) at the document level.  Based on the technique \nof bootstrapping, a classification accuracy of 91% is achieved over a benchmark movie dataset (Pang et. al. 2002). \nInstead of employing machine learning approaches, our opinion mining system utilizes a statistical learning method \ni.e., a variant of Kullback-Leibler divergence, to detect the polarity of sentiments at the product feature level. \nIn the field of IR, Probabilistic Latent Semantic Analysis (PLSA) which is underpinned by the unigram language \nmodeling approach is proposed to predict sentiment orientations in movie blog posts (Liu et. al. 2007). The PLSA \nmodel is combined with a time series analysis model (called autoregressive model) to predict the gross revenues of \nmovies.  PLSA is also applied to combine opinions expressed in a well-written expert review with those retrieved \nfrom Web 2.0 sources such as blog posts to generate a comprehensive opinion summary about a product or a \npolitical figure (Lu and Zhai 2008). Probabilistic generation language models are explored to identify and rank \nsentiment expressions at the document level (Zhang and Ye 2008).  Instead of applying a probabilistic language \nmodeling approach to opinion mining, we propose to address the problem of opinion mining using a fuzzy approach \ne.g., modeling the association between a product feature and a sentiment in terms of a fuzzy relation. In the field of \nmachine learning, the problem of automatically identifying sentiment orientations across different domains is called \nthe “Domain-Transfer” problem (Tan et. al. 2007; Tan et. al. 2008). A method called Relative Similarity Ranking \n(RSR) is proposed to select the most informative unlabeled opinionated documents from a training set to re-train a \nclassifier (e.g., Support Vector Machine). Instead of identifying the most informative training examples, we employ \nan efficient statistical learning technique to automatically build a domain dependent sentiment lexicon based on the \ntraining set pertaining to each product domain. \nLinguistic rules are applied to detect the context-sensitive orientations of sentiments extracted from online customer \nreviews (Ding and Liu 2007).  For example, for the sentence “This camera takes great pictures and has a long \nbattery life”, the orientation of the sentiment “long” is classified as positive because it is conjoined with the positive \nseeding sentiment “great”. An inference-based opinion mining method called Semantic Orientation (SO) analysis \nhas been developed to estimate the polarity of sentiments (Hatzivassiloglou and McKeown 1997; Turney and \nLittman 2003). The SO of an arbitrary word can be estimated based on the strength of association between the word \nand fourteen seeding sentiment words such as good, nice, bad, poor, and so on. Point-wise Mutual Information \n(PMI) is proposed to compute the strength of association between any pair of words. Our system also employs a \nvariant of Mutual Information to estimate the strength of associations between product features and sentiment \nwords. However, polarity detection is underpinned by a variant of Kullback-Leibler divergence.  \nContext-sensitive sentiment analysis has been an active research topic in the Natural Language Processing (NLP) \nresearch community (Wilson et. al. 2005; Wilson et. al. 2006).  A sentence is first parsed and represented by a \ndependency tree. A set of linguistic features are used to train the AdaBoost classifier to predict the sentiment \norientation of a target word.  An appraisal group is represented by a set of attribute values in some task-independent \nsemantic taxonomies such as attitude, orientation, graduation, and polarity (Whitelaw et. al. 2005). The appraisal \ngroup method has been applied to analyze the sentiments of a movie review corpus. Apart from utilizing the fuzzy \ndomain ontology, our system also employs basic syntactical features to infer sentiment polarity. However, instead of \nusing sophisticated NLP techniques which are computationally expensive, we adopt a light-weight NLP approach so \nthat our opinion mining system can scale up for the sheer volume of users contributed feedback data generated in the \nera of Web 2.0. \n \n                                                          \n1\n An n-gram is a term with n consecutive words. \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 5 \nThe Architectural Design of an Ontology Based Opinion Mining System  \nThe general system architecture of our Ontology Based Product Review Miner (OBPRM) system is depicted in \nFigure 1. A user first selects a product category and a specific product for opinion mining (Task 1 in Figure 1). \nBased on the selected target product, the OBPRM system will use the Web services or APIs provided by e-\nCommerce sites (e.g., Amazon.com2 and Cnet.com3) and Internet Search Engines (e.g., Google4) to retrieve the \nconsumer reviews for the particular product (Task 2 in Figure 1). In addition, the crawlers of our system can also be \ninvoked to retrieve information about product features and download consumer reviews (Task 3 in Figure 1). \nTraditional document pre-processing procedures (Salton et. al. 1975; Salton and McGill 1983) such as stop word \nremoval, Part-of-Speech (POS) tagging, and stemming (Porter 1980) are then invoked to process the consumer \nreviews and product descriptions (Task 4 in Figure 1). We develop our POS tagger based on the WordNet lexicon \n(Miller et. al. 1990) and the publicly available WordNet API5.  Similar to previous studies, a product feature is \nrepresented by a Noun or a Noun compound (Archak et. al. 2007; Hu and Liu 2004; Popescu and Etzioni 2005), and \nsentiment words are represented by Adjective or Adverb (Subrahmanian and Reforgiato 2008).  \n  \n \nFigure 1.  The General System Architecture of OBPRM  \n \nOntology extraction (Task 5 in Figure 1) is carried out offline and it must be performed before context-sensitive \nmining (Task 6 in Figure 1) is conducted. The fuzzy domain ontology captures taxonomic information such as \n“iPhone” (product) “is-a” mobile phone (product category), and non-taxonomic relationship such as “screen” \n(product feature) is “associated with” “iPhone” (product). In addition, context-sensitive sentiment orientation (e.g., \n“excellent”) of a product feature (e.g., “screen”) is also captured in the fuzzy domain ontology.  Consumer reviews, \nproduct ratings, and product descriptions can be retrieved from e-Commerce sites; this information is fed to the \nontology extraction module to automatically build the fuzzy domain ontology. The details about ontology extraction \nwill be described in the following section.  Based on the fuzzy domain ontology, manually crafted sentiment \nlexicons, and basic NLP rules, the opinion mining module can analyze each pair of product feature and sentiment (f, \n                                                          \n2\n http://ecs.amazonaws.com/AWSECommerceService/AWSECommerceService.wsdl \n3\n http://api.cnet.com/ \n4\n http://code.google.com/apis/ajaxsearch/ \n5\n http://wordnet.princeton.edu/ \nData and Web Mining Track \n \n6 Thirtieth International Conference on Information Systems, Phoenix 2009  \ns) and determine its polarity. By aggregating the polarity scores of product features from all the reviews, a final \nsentiment score and a polarity label can be generated for the target product. The presentation manager is responsible \nfor delivering the opinion mining results as well as visualizing the fuzzy domain ontology (Task 7 in Figure 1). Our \nprototype system6 was developed using Java (J2SE v 1.4.2), Java Server Pages (JSP) 2.1, and Servlet 2.5. The \nsystem is hosted on a DELL 1950 III Server with 16GB main memory and running under Apache Tomcat 6.0. \nThe design choices of OBPRM can be explained based on the merits of proven technologies. Firstly, an ontology \nbased knowledge representation for the automatically generated sentiment lexicon is justified because formal \nontology such as the W3C’s Web Ontology Language (OWL)7 facilitates knowledge exchange between humans and \nsystems on the Web (Fikes et. al. 2004).  Therefore, representing our sentiment lexicon as domain ontology \nfacilitates the extraction and reuse of the sentiment knowledge across various Web applications. Secondly, as the \nproblem of sentiment lexicon construction is viewed as the process of ontology learning, existing ontology \nextraction techniques (Lau et. al. 2009; Tho et. al. 2006) can be applied to build a sentiment lexicon automatically.  \nIn particular, the notions of fuzzy set and fuzzy relation can be applied to capture the uncertainty presented in the \nproblem domain (Zadeh 1965). Although machine learning techniques have been explored for context-sensitive \nopinion mining, a large number of manually labeled training examples at the phrase level are often required to train \nan accurate classifier. Given the sheer volume of user contributed opinion data in the era of Web 2.0, our proposed \nstatistical learning approach which does not rely on manually annotated training examples is desirable; our method \ncan scale up to process the ever growing opinion data on the Web. Finally, our proposed method also utilizes proven \nIR techniques such as Term Frequency Inverse Document Frequency (TFIDF) (Salton 1990) and Rocchio learning \n(Rocchio 1971) for product feature extraction, and the Keyword Classifier (Kindo et. al. 1997; Lau et. al. 2008) for \nsentiment polarity prediction. These methods have been empirically tested and they are efficient enough to process \nopinionated documents of a Web scale.           \n \nFuzzy Domain Ontology Extraction \nOntology is generally considered as a formal specification of conceptualization which consists of concepts and their \nrelationships (Gruber 1993). Domain ontology is one kind of ontology which is used to represent the knowledge for \na particular type of application domain (e.g., a consumer product domain) (Dittenbach et. al. 2004). Our model of \nfuzzy domain ontology is underpinned by fuzzy sets and fuzzy relations (Zadeh 1965). The fuzzy domain ontology \noffers the expressive power such that the uncertainty related to sentiment polarity prediction can be properly \ncaptured. In particular, the light weight fuzzy domain ontology model is developed based on the formal model \npublished in (Lau et. al. 2009). Our light weight fuzzy domain is defined as follows:  \nDefinition 1. Fuzzy Set: A fuzzy set F  consists of a set of objects drawn from a domain X  and the membership of \neach object ix  in F  is defined by a membership function [0 1]F Xµ : ,a .  \nDefinition 2. Fuzzy Relation: A fuzzy relation XYR  is defined as the fuzzy set R  on a domain X Y×  where X  \nand Y  are two crisp sets. The membership of each object ( )i ix y,  in R  is defined by a membership function \n[0 1]R X Yµ : × ,a .  \nDefinition 3. Fuzzy Domain Ontology: A fuzzy domain ontology is a triple \nNTAX TAXOnt C R R= , , where C is a set of \nconcepts (classes). The fuzzy relation [0 1]NTAXR C C: × ,a  defines the strength of the non-taxonomic relationship \nfor each pair (\ni ic c, ) in NTAXR , and the fuzzy relation [0 1]TAXR C C: × ,a  defines the strength of the taxonomic (sub-\nclass/super-class) relationship for each pair ( i ic c, ).  \nWith reference to our application, C  represents the set of products, product categories, sentiments, and so on. For \nour application, the taxonomy relations TAXR  are specified by the users. When a user inquires about the sentiment \nof a product, they will select the product category pertaining to the product via our system interface. The main focus \nof our fuzzy domain ontology extraction method is to automatically learn the non-taxonomic fuzzy relation \n                                                          \n6\n http://quantum.is.cityu.edu.hk/OM_web/login.jsp \n7\n http://www.w3.org/TR/owl-features/ \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 7 \nNTAXR (e.g., “Associated”) among the classes ic C∈ . A conceptual view of the fuzzy domain ontology when it is \napplied to our opinion mining problem is depicted in Figure 2. Our fuzzy domain ontology can be formally \nrepresented by a standard representation language such as OWL. \n \n \nFigure 2.  The Fuzzy Domain Ontology \n \nExtracting Product Features \nFor each product category defined in our system, the set of product features associated with the category is acquired \nvia an offline ontology building process. Since each product belongs to a product category, the common product \nfeatures associated with each product will implicitly be associated with a product category via the “is-a” relation. \nUsing the APIs provided by e-Commerce sites, our system can retrieve the product descriptions of a set of products \nunder a product category. Crawler programs and the APIs of Google can also be used to collect product descriptions \nfor each relevant product. Standard document-processing procedures (Task 4 in Figure 1) are applied to each \nproduct description document retrieved from the Web. Normalized TFIDF weighting scheme (Salton 1990) is \napplied to extract the most informative noun patterns to represent the product features of a particular product ip . \nFor each product description document d, the weight ( , ) [0,1]iw f d ∈ of a product feature if  is derived by: \n2\n2\n2\n( )0.5 0.5 log( ) ( )( , ) \n( )(0.5 0.5 ) log( ) ( )j\ni\ni\ni\nj\nf d j\ntf f N\nMaxtf d df f\nw f d\ntf f N\nMaxtf d df f∈\n \n+ ⋅ \n =\n \n+ ⋅  \n \n∑\n                                                                  (1) \nwhere the term ( )itf f  is the term frequency of if  in d, and ( )idf f  is the document frequency of if  in the \ncollection of product descriptions retrieved from the Web (i.e., how many times if  occurs in the product \ndescriptions). The function ( )Maxtf d  returns the maximal term frequency from a product description d. \n| |\nip\nN D=  is cardinality of the set of product descriptions \nip\nD retrieved for the product ip . Finally, the fuzzy \nmembership of a product feature for a product is approximated by the mean TFIDF weights of if  over the \ncollection of product descriptions \nip\nD :  \nData and Web Mining Track \n \n8 Thirtieth International Conference on Information Systems, Phoenix 2009  \n( , )\n(f ,p ) | |\npi\nNTAX\ni\ni\nd D\nR i i\np\nw f d\nD\nµ\n∈\n≈\n∑\n                                                                                                 (2) \nDuring opinion mining of consumer reviews (Task 6 in Figure 1), the product features if  of each review d are also \nextracted using the aforementioned process. Let d\nr\n be a vector of product feature weights extracted from a review d. \nAfter the opinion mining process, the vectors of product feature weights derived from the set of consumer reviews \nRe vD are applied to update the (f ,p )NTAXR i iµ  of the fuzzy domain ontology using an approach similar to the \nRocchio learning  method  (Rocchio 1971):  \nRe\n, 1 ,\nRe v\ni t i t\nd Dv\ndF F\nD d\nβ\nα+\n∈\n= × + × ∑\nr\nr r\nr               (3) \nwhere \n, 1 2( , ), ( , ), , ( , )NTAX NTAX NTAXi t R i R i R n iF f p f p f pµ µ µ=< >\nr\nL  is the original vector of product feature weights \n(i.e., ( , )\nNTAXR i i\nf pµ ) for product ip . The parameters α = β = 0.5 were applied to our experiments; dr  is the norm \n(length) of a product feature vector d\nr\n. After Rocchio learning, an updated set of product features and their weights \n, 1i tF +\nr\n is obtained for the product. While new product features may be added, the weakest product features are \nremoved after Rocchio learning.  The parameter fϖ =50 controls how many product features retained for the \nproduct feature vector \n, 1i tF +\nr\n.  \nExtracting Sentiments Related to Product Features \nSimilar to product feature extraction, a set of consumer reviews is used to build the non-taxonomic relations \nbetween sentiments and product features via an offline learning process. The Adjectives or Adverbs associated with \nthe product features (measured by a text window of size \nwinϖ ) within a review are extracted as the candidate \nsentiments (Subrahmanian and Reforgiato 2008). The meaning of 1winϖ =  is that the adjective or adverb next to \nproduct feature from both sides are extracted. Our system takes into account the sentence boundary as well. Within a \ntext window, the negation of sentiment will be taken into account. For instance, if words such as “no”, “not”, \n“except”, and so on is found, the negation of the sentiment word is assumed (Das and Chen 2007; Ding and Liu \n2007). By means of Balanced Mutual Information (BMI) which has successfully been applied to build fuzzy domain \nontology (Lau et. al. 2009), we can identify the sentiments that are highly associated with a given product feature: \n2\n2\n2\n(s ,f ) ( )\n( ) 1[ ( ) log ( )( ) ( )\n( ) 1\n  ( ) log ( )]( ) ( )\n( ) 1\n                 (1 ) [ ( ) log ( )( ) ( )\n             \nNTAXR i i i j\ni j\nBMI i j\ni j\ni j\ni j\ni j\ni j\nBMI i j\ni j\nBMI t t\nPr t t\nPr t t\nPr t Pr t\nPr t t\nPr t t\nPr t Pr t\nPr t t\nPr t t\nPr t Pr t\nµ\nϖ\nϖ\n≈ ,\n, +\n= × , +\n¬ ,¬ +\n¬ ,¬ −\n¬ ¬\n,¬ +\n− × ,¬ +\n¬\n2\n( ) 1\n      ( ) log ( )]( ) ( )\ni j\ni j\ni j\nPr t t\nPr t t\nPr t Pr t\n¬ , +\n¬ ,\n¬\n                 (4) \nwhere ( , )\nNTAXR i i\ns fµ  is the membership function to estimate the degree of association between a sentiment is  and a \nproduct feature if .  The advantage of the BMI measure is that it takes into account both term presence and term \nabsence as the evidence of the implicit term association. The parameter [0.5,0.7]BMIϖ ∈  was used to adjust the \nrelative weight of positive and negative evidence respectively (Lau et. al. 2009; Lau 2003). ( )i jPr t t,  is the joint \nprobability that both terms appear in a text window, and ( )iPr t  is the probability that a term it  appears in a text \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 9 \nwindow. The probability ( )iPr t  is estimated based on \ntw\nw\n| |\n| |  where tw| |  is the number of windows containing the \nterm t  and w| |  is the total number of windows constructed from a corpus. Similarly, ( )i jPr t t,  is the fraction of the \nnumber of windows containing both terms out of the total number of windows.  After computing the BMI scores, \nthe top 30\nS\nϖ = sentiments associated with a product feature can be extracted.  All the BMI scores are subject to a \nlinear normalization process (i.e., Min\nMax Min\nBMI BMI\nnormal BMI BMIBMI\n−\n− |= ) such that ( , ) [0,1]NTAXR i is fµ ∈  is maintained. The degree \nof association between the pair ( , )i is f is incrementally updated based on \n1 ( , ) ( , ) ( , )\nNTAX NTAX\nt t\nR i i R i i normal is f s f BMI s fµ α µ β+ = × + × after scanning a new collection of consumer reviews. \nLearning Context-Sensitive Sentiment Polarity \nOur objective is to develop a fuzzy membership function to estimate (( , ), )\nNTAXR i i i\ns f oµ  given a product ip  and a \nfeature if , where { , , }io positive negative neutral∈  is a sentiment orientation label. Instead of relying on \nmanually tagged examples to train a classification function as in (Wilson et. al. 2005), we would like to develop an \nautomated method such that context-sensitive sentiment polarity can be acquired across different product domains. \nThe basic intuition is that a positive consumer review is more likely to contain positive sentiment and feature pairs \n( , )i is f  than a negative review does. Therefore, we may use the sentiment polarity label of a consumer review to \ninfer the sentiment polarity of an individual product feature within the review. Based on the theory of Kullback-\nLeibler (KL) divergence (Kullback and Leibler 1951), an effective measure called Keyword Classifier (KC) has \nbeen developed to identify positive, negative, and neutral keywords representing an information seeker’s positive, \nnegative, or neutral information needs (Kindo et. al. 1997; Lau et. al. 2008).  Instead of summing the probabilities \ncharacterizing the positive and negative events as in the original KL divergence formulation, the KC measure takes a \nsubtraction between the conditional probabilities related to the positive and the negative events.  Such a formulation \ncorresponds to our intuition of weighting positive and negative sentiments presented in consumer reviews. For the \nOBPRM system, we apply the KC measure to determine the polarity and the corresponding strength of a \n( , )i it s f=  pair extracted from a review. As the ratings of consumer reviews are readily available from e-\nCommerce sites, we can obtain the polarity label for a review by using the respective APIs. For example, an \nAmazon rating of 4-5 can be regarded as positive, and a rating of 1 can be taken as negative; a mid range rating of 2-\n3 is considered neutral.  The KC formulation is shown as follows (Kindo et. al. 1997; Lau et. al. 2008): \n2\n2\n( ) Pr( | )( ) tanh( Pr( | ) log\nPr( )\n( ) Pr( | )Pr( | ) log )\nPr( )\npos\nneg\ndf t pos tKC t pos t\npos\ndf t neg t\nneg t\nneg\nϖ\nϖ\n= × × −\n× ×\n                 (5) \n \n \n( )\n      if ( )\n1\n| ( ) |( )          if ( )\n1\n0 otherwise\nKC\nKC\nKC\nKC\nOnt KC\nKC\nKC t KC t\nKC t\npolarity t KC t\nϖ\nϖ\nϖ\nϖ\nϖ\nϖ\n− > −\n  −\n= − < −  − \n\n\n\n                      (6) \nThe parameters posϖ  and negϖ control the learning rates for positive and negative evidences respectively, and they \ncan be established empirically (Kindo et. al. 1997; Lau et. al. 2008). The hyperbolic tangent function tanh ensures \nthe induced polarity scores fall in the unit interval. ( )Pr( | ) ( )\nposdf tpos t\ndf t=\n is the estimated conditional probability that a \nreview is positive given that it contains the particular sentiment feature pair ( , )i it s f= ; it can be derived based on \na set of consumer reviews (i.e., the context). Pr( | )pos t  is estimated based on the fraction of the number of positive \nData and Web Mining Track \n \n10 Thirtieth International Conference on Information Systems, Phoenix 2009  \nreviews which contain the pair (i.e., ( )posdf t ) over the total number of reviews which contain the pair (i.e., \n( )df t ). Similarly,   neg( )Pr( | ) ( )\ndf t\nneg t\ndf t=\n is the estimated conditional probability that a review is negative if it \ncontains the pair t. The document frequency ( )negdf t  represents the number of negative reviews which contain the \npair t.  In addition, Re\nRe Re\n| |Pr( ) | | | |\nv\nv v\nDpos\nD D\n+\n+ −= +\n  ( Re\nRe Re\n| |Pr( ) | | | |\nv\nv v\nD\nneg\nD D\n−\n+ −\n=\n+\n)  is the priori probability that a review is \npositive (negative) respectively; RevD+  ( RevD− ) is the set of positive (negative) reviews rated by consumers. A \npositive ( )Ontpolarity t score indicates that the underlying pair of sentiment and product feature is positive, \nwhereas a negative ( )Ontpolarity t score implies that the pair is negative. If the polarity score is zero, the pair is \nconsidered neutral. Similarly, the membership function can be incrementally updated by \n1 (( , ), ) (( , ), ) ( , )\nNTAX NTAX\nt t\nR i i i R i i i Ont is f o s f o polarity s fµ α µ β+ = × + ×  at training stage 1t + . \nContext-Sensitive Opinion Mining \nIdentification of Product Features and Sentiments  \nGiven a fuzzy domain ontology which holds the context-sensitive sentiment polarities of product features, it is \npossible to improve the accuracy of opinion mining (Task 6 in Figure 1). The main input to the OBPRM system is a \nuser’s query about a product. Driven by such a query, all the consumer reviews are retrieved by means of OBPRM’s \nAPIs and crawlers. Before sentiment analysis is applied to each consumer review, standard document pre-processing \nprocedures (e.g., stop word removal, POS tagging, and stemming) are applied to the review document (Task 4 in \nFigure 1).  As illustrated in the previous section, normalized TFIDF weighting is applied to extract the most \nrepresentative product features from each review. In addition, low frequency candidate product features (Hu and Liu \n2004; Yang et. al. 2007) are identified by matching the target tokens with the common product features stored in the \nfuzzy domain ontology.  Candidate sentiments which are close (within a text window of size \nwin\nϖ ) to the product \nfeatures are then identified and selected according to the normalized BMI scores. As mentioned in the previous \nsection, the negation of a sentiment word is taken into account by our system if a negation indicator such as “no”, \n“not”, “except”, and so on is found in the same text window of the sentiment (Das and Chen 2007; Ding and Liu \n2007).  \nPredicting the Polarities of Reviews and Products \nOur system first applies the fuzzy domain ontology to determine the strength and the polarity for each ( , )i is f  pair. If \nthere is a sentiment polarity that cannot be resolved, the system will apply the linguistic rules (Ding and Liu 2007) \nstored in the NLP rule base to determine the sentiment polarity. If the NLP rules cannot be applied to the current \ntarget, a default sentiment lexicon such as SentiWordNet (Esuli and Sebastiani 2005) is invoked to determine the \npair’s polarity. If no classification can be done after utilizing all the sentiment sources, a neutral polarity will be \nassigned to the ( , )i is f  pair. The overall polarity score Re ( , )vD i ipolarity s f  of the pair ( , )i is f over a collection of \nconsumer reviews \nRe vD is the mean polarity score computed based on the weighted polarity scores generated from \nindividual reviews.  The polarity score ( )docpolarity d  for a review d is derived by:  \n( , )\n( , )\n( ) | |\ni i\nsource i i\ns f d\ndoc\npolarity s f\npolarity d\nd\nϖ\n∈\n×\n=\n∑\n                  (7) \nwhere ( , ) :: ( , ) | ( , ) | ( )i i Ont i i NLP i i Lexicon ipolarity s f polarity s f polarity s f polarity s=  represents the context-sensitive \nsensitive polarity score derived from the fuzzy domain ontology ( , )Ont i ipolarity s f , or  the polarity score inferred \nbased on our linguistic rules and the seeding sentiment words ( , )NLP i ipolarity s f ,  or the context-free polarity score \ndetermined based on the generic sentiment lexicons ( )Lexicon ipolarity s . If the polarity of a pair ( , )i is f  is defined in \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 11 \nthe system generated sentiment lexicon, the polarity inferred based on the linguistic rules module or the manual \nlexicon module will be ignored.  The weight factor  sourceϖ  defines the importance of the polarity score when it is \nextracted from different sources e.g., system generated lexicon with weight Ontϖ , linguistic rules with weight \nNLPϖ , or generic sentiment lexicon with weight Lexiconϖ .  If a negation indicator is associated with the pair \n( , )i is f , the sign of the polarity score ( , )i ipolarity s f  will be reversed. The parameters \n( 1, 0.5Ont NLP Lexiconϖ ϖ ϖ= = = ) are used to tune the relative weights of the polarity scores generated from the \nrespective sources. The term | |d returns the cardinality of d in terms of the number of sentiment and product feature \npair found in d. Three simple linguistic rules proposed by (Ding and Liu 2007) and fourteen seeding sentiment \nwords used by (Turney and Littman 2003) are used to build our NLP module for polarity detection.  Finally, the \npolarity score Pr ( )o ipolarity p for a product ip  is derived by: \n  \nRe( ), , ( , )\nPr\nRe\n(f ,p ) (s ,f ) ( , )\n( ) | | | |\nNTAX NTAX\ni i v i i\nR i i R i i i i\nf F p d D s f d\no i\nv\npolarity s f\npolarity p\nD d\nµ µ\n∈ ∈ ∈\n× ×\n=\n×\n∑ ∑\n                    (8) \nwhere (f ,p )\nNTAXR i i\nµ  is the product and product feature association weight (i.e., the fuzzy membership) defined in the \nfuzzy domain ontology and (s ,f )\nNTAXR i i\nµ  is the fuzzy membership of the relation ( , )i is f . The polarity score \n( , )i ipolarity s f  of a ( , )i is f pair is computed based on the fuzzy domain ontology, the NLP module, or the sentiment \nlexicon. A threshold Pr 0.05oϖ =  is established empirically to determine the ultimate polarity of the product. If the \nproduct score \nPr ( )o ipolarity p  is greater than Pr oϖ , the product will be labeled as positive; if the product score is \nleas than - Pr oϖ , the product will be labeled as negative; otherwise it is considered neutral. \nExperiments and Results \nSimilar to previous studies (Archak et. al. 2007; Hu and Liu 2004), we retrieved real consumer reviews from \nAmazon.com using the Amazon Web services APIs.  Our evaluation work was based on eight Amazon product \ncategories such as Cameras, Mobile Phones, Watches, Laptops, Sport Equipment, and so on. The average length of \nthese reviews is 139.4 words, and the average number of unique words per product category is 34,549.4 words. For \neach product category, 5,000 consumer reviews together with the corresponding product ratings (in the scale of 1 to \n5) were downloaded. So, our dataset included 40K reviews in total. For our experiments, we treated the ratings of 4-\n5 as positive and the rating of 1 as negative. The evaluation metrics included precision, recall, accuracy, and F-\nmeasure (van Rijsbergen 1979): \naprecision\na b\n=\n+\n                 (9) \n \na\nrecall\na c\n=\n+\n                 (10) \n \n2\n1 2\n(1 ) 2\n2\nprecision recall aF\nprecision recall a b cβ\nβ\nβ=\n+ × ×\n= =\n× + + +\n            (11) \na d\naccuracy\na b c d\n+\n=\n+ + +\n               (12) \nWith reference to a confusion matrix, a, b, c, d refer to the number of correctly classified positive (negative) \nreviews, the number of classified non-positive (negative) reviews, the number of non-classified positive (negative) \nreviews, and the number of non-classified non-positive (negative) reviews. Ten-fold cross validation was employed \nto evaluate our system (Mitchell 1997). In each experimental run, one randomly selected sub-sample (a fold) was \nretained as the validation set and the remaining 9 sub-samples were used as the training set (Archak et. al. 2007). \nData and Web Mining Track \n \n12 Thirtieth International Conference on Information Systems, Phoenix 2009  \nThis procedure was repeated 10 times and we took the average result as the respective measurement score.  With our \nnovel automated domain ontology learning method, manual tagging of the consumer reviews is not required. \nExperiment One \nIn the first experiment, we tried to compare the performance between context-sensitive and context-free opinion \nmining methods. For each product pertaining to some consumer reviews, we used our crawler to retrieve the product \nfeature or the product description section from the respective product page of Amazon.com. (Eq. 1) and (Eq. 2) were \nthen used to select at most fifty features (i.e., fϖ =50) with each feature represented by a single noun. (Eq. 4-6) were \napplied to learn the sentiment orientations from the training set of reviews; the automatically acquired contextual \nsentiment knowledge is stored in our fuzzy domain ontology. Based on the empirical testing for several sub-\nsamples, the parameters ( 100, 10, 0.02pos neg KCϖ ϖ ϖ= = = ) were established. After learning the fuzzy domain \nontology in each run, we applied (Eq. 7) to predict the polarities of the reviews of the validation set.  To facilitate the \ncomparison with published classification results (Hu and Liu 2004; Popescu and Etzioni 2005), we also adopted a \ntwo-class (i.e., positive and negative) classification procedure. If the polarity score ( )docpolarity d of a review d was \ngreater than zero, the review was classified as positive; otherwise it was negative. For the baseline runs (context-free \nopinion mining), ontology learning was not invoked. Polarity prediction was conducted based on the NLP module \nand the sentiment lexicon module only.  \n \nTable 1. Comparative Performance Between the Baseline System and OBPRM \nBaseline System OBPRM Improvement Product Category F-Measure Accuracy F-Measure Accuracy F-Measure Accuracy \nToys 0.7672 0.6588 0.8345 0.7562 +8.78% +14.78% \nSport Equipment 0.7683 0.6616 0.8274 0.7526 +7.70% +13.75% \nWatches 0.8091 0.7044 0.8871 0.8260 +9.64% +17.26% \nLaptops 0.7994 0.6922 0.8918 0.8342 +11.55% +20.51% \nMotorcycle Parts 0.7845 0.6920 0.8624 0.8044 +9.93% +16.24% \nCameras 0.7928 0.6872 0.8740 0.8100 +10.25% +17.87% \nMP3 Players 0.7830 0.6902 0.8718 0.8174 +11.33% +18.43% \nMobile Phones 0.7630 0.6616 0.8479 0.7856 +11.12% +18.74% \nAverage 0.7834 0.6810 0.8621 0.7983 +10.04% +17.22% \n \nTable 1 tabulates the F-measure and the accuracy results pertaining to various product categories for the baseline \nsystem and the ontology-based opinion mining system (OBPRM) respectively. Figure 3 plots the comparative \nperformance between the two systems in terms of accuracy scores across the eight product categories. As shown in \nTable 1, the average improvements across the eight product categories are +10.04% and +17.22% in terms of F-\nmeasure and Accuracy respectively. The performance (F-measure and Accuracy) of the OBPRM system is \nsignificantly better than that of the baseline system according to Wilcoxon signed rank test (p<0.01). The product \ncategory of laptop computers has the largest improvement in terms of Accuracy and F-measure. The intuitive \nexplanation of this improvement is that sentiments such as “small”, “tiny”, “little”, and so on usually have negative \npolarities defined in sentiment lexicons such as SentiWordNet (Esuli and Sebastiani 2005) or OpinionFinder \n(Wilson et. al. 2005). However, for the context of laptop computers, these sentiments often imply a positive \norientation (e.g., “a small laptop consuming little time for configuration”). According to our data analysis, 60.5% of \nthe sentiments found in the laptop category are “weak subjective” as defined according to OpinionFinder. These \nweak subjective sentiments (e.g., “tiny”) are usually context-dependent, and so they become the source of \nperformance improvement after our context-sensitive sentiment polarity detection.  \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 13 \n \nFigure 3.  Comparative Performance Between OBPRM and the Baseline \n \nTable 2. Context-Sensitive Sentiments for the Product Category of Laptops \nPositive Sentiments Negative Sentiments \nFeatures Sentiments Polarity Features Sentiments Polarity \ndrive hard 0.999997 battery defective -0.85361 \ngb hard 0.999567 screen blue -0.83926 \nkeyboard great 0.997659 laptop hopeless -0.82427 \nlaptop great 0.996959 problem called -0.82290 \nlaptop small 0.996951 product average -0.79484 \nbattery long 0.995394 light lit -0.78244 \nbattery great 0.992897 warranty damaged -0.78244 \ncomputer great 0.989026 laptop blue -0.73699 \nwork great 0.986654 hp called -0.73208 \nsize small 0.969728 acer average -0.73208 \nscreen size 0.965520 support called -0.73208 \nbattery good 0.964125 laptop damaged -0.71784 \ndrive external 0.963565 product defective -0.67862 \nkeyboard easy 0.959373 customer called -0.67216 \nkeyboard good 0.959208 machine frustrating -0.63849 \ncomputer small 0.952062 no stopped -0.61123 \nmachine great 0.949557 problem searching -0.61031 \nlife great 0.946326 product faulty -0.60185 \nweight light 0.945798 wireless hopeless -0.60185 \nram hard 0.942497 pixels poor -0.60185 \n \nSamples of the product features, sentiments, and the corresponding polarity scores captured in our fuzzy domain \nontology are illustrated in Table 2.  Only the top 20 positive and negative sentiments are depicted in the table. As \nshown in Table 2, it is interesting to find that the usual negative sentiment “small” as defined in generic sentiment \nlexicons (Esuli and Sebastiani 2005; Wilson et. al. 2005) is shown to be positive (e.g., small size laptop) in the \ncontexts of Laptops. Another interesting finding is that general product name such as “laptop” which is not usually \nregarded as product feature is treated as an implicit feature from the perspective of the review writers. Therefore, \nData and Web Mining Track \n \n14 Thirtieth International Conference on Information Systems, Phoenix 2009  \nwhen a product feature based sentiment lexicon is built, explicit or implicit product features extracted from \nconsumer reviewers should be used to substitute the initial product features extracted from generic product \ndescription by applying an appropriate learning mechanism such as (Eq.3).   \nExperiment Two \nEarlier studies indicated that representation schemes of product features and the proximity consideration for \nsentiments identification might have impact on the accuracy of opinion mining (Hu and Liu 2004; Popescu and \nEtzioni 2005). Our second experiment examined the impact of product feature representation (with varying length of \nnoun patterns) and sentiment identification (with different proximity to the identified product feature) on the overall \nperformance of context-sensitive opinion mining. We tried a proximity factor from 1 (sentiments immediately next \nto the identified product feature on both sides) to 10 words, and representing product features by “Noun (N)”, “Noun \nNoun (NN)”, or “Noun Noun Noun (NNN)” patterns. The other parameters and the dataset were the same as those \nused in experiment one. Figure 4 depicts the average accuracy of the OBPRM system with varying proximity factor \nand length of product feature for the 8 product categories.  It is shown that a unigram (single noun) representation of \nproduct feature and a proximity factor of 5 to 6 words produce the best opinion mining accuracy. Consistent with the \nprevious observation (Hu and Liu 2004), a proximity factor of 5 words is effective for extracting sentiments related \nto the identified product features.  With a small proximity factor, many candidate sentiments were missed. On the \nother hand, a large proximity factor might introduce too many irrelevant tokens (noises) to the process of sentiment \nextraction. However, unlike the previous studies (Hu and Liu 2004; Dave et. al. 2003; Popescu and Etzioni 2005), \nwe found that unigram (a single noun) was more effective than bigram (two nouns) or trigram (three nouns) for the \nrepresentation of product features. According to our in-depth analysis, it was revealed that standard names of \nproduct features such as “battery life” for mobile phones or laptops were not frequently used in consumer reviews. \nIndeed, in more than a half of the reviews related to mobile phones, the word “battery” alone (e.g., “the battery can \nlast for 5 days”) was referred to. As a result, quite a number of important product features were not extracted from \nthe reviews if we used the “Noun Noun” pattern to represent product features. Similarly, as most of the names of \nproduct features comprise one to two words, using the pattern of “Noun Noun Noun” to represent product features \nwill lead to a very poor recall. In fact, a previous study also found that a single noun is sufficient to represent a \nproduct feature (Archak et. al. 2007). Irrespective of the word length of the product features, it seems that a \nproximity factor of five to six words is appropriate according to our empirical study. \n \nFigure 4.  The Impact of Feature Representation and Proximity Factor on Prediction Accuracy \nExperiment Three \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 15 \nFor our last experiment, we tried to evaluate the performance of the OBPRM system based on the publicly available \nbenchmark dataset originally retrieved from Amazon.com (Hu and Liu 2004).  The benchmark dataset8 consists of \nthe consumer reviews for five products such as cameras, mobile phones, and MP3 players. Each sentence of a \nreview was manually tagged by two researchers, but the actual consumer rating of the review was not included. For \ninstance, the tagged sentence “camera[+2]##this is a great camera for you!” means that the  feature “camera” is \npositive with a polarity score 2. As shown in this example, a feature tagged in the benchmark dataset may not be an \nexplicit product feature at all. In order to evaluate our system based on this benchmark dataset, we treated each \nsentence which had a polarity tag as a review. Ten-fold cross validation procedure was still applied in this \nexperiment. If the polarity (e.g., positive or negative) classified by our system is the same as the manually tagged \npolarity in the benchmark dataset, it is considered a correct classification.  The performance of our system and that \nof the published results are tabulated in Table 3.  It is shown that the precision, recall, and F-measure scores of our \nsystem outperform that of FBS (Hu and Liu 2004) and OPINE (Popescu and Etzioni 2005) and it is comparable with \nthat of Opinion Observer (Ding and Liu 2007). Our system achieves the top F-measure score because of the \nautomatic learning of proper sentiment contextual knowledge during the training phrase. The contextual knowledge \nwas then applied to bootstrap opinion mining during the testing phrase. It should be noted that our system does not \nrely on the manually tagged polarity label to bootstrap classification performance; it only requires the readily \navailable consumer ratings at e-Commerce sites to infer the polarities of sentiments presented in consumer reviews.    \n Table 3. Comparative Performance Based on Benchmark Test \nSystems Recall Precision F-Measure \nFBS 0.693 0.642 0.667 \nOPINE 0.890 0.860 0.875 \nOpinion Observer 0.910 0.920 0.910 \nOBPRM 0.913 0.919 0.916 \n \nAccording to published results (Abbasi et. al. 2008; Pang et. al. 2002), the accuracy of Support Vector Machine \n(SVM) based sentiment polarity classification falls in the range of [0.8, 0.9]. Although a direct comparison between \nour work and early studies is not possible due to varying experimental settings, the accuracy of our context-sensitive \nopinion mining method (e.g., 0.83 for the laptop category and 0.89 for the benchmark dataset) is comparable with \nthat of the early work which utilizes machine learning techniques.    \nConclusions and Future Work \nGuided by the design science research methodology, we illustrate the design, development, and evaluation of a \nnovel fuzzy domain ontology based context-sensitive opinion mining system in this paper. In particular, we show \nthat a domain specific sentiment lexicon can be automatically constructed to facilitate context-sensitive opinion \nmining based on existing fuzzy domain ontology extraction method. By applying a variant of the Kullback-Leibler \ndivergence statistical learning technique, our system can accurately predict the polarities of sentiments without \nrequiring extra human effort to annotate training examples. Based on real consumer reviews collected from \nAmazon.com, the effectiveness of our OBPRM context-sensitive opinion mining system is empirically tested; the \nproposed system performs significantly better than a baseline system which is based on context-free sentiment \nclassification approach. Experimental results from a benchmark test also reveal that the performance of the OBPRM \nsystem is better than that of similar opinion mining systems. The business implication of our research is tremendous; \nour context-sensitive opinion mining methodology assists organizations to analyze a large number of consumer \nreviews efficiently. As a result, organizations can develop effective business strategies related to marketing, \ncustomer support, and product design functions in a timely fashion. Our work also facilitates individual consumers’ \ncomparison shopping processes. Future research involves examining the correlation of the product sentiment scores \ngenerated by our system and the actual sales ranks or sales volumes of various products. Another direction of \nresearch is to examine the validity (e.g., review spam) and the usefulness of reviews posted to the Web. A direct \ncomparison of the performance of our method with that of some machine learning classification methods such as \nSVM will be conducted in the future. \n                                                          \n8\n http://www.cs.uic.edu/~liub/FBS/CustomerReviewData.zip \nData and Web Mining Track \n \n16 Thirtieth International Conference on Information Systems, Phoenix 2009  \nAcknowledgements  \nThe work reported in this paper has been funded in part by the City University of Hong Kong research grants \n(Project No.: 7200126 and 7002426). The authors would like to thank the anonymous reviewers, the associate \neditor, and the track chair for valuable suggestions that help a lot in improving the quality of the paper. \n \nReferences \nAbbasi, A., Chen, H., and Salem, A. “Sentiment analysis in multiple languages: Feature selection for opinion \nclassification in Web forums”, ACM Transactions on Information Systems (26:3), June 2008, Article 12. \nAgarwal, A., Biadsy, F., and Mckeown, F. “Contextual Phrase-Level Polarity Analysis Using Lexical Affect \nScoring and Syntactic N-Grams”, in Proceedings of the 12th Conference of the European Chapter of the \nAssociation for Computational Linguistics, 2009, pp.24-32. \nArchak, N., Ghose, A., and Ipeirotis, P. “Show me the money!: deriving the pricing power of product features by \nmining consumer reviews”, in Proceedings of the 13th ACM SIGKDD International Conference on Knowledge \nDiscovery and Data Mining, San Jose, California, 12-15 August 2007, pp. 56 - 65. \nBao, S., Li, R., Yu, Y., and Cao, Y. “Competitor Mining with the Web”, IEEE Transactions on Knowledge and \nData Engineering, (20:10), 2008, pp. 1297–1310. \nBickart, B. and Schindler, R. “Internet forums as influential sources of consumer information”, Journal of \nInteractive Marketing, (15:3), 2001, pp. 31–40. \nCimiano, P., Hotho, A., and Staab, S. “Learning concept hierarchies from text corpora using formal concept \nanalysis”, Journal of Artificial Intelligence Research (24), 2005, pp. 305–339. \nDanescu-Niculescu-Mizil, C., Kossinets, G., Kleinberg, J., and Lee, L. “How opinions are received by online \ncommunities: a case study on amazon.com helpfulness votes”, in Proceedings of the 18th international \nconference on World Wide Web, Madrid, Spain, 20-24 April 2009, pp.141-150. \nDas, S. and Chen, M. “Yahoo! for Amazon: Sentiment Extraction from Small Talk on the Web”, Management \nScience, (53:9), September 2007, pp. 1375–1388. \nDave, K., Lawrence, S., and Pennock, D. “Mining the peanut gallery: opinion extraction and semantic classification \nof product reviews”, in Proceedings of the 12th international conference on World Wide Web, Budapest, \nHungary, 20-24 May 2003, pp.519-528. \nDing, X. and Liu, B. “The Utility of Linguistic Rules in Opinion Mining”, in Proceedings of the 30th Annual \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, \nNetherlands, 23-27 July 2007, pp. 811 - 812. \nDittenbach, D., Berger, H., and Merkl, D. “Improving domain ontologies by mining semantics from text”, in \nProceedings of the First Asia-Pacific Conference on Conceptual Modelling (APCCM2004), 2004, pp. 91–100.  \nEsuli, A. and Sebastiani, F. “Determining the semantic orientation of terms through gloss classification”, in \nProceedings of the 14th ACM International Conference on Information and Knowledge Management, Bremen, \nGermany, October 2005, pp. 617 - 624. \nFikes, R., Hayes, P., and Horrocks, I. “OWL-QL - A Language for Deductive Query Answering on the Semantic \nWeb”, Journal of Web Semantics, (2:1), 2004, pp. 19-29. \nGruber, T.R. “A translation approach to portable ontology specifications”, Knowledge Acquisition, (5:2), 1993, pp. \n199–220. \nHatzivassiloglou, V. and McKeown, K. “Predicting the semantic orientation of adjectives”, in Proceedings of the \n35th Annual Meeting of the Association for Computational Linguistics, 1997, pp. 174 - 181. \nHevner, A., March, S., Park, J., Ram, S. “Design science in information systems research”, MIS Quarterly (28:1), \n2004, pp. 75-105. \nHu, M. and Liu, B. “Mining and summarizing customer reviews”, in Proceedings of the tenth ACM SIGKDD \nInternational Conference on Knowledge Discovery and Data Mining, Seattle, Washington, 22-25 August 2004, \npp. 168-177. \nKindo, T., Yoshida, H., Morimoto, T., and Watanabe, T. “Adaptive personal information filtering system that \norganises personal profiles automatically”, in Proceedings of the Fifteenth International Joint Conference on \nArtificial Intelligence (IJCAI’97), 1997, pp. 716-721. \nKullback, S. and Leibler, R.A. \"On Information and Sufficiency\", The Annals of Mathematical Statistics, (22:1), \n1951, pp. 79–86. \nLau, R.Y.K. “Context-Sensitive Text Mining and Belief Revision for Intelligent Information Retrieval on the Web”, \n Lau et. al. / Domain Ontology Extraction for Context-Sensitive Opinion Mining \n  \n Thirtieth International Conference on Information Systems, Phoenix 2009 17 \nJournal of Web Intelligence and Agent Systems, (1:3-4), 2003, pp. 151-172. \nLau, R.Y.K., Bruza, P.D., and Song, D. “Towards a Belief Revision Based Adaptive and Context Sensitive \nInformation Retrieval System”, ACM Transactions on Information Systems, (26:2), March 2008, pp. 8:1-8:38. \nLau, R.Y.K., Song, D., Li, Y., Cheung, C.H., Hao, J.X. “Towards A Fuzzy Domain Ontology Extraction Method for \nAdaptive e-Learning”, IEEE Transactions on Knowledge and Data Engineering, (21:6), 2009, pp. 800-813. \nLau, R.Y.K. and Lai, C.L. “Information Granulation for the Design of Granular Information Retrieval Systems”, in \nProceedings of the 2008 International Conference on Information Systems, Paris, France, 14-17 December \n2008, Paper 179. \nLiu, Y., Huang, X., An, A., and Yu, X. “ARSA: A Sentiment- Aware Model for Predicting Sales Performance Using \nBlogs”, in Proceedings of the 30th Annual International ACM SIGIR Conference on Research and \nDevelopment in Information Retrieval, Amsterdam, Netherlands, 23-27 July 2007, pp. 607-614. \nLu, Y. and Zhai, C. “Opinion integration through semi-supervised topic modeling”, in Proceedings of the 17the \nInternational Conference on World Wide Web, Beijing, China, 21-25 April 2008, pp. 121-130. \nMacdonald, C. and Ounis, I. “Overview of the TREC 2007 Blog Track”, in Proceedings of the Sixteenth Text \nREtrieval Conference, Gaithersburg, Maryland, 2007. Available from http://trec.nist.gov/pubs/trec16/. \nMiao, Q., Li, Q., and Dai, R. “An integration strategy for mining product features and opinions”, in Proceedings of \nthe 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, 26-30 \nOctober 2008, pp. 1369-1370. \nMiller, G., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K. “Introduction to WordNet: An on-line lexical \ndatabase”, Journal of Lexicography, (3:4), 1990, pp. 234–244. \nMitchell, T. Machine Learning. New York: McGraw-Hill, 1997. \nPang, B., Lee, L., and Vaithyanathan, S. “Thumbs up? Sentiment Classification using Machine Learning \nTechniques”, in Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,  \nPhiladelphia, 2002, pp. 79-86. \nPorter, M. “An algorithm for suffix stripping”, Program, 1980, (14:3), pp.130–137. \nPopescu, A.M. and Etzioni, O. “Extracting Product Features and Opinions from Reviews”, in Proceedings of the \n2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language \nProcessing, Vancouver, Canada, October 2005, pp. 339-346. \nRocchio, J. “Relevance Feedback in Information Retrieval”, in G. Salton, editor, The SMART retrieval system: \nexperiments in automatic document processing, 1971, pp. 313-323. \nSalton, G. and McGill, H.J. Introduction to Modern Information Retrieval. New York: McGraw-Hill, 1983. \nSalton, G., Wong, A., and Yang, C.S.  “A vector space model for automatic indexing”, Communications of the ACM  \n(18:11), November 1975, pp. 613-620. \nSalton, G. “Full text information processing using the smart system”, Database Engineering Bulletin, (13:1), March \n1990, pp. 2–9. \nStone, P., Dunphy, D., Smith, M., and Ogilvie, D. The General Inquirer: A Computer Approach to Content Analysis. \nCambridge: MIT Press, 1966. \nSubrahmanian, V. and Reforgiato, D. “AVA: Adjective-Verb-Adverb Combinations for Sentiment Analysis”, IEEE \nIntelligent Systems, July/August 2008, pp. 43-50. \nTan, S., Wang, Y., and Cheng, X. “Combining learn-based and lexicon-based techniques for sentiment detection \nwithout using labeled examples”, in Proceedings of the 31st annual international ACM SIGIR Annual \nInternational Conference on Research and Development in Information Retrieval, Singapore, 20-24 July 2008, \npp.743-744. \nTan, S., Wu, G., Tang, H., and Cheng, X. “A novel scheme for domain-transfer problem in the context of sentiment \nanalysis”, in Proceedings of the sixteenth ACM conference on Conference on information and knowledge \nmanagement, Lisboa, Portugal, 6-8 November 2007, pp.979-982. \nTho, Q.T., Hui, S.C., Fong, A., and Cao, T.H. “Automatic Fuzzy Ontology Generation for Semantic Web”, IEEE \nTransactions on Knowledge and Data Engineering, (18:6), June 2006, pp. 842-856. \nTurney, P. and Littman, M. “Measuring praise and criticism: Inference of semantic orientation from association”, \nACM Transactions on Information Systems, (21:4), October 2003, pp. 315–346. \nvan  Rijsbergen, C. Information Retrieval. London:Butterworths, 1979. \nWhitelaw, C., Garg, N. and Argamon, S. “Using appraisal groups for sentiment analysis”, in Proceedings of the 14th \nACM International Conference on Information and Knowledge Management, Bremen, Germany, October \n2005, pp. 625 - 631. \nWille, R. “Formal concept analysis as mathematical theory of concepts and concept hierarchies”, in Bernhard \nGanter, Gerd Stumme, and Rudolf Wille, editors, Formal Concept Analysis, Foundations and Applications, \nData and Web Mining Track \n \n18 Thirtieth International Conference on Information Systems, Phoenix 2009  \nvolume 3626, 2005, pp. 1–33. \nWilson, T., Wiebe, J., and Hwa, R. “Just how mad are you? finding strong and weak opinion clauses”, in \nProceedings of the 19th National Conference on Artificial Intelligence, San Jose, California, 25-29 July 2004, \npp. 761–769.  \nWilson, T., Wiebe, J., and Hoffmann, P. “Recognizing contextual polarity in phrase-level sentiment analysis”, in \nProceedings of the Human Language Technology Conference and Conference on Empirical Methods in \nNatural Language Processing, 2005, pp. 347–354.  \nWilson, T., Wiebe, J., and Hwa, R. “Recognizing strong and weak opinion clauses”, Computational Intelligence \n(22:2), 2006, pp. 73-99.  \nWright, A. “Our sentiments, exactly”, Communications of the ACM, (52:4), April 2009, pp. 14-15. \nXu, R., Wong, K.F., Lu, Q., Xia, Y., and Li, W. “Learning Knowledge from Relevant Webpage for Opinion \nAnalysis”, in Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and \nIntelligent Agent Technology, Sydney, Australia, 9-12 December 2008, pp. 307–313. \nYang, K., Yu, N., and Zhang, H. “WIDIT in TREC-2007 Blog Track: Combining lexicon based Methods to Detect \nOpinionated Blogs”, in Proceedings of the Sixteenth Text REtrieval Conference, Gaithersburg, Maryland, \n2007, Available from http://trec.nist.gov/pubs/trec16/. \nZadeh, L.A.. “Fuzzy sets”, Journal of Information and Control, (8), 1965, pp. 338-353. \nZhang, M. and Ye, X. “A generation model to unify topic relevance and lexicon-based sentiment for opinion \nretrieval”, in Proceedings of the 31st Annual International ACM SIGIR Conference on Research and \nDevelopment in Information Retrieval, Singapore, 20-24 July 2008, pp. 411-418.  \n \n \n \n",
      "id": 4591837,
      "identifiers": [
        {
          "identifier": "oai:eprints.qut.edu.au:42065",
          "type": "OAI_ID"
        },
        {
          "identifier": "301346171",
          "type": "CORE_ID"
        },
        {
          "identifier": "10904076",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:aisel.aisnet.org:icis2009-1170",
          "type": "OAI_ID"
        }
      ],
      "title": "Automatic domain ontology extraction for context-sensitive opinion mining",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:aisel.aisnet.org:icis2009-1170",
        "oai:eprints.qut.edu.au:42065"
      ],
      "publishedDate": "2009-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://eprints.qut.edu.au/42065/",
        "https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1170&amp;context=icis2009"
      ],
      "updatedDate": "2022-05-15T09:14:38",
      "yearPublished": 2009,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/10904076.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/10904076"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/10904076/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/10904076/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4591837"
        }
      ]
    },
    {
      "acceptedDate": "2009-11-11T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Davy, Michael"
        },
        {
          "name": "Ferguson, Paul"
        },
        {
          "name": "Gurrin, Cathal"
        },
        {
          "name": "O'Hare, Neil"
        },
        {
          "name": "Sheridan, Páraic"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Maojin",
        "The Pennsylvania State University CiteSeerX Archives",
        "Neil"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/11309150",
        "https://api.core.ac.uk/v3/outputs/143907304",
        "https://api.core.ac.uk/v3/outputs/192372010",
        "https://api.core.ac.uk/v3/outputs/147598409"
      ],
      "createdDate": "2013-07-10T11:53:33",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        }
      ],
      "depositedDate": "2009-01-01T00:00:00",
      "abstract": "While most work in sentiment analysis in the financial domain has focused on the use of content from traditional finance news, in this work we concentrate on  more subjective sources of information,  blogs. We aim  to automatically determine the sentiment of financial bloggers towards companies and their stocks. To do this we develop a corpus of financial blogs, annotated with polarity of sentiment with respect to a number of companies. We conduct an analysis of the annotated corpus, from which we show there is a significant level of topic shift within this collection, and also illustrate the difficulty that human annotators have when annotating certain sentiment categories. To deal with the problem of topic shift within blog articles, we propose text extraction techniques to create topic-specific sub-documents, which we use to train a sentiment  classifier. We show that such approaches provide a substantial improvement over full documentclassification and that word-based approaches perform better than sentence-based or paragraph-based approaches",
      "documentType": "research",
      "doi": "10.1145/1651461.1651464",
      "downloadUrl": "https://core.ac.uk/download/11309150.pdf",
      "fieldOfStudy": "computer science",
      "fullText": "Topic-Dependent Sentiment Analysis of Financial Blogs\nNeil O’Hare1, Michael Davy2, Adam Bermingham1, Paul Ferguson1,\nPáraic Sheridan2, Cathal Gurrin1, Alan F. Smeaton1\n1CLARITY: Centre for Sensor Web Technologies, Dublin City University, Ireland\n2National Centre for Language Technology, Dublin City University, Ireland\nnohare@computing.dcu.ie\nABSTRACT\nWhile most work in sentiment analysis in the financial do-\nmain has focused on the use of content from traditional fi-\nnance news, in this work we concentrate on more subjec-\ntive sources of information, blogs. We aim to automatically\ndetermine the sentiment of financial bloggers towards com-\npanies and their stocks. To do this we develop a corpus of\nfinancial blogs, annotated with polarity of sentiment with re-\nspect to a number of companies. We conduct an analysis of\nthe annotated corpus, from which we show there is a signifi-\ncant level of topic shift within this collection, and also illus-\ntrate the difficulty that human annotators have when anno-\ntating certain sentiment categories. To deal with the prob-\nlem of topic shift within blog articles, we propose text ex-\ntraction techniques to create topic-specific sub-documents,\nwhich we use to train a sentiment classifier. We show that\nsuch approaches provide a substantial improvement over full\ndocumentclassification and that word-based approaches per-\nform better than sentence-based or paragraph-based approaches.\nCategories and Subject Descriptors\nH3.1 [Information Storage and Retrieval]: Content Anal-\nysis and Indexing\nGeneral Terms\nAlgorithms\nKeywords\nSentiment Analysis, Opinion Mining, Financial Blogs\n1. INTRODUCTION\nThe blogosphere is acknowledged as a source of subjective\nopinions on a wide variety of topics, as has been recognised\nin the TREC Blog Track [22]. This track has run since\n2006, focussing on the retrieval of subjective text from Blog\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nTSA’09, November 6, 2009, Hong Kong, China.\nCopyright 2009 ACM 978-1-60558-805-6/09/11 ...$10.00.\narticles. In the domain of finance, many bloggers publish\nopinions about specific companies and on markets in gen-\neral1. However, as far as we are aware, no existing work on\nsentiment analysis in the financial domain has used blogs as\nsources, instead using traditional news and finance media\n(e.g. [1],[14]). Blogs have the advantage that their authors\nare more likely to express opinions and to make predictions\nabout the performance of stocks than traditional media –\nwhich are more likely to report news relating to a stock’s\npast performance but may contain few explicit statements\nof opinion regarding the future.\nOur work has developed from a collaboration between\nDublin City University (DCU) and an industrial partner\nworking in online stock trading2. The aim is to automati-\ncally extract the subjective opinions uniquely found on blogs\nand track the changing sentiment from the blogosphere to-\nwards individual stocks and the market in general. This\ninvolves crawling financial weblogs, retrieving articles rele-\nvant to certain companies and their stocks, running senti-\nment polarity classification (positive, neutral, negative) on\nthose articles. The extracted sentiment will then be aggre-\ngated to obtain a snapshot of the general sentiment of the\nblogosphere towards that company. We believe that such\ninformation will prove useful to users of online stock trading\nservices.\nThere is a tendency in financial blogs to discuss multi-\nple companies (or their stocks) in a single article, meaning\nthat document level sentiment classification will not always\nbe suited. In this work we explore simple approaches to\ncoping with such topic shift by extracting topic specific sub-\ndocuments (i.e. subsets of the documents considered rele-\nvant to the topic) and training sentiment polarity classifiers\nbased on these sub-documents. Since there are no existing\ncorpora for sentiment in the financial domain for blogs, we\nalso constructed a new corpus for developing and evaluating\nsentiment analysis approaches.\nThe main contributions of this paper are twofold. Firstly,\nwe develop a new corpus of financial blogs, annotated with\npolarity of sentiment, and analyse this corpus with respect\nto annotator’s ability to create consistent sentiment polarity\nannotations. Furthermore, we explore the extent to which\ntopic shift within financial blog articles occurs. Having de-\ntermined that it is a genuine problem, we propose approaches\nto topic-based text extraction for sentiment polarity classi-\nfication and evaluate these approaches on this corpus.\n1blogged.com, for example, lists over 2,000 blogs in the cat-\negory ‘investing’\n2Zignals: http://www.zignals.com\nThe remainder of the paper is organised as follows. In the\nnext section we introduce related work in sentiment annota-\ntion, topic-based sentiment analysis and sentiment analysis\nin the financial domain. Then, in Section 3 we discuss the\ncreation of our corpus and present some analysis of it, in-\ncluding examining the ability of humans to annotate certain\nsentiment labels consistently and the extent to which topic\nshift is an issue in this corpus. In Section 4 we propose\nour approach to topic-based sub-document extraction and\ngive details of the sentiment classifier system. Section 5 de-\nscribes our experiments in sentiment polarity classification\nand results. Finally, in Section 6 we report conclusions and\ndirections for future work.\n2. RELATED WORK\nWe first present existing work in corpus annotation for\nsentiment, and then place our work in context of existing\ntopic-based sentiment analysis approaches and existing sen-\ntiment analysis work in the financial domain.\n2.1 Sentiment Annotation\nSupervised learning relies on labelled training data to in-\nduce and evaluate classification strategies. In some domains\ndocuments labelled for sentiment by the document author\nare available, notable examples being Pang and Lee’s work\non movie reviews [25] and Dave et al.’s work on product re-\nviews [7]. In domains where author labels are not available,\nwe must rely on human annotators to provide sentiment\njudgements. Notable manual sentiment annotation efforts\ninclude the Blogs06 corpus [20], Wilson’s MPQA [31] cor-\npus and the NTCIR corpus for their Multilingual Opinion\nAnalysis Task (MOAT) [27]. Of these, the Blogs06 corpus\nwas annotated at the document level, and the MOAT and\nMPQA corpora were annotated at the sentence and phrase\nlevel respectively. Each corpus required a significant amount\nof annotator training, in particular the MPQA corpus which\nfeatured very detailed annotation.\nAnnotation of sentiment can be a relatively difficult chal-\nlenge, as interpretation of sentiment is subject to a number\nof human factors such as domain expertise, the annotator’s\nprivate state and inferences the annotator has made into the\ntext to be annotated. The MOAT corpus has, for example,\nmoderately high rates of agreement for Japanese and Chi-\nnese but low agreement for English texts, while the MPQA\ncorpus achieved a high level of agreement.\nOur own previous work evaluated inter-annotator agree-\nment on a representative subset of the Blogs06 corpus [3].\nWe evaluated sentiment annotation at the sentence and doc-\nument level, observing a similar moderate level of agreement\nfor both sentence and document-level annotations. We also\nfound that annotating for the label mixed sentiment was\ntroublesome for annotators and agreement for this class was\nsignificantly lower than that for the other sentiment classes,\na finding which is further explored in the current work.\n2.2 Topic-based Sentiment Analysis\nMuch of the work to date in sentiment analysis has focused\non domains where topic relevance is assumed. Examples\nof this are to be found in the product review [29] [7] [19]\nand film review [25] domains. This simplifying assumption\nallows systems to focus specifically on the identification of\nsentiment, without regarding topic relevance.\nWith more ad-hoc information sources, such as blogs, topic\nrelevance may not be assumed and relevance determination\nmust be incorporated into the sentiment analysis process.\nOne approach is to first estimate the likelihood of topic rel-\nevance using techniques from the field of information re-\ntrieval. The relevance probability may then inform the sen-\ntiment analysis algorithm, which in turn produces a final\ntopic-sentiment score. This allows us to rank the documents\nfor likelihood of containing topic-directed sentiment. This\ntwo-stage approach is the most common approach used in\nthe opinion finding tasks at TREC [22], which evaluated\ntwo separate sentiment related tasks: opinion finding (find\nany opinionated documents) and sentiment polarity ranking\n(find only positive, or only negative, documents). In some\napplications, such as the current work, a sentiment ranking\nis not appropriate and a summary of sentiment in known or\nassumed relevant documents is required. In this scenario,\ndocuments are first labeled for binary topical relevance, and\nthe relevant documents are analysed for sentiment.\nAnother problem with more freeform domains is topic\nshift, where several topics are discussed in a single docu-\nment. A number of proximity-based models have been tried,\nwith some success. The underlying assumption in proximity-\nbased models is that portions of text which co-occur with\ntopic-related terms are likely to be indicative of the senti-\nment towards that topic. There are many examples of this in\nthe literature which show some degree of success, for exam-\nple [13] [21]. Zhang et al. [33] propose a method for jointly\nmodelling proximity and sentiment using a generative model\nand a degradation is observed as the proximity window is\nmade smaller. Rather than incorporate the proximity model\ninto the sentiment analysis directly, an alternative approach\nis to extract relevant information from the source document\nbefore conducting sentiment analysis, with passage retrieval\napproaches showing some success [16], [2]. Our work differs\nfrom such work in that we are interested in sentiment polar-\nity rather than subjectivity detection, and we are interested\nin hard classification of a set of relevant documents, rather\nthan an information retrieval style ranking.\nWe also take inspiration from the passage retrieval and\nproximity models and from the area of topic-based text ex-\ntraction used in both the citation analysis and Web retrieval\nfields [15] [12] [5], [4]. In these domains the goal is to identify\nadditional text from the referencing document or web page,\nand associate this text with the document being referenced.\nThe most common approach to identify words that are to\nbe associated with the document that is being referenced is\nto take a ‘window’ of text either side of the reference. We\nhave taken a similar approach in that we identify the areas\nwithin the articles that are associated with specific stocks,\nand then use a variety of windowing techniques in order to\nidentify the text that is associated with that stock. This\nprocess of extracting text relating to a specific topic is im-\nportant as topic shift is a potential problem in our corpus\nthat we are using (see Section 3.2.3) and so we extract topic-\nbased sub-documents for sentiment analysis (Section 4).\n2.3 Sentiment Analysis in the Financial Do-\nmain\nIn the financial domain, Ahmed et al. have studied meth-\nods for identifying positive and negative news in news streams\n[1] and for identifying affect in news text [8]. They identify\na controversial news event likely to elicit emotive content\nand use the subsequent news articles as a corpus. Koppel\net al. used market price movements as a ground truth for\ntheir financial sentiment analysis [14]. They too used news\ndata as their corpus and achieved an accuracy of 70%. How-\never, their work is unique in the literature in that it is not\nevaluated against human judgements. The goal of both of\nthe above approaches is to mine sentiment from broadcast\nnews data. We believe that news data is generally objective\nand not an ideal source for mining and aggregating senti-\nment. Instead we use blogs which have been shown to be\nhighly subjective [22]. This is particularly true in the finan-\ncial analysis domain as authors frequently make evaluations\nand projections targetted at markets, stocks, companies and\nprominent figures. To the best of our knowledge, our work\nis the first to mine sentiment in blogs specifically targetted\nat the financial domain.\n3. FINANCIAL BLOG CORPUS\nIn this section we outline the creation of our blog articles,\nfollowed by an analysis of the corpus.\n3.1 Development of Corpus\n3.1.1 Crawl and Noise Removal\nThe corpus we use is made up of financial blog articles\ncollected automatically from a predefined set of sources.\nWe identified 232 financial blog sources, and crawled these\nsources on two separate occasions: for 3 weeks in Febru-\nary 2009 (Crawl 1), and for 5 weeks from May to June 2009\n(Crawl 2). Since there was a significant change in the overall\nmood between these snapshots (relating to the 2009 global\nfinancial crisis), splitting the dataset in this way should cap-\nture overall shifts in sentiment in relation to the markets.\nAfter crawling these blog sources and extracting the HTML\nsource for all articles it is necessary to remove irrelevant in-\nformation contained in those pages, such as links to other\npages, advertisements, etc. We use the DiffPost algorithm,\nproposed by Lee et al [16], to remove noise from the docu-\nments in the collection. This approach exploits the fact that,\nwithin a given blog feed, the noise, or unwanted content,\nwill tend to be repeated across multiple articles, while the\nrelevant text from the article will be unique to that individ-\nual article. Accordingly, each article is first broken up into\nHTML segments, and each of these segments is compared\nto segments extracted from articles from the same source.\nOnly unique segments are kept, with non-unique segments\nbeing considered as noise and so are removed.\n3.1.2 Annotation Granularity\nIn general the input to the polarity classifier will be doc-\numents and sentiment analysis will be first applied at the\ndocument level. However, it may be the case that finer gran-\nularity is required since documents can contain a mixture of\nsentiments for a variety of topics (e.g. one blog post about\na number of different stocks). In addition to document level\nannotation, we annotated at the paragraph level. In the lit-\nerature sentence and phrase level [31] granularity have been\nexplored. While this mitigates against the problem of mixed\ntopics found at the document level and also paragraph level\ngranularity, a number of new challenges arise. First is the\nextra demands incurred when annotation is performed at the\nsentence level. Second, it can be more difficult to accurately\nlabel sentences (or even phrases) since the contextual infor-\nmation is not available. The manual annotation effort for\nparagraph granularity is less than that of sentence or phrase\nlevel granularity while contextual information is maintained.\n3.1.3 Labels\nThe labels used for annotation include a five-point scale\nfrom Very Negative to Very Positive: Very Negative, Nega-\ntive, Neutral, Positive, Very Positive. Annotators could also\nannotate paragraphs or documents as mixed, which indicates\na mixture of positive and negative sentiment, and not rele-\nvant. For paragraph-level annotations only, the noise label\nindicates that the paragraph should not be considered to\nbe part of the article body but it an unwanted part of the\nHTML page containing the article. Finally, we also gave the\nannotators the option of annotating as I don’t know (IDK),\nwhich means that the annotator is not confident in making\nan annotation. We included this class in acknowledgement of\nthe fact that sentiment annotation is an inherently difficult\ntask, and even human annotators sometimes have difficulty\nannotating documents with confidence.\n3.1.4 Annotation Tool\nTo facilitate the annotation of our corpus, we developed\na web-based annotation tool to present annotators with a\nqueue of documents to be annotated with the labels de-\nscribed in Section 3.1.3, and allowed annotators to annotate\nat the document level and the paragraph level.\n3.1.5 Annotators and Training\nAs sentiment annotation is a difficult task, and since do-\nmain knowledge of financial markets is necessary for anno-\ntating this corpus, it was important that our annotators were\ntrained before undertaking this annotation task. The cor-\npus was annotated by 7 people, 5 of these being computer\nscience researchers from DCU, and 2 employees of our in-\ndustrial partner. The training phase involved two rounds of\npilot annotations consisting of 5 training documents each,\nfollowed by extensive discussions of these annotations, until\na consensus annotation was reached. Following this, a set of\nguidelines for annotations was produced for the annotators.\n3.1.6 Topics and Retrieval\nWe identified the 500 companies that make up the Stan-\ndard & Poor’s S&P 500 Index as topics of interest for sen-\ntiment analysis. In order to retrieve candidate documents\nfor annotation with respect to a certain stock, we ran a case-\nsensitive phrase search on the name of the stock i.e. rele-\nvant articles must contain the whole phrase of the company\nname, and the case must also match (typically the name is\ncapitalised). Since each document can be annotated with\nrespect to more than one company (or stock), unique an-\nnotations are identified by the combinations of document\nand topic, which we will refer to as a doc-topic or doc-topic\npair. In addition to annotating documents with respect to\nstocks, we are also interested in the sentiment of documents\nwith respect to the market in general. For this reason we\nannotate a number of documents with respect to their senti-\nment towards stocks or equities in general: these documents\nwere randomly selected. In total, we annotated 1526 unique\ndoc-topic pairs, 167 of which were annotated for stocks in\ngeneral, and 164 of which were annotated by two annotators\nto facilitate inter-annotator agreement analysis.\n3.2 Analysis of Corpus\nIn this section we give details of the corpus, which contains\nfinancial blog articles annotated by 7 users.\n3.2.1 Annotation Statistics\nTable 1 summarises the document-level annotations; since\na number of documents were annotated more than once (i.e.\nwith respect to different topics) the number of unique docu-\nments annotated is much less than the total number of anno-\ntations. There is a clear bias towards negative sentiment in\nCrawl 1, with approximately twice as many negative labels\nas positive labels, while Crawl 2 shows the opposite bias.\nOverall, though, there is a roughly even balance between\npositive, negative and neutral annotations. Comparatively\nfew documents are annotated as Very Positive or Very Neg-\native, and 90 annotations (just over 5%) were I don’t know.\nCrawl 1 Crawl 2 Total\nTotal Annotations 541 1150 1691\nUnique Documents 311 668 979\nUnique Doc/Topic Pairs 476 1050 1526\nVery Positive 21 47 68\nPositive 54 251 305\nNeutral 80 187 267\nNegative 124 177 301\nVery Negative 43 56 99\nMixed 27 75 102\nNot Relevant 154 305 459\nI don’t know 38 52 90\nTable 1: Statistics for document-level annotations.\n3.2.2 Inter-Annotator Agreement\nTable 2 shows the Kappa score for inter-annotator agree-\nment for various levels of granularity. The 7-point scale,\nmade up of all document level annotations (except I don’t\nknow, which we interpret as abstaining from annotating) has\na Kappa of 0.462, indicating only a moderate level of agree-\nment. This increases to 0.593 for a 5-point scale, suggesting\na low level of agreement for annotation of degree or strength\nof polarity, as merging positive and very positive, and neg-\native with negative, greatly improves the agreement.\nSince our sentiment analysis classifier will not be inter-\nested in learning the not relevant class (indeed, it would not\nbe feasible to create separate relevance classifiers for all 500\ntopics) it is also worth looking at the agreement with the\nnot relevant class excluded. The 4-point granularity, which\nremoves the not relevant class, has a Kappa of 0.592, equiv-\nalent to the 5-Point Kappa, suggesting that relevance was\nannotated consistently. Combining the mixed and neutral\nclasses to create the 3-PointMN granularity gives a kappa of\n0.596. Removing the mixed class completely, however, leads\nto a Kappa of 0.712, a huge improvement which suggests\nthat the mixed category is a difficult one for annotators to\nagree on (of the doubly annotated doc-topics, of 10 mixed\nannotations, only 1 of these was annotated consistently by\nboth annotators). If we only look at the positive and nega-\ntive classes, there is perfect agreement in the annotations.\nBased on these agreement scores, we believe that it is\nmost appropriate to train a polarity classifier at either the\n3-PointN or the binary granularity, and it is at these granu-\nGranularity Kappa\n7-Point\n(VP / P / Nu / N / VN / M / NR) 0.466\n5-Point\n(VP&P / Nu / N&VN / M / NR) 0.59\n4-Point\n(VP&P / Nu / N&VN / M) 0.59\n3-PointMN\n(VP&P / Nu&M / N&VN) 0.6\n3-Point\n(V&P / Nu / N&VN) 0.712\nBinary\n(V&P / N&VN) 1\nTable 2: Kappa score for document-level inter-\nannotator agreement at various levels of granularity.\nlarities that we will evaluate our topic-dependent sentiment\nanalysis in Section 5.\n3.2.3 Topic Relevance\nTo determine the level of topic shift within the documents\nin our collection, we analysed the relevance statistics of the\ndocuments in Crawl 2 of our collection. For our purposes\nhere a document is considered relevant if it is retrieved by\nan initial retrieval process, as described in Section 3.1.6.\nWe can see from Table 3 that, although the number of doc-\ntopic pairs is roughly equal to the number of documents in\nthe crawl, only about 30% (2,249) of these are relevant to at\nleast one stock meaning that, when a document is relevant to\na stock, it is, on average, relevant to 3 stocks. The table also\nshows that over half of the relevant documents are relevant\nto 2 stocks or more, and approximately a quarter of them\nare relevant to 4 stocks or more. This indicates that, as\nexpected, this dataset does contain a lot of topic shift within\nrelevant documents, and that documents that mention one\nstock will very often mention other stocks also, supporting\nour argument that sub-document extraction for sentiment\nclassification are necessary.\nTotal Documents 6,561\nDoc-Topic Pairs 6,614\nDocs Relevant to at least:\n1 Stock 2,249\n2 Stocks 1292\n3 Stocks 820\n4 Stocks 560\n5 Stocks 403\n6 Stocks 284\n7 Stocks 173\n8 Stocks 137\n9 Stocks 110\n10 Stocks 86\nTable 3: Article Relevence Statistics for Crawl 2.\nOf course, looking at the annotation statistics in Table 3,\nwe can see that 459 out of 1,691 annotations (approx. 27%)\nare non-relevant. However, we do not believe that this biases\nthe observations made above, but rather inflates the number\nof relevant documents reported. Anecdotal reports from the\nannotators suggest that the majority of these non-relevant\ndocuments are retrieved due to failure of the noise removal\ncomponent, and we believe that improving the noise removal\nwould alleviate this problem.\n4. TOPIC-BASED SENTIMENT ANALYSIS\nIn this Section we introduce our approach to topic-based\nsentiment classification, first introducing our topic-based text\nextraction approaches, and then outlining the sentiment anal-\nysis classifier used.\n4.1 Topic-Based Text Extraction\nSince blog articles often contain discussion of multiple top-\nics, it is useful to extract those segments from the documents\nthat are most relevant to the topic of interest. This topic-\nbased text extraction will enable sentiment analysis to be\ncarried out at a sub-document level, ensuring that we re-\nstrict our analysis to the portions of a document relevant\nto a specified topic, and this should alleviate the topic shift\nproblems discussed in Section 3.2.3. As discussed in Sec-\ntion 2.2, there has been similar prior work carried out which\nattempts to calculate an optimal window of text around a\ntopic word in order to retain the most relevant words asso-\nciated with that topic. We investigate the use of three dif-\nferent sub-document extraction approaches, while also using\nthe output for the task of polarity detection, which is quite\ndistinct from work carried out by [33] for the task of opin-\nion finding – our task is essentially a classification task, as\nopposed to a retrieval oriented task. In addition, we thor-\noughly investigate a spreading window-size approach that\nuses a number of different extraction methods to find the\nmost effective input for our sentiment classifier.\nEach of our three segmentation algorithms take a topic\n(a text string containing one or more terms) and extracts\nsub-segments of the document that occur adjacent to any of\nthe topic terms. We implemented the following approaches:\n• N-word extraction. Based on natural sequence of words\nin article, we extract a given number, n, of words either\nside of any topic word. Figure 1 shows an example of\nn-words (N=1 and N=3) extracted from either side of\na topic word.\n• N-sentence extraction. Similar to n-word segmenta-\ntion, n-sentence segmentation extracts n sentences side\nof a sentence containing a topic term.\n• N-paragraph extraction. Extracts n paragraphs adja-\ncent to paragraphs containing any of the topic terms.\nFigure 1: N-word text extraction\n4.2 Sentiment Classification\nTwo distinct approaches to automatic sentiment polarity\nclassification have been proposed in the literature. The first\nuses domain independent lexical resources to classify text\n[29, 6, 8], while the other builds domain dependent models\nusing machine learning techniques [23, 17, 11]. In this work\nwe focus on the latter, and use two alternative classifiers.\nWe use a multinomial na¨ıve Bayes (MNB) classifier, since it\nhas been shown to give strong performance [28] without re-\nquiring parameter tuning. The second classifier is a Support\nvector machine (SVM), the current state-of-the-art in topic\nclassification, which has also been shown to perform well in\nthe task of sentiment polarity classification [23, 11].\nThe classification task attempts to model a function f :\nX 7→ Y which maps from doc-topic pairs (X) to a set of pre-\ndefined categories (Y ). We explore two classification tasks:\n• Binary classification, which predicts whether an article\nis either positive or negative to a given topic (Y ∈\n{positive, negative}).\n• 3-Point classification, which is a finer level of classi-\nfication granularity. In this case we include neutral\ndocuments (Y ∈ {positive, negative, neutral}).\nOf the two classification tasks performed, 3-Point classi-\nfication is considered more challenging than binary.\nAs a pre-processing step, the dataset was firstly tokenised\non whitespace, digits and punctuation characters. Following\nthis, we removed stopwords (using the list from the RCV1\n[18] corpus), stemmed all tokens (using the Porter stemming\nalgorithm [26]), and transforming all tokens to lowercase.\nFrom this we used the bag-of-words representation to con-\nstruct feature vectors for each document and sub-document.\nA binary weighting scheme was employed, since it has been\nfound to outperform traditional weighting schemes (such as\ntf-idf) for sentiment classification [10, 24].\nTrivial SVM MNB\nBinary 50.876 66.0601 69.5447\n3-Point 38.143 49.719 54.454\nTable 4: Baseline Accuracy\n5. EXPERIMENTS\nWe evaluate our proposed sentiment polarity classification\napproaches using the corpus described in Section 3. Exam-\nples not having the labels Y (see Section 4.2 above) are\ndiscarded, while those examples that were labelled inconsis-\ntently by more than one annotator are also discarded. This\ngives a total of 687 labelled documents for binary classifica-\ntion and 917 labelled documents for 3-Point classification.\nWe consider the different representations of a doc-topic\npair given by each of the text extraction techniques out-\nlined in Section 4.1, and compare the accuracy obtained by\nconstructing a classifier trained on each of the approaches.\nWe compare three classifiers: a multinomial na¨ıve Bayes, a\nSupport Vector Machine [30] and a baseline trivial classifier.\nFor the SVM classifier, we used a linear kernel with de-\nfault parameters (C = 1). The trivial classifier predicts the\nmode of the classes in the training data, and is included as a\nParagraph Sentence Words\nN SVM MNB SVM MNB N SVM MNB\n0 67.9462 73.3429 69.2377 71.8958 5 68.9565 71.2904\n1 64.5829† 71.8679301 70.7022 72.5925 10 72.6119 72.1599\n2 66.3369 70.99617 68.0999 72.1534 15 72.1901 73.6156\n3 66.9230 70.855509 70.5656 72.5839 20 73.3301 74.6280\n4 67.0724 69.83894466 67.9247 71.7143 25 74.3683 74.0460\n5 67.7949 69.09919† 66.4883 70.5636 30 71.8807 75.0691\n6 68.2383 69.38905† 66.7825 71.4331 40 72.1728 74.4787\n7 64.8618† 69.8217 64.8791† 70.4143 50 71.5779 74.0482\n8 63.4127† 69.96663 67.7925 69.8367 60 68.3722 74.3511\n9 65.1604† 69.966631 66.1920 70.2758 70 68.3301 73.3190\n10 64.8749† 70.260748 65.4586† 69.6789 80 69.0925 73.1722\nBaseline 66.0601 69.5447 66.0601 69.5447 66.0601 69.5447\nTable 5: Binary classification results for paragraph, sentence and word text extraction. Maximum accuracy\nis represented by bold text, while accuracy below that of the baseline are indicated with †\nbaseline to show that the more advanced classification tech-\nniques offer significant advantage in terms of effectiveness.\nThe WEKA [32] machine learning library implementation of\neach classifier was used in all experiments.\nTen-fold cross validation was used for each of the seg-\nmentation experiments, with the results averaged over the\nten folds. We use classification accuracy as the performance\nmetric, with a baseline measurement being calculated us-\ning the entire document for each doc-topic pair (with the\ntopic terms removed). Table 4 displays the baseline results,\nshowing that for both binary and 3-Point classification, the\ndocument-level SVM and na¨ıve Bayes classifiers achieve a\nlarge improvement in performance over the trivial classifier.\n5.1 Results\nBinary classification results using sentence-, paragraph-\nand word- based sub-document extraction are shown in Ta-\nble 5. Each row corresponds to the results at the given\nlevel of the extraction (e.g. N=10 indicates that 10 para-\ngraphs / sentences / words either side of a topic term are\nincluded). All approaches are shown to give a large improve-\nment over the baseline performance of 69.54% accuracy for\nfull document classification with the MNB classifier (66.06%\nfor SVM), with the paragraph achieving accuracy of over\n73% and the sentence approach achieving accuracy of over\n72%. The largest improvement was achieved by word-based\nextraction, with a classification accuracy of 75.07% using the\nMNB classifier, an improvement of almost 5.52% in absolute\nterms (or a relative improvement of 8%).\nAlthough performance is lower for the 3-point classifi-\ncation task, as shown in Table 6, the improvement over\ndocument-based classification is similar, improving from 54.45%\nto 59.46% (a relative improvement of over 9%) for word-\nbased text extraction (N=30), with sentence- and paragraph-\nbased approaches also giving large improvements over the\nbaseline.\nThe performance of the na¨ıve Bayes classifier was consis-\ntently better than that of the SVM, which may be due to\nthe fact that a linear kernel used in conjunction with default\nparameter values were not appropriate for this domain, and\nwhich warrants further investigation.\nWe conducted a detailed analysis of the binary classifi-\n50% \n55% \n60% \n65% \n70% \n75% \n80% \n85% \n90% \n95% \n100% \n1 (95) 2 (48) 3 (21) 4 (6) 5 (9) 6 (9) 7 (3) 8 (6) 9 (5) 10 (4) 11+ (6) \nC\nla\nss\nifi\nca\ntio\nn \nA\ncc\nur\nac\ny \nAvg Number of Training Documents  (Number of Stocks) \nBinary Classification Accuracy Grouped by Training Set Frequency \nFigure 2: Binary classification accuracy, grouped by\nthe number of times a topic stock was present in the\ntraining set. Since 10-fold cross-validation was used,\nthese figures are averaged across the 10 folds.\ncation results from the optimally performing word-based\n(n=30) text extraction approach, examining variation in\nperformance as the amount of training data for specific stocks\nis increased. Figure 2 shows the classification accuracy grouped\nby the number of training instances for specific stocks. There\nis some variation in performance, with stocks represented\nby 1 training instance achieving a classification accuracy of\n75.78% (95 stocks), and those represented by 11 or more\ndocuments achieving a classification accuracy of 80.9% (6\nstocks). Nevertheless, there is no clear trend towards higher\naccuracy for stocks that are over-represented in the train-\ning set, encouragingly suggesting that our classifier is not\novertly biased towards those stocks, but rather is general\nenough to perform similarly for all stocks.\nIn general, the results indicate that it is possible to achieve\nlarge improvements over document-based sentiment classifi-\ncation using quite simple text-extraction approaches to ex-\ntract the most relevant segments of those documents. For\nboth binary and 3-point classification, the best results were\nachieved when word-based text extraction approaches were\nused, suggesting that for this dataset at least, paragraphs\nand sentences do not necessarily correspond to the unit of\nexpression. This result differs from the result obtained by\nZhang el al [33], who found that the full document gave\nParagraph Sentence Words\nN SVM MNB SVM MNB N SVM MNB\n0 53.3143 57.7242 51.7928 54.9983 5 50.0378 55.5285\n1 49.9402 56.9427 53.5476 56.8389 10 53.2130 56.1830\n2 51.2468 55.6225 52.5535 56.2869 15 52.9150 56.6178\n3 50.9255 55.5175 53.8737 57.4825 20 54.3244 57.8258\n4 50.7082 55.1987 51.5643 56.9378 25 55.4064 58.0346\n5 52.2348 54.8665 50.4724 55.6396 30 55.3001 59.4621\n6 51.46914 55.4124 51.8036 57.1662 40 56.6019 58.2458\n7 50.1744 55.4075 50.0402 56.0767 50 55.0522 58.1443\n8 49.3822† 55.2988 51.0207 55.8642 60 52.7524 58.5877\n9 50.48714 55.7373 50.9120 55.9717 70 54.8311 58.3605\n10 49.7301 55.6334 50.6812 56.0743 80 53.7307 58.5779\nBaseline 49.7190 54.4540 49.7190 54.4540 49.7190 54.4540\nTable 6: 3-Point classification results for paragraph, sentence and word text extraction. Maximum accuracy\nis represented by bold text, while accuracy below that of the baseline are indicated with †\nthe best performance. They are interested in opinion de-\ntection, however, not sentiment polarity classification, and\nthe information retrieval paradigm in which they conduct\ntheir evaluation means that relevance and opinion detection\nare being evaluated simultaneously. Our work, on the other\nhand, is exclusively concerned with sentiment polarity clas-\nsification and shows the performance that can be achieved if\ntopic relevance is assumed. It would be of interest to explore\nwhether this result is specific to this dataset, or if similar a\napproach would prove useful in alternative domains.\n6. CONCLUSIONS AND FUTURE WORK\nIn this paper we have explored the use of blog sources for\nsentiment analysis in the financial domain, and developed a\ncorpus of over 1,500 document-level annotations. Analysis of\nthis annotation effort suggests that humans have particular\ndifficulty annotating for strength or degree of polarity, and\nin annotating documents as having mixed sentiment. Topic\nshift, which is where a single blog article discuss more than\none topic, was identified in a significant percentage of the\nblog articles collected, with articles relative to at least one\nstock also relevant to an average of 2 other stocks.\nIn order to tackle the problem of topic shift, we proposed\nand evaluated simple text-extraction approaches to extract\nthe most relative segments of a document with respect to\na given topic, then trained and tested sentiment classifiers\non the extracted sub-document representation. Empirical\nevaluation revealed that word-, sentence- and paragraph-\nbased text extraction all achieved improvements over base-\nline (full document) effectiveness, with the best performance\nrecorded when word-based text extraction techniques are\nused. Paragraph-based appoaches performed slightly bet-\nter than sentence-based approaches, suggesting that, in this\ndataset at least, the paragraph is a more natural unit for\nthe expression of sentiment than the sentence.\nThe features (bag of words) that we use for our classifier\nare quite simple compared to what has been used by other\nresearchers in sentiment analysis. We plan to explore the\nuse of linguistic features and domain independent resources\n(such as SentiWordNet [9]) in subsequent experiments. We\nhave made exclusive use of document-level annotations in\nthis paper, even though we have annotated our corpus at\nthe paragraph level, and we plan to use the paragraph-level\nannotations in future work.\nAs discussed previously, this work is part of a project to\nmonitor the overall sentiment of the blogosphere towards in-\ndividual companies and the market in general. Accordingly,\nin addition to improving our sentiment polarity classifier,\nwe will explore methods of aggregrating these sentiment re-\nsults, and we are currently developing a user interface for\ndisplaying and interacting with this data.\nAcknowledgements\nThis work is supported by Science Foundation Ireland under\ngrant 07/CE/I1147, and by Enterprise Ireland under grant\nIP/2008/0549.\n7. REFERENCES\n[1] K. Ahmad, D. Cheng, and Y. Almas. Multi-lingual\nsentiment analysis of financial news streams. In\nProceedings of the 1st International Conference on\nGrid in Finance, Palermo, 2006.\n[2] G. Attardi and M. Simi. Blog mining through\nopinionated words. In E. M. Voorhees and L. P.\nBuckland, editors, TREC, volume Special Publication\n500-272. National Institute of Standards and\nTechnology (NIST), 2006.\n[3] A. Bermingham and A. F. Smeaton. A study of\ninter-annotator agreement for opinion retrieval. In\nSIGIR ’09: Proceedings of the 32nd annual\ninternational ACM SIGIR conference on Research and\ndevelopment in information retrieval, 2009.\n[4] S. Bradshaw. Reference directed indexing: Redeeming\nrelevance for subject search in citation indexes. In\nProc. of the 7th ECDL, pages 499–510, 2003.\n[5] S. Chakrabarti, B. Dom, P. Raghavan, S. Rajagopalan,\nD. Gibson, and J. Kleinberg. Automatic resource\ncompilation by analyzing hyperlink structure and\nassociated text. In WWW7: Proceedings of the seventh\ninternational conference on World Wide Web 7, pages\n65–74, Amsterdam, The Netherlands, The\nNetherlands, 1998. Elsevier Science Publishers B. V.\n[6] S. Das and M. Chen. Yahoo! for amazon: Sentiment\nextraction from small talk on the web. Management\nScience, 53(9):1375–1388, 2007.\n[7] K. Dave, S. Lawrence, and D. M. Pennock. Mining the\npeanut gallery: opinion extraction and semantic\nclassification of product reviews. In WWW ’03:\nProceedings of the 12th international conference on\nWorld Wide Web, pages 519–528, New York, NY,\nUSA, 2003. ACM.\n[8] A. Devitt and K. Ahmad. Sentiment polarity\nidentification in financial news: A cohesion-based\napproach. In Annual Meeting of the Association of\nComputational Linguistics, volume 45, page 984, 2007.\n[9] A. Esuli and F. Sebastiani. Sentiwordnet: A publicly\navailable lexical resource for opinion mining. In 5th\nConference on Language Resources and Evaluation,\npages 417–422, 2006.\n[10] G. Forman. An extensive empirical study of feature\nselection metrics for text classification. Journal of\nMachine Learning Research, 3(1):1533–7928, 2003.\n[11] A. Funk, Y. Li, H. Saggion, K. Bontcheva, and\nC. Leibold. Opinion analysis for business intelligence\napplications. 2008.\n[12] T. H. Haveliwala, A. Gionis, D. Klein, and P. Indyk.\nEvaluating strategies for similarity search on the web.\nIn WWW ’02: Proceedings of the 11th international\nconference on World Wide Web, pages 432–442, New\nYork, NY, USA, 2002. ACM.\n[13] B. He, C. Macdonald, J. He, and I. Ounis. An effective\nstatistical approach to blog post opinion retrieval. In\nCIKM ’08: Proceeding of the 17th ACM conference on\nInformation and knowledge management, pages\n1063–1072, New York, NY, USA, 2008. ACM.\n[14] M. Koppel and I. Shtrimberg. Good news or bad\nnews?: Let the market decide. In AAAI Spring\nSymposium on Exploring Attitude and Affect in Text,\npages 86–88. Springer, 2004.\n[15] R. Kraft and J. Zien. Mining anchor text for query\nrefinement. In WWW ’04: Proceedings of the 13th\ninternational conference on World Wide Web, pages\n666–674, New York, NY, USA, 2004. ACM.\n[16] Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and\nJ.-H. Lee. Kle at trec 2008 blog track: Blog post and\nfeed retrieval. In TREC, 2008.\n[17] K. Lerman, A. Gilder, M. Dredze, and F. Pereira.\nReading the markets: Forecasting public opinion of\npolitical candidates by news analysis. In Proceedings of\nthe 22nd International Conference on Computational\nLinguistics (Coling 2008), pages 473–480, Manchester,\nUK, August 2008. Coling 2008 Organizing Committee.\n[18] D. Lewis, Y. Yang, T. Rose, and F. Li. Rcv1: A new\nbenchmark collection for text categorization research.\nJournal of Machine Learning Research, 5:361–397,\nApril 2004.\n[19] B. Liu, M. Hu, and J. Cheng. Opinion observer:\nanalyzing and comparing opinions on the web. In\nWWW ’05: Proceedings of the 14th international\nconference on World Wide Web, pages 342–351, New\nYork, NY, USA, 2005. ACM.\n[20] C. Macdonald and I. Ounis. The trec blogs06\ncollection : Creating and analysing a blog test\ncollection. Technical report, University of Glasgow,\nDepartment of Computing Science, 2006.\n[21] N. Nicolov, F. Salvetti, and S. Ivanova. Sentiment\nanalysis: Does coreference matter? In Symposium on\nAffective Language in Human and Machine, 2008.\n[22] I. Ounis, C. MacDonald, and I. Soboroff. Overview of\nthe TREC-2008 Blog Track. In The Seventeenth Text\nREtrieval Conference (TREC 2008) Proceedings.\nNIST, 2008.\n[23] B. Pang and L. Lee. Seeing stars: Exploiting class\nrelationships for sentiment categorization with respect\nto rating scales. pages 115–124, 2005.\n[24] B. Pang and L. Lee. Opinion mining and sentiment\nanalysis. Now Publishers, 2008.\n[25] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?:\nSentiment classification using machine learning\ntechniques. In Proceedings of the 2002 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 79–86, 2002.\n[26] M. Porter. An algorithm for suffix stripping. Program,\n14(3):130–137, 1980.\n[27] Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and\nN. Kando. Overview of multilingual opinion analysis\ntask at NTCIR-7. 2008.\n[28] H. Tang, S. Tan, and X. Cheng. A survey on\nsentiment detection of reviews. Expert Systems With\nApplications, 2009.\n[29] P. Turney. Thumbs up or thumbs down? semantic\norientation applied to unsupervised classification of\nreviews. In Proceedings of 40th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n417–424, Philadelphia, Pennsylvania, USA, July 2002.\nAssociation for Computational Linguistics.\n[30] V. Vapnik. The nature of statistical learning theory.\n1995.\n[31] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing\ncontextual polarity in phrase-level sentiment analysis.\nIn Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural\nLanguage Processing, pages 347–354. Association for\nComputational Linguistics Morristown, NJ, USA,\n2005.\n[32] I. H. Witten and E. Frank. Data Mining: Practical\nmachine learning tools and techniques. Morgan\nKaufmann, 2nd edition, 2005.\n[33] M. Zhang and X. Ye. A generation model to unify\ntopic relevance and lexicon-based sentiment for\nopinion retrieval. In SIGIR ’08: Proceedings of the\n31st annual international ACM SIGIR conference on\nResearch and development in information retrieval,\npages 411–418, New York, NY, USA, 2008. ACM.\n",
      "id": 4760856,
      "identifiers": [
        {
          "identifier": "192372010",
          "type": "CORE_ID"
        },
        {
          "identifier": "21152935",
          "type": "CORE_ID"
        },
        {
          "identifier": "147598409",
          "type": "CORE_ID"
        },
        {
          "identifier": "143907304",
          "type": "CORE_ID"
        },
        {
          "identifier": "22872249",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.155.4864",
          "type": "OAI_ID"
        },
        {
          "identifier": "11309150",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.1145/1651461.1651464",
          "type": "DOI"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.323.6489",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:doras.dcu.ie:14830",
          "type": "OAI_ID"
        },
        {
          "identifier": "2088627781",
          "type": "MAG_ID"
        }
      ],
      "title": "Topic-dependent sentiment analysis of financial blogs",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:citeseerx.psu:10.1.1.323.6489",
        "oai:citeseerx.psu:10.1.1.155.4864",
        "oai:doras.dcu.ie:14830"
      ],
      "publishedDate": "2009-01-01T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [
        {
          "id": 16570552,
          "title": "A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval.",
          "authors": [],
          "date": "2008",
          "doi": "10.1145/1390334.1390405",
          "raw": "M. Zhang and X. Ye. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 411{418, New York, NY, USA, 2008. ACM.",
          "cites": null
        },
        {
          "id": 16570403,
          "title": "A study of inter-annotator agreement for opinion retrieval.",
          "authors": [],
          "date": "2009",
          "doi": "10.1145/1571941.1572127",
          "raw": "A. Bermingham and A. F. Smeaton. A study of inter-annotator agreement for opinion retrieval. In SIGIR '09: Proceedings of the 32nd annual international ACM SIGIR conference on Research and development in information retrieval, 2009.",
          "cites": null
        },
        {
          "id": 16570529,
          "title": "A survey on sentiment detection of reviews. Expert Systems With Applications,",
          "authors": [],
          "date": "2009",
          "doi": "10.1016/j.eswa.2009.02.063",
          "raw": "H. Tang, S. Tan, and X. Cheng. A survey on sentiment detection of reviews. Expert Systems With Applications, 2009.",
          "cites": null
        },
        {
          "id": 16570520,
          "title": "An algorithm for sux stripping.",
          "authors": [],
          "date": "1980",
          "doi": null,
          "raw": "M. Porter. An algorithm for sux stripping. Program, 14(3):130{137, 1980.",
          "cites": null
        },
        {
          "id": 16570437,
          "title": "An eective statistical approach to blog post opinion retrieval.",
          "authors": [],
          "date": "2008",
          "doi": "10.1145/1458082.1458223",
          "raw": "B. He, C. Macdonald, J. He, and I. Ounis. An eective statistical approach to blog post opinion retrieval. In CIKM '08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 1063{1072, New York, NY, USA, 2008. ACM.",
          "cites": null
        },
        {
          "id": 16570428,
          "title": "An extensive empirical study of feature selection metrics for text classi",
          "authors": [],
          "date": "2003",
          "doi": null,
          "raw": "G. Forman. An extensive empirical study of feature selection metrics for text classication. Journal of Machine Learning Research, 3(1):1533{7928, 2003.",
          "cites": null
        },
        {
          "id": 16570410,
          "title": "Automatic resource compilation by analyzing hyperlink structure and associated text.",
          "authors": [],
          "date": "1998",
          "doi": "10.1016/s0169-7552(98)00087-7",
          "raw": "S. Chakrabarti, B. Dom, P. Raghavan, S. Rajagopalan, D. Gibson, and J. Kleinberg. Automatic resource compilation by analyzing hyperlink structure and associated text. In WWW7: Proceedings of the seventh international conference on World Wide Web 7, pages 65{74, Amsterdam, The Netherlands, The Netherlands, 1998. Elsevier Science Publishers B. V.[6] S. Das and M. Chen. Yahoo! for amazon: Sentiment extraction from small talk on the web. Management Science, 53(9):1375{1388, 2007.",
          "cites": null
        },
        {
          "id": 16570398,
          "title": "Blog mining through opinionated words.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "G. Attardi and M. Simi. Blog mining through opinionated words. In E. M. Voorhees and L. P. Buckland, editors, TREC, volume Special Publication 500-272. National Institute of Standards and Technology (NIST), 2006.",
          "cites": null
        },
        {
          "id": 16570547,
          "title": "Data Mining: Practical machine learning tools and techniques.",
          "authors": [],
          "date": "2005",
          "doi": "10.1016/b978-0-12-374856-0.00005-5",
          "raw": "I. H. Witten and E. Frank. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2nd edition, 2005.",
          "cites": null
        },
        {
          "id": 16570433,
          "title": "Evaluating strategies for similarity search on the web. In",
          "authors": [],
          "date": "2002",
          "doi": "10.1145/511446.511502",
          "raw": "T. H. Haveliwala, A. Gionis, D. Klein, and P. Indyk. Evaluating strategies for similarity search on the web. In WWW '02: Proceedings of the 11th international conference on World Wide Web, pages 432{442, New York, NY, USA, 2002. ACM.",
          "cites": null
        },
        {
          "id": 16570440,
          "title": "Good news or bad news?: Let the market decide.",
          "authors": [],
          "date": "2004",
          "doi": "10.1007/1-4020-4102-0_22",
          "raw": "M. Koppel and I. Shtrimberg. Good news or bad news?: Let the market decide. In AAAI Spring Symposium on Exploring Attitude and Aect in Text, pages 86{88. Springer, 2004.",
          "cites": null
        },
        {
          "id": 16570448,
          "title": "Kle at trec",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and J.-H. Lee. Kle at trec 2008 blog track: Blog post and feed retrieval. In TREC, 2008.",
          "cites": null
        },
        {
          "id": 16570444,
          "title": "Mining anchor text for query re",
          "authors": [],
          "date": "2004",
          "doi": "10.1145/988672.988763",
          "raw": "R. Kraft and J. Zien. Mining anchor text for query renement. In WWW '04: Proceedings of the 13th international conference on World Wide Web, pages 666{674, New York, NY, USA, 2004. ACM.",
          "cites": null
        },
        {
          "id": 16570416,
          "title": "Mining the peanut gallery: opinion extraction and semantic classi of product reviews.",
          "authors": [],
          "date": "2003",
          "doi": "10.1145/775152.775226",
          "raw": "K. Dave, S. Lawrence, and D. M. Pennock. Mining the peanut gallery: opinion extraction and semantic classication of product reviews. In WWW '03: Proceedings of the 12th international conference on World Wide Web, pages 519{528, New York, NY, USA, 2003. ACM.",
          "cites": null
        },
        {
          "id": 16570393,
          "title": "Multi-lingual sentiment analysis of  news streams.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "K. Ahmad, D. Cheng, and Y. Almas. Multi-lingual sentiment analysis of nancial news streams. In Proceedings of the 1st International Conference on Grid in Finance, Palermo, 2006.",
          "cites": null
        },
        {
          "id": 16570431,
          "title": "Opinion analysis for business intelligence applications.",
          "authors": [],
          "date": "2008",
          "doi": "10.1145/1452567.1452570",
          "raw": "A. Funk, Y. Li, H. Saggion, K. Bontcheva, and C. Leibold. Opinion analysis for business intelligence applications. 2008.",
          "cites": null
        },
        {
          "id": 16570510,
          "title": "Opinion mining and sentiment analysis.",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "B. Pang and L. Lee. Opinion mining and sentiment analysis. Now Publishers, 2008.",
          "cites": null
        },
        {
          "id": 16570458,
          "title": "Opinion observer: analyzing and comparing opinions on the web. In",
          "authors": [],
          "date": "2005",
          "doi": "10.1145/1060745.1060797",
          "raw": "B. Liu, M. Hu, and J. Cheng. Opinion observer: analyzing and comparing opinions on the web. In WWW '05: Proceedings of the 14th international conference on World Wide Web, pages 342{351, New York, NY, USA, 2005. ACM.",
          "cites": null
        },
        {
          "id": 16570524,
          "title": "Overview of multilingual opinion analysis task at NTCIR-7.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multilingual opinion analysis task at NTCIR-7. 2008.",
          "cites": null
        },
        {
          "id": 16570500,
          "title": "Overview of the TREC-2008 Blog Track.",
          "authors": [],
          "date": "2008",
          "doi": "10.1145/1842890.1842899",
          "raw": "I. Ounis, C. MacDonald, and I. Soboro. Overview of the TREC-2008 Blog Track. In The Seventeenth Text REtrieval Conference (TREC 2008) Proceedings. NIST, 2008.",
          "cites": null
        },
        {
          "id": 16570454,
          "title": "Rcv1: A new benchmark collection for text categorization research.",
          "authors": [],
          "date": "2004",
          "doi": null,
          "raw": "D. Lewis, Y. Yang, T. Rose, and F. Li. Rcv1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361{397, April 2004.",
          "cites": null
        },
        {
          "id": 16570450,
          "title": "Reading the markets: Forecasting public opinion of political candidates by news analysis.",
          "authors": [],
          "date": "2008",
          "doi": "10.3115/1599081.1599141",
          "raw": "K. Lerman, A. Gilder, M. Dredze, and F. Pereira. Reading the markets: Forecasting public opinion of political candidates by news analysis. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 473{480, Manchester, UK, August 2008. Coling 2008 Organizing Committee.",
          "cites": null
        },
        {
          "id": 16570544,
          "title": "Recognizing contextual polarity in phrase-level sentiment analysis.",
          "authors": [],
          "date": "2005",
          "doi": "10.3115/1220575.1220619",
          "raw": "T. Wilson, J. Wiebe, and P. Homann. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347{354. Association for Computational Linguistics Morristown, NJ, USA, 2005.",
          "cites": null
        },
        {
          "id": 16570406,
          "title": "Reference directed indexing: Redeeming relevance for subject search in citation indexes.",
          "authors": [],
          "date": "2003",
          "doi": "10.1007/978-3-540-45175-4_45",
          "raw": "S. Bradshaw. Reference directed indexing: Redeeming relevance for subject search in citation indexes. In Proc. of the 7th ECDL, pages 499{510, 2003.",
          "cites": null
        },
        {
          "id": 16570505,
          "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.",
          "authors": [],
          "date": "2005",
          "doi": "10.3115/1219840.1219855",
          "raw": "B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. pages 115{124, 2005.",
          "cites": null
        },
        {
          "id": 16570464,
          "title": "Sentiment analysis: Does coreference matter? In",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "N. Nicolov, F. Salvetti, and S. Ivanova. Sentiment analysis: Does coreference matter? In Symposium on Aective Language in Human and Machine, 2008.",
          "cites": null
        },
        {
          "id": 16570421,
          "title": "Sentiment polarity identi in  news: A cohesion-based approach.",
          "authors": [],
          "date": "2007",
          "doi": null,
          "raw": "A. Devitt and K. Ahmad. Sentiment polarity identication in nancial news: A cohesion-based approach. In Annual Meeting of the Association of Computational Linguistics, volume 45, page 984, 2007.",
          "cites": null
        },
        {
          "id": 16570426,
          "title": "Sentiwordnet: A publicly available lexical resource for opinion mining.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "A. Esuli and F. Sebastiani. Sentiwordnet: A publicly available lexical resource for opinion mining. In 5th Conference on Language Resources and Evaluation, pages 417{422, 2006.",
          "cites": null
        },
        {
          "id": 16570539,
          "title": "The nature of statistical learning theory.",
          "authors": [],
          "date": "1995",
          "doi": "10.1007/978-1-4757-2440-0",
          "raw": "V. Vapnik. The nature of statistical learning theory. 1995.",
          "cites": null
        },
        {
          "id": 16570461,
          "title": "The trec blogs06 collection : Creating and analysing a blog test collection.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "C. Macdonald and I. Ounis. The trec blogs06 collection : Creating and analysing a blog test collection. Technical report, University of Glasgow, Department of Computing Science, 2006.",
          "cites": null
        },
        {
          "id": 16570534,
          "title": "Thumbs up or thumbs down? semantic orientation applied to unsupervised classi of reviews.",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1073083.1073153",
          "raw": "P. Turney. Thumbs up or thumbs down? semantic orientation applied to unsupervised classication of reviews. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 417{424, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.",
          "cites": null
        },
        {
          "id": 16570515,
          "title": "Thumbs up?: Sentiment classi using machine learning techniques.",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1118693.1118704",
          "raw": "B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: Sentiment classication using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79{86, 2002.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/14830/1/tsa20-ohare.pdf",
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.155.4864"
      ],
      "updatedDate": "2021-10-14T16:15:04",
      "yearPublished": 2009,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11309150.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11309150"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11309150/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11309150/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4760856"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1512.04466",
      "authors": [
        {
          "name": "Zhai, Shuangfei"
        },
        {
          "name": "Zhang, Zhongfei"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/386118113"
      ],
      "createdDate": "2016-08-03T02:35:13",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "In this paper, we investigate the usage of autoencoders in modeling textual\ndata. Traditional autoencoders suffer from at least two aspects: scalability\nwith the high dimensionality of vocabulary size and dealing with\ntask-irrelevant words. We address this problem by introducing supervision via\nthe loss function of autoencoders. In particular, we first train a linear\nclassifier on the labeled data, then define a loss for the autoencoder with the\nweights learned from the linear classifier. To reduce the bias brought by one\nsingle classifier, we define a posterior probability distribution on the\nweights of the classifier, and derive the marginalized loss of the autoencoder\nwith Laplace approximation. We show that our choice of loss function can be\nrationalized from the perspective of Bregman Divergence, which justifies the\nsoundness of our model. We evaluate the effectiveness of our model on six\nsentiment analysis datasets, and show that our model significantly outperforms\nall the competing methods with respect to classification accuracy. We also show\nthat our model is able to take advantage of unlabeled dataset and get improved\nperformance. We further show that our model successfully learns highly\ndiscriminative feature maps, which explains its superior performance.Comment: To appear in AAAI 201",
      "documentType": "research",
      "doi": "10.1609/aaai.v30i1.10159",
      "downloadUrl": "http://arxiv.org/abs/1512.04466",
      "fieldOfStudy": null,
      "fullText": "ar\nX\niv\n:1\n51\n2.\n04\n46\n6v\n1 \n [c\ns.L\nG]\n  1\n4 D\nec\n 20\n15\nSemisupervised Autoencoder for Sentiment Analysis\nShuangfei Zhai Zhongfei (Mark) Zhang\nComputer Science Department, Binghamton University\n4400 Vestal Pkwy E, Binghamton, NY 13902\nszhai2@binghamton.edu zhongfei@cs.binghamton.edu\nAbstract\nIn this paper, we investigate the usage of autoencoders\nin modeling textual data. Traditional autoencoders suf-\nfer from at least two aspects: scalability with the high\ndimensionality of vocabulary size and dealing with\ntask-irrelevant words. We address this problem by in-\ntroducing supervision via the loss function of autoen-\ncoders. In particular, we first train a linear classifier on\nthe labeled data, then define a loss for the autoencoder\nwith the weights learned from the linear classifier. To\nreduce the bias brought by one single classifier, we de-\nfine a posterior probability distribution on the weights\nof the classifier, and derive the marginalized loss of the\nautoencoder with Laplace approximation. We show that\nour choice of loss function can be rationalized from the\nperspective of Bregman Divergence, which justifies the\nsoundness of our model. We evaluate the effectiveness\nof our model on six sentiment analysis datasets, and\nshow that our model significantly outperforms all the\ncompeting methods with respect to classification accu-\nracy. We also show that our model is able to take ad-\nvantage of unlabeled dataset and get improved perfor-\nmance. We further show that our model successfully\nlearns highly discriminative feature maps, which ex-\nplains its superior performance.\nIntroduction\nIn machine learning, documents are usually represented as\nBag of Words (BoW), which nicely reduces a piece of text\nwith arbitrary length to a fixed length vector. Despite its sim-\nplicity, BoW remains the dominant representation in many\napplications including text classification. There has also\nbeen a large body of work dedicated to learning useful rep-\nresentations for textual data (Turney and Pantel 2010;\nBlei, Ng, and Jordan 2003; Deerwester et al. 1990;\nMikolov et al. 2013; Glorot, Bordes, and Bengio 2011).\nBy exploiting the co-occurrence pattern of words, one can\nlearn a low dimensional vector that forms a compact and\nmeaningful representation for a document. The new repre-\nsentation is often found useful for subsequent tasks such as\ntopic visualization and information retrieval. In this paper,\nwe investigate the application of one of the most popular\nrepresentation learning methods, namely autoencoders\nCopyright c© 2015, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(Bengio 2009), to learn task-dependent representations for\ntextual data. Our model differs from most of the existing\nwork as it naturally incorporates label information into its\nobjective function, which allow the learned representation\nto be directly coupled with the task of interest.\nIn this paper we focus on a specific class of task in text\nmining: Sentiment Analysis (SA). We further focus on a spe-\ncial case of SA as a binary classification problem, where a\ngiven piece of text is either of positive or negative attitude.\nThis problem is interesting largely due to the emergence of\nonline social networks, where people consistently express\ntheir opinions about certain subjects. Also, it is easy to ob-\ntain a large amount of clean labeled data for SA by crawling\nreviews from websites such as IMDB or Amazon. Thus, SA\nis an ideal benchmark for evaluating text classification mod-\nels (and features).\nAutoencoders have attracted a lot of attention in recent\nyears as a building block of Deep Learning (Bengio 2009).\nThey act as the feature learning methods by reconstructing\ninputs with respect to a given loss function. In a neural net-\nwork implementation of autoencoders, the hidden layer is\ntaken as the learned feature. While it is often trivial to ob-\ntain good reconstructions with plain autoencoders, much ef-\nfort has been devoted on regularizations in order to prevent\nthem against overfitting (Bengio 2009; Vincent et al. 2008;\nRifai et al. 2011b). However, little attention has been de-\nvoted to the loss function, which we argue is critical for\nmodeling textual data. The problem with the commonly\nadopted loss functions (squared Euclidean distance and\nelement-wise KL Divergence, for instance) is that they try to\nreconstruct all dimensions of input independently and undis-\ncriminatively. However, we argue that this is not the optimal\napproach when our interest is text classification. The reason\nis two folds. First, it is well known that in natural language\nthe distribution of word occurrences follows the power-law.\nThis means that a few of the most frequent words will ac-\ncount for most of the probability mass of word occurrences.\nAn immediate result is that the Autoencoder puts most of\nits effort on reconstructing the most frequent words well but\n(to a certain extent) ignores the less frequent ones. This may\nlead to a bad performance especially when the class distribu-\ntion is not well captured by merely the frequent words. For\nsentiment analysis, this problem is especially severe because\nit is obvious that the truly useful features (words or phrases\nexpressing a clear polarity) only occupy a small fraction of\nthe whole vocabulary; and reconstructing irrelevant words\nsuch as ’actor’ or ’movie’ very well is not likely to help\nlearn more useful representations to classify the sentiment\nof movie reviews. Second, explicitly reconstructing all the\nwords in an input text is expensive, because the latent rep-\nresentation has to contain all aspects of the semantic space\ncarried by the words, even if they are completely irrelevant.\nAs the vocabulary size can easily reach the range of tens\nof thousands even for a moderate sized dataset, the hidden\nlayer size has to be chosen very large to obtain a reasonable\nreconstruction, which causes a huge waste of model capacity\nand makes it difficult to scale to large problems.\nIn fact, the reasoning above applies to all the unsuper-\nvised learning methods in general, which we argue is one\nof the most important problems to address in order to learn\ntask-specific representations. This naturally leads us to the\nsemisupervised approach, where label information is intro-\nduced to guide the feature learning procedure. In particular,\nwe propose a novel loss function for training autoencoders\nthat are directly coupled with the classification task. We first\ntrain a linear classifier on BoW, then a Bregman Divergence\n(Banerjee et al. 2004) is derived as the loss function of a\nsubsequent autoencoder. The new loss function gives the au-\ntoencoder the information about directions along which the\nreconstruction should be accurate, and where larger recon-\nstruction errors are tolerated. Informally, this can be con-\nsidered as a weighting of words based on their correlations\nwith the class label: predictive words should be given large\nweights in the reconstruction even they are not frequent\nwords, and vice versa. Furthermore, to reduce the bias in-\ntroduced by the linear classifier, we take a Bayesian view\nby defining a posterior distribution on the weights of the\nclassifier. We then approximate the posterior with Laplace\napproximation and derive the marginalized loss function for\nthe autoencoder. We show that our model successfully learns\nfeatures that are highly discriminative with respect to class\nlabels, and also outperform all the competing methods eval-\nuated by classification accuracy. Moreover, the derived loss\ncan also be applied to unlabeled data, which allows the\nmodel to learn further better representations.\nModel\nDenoising Autoencoders\nAutoencoders learn functions that can reconstruct the inputs.\nThey are typically implemented as a neural network with\none hidden layer, and one can extract the activation of the\nhidden layer as the new representation. Mathematically, we\nare given a collection of data pointsX = {xi}, xi ∈ Rd, i ∈\n[1,m], the objective function of an autoencoder is thus:\nmin\n∑\ni\nD(x˜i, xi)\ns.t. hi = g(Wxi + b), x˜i = f(W\n′hi + b\n′)\n(1)\nwhere W ∈ Rk×d, b ∈ Rk,W ′ ∈ Rd×k, b′ ∈ Rd are the\nparameters to be learned; D is a loss function, such as the\nsquared Euclidean Distance ‖x˜−x‖22; g and f are predefined\nnonlinear functions, which we set as g(x) = max(0, x),\nf(x) = (1+exp(−x))−1 in this paper; hi is the learned rep-\nresentation; x˜i is the reconstruction. A common approach is\nto use tied weights by setting W = W ′; this usually works\nbetter as it speeds up learning and prevents overfitting at the\nsame time. For this reason, we always use tied weights in\nthis paper.\nAutoencoders transform an unsupervised learning prob-\nlem to a supervised one by the self reconstruction criteria.\nThis enables one to use all the tools developed for supervised\nlearning such as back propagation to efficiently train the au-\ntoencoders. Moreover, thanks to the nonlinear functions f\nand g, autoencoders are able to learn non-linear and possibly\novercomplete representations, which give the model much\nmore expressive power than their linear counter parts such\nas PCA (LSA) (Deerwester et al. 1990).\nIn this paper, we adopt one of the most popular variants\nof autoencoders, namely Denoising Autoencoder. Denois-\ning Autoencoder works by reconstructing the input from\na noised version of itself. The intuition is that a robust\nmodel should be able to reconstruct the input well even in\nthe presence of noises, due to the high correlation among\nfeatures. For example, imagine deleting or adding a few\nwords from/to a document, the semantics should still remain\nunchanged, thus the autoencoder should learn a consistent\nrepresentation from all the noisy inputs. In the high level,\nDenoising Autoencoders are equivalent to ordinary autoen-\ncoders trained with dropout (Srivastava et al. 2014), which\nhas been shown as an effective regularizer for (deep) neu-\nral networks. Formally, let q(x¯|x) be a predefined noising\ndistribution, and x¯ be a noised sample of x: x¯ ∼ q(x¯|x).\nThe objective function takes the form of sum of expectations\nover all the noisy samples:\nmin\n∑\ni\nEq(x¯i|xi)D(x˜i, xi)\ns.t. hi = g(Wx¯i + b), x˜i = f(W\n′hi + b\n′)\n(2)\nwhere we have slightly overloaded the notation to let x˜i de-\nnote the reconstruction calculated from the noised input x¯i.\nWhile the marginal objective function requires infinite many\nnoised samples per data point, in practice it is sufficient to\nsimulate it stochastically. That is, for each example seen in\nthe stochastic gradient descent training, we randomly sam-\nple a x¯i from q(x¯i|xi) and calculate the gradient with ordi-\nnary back propagation.\nLoss Function as Bregman Divergence\nWe then discuss the proper choice of the loss function D\nin (2) as a specific form of Bregman Divergence. Bregman\nDivergence (Banerjee et al. 2004) generalizes the notion of\ndistance in a d dimensional space. To be concrete, given two\ndata points x˜, x ∈ Rd and a convex function f(x) defined\non Rd, the Bregman Divergence of x˜ from x with respect to\nf is:\nDf (x˜, x) = f(x˜)− (f(x) +∇f(x)\nT\n(x˜− x)). (3)\nNamely, Bregman Divergence measures the distance be-\ntween two points x˜, x as the deviation between the function\nvalue of f and the linear approximation of f around x at x˜.\nTwo of the most commonly used loss functions for au-\ntoencoders are the squared Euclidean distance and element-\nwise KL divergence. It is not difficult to verify that they both\nfall into this family by choosing f as the squared ℓ2 norm\nand the sum of element-wise entropy respectively. What the\ntwo loss functions have in common is that they make no\ndistinction among dimensions of the input. In other words,\neach dimension of the input is pushed to be reconstructed\nequally well. While autoencoders trained in this way have\nbeen shown to work very well on image data, learning much\nmore interesting and useful features than the original pixel\nintensity features, they are less appropriate for modeling tex-\ntual data. The reason is two folds. First, textual data are\nextremely sparse and high dimensional, where the dimen-\nsionality is equal to the vocabulary size. To maintain all the\ninformation of the input in the hidden layer, a very large\nlayer size must be adopted, which makes the training cost\nextremely large. Second, ordinary autoencoders are not able\nto deal with the power law of word distributions, where a few\nof the most frequent words account for most of the word oc-\ncurrences. As a result, frequent words naturally gain favor\nto being reconstructed accurately, and rare words tend to be\nreconstructed with less precision. This problem is also anal-\nogous to the imbalanced classification setting. This is es-\npecially problematic when frequent words carry little infor-\nmation about the task of interest, which is not uncommon.\nExamples include stop words (the, a, this, from) and topic\nrelated terms (movie, watch, actress) in a movie review sen-\ntiment analysis task.\nSemisupervised Autoencoder with Bregman\nDivergence\nTo address the problems mentioned above, we propose to\nintroduce supervision to the training of autoencoders. To\nachieve this, we first train a linear classifier on Bag of Words,\nand then use the weight of the learned classifier to define a\nnew loss function for the autoencoder. Now let us first de-\nscribe our choice of loss function, and then elaborate the\nmotivation later:\nD(x˜, x) = (θT (x˜− x))2. (4)\nwhere θ ∈ Rd are the weights of the linear classifier, and we\nhave omitted the bias for simplicity. Before we delve into\nmore details, note that Equation (4) is a valid distance, as\nit is non-negative and reaches zeros if and only if x˜ = x.\nMoreover, the reconstruction error is only measured after\nprojecting on θ; this guides the reconstruction to be accurate\nonly along directions where the linear classifier is sensitive\nto. Note also that Equation (4) on the one hand uses label\ninformation (θ has been trained with labeled data), on the\nother hand no explicit labels are directly referred to (only re-\nquires xi). Thus one is able to train an autoencoder on both\nlabeled and unlabeled data with the loss function in Equa-\ntion (4). This subtlety distinguishes our method from pure\nsupervised or unsupervised learning, and allows us to enjoy\nthe benefit from both worlds.\nAs a design choice, we consider SVM with squared hinge\nloss (SVM2) and ℓ2 regularization as the linear classifier,\nbut other classifiers such as Logistic Regression can be used\nand analyzed similarly. Let us denote {xi}, xi ∈ Rd as the\ncollection of samples, and {yi}, yi ∈ {1,−1} as the class\nlabels; the objective function SVM2 is:\nL(θ) =\n∑\ni\n(max(0, 1− yiθ\nTxi))\n2 + λ‖θ‖2. (5)\nHere θ ∈ Rd is the weight; λ is the weight decay parameter.\nEquation (5) is continuous and differentiable everywhere\nwith respect to θ, so the model can be easily trained with\nstochastic gradient descent. The next (and most critical) step\nof our approach is to transfer label information from the lin-\near classifier to the autoencoder. With this in mind, we ex-\namine the loss induced by each sample as a function of the\ninput, while with θ fixed:\nf(xi) = (max(0, 1− yiθ\nTxi))\n2 (6)\nNote that f(xi) is defined on the input space Rd, which\nshould be contrasted with L(θ) in Equation (5) which is a\nfunction of θ. We are interested in f(xi) because if we con-\nsider moving each input xi to x˜i, f(xi) indicates the direc-\ntion along which the loss is sensitive to. If we think of x˜\nas the reconstruction of xi obtained from an autoencoder, a\ngood x˜i should be in a way such that the deviation of x˜i from\nxi is small evaluated by f(xi). In other words, we would\nlike x˜i to still be correctly classified by the pretrained linear\nclassifier. Therefore, f(xi) should be a much better function\nto evaluate the deviation of two samples. if we can derive a\nBregman Divergence from f(xi) and use it as the loss func-\ntion of the subsequent autoencoder training, the autoencoder\nshould be guided to give reconstruction errors that do not\nconfuse the classifier. Note that f(xi) is a quadratic func-\ntion of xi whenever f(xi) > 0, so we only need to derive\nthe Hessian matrix in order to achieve the Bregman Diver-\ngence. The Hessian follows as:\nH(xi) =\n{\nθθT , if 1− yiθTxi > 0\n0, otherwise. (7)\nRecall that for a quadratic function with Hessian matrix H ,\nthe Bregman Divergence is simply (x˜− x)TH(x˜− x); then\nwe have:\nD(x˜i, xi) =\n{\n(θT (x˜i − xi))2, if 1− yiθTxi > 0\n0, otherwise (8)\nIn words, Equation (8) says that we measure the recon-\nstruction loss for difficult examples (those that satisfy 1 −\nyiθ\nTxi > 0) with Equation (4); and there is no reconstruc-\ntion loss at all for easy examples. This discrimination is un-\ndesirable, because in this case the Autoencoder would com-\npletely ignore easy examples, and there is no way to guar-\nantee that the x˜i can be correctly classified. Actually, this\nsplit is just an artifact of the hinge loss and the asymmetri-\ncal property of Bregman Divergence. Hence, we perform a\nsimple correction by ignoring the condition in Equation (8),\nwhich basically pretends that all the examples induce a loss.\nThis directly yields the loss function as in Equation (4).\nThe Bayesian Marginalization\nIn principle, one may directly apply Equation (4) as the loss\nfunction in place of the squared Euclidean distance and train\nan autoencoder. However, doing so might introduce a bias\nbrought by one single classifier. As a remedy, we resort to\nthe Bayesian approach, which defines a probability distribu-\ntion over θ. Although SVM2 is not a probabilistic classifier\nlike Logistic Regression, we can borrow the idea of Energy\nBased Model (Bengio 2009) and use L(θ) as the negative\nlog likelihood of the following distribution:\np(θ) =\nexp(−βL(θ))∫\nexp(−βL(θ))dθ\n(9)\nwhere β > 0 is the temperature parameter which controls\nthe shape of the distribution p. Note that the larger β is, the\nsharper p will be. In the extreme case, p(θ) is reduced to a\nuniform distribution as β approaches 0, and collapses into a\nsingle δ function as β goes to positive infinity.\nGiven p(θ), we rewrite Equation (4) as an expectation\nover θ:\nD(x˜, x) = Eθ∼p(θ)(θ\nT (x˜ − x))2 =\n∫\n(θT (x˜− x))2p(θ)dθ.\n(10)\nObviously there is now no closed form expression for\nD(x˜, x). To solve it one could use sampling methods such\nas MCMC, which provides unbiased estimates of the ex-\npectation but could be slow in practice. Instead, we use\nthe Laplace approximation, which approximates p(θ) by a\nGaussian distribution p˜(θ) = N (θˆ,Σ). As estimating the\nfull covariance matrix is prohibitive, we further constrain Σ\nto be diagonal. The benefit of doing so is that the expectation\ncan now be computed directly in closed form. To see this, by\nsimply replacing p(θ) with p˜(θ) in Equation (11):\nD(x˜, x) =Eθ∼p˜(θ)(θ\nT (x˜− x))2\n=(x˜− x)TEθ∼p˜(θ)(θθ\nT )(x˜− x)\n=(x˜− x)T (θˆθˆT +Σ)(x˜− x)\n=(θˆT (x˜− x))2 + (Σ\n1\n2 (x˜− x))T (Σ\n1\n2 (x˜ − x)).\n(11)\nwhere D now involves two parts, corresponding to the mean\nand variance term of the Gaussian distribution respectively.\nNow let us derive p˜(θ) for p(θ). In Laplace approximation, θˆ\nis chosen as the mode of p(θ), which is exactly the solution\nto the SVM2 optimization problem. For Σ, we have:\nΣ =(diag(\n∂2L(θ)\n∂θ2\n))−1\n=\n1\nβ\n(diag(\n∑\ni\nI(1− yiθ\nTxi > 0)x\n2\ni ))\n−1\n(12)\nHere we have overridden diag but letting it denote a diago-\nnal matrix induced either by a square matrix or a vector; I\nis the indicator function; (·)−1 denotes matrix inverse. Inter-\nestingly, the second term in Equation (11) is now equivalent\nTable 1: Statistics of the datasets.\nIMDB books DVD music electronics kitchenware\n# train 25,000 10,000 10,000 18,000 6,000 6,000\n# test 25,000 3,105 2,960 2,661 2,862 1,691\n# unlabeled 50,000 N/A N/A N/A N/A N/A\n# features 8,876 9,849 10,537 13,099 5,091 3,907\n% positive 50 49.81 49.85 50.16 49.78 50.08\nto the squared Euclidean distance after performing element-\nwise normalizing the input using all difficult examples. The\neffect of this normalization is that the reconstruction errors\nof frequent words are down weighted; on the other hand, dis-\ncriminative words are given higher weights as they would\noccur less frequently in difficult examples. Note that it is\nimportant to use a relatively large β in order to avoid the\nvariance term dominating the mean term. In other words, we\nneed to ensure p(θ) to be reasonable peaked around θˆ to ef-\nfective take advantage of label information.\nExperiments\nDatasets\nWe evaluate our model on six Sentiment Analysis\nbenchmarks. The first one is the IMDB dataset 1\n(Maas et al. 2011), which consists of movie reviews col-\nlected from IMDB. The IMDB dataset is one of the\nlargest sentiment analysis dataset that is publicly avail-\nable; it also comes with an unlabeled set which al-\nlows us to evaluate semisupervised learning methods.\nThe rest five datasets are all collected from Amazon\n2(Blitzer, Dredze, and Pereira 2007), which corresponds to\nthe reviews of five different products: books, DVDs, music,\nelectronics, kitchenware. All the six datasets are already to-\nkenized as either uni-gram or bi-gram features. For compu-\ntational reasons, we only select the words that occur in at\nleast 30 training examples. We summarize the statistics of\ndatasets in Table 1.\nMethods\n• Bag of Words (BoW). Instead of using the raw word\ncounts directly, we take a simple step of data normaliza-\ntion:\nxi,j =\nlog(1 + ci,j)\nmaxj log(1 + ci,j)\n(13)\nwhere ci,j denotes the number of occurrences of the jth\nword in the ith document, xi,j denotes the normalized\ncount. We choose this normalization because it preserves\nthe sparsity of the Bag of Words features; also each fea-\nture element is normalized to the range [0, 1]. Note that\nthe very same normalized Bag of Words features are fed\ninto the autoencoders.\n1http://ai.stanford.edu/ amaas/data/sentiment/\n2http://www.cs.jhu.edu/ mdredze/datasets/sentiment/\n• Denoising Autoencoder (DAE) (Vincent et al. 2008).\nThis refers to the regular Denoising Autoencoder de-\nfined in Equation (1) with squared Euclidean dis-\ntance loss: D(x˜, x) = ‖x˜ − x‖22. This is also used\nin (Glorot, Bordes, and Bengio 2011) on the Amazon\ndatasets for domain adaptation. We use ReLu max(0, x)\nas the activation function, and Sigmoid as the decoding\nfunction.\n• Denoising Autoencoder with Finetuning (DAE+)\n(Vincent et al. 2008). This denotes the common approach\nto continue training an DAE on labeled data by replacing\nthe decoding part of DAE with a Softmax layer.\n• Feedforward Neural Network (NN). This is the standard\nfully connected neural network with one hidden layer and\nrandom initialization. We use the same activation function\nas that in Autoencoders, i.e., ReLU.\n• Logistic Regression with Dropout (LrDrop)\n(Wager, Wang, and Liang 2013). This is a model where\nlogistic regression is regularized with the marginalized\ndropout noise. LrDrop differs from our approach as it\nuses feature noising as an explicit regularization. Another\ndifference is that our model is able to learn nonlinear\nrepresentations, not merely a classifier, and thus is\npotentially able to model more complicated patterns in\ndata.\n• Semisupervised Bregman Divergence Autoencoder (SB-\nDAE). This corresponds to our model with Denoising Au-\ntoencoder as the feature learner. The training process is\nroughly equivalent to training on BoW followed by the\ntraining of DAE, except that the loss function of DAE is\nreplaced with the loss function defined in Equation (11).\nWe cross validate β from the set {104, 105, 106, 107, 108}\n(note that larger β corresponds to weaker Bayesian regu-\nlarization).\n• Semisupervised Bregman Divergence Autoencoder with\nFinetuning (SBDAE+).\nNote that except for BoW and LrDrop, all the other meth-\nods require a predefined dimensionality of representation.\nWe use fixed sizes on all the datasets. For SBDAE and NN, a\nsmall hidden size is sufficient, so we use 200. For DAE, we\nobserve that it benefits from very large hidden sizes; how-\never, due to computational constraints, we take 2000. For\nBoW, DAE, SBDAE, we use SVM2 as the classifier. All the\nmodels are trained with mini-batch Stochastic Gradient De-\nscent with momentum of 0.9.\nResults\nWe first summarize the results as in classification error rate\nin Table 2. First of all, our model consistently beats BoW\nwith a margin, and it achieves the best results on four (larger)\ndatasets out of six. On the other hand, DAE, DAE+ and NN\nall fail to outperform BoW, although they share the same\narchitecture as nonlinear classifiers. This suggests that SB-\nDAE be able to learn a much better nonlinear feature trans-\nformation function by training with a more informed objec-\ntive (than that of DAE). Moreover, note also that finetun-\ning on labeled set (DAE+) significantly improves the perfor-\nmance of DAE, which is ultimately on a par with training a\nneural net with random initialization (NN). However, fine-\ntuning offers little help to SBDAE, as it is already implicitly\nguided by labels during the training.\nLrDrop is the second best method that we have tested.\nThanks to the usage of dropout regularization, it consis-\ntently outperforms BoW, and achieves the best results on\ntwo (smaller) datasets. Compared with LrDrop, it appears\nthat our model works better on large datasets (≈ 10K words,\nmore than 10K training examples) than smaller ones. This\nindicates that in high dimensional spaces with sufficient\nsamples, SBDAE benefits from learning a nonlinear feature\ntransformation that disentangles the underlying factors of\nvariation, while LrDrop is incapable of doing so due to its\nnature as a linear classifier.\nAs the training of the autoencoder part of SBDAE does\nnot require the availability of labels, we also try incorporat-\ning unlabeled data after learning the linear classifier in SB-\nDAE. As shown in Table 2, doing so further improves the\nperformance over using labeled data only. This justifies that\nit is possible to bootstrap from a relatively small amount of\nlabeled data and learn better representations with more un-\nlabeled data with SBDAE.\nTo gain more insights of the results, we further visual-\nize the filters learned by SBDAE and DAE on the IMDB\ndataset in Table 3. In particular, we show the top 5 most\nactivated and deactivated words of the first 8 filters (corre-\nsponding to the first 8 rows of W ) of SBDAE and DAE, re-\nspectively. First of all, it seems very difficult to make sense\nof the filters of DAE as they are mostly common words with\nno clear co-occurrence pattern. By comparison, if we look at\nthe filters from SBDAE, they are mostly sensitive to words\nthat demonstrate clear polarity. In particular, all the 8 filters\nseem to be most activated by certain negative words, and are\nmost deactivated by certain positive words. In this way, the\nactivation of each filter of SBDAE is much more indicative\nof the polarity than that of DAE, which explains the better\nperformance of SBDAE over DAE. Note that this difference\nonly comes from reweighting the reconstruction errors in a\ncertain way, with no explicit usage of labels.\nRelated Work and Discussion\nOur work falls into the general category of learning rep-\nresentations for text data. In particular, there have been a\nlot of efforts that try to learn compact representations for\neither words or documents (Turney and Pantel 2010;\nBlei, Ng, and Jordan 2003; Deerwester et al. 1990;\nMikolov et al. 2013; Le and Mikolov 2014;\nMaas et al. 2011). LDA (Blei, Ng, and Jordan 2003)\nexplicitly learns a set of topics, each of which is de-\nfined as a distribution on words; a document is thus\nrepresented as the posterior distribution on topics,\nwhich is a fixed-length, non-negative vector. Closely\nrelated are matrix factorization models such as LSA\n(Deerwester et al. 1990) and Non-negative Matrix Fac-\ntorization (NMF) (Xu, Liu, and Gong 2003). While LSA\nfactorizes the doc-term matrix via Singular Value Decom-\nposition, NMF learns non-negative basis and coefficient\nvectors. Similar to these efforts, our model also works\nTable 2: Left: our model achieves the best results on four (large ones) out of six datasets. Right: our model is able to take\nadvantage of unlabeled data and gain better performance.\nbooks DVD music electronics kitchenware IMDB IMDB + unlabled\nBoW 10.76 11.82 11.80 10.41 9.34 11.48 N/A\nDAE 15.10 15.64 15.44 14.74 12.48 14.60 13.28\nDAE+ 11.40 12.09 11.80 11.53 9.23 11.48 11.47\nNN 11.05 11.89 11.42 11.15 9.16 11.60 N/A\nLrDrop 9.53 10.95 10.90 9.81 8.69 10.88 10.73\nSBDAE 9.16 10.90 10.59 10.02 8.87 10.52 10.42\nSBDAE+ 9.12 10.90 10.58 10.01 8.83 10.50 10.41\nTable 3: Visualization of learned feature maps. From top to bottom: most activated and deactivated words for SBDAE; most\nactivated and deactivated words for DAE.\nnothing disappointing badly save even dull excuse ridiculously\ncannon worst disappointing redeeming attempt fails had dean\noutrageously unfortunately annoying awful unfunny stupid failed none\nlends terrible worst sucks couldn’t worst rest ruined\nteacher predictable poorly convince worst avoid he attempt\nfirst tears loved amazing excellent perfect years with\nclassic wonderfully finest incredible surprisingly ? terrific best\nman helps noir funniest beauty powerful peter recommended\nhard awesome magnificent unforgettable unexpected excellent cool perfect\nstill terrific scared captures appreciated favorite allows heart\nlong wasn’t probably to making laugh tv someone\nworst guy fan the give find might yet\nkids music kind and performances where found goes\nanyone work years this least before kids away\ntrying now place shows comes ever having poor\ndone least go kind recommend although ending worth\nfind book trying takes instead everyone once interesting\nbefore day looks special wife anything wasn’t isn’t\nwork actors everyone now shows comes american rather\nwatching classic performances someone night away sense around\ndirectly on the doc-term matrix. However, thanks to the\nusage of autoencoder, the representation for documents\nare calculated instantly via direct matrix product, which\neliminates the need of expensive inference. Our work also\ndistinguishes itself from other work as a semisupervised\nrepresentation learning model, where label information can\nbe effectively leveraged.\nRecently, there has also been an active thread of\nresearch on learning word representations. Notably,\n(Mikolov et al. 2013) shows that we can learn inter-\nesting word embeddings via very simple architecture\non a large amount of unlabeled dataset. Moreover,\n(Le and Mikolov 2014) proposed to jointly learn repre-\nsentations for sentences and paragraphs together with\nwords in a similar unsupervised fashion. While our work\ndoes not explicitly model the representations for words,\nit is straightforward to incorporate this idea by adding an\nadditional linear layer at the bottom of the autoencoder.\nFrom the perspective of machine learning methodology,\nour approach resembles the idea of layer-wise pretraining\nin deep Neural Networks (Bengio 2009). Our model dif-\nfers from the traditional training procedure of autoencoders\nin that we effectively utilize the label information to guide\nthe representation learning. Related idea has been proposed\nin (Socher et al. 2011), where they train Recursive autoen-\ncoders on sentences jointly with prediction of sentiment.\nDue to the delicate recursive architecture, their model only\nworks on sentences with given parsing trees, and could not\ngeneralize to documents. MTC (Rifai et al. 2011a) is an-\nother work that models the interaction of autoencoders and\nclassifiers. However, their training of autoencoders is purely\nunsupervised, the interaction comes into play by requiring\nthe classifier to be invariant along the tangents of the learned\ndata manifold. It is not difficult to see that the assumption of\nMTC would not hold when the class labels did not align well\nwith the data manifold, which is a situation our model does\nnot suffer from.\nConclusion\nIn this paper, we have proposed a novel extension to autoen-\ncoders for learning task-specific representations for textual\ndata. We have generalized the traditional autoencoders by\nrelaxing their loss function to the Bregman Divergence, and\nthen derived a discriminative loss function from the label\ninformation. Experiments on text classification benchmarks\nhave shown that our model significantly outperforms Bag of\nWords, traditional Denoising Autoencoder, and other com-\npeting methods. We have also qualitatively visualized that\nour model successfully learns discriminative features, which\nunsupervised methods fail to do.\nAcknowledgments\nThis work is supported in part by NSF (CCF-1017828).\nReferences\n[Banerjee et al. 2004] Banerjee, A.; Merugu, S.; Dhillon,\nI. S.; and Ghosh, J. 2004. Clustering with bregman diver-\ngences. In Proceedings of the Fourth SIAM International\nConference on Data Mining, Lake Buena Vista, Florida,\nUSA, April 22-24, 2004, 234–245.\n[Bengio 2009] Bengio, Y. 2009. Learning deep architec-\ntures for AI. Foundations and Trends in Machine Learning\n2(1):1–127.\n[Blei, Ng, and Jordan 2003] Blei, D. M.; Ng, A. Y.; and Jor-\ndan, M. I. 2003. Latent dirichlet allocation. Journal of\nMachine Learning Research 3:993–1022.\n[Blitzer, Dredze, and Pereira 2007] Blitzer, J.; Dredze, M.;\nand Pereira, F. 2007. Biographies, bollywood, boom-boxes\nand blenders: Domain adaptation for sentiment classifica-\ntion. In ACL 2007, Proceedings of the 45th Annual Meeting\nof the Association for Computational Linguistics, June 23-\n30, 2007, Prague, Czech Republic.\n[Deerwester et al. 1990] Deerwester, S. C.; Dumais, S. T.;\nLandauer, T. K.; Furnas, G. W.; and Harshman, R. A. 1990.\nIndexing by latent semantic analysis. JASIS 41(6):391–407.\n[Glorot, Bordes, and Bengio 2011] Glorot, X.; Bordes, A.;\nand Bengio, Y. 2011. Domain adaptation for large-scale\nsentiment classification: A deep learning approach. In Pro-\nceedings of the 28th International Conference on Machine\nLearning, ICML 2011, Bellevue, Washington, USA, June 28\n- July 2, 2011, 513–520.\n[Le and Mikolov 2014] Le, Q. V., and Mikolov, T. 2014. Dis-\ntributed representations of sentences and documents. In Pro-\nceedings of the 31th International Conference on Machine\nLearning, ICML 2014, Beijing, China, 21-26 June 2014,\n1188–1196.\n[Maas et al. 2011] Maas, A. L.; Daly, R. E.; Pham, P. T.;\nHuang, D.; Ng, A. Y.; and Potts, C. 2011. Learning word\nvectors for sentiment analysis. In The 49th Annual Meeting\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Proceedings of the Conference, 19-\n24 June, 2011, Portland, Oregon, USA, 142–150.\n[Mikolov et al. 2013] Mikolov, T.; Sutskever, I.; Chen, K.;\nCorrado, G. S.; and Dean, J. 2013. Distributed representa-\ntions of words and phrases and their compositionality. In Ad-\nvances in Neural Information Processing Systems 26: 27th\nAnnual Conference on Neural Information Processing Sys-\ntems 2013. Proceedings of a meeting held December 5-8,\n2013, Lake Tahoe, Nevada, United States., 3111–3119.\n[Rifai et al. 2011a] Rifai, S.; Dauphin, Y.; Vincent, P.; Ben-\ngio, Y.; and Muller, X. 2011a. The manifold tangent classi-\nfier. In Advances in Neural Information Processing Systems\n24: 25th Annual Conference on Neural Information Process-\ning Systems 2011. Proceedings of a meeting held 12-14 De-\ncember 2011, Granada, Spain., 2294–2302.\n[Rifai et al. 2011b] Rifai, S.; Vincent, P.; Muller, X.; Glorot,\nX.; and Bengio, Y. 2011b. Contractive auto-encoders: Ex-\nplicit invariance during feature extraction. In Proceedings\nof the 28th International Conference on Machine Learning,\nICML 2011, Bellevue, Washington, USA, June 28 - July 2,\n2011, 833–840.\n[Socher et al. 2011] Socher, R.; Pennington, J.; Huang,\nE. H.; Ng, A. Y.; and Manning, C. D. 2011. Semi-\nsupervised recursive autoencoders for predicting sentiment\ndistributions. In Proceedings of the 2011 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2011, 27-31 July 2011, John McIntyre Conference Centre,\nEdinburgh, UK, A meeting of SIGDAT, a Special Interest\nGroup of the ACL, 151–161.\n[Srivastava et al. 2014] Srivastava, N.; Hinton, G. E.;\nKrizhevsky, A.; Sutskever, I.; and Salakhutdinov, R.\n2014. Dropout: a simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Research\n15(1):1929–1958.\n[Turney and Pantel 2010] Turney, P. D., and Pantel, P. 2010.\nFrom frequency to meaning: Vector space models of seman-\ntics. J. Artif. Intell. Res. (JAIR) 37:141–188.\n[Vincent et al. 2008] Vincent, P.; Larochelle, H.; Bengio, Y.;\nand Manzagol, P. 2008. Extracting and composing robust\nfeatures with denoising autoencoders. In Machine Learn-\ning, Proceedings of the Twenty-Fifth International Confer-\nence (ICML 2008), Helsinki, Finland, June 5-9, 2008, 1096–\n1103.\n[Wager, Wang, and Liang 2013] Wager, S.; Wang, S. I.; and\nLiang, P. 2013. Dropout training as adaptive regularization.\nIn Advances in Neural Information Processing Systems 26:\n27th Annual Conference on Neural Information Processing\nSystems 2013. Proceedings of a meeting held December 5-8,\n2013, Lake Tahoe, Nevada, United States., 351–359.\n[Xu, Liu, and Gong 2003] Xu, W.; Liu, X.; and Gong, Y.\n2003. Document clustering based on non-negative matrix\nfactorization. In SIGIR 2003: Proceedings of the 26th An-\nnual International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, July 28 - August 1,\n2003, Toronto, Canada, 267–273.\n",
      "id": 24752418,
      "identifiers": [
        {
          "identifier": "386118113",
          "type": "CORE_ID"
        },
        {
          "identifier": "42664313",
          "type": "CORE_ID"
        },
        {
          "identifier": "1512.04466",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1512.04466",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1609/aaai.v30i1.10159",
          "type": "DOI"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/10159",
          "type": "OAI_ID"
        }
      ],
      "title": "Semisupervised Autoencoder for Sentiment Analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:ojs.aaai.org:article/10159",
        "oai:arxiv.org:1512.04466"
      ],
      "publishedDate": "2015-12-14T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1512.04466"
      ],
      "updatedDate": "2024-02-09T07:01:52",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1512.04466"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24752418"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/143912506",
        "https://api.core.ac.uk/v3/outputs/147601859",
        "https://api.core.ac.uk/v3/outputs/386120791",
        "https://api.core.ac.uk/v3/outputs/11310678"
      ],
      "createdDate": "2013-07-10T11:53:36",
      "dataProviders": [
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The recent prominence of the real-time web is proving both challenging and disruptive for information retrieval and web data mining research. User-generated content on the real-time web is perhaps best epitomised by content on microblogging platforms, such as Twitter. Given the substantial quantity of microblog posts that may be relevant to a user's query at a point in time, automated methods are required to sift through this information. Sentiment analysis offers a promising direction for modelling microblog content. We build and evaluate a sentiment-based filtering system using real-time user studies. We find a significant role played by sentiment in the search scenarios, observing detrimental effects in filtering out certain sentiment types. We make a series of observations regarding associations between document-level sentiment and user feedback, including associations with user profile attributes, and users' prior topic sentiment",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/11310678.pdf",
      "fieldOfStudy": null,
      "fullText": "An Evaluation of the Role of Sentiment in Second Screen Microblog Search TasksAdam Bermingham and Alan F. SmeatonCLARITY: Centre for Sensor Web TechnologiesSchool of Computing, Dublin City UniversityAbstractThe recent prominence of the real-time web is prov-ing both challenging and disruptive for information re-trieval and web data mining research. User-generatedcontent on the real-time web is perhaps best epitomisedby content on microblogging platforms, such as Twitter.Given the substantial quantity of microblog posts thatmay be relevant to a user’s query at a point in time, au-tomated methods are required to sift through this infor-mation. Sentiment analysis offers a promising directionfor modelling microblog content. We build and evalu-ate a sentiment-based filtering system using real-timeuser studies. We find a significant role played by senti-ment in the search scenarios, observing detrimental ef-fects in filtering out certain sentiment types. We makea series of observations regarding associations betweendocument-level sentiment and user feedback, includingassociations with user profile attributes, and users’ priortopic sentiment.IntroductionRecently, sentiment analysis classification accuracies havebecome comparable with the traditionally easier task of top-ical classification. Given sentiment analysis’s capabilitiesand limitations, we endeavour to demonstrate its benefitin application areas. For the task of search through user-generated content, analysis of query logs have shown thatinformation needs frequently have a subjective component,for example in blog search (Mishne 2007). Our observationsare that real-time events tend to be polarising and content ispartisan (e.g. politics, sports) or critical (e.g. television). Weargue that in the real-time social web, users’ real-time needshave prominent sentiment components.In this work, we describe a system we have developed forallowing users to view a stream of real-time content fromthe microblogging service, Twitter1, while observing events.Our work has two aims: (i) to present a system and method-ology for evaluating the role of sentiment in real-time mi-croblog search, and (ii) to explore the relationship betweensentiment and users in a real-time context.Copyright c© 2012, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.1http://www.twitter.comBackground and Related WorkMicroblog search is perhaps most closely related to blogsearch. Two primary categories of blog search query areconcept and context (Mishne and de Rijke 2006). Whereasconcept queries concern a topic or area of interest, contextqueries aim to find commentary on real-world entities suchas products or public figures. Mishne and de Rijke remarkhow this significantly differs from web search informationneeds, which are described as informational, navigational,or transactional (Broder 2002).The prevalance of context queries in blog search has in-spired much work on opinion-based search, for example inthe Blog Track at TREC (Macdonald et al. 2010), and theMultilingual Opinion Analysis Task at NTCIR (Seki et al.2010). One conclusion from the TREC Blog Track, was thatdue to the inherently opinionated nature of blog content,strong ad-hoc retrieval systems with no sentiment-specifictechniques performed well on the opinion-finding task (Ou-nis, Macdonald, and Soboroff 2008), suggesting sentimentplays only a minor role in blog search. We argue that senti-ment is more important in microblog search.There are two types of microblog search query we mightconsider: (i) Ad-hoc: A user has an instantaneous informa-tion need at a point in time, and desires a single set of docu-ments, and (ii) Persistent: A user wishes to state an informa-tion need, and receive documents which satisfy this need, asand when they become available. The former type of queryis perhaps easier to formulate in a traditional information re-trieval evaluation. Such a task has recently been run at theTREC Microblog Track2. The focus of our evaluation, per-sistent queries, allow people to track live events such as tele-vision programmes, breaking news stories and sports, as theevent is unfolding. A common form of this is following anevent on the social web, while also watching the event ontelevision, known as second-screen viewing (Deller 2011).A persistent microblog query may be thought of as theuser expressing a wish to be shown documents which pro-vide them with additional contextual information and com-mentary related to the query over time. Just like blog search,this does not conform to the notions of information needwhich epitomise web search, and is more like an informa-2https://sites.google.com/site/trecmicroblogtrack/tion filtering system (Belkin and Croft 1992).One work that has tackled microblog search improves re-trieval performance over a Boolean search recency-rankedbaseline using query expansion and quality indicators to ex-tend a language model information retrieval approach to mi-croblogs (Massoudi et al. 2011). Another work also useslanguage models to tackle microblog search (Efron andGolovchinksy 2011). In other research, Efron identifies theprimary information retrieval tasks in microblogs as ques-tion answering, and what we refer to as ad hoc queries(Efron 2011). In this work, emphasis is placed on the preva-lence of named entities as topics, and the implications of thepresence of temporal context and meta-information. Boththis and the previous work from Massoudi et al. treat the in-formation need as instantaneous, and derive their methodol-ogy from traditional, static information retrieval evaluation.A recent comparison of web search and microblog searchthrough query log analysis notes that Twitter queries tend tobe shorter than web queries, and are likely to be related tohashtags (Teevan, Ramage, and Morris 2011). The authorsfind, “Twitter search is used to monitor content, while Websearch is used to develop and learn about a topic.” Somerecent works tackle microblog search as a filtering prob-lem, filtering tweets into general categories such as newsand events (Sriram et al. 2010) or using social informationto generate user profiles (Churchill, Liodakis, and Ye 2010).This area is, however, largely unexplored, perhaps due to thepoorly understood information needs of persistent queries,and difficulties in evaluating such.Experimental DesignEvaluating real-time microblog search with a static corpusevaluation is problematic. Relevance measures may not ad-equately discriminate in a large set of relevant documents.Also, static evaluations rely on the objective judgments ofassessors, which for real-time search is subject to hindsightbias. A third problem is that objective judgments do not ac-count for a user’s internal knowledge or outlook. One recentreview of search in microblogs concluded, “...naturalisticand behavioral methods of assessing system performancewill no doubt have a large impact on future research, aswe work to make our studies both realistic and generaliz-able.” (Efron 2011) This motivated our decision to conductour evaluation with controlled user studies.We characterised three aspects of persistent microblogsearch with respect to a given topic: (i) document senti-ment, (ii) stream sentiment distribution and (iii) user sen-timent, constituting our independent variables. We captureinformation for each user concerning their task familiarityand demographics and evaluate these as secondary, inde-pendent variables. Using a repeated measures experimentaldesign, we exposed participants to various sentiment condi-tions, and recorded their feedback (our dependent variable).We adapted a familiar web interaction metaphor, thumbs upand thumbs down, and instructed participants to approachliking and disliking a document as they would if they enoun-tered it in their normal Internet use. We also recorded peri-odic stream-level feedback by prompting users for a ratingfrom 1 (poor) to 7 (excellent).Participant Profile XF GE11Age ≥25 17 18<25 18 3Task slightly or not familiar 19 9Familiarity somewhat or more familiar 16 12Gender female 12 10male 23 11Prior positive 16 4Sentiment negative 9 4neutral 7 9unfamiliar 3 4Table 1: Participant sample sizes for profile attributesXF % GE11 %positive 2,131 30.8 884 12.2negative 3,640 52.7 2,716 37.6neutral 843 12.2 3,628 50.2mixed 296 4.3 153 2.1Total 6,910 7,381Table 2: Labelled training documents for sentimentWe chose two real-time topics: the final of a singing com-petition on ITV, the X Factor (11th and 12th of December,2010), and the Leaders’ Debate during the Irish GeneralElection (14th February, 2011). We recruited participantsthrough the university staff and students (see Table 1). Par-ticipants observed the topic event live on a shared screen.When the event began, the system allocated each partici-pant a random sentiment algorithm. At intervals of 15 min-utes, the system prompted participants to provide stream-level feedback and the algorithms were rotated.For a set of documents, TUp is thumbs up frequency, andTDn is thumbs down frequency.TNet is TUp−TDn. Our ex-periment has four experimental conditions: (i) positive doc-uments only (pos), (ii) negative documents only (neg), (iii)positive and negative documents only (posneg) and (iv)random sampling (control). We use the general linearmodel for repeated measures to compare the feedback distri-bution under the four conditions. In addition, the control al-gorithm is compared to the others using a paired, two-tailed,t-test. Using the general linear model also enables us to lookat between-subjects main interaction, i.e. if there is a dif-ference in the main effect, that corresponds to attribute dif-ferences between participants. Where we examine categori-cal associations, statistical significance is noted according toPearson’s chi-squared test.The sentiment targets we use for the X Factor are thejudges and contestants. Similarly, for the election our senti-ment targets are the parties and their leaders. Our annotatorslabelled documents with respect to these sentiment targets(see Table 2). For the X Factor, we observed an agreementof 0.78 (Krippendorff’s α) for 3 classes: positive, negativeand neutral. For the election, the agreement was lower at0.48, possibly reflecting a more difficult annotation task.At search-time, we consider a positive document to beone which refers positively to each sentiment target that itmentions, and a negative document to be one which refersFeedback posneg pos neg controlXFOverall* 4.21 4.06 4.61 4.35TUp Rate** 0.19* 0.15** 0.24 0.23TDn Rate* 0.21 0.22* 0.17 0.17TNet** -0.01** -0.07** 0.07 0.05GE11Overall 4.05 4.14 4.24 4.38TUp Rate* 0.31 0.26* 0.32 0.32TDn Rate 0.13 0.13 0.12 0.13TNet 0.17 0.13 0.20 0.18Table 3: Mean feedback for sentiment filtering algorithmsnegatively to each of the sentiment targets it mentions. Aneutral document is then one which mentions one or moresentiment targets but does not contain sentiment towardsthose targets. During the X Factor we used two binary SVMclassifiers, one for positive, one for negative. Using 10-foldcross validation, the accuracies for these classifiers on thetraining data were 82.47% and 75.57%, respectively. For theLeaders’ Debate we used a three-way Adaboost multinomialnaive Bayes classifer (positive, negative, neutral). This clas-sifier as well as our feature vector and annotation method-ology are described in our earlier work (Bermingham andSmeaton 2011).Results and DiscussionIn this section we present and discuss the results of our ex-periments.Algorithm SentimentThe average overall ratings for the sentiment filtering al-gorithms were slightly better than the midpoint of the 7-point scale (see Table 3). The streams, which upweight posi-tive documents (posneg, pos), received lower ratings thanthose that do not (neg, control). However, this differenceis only significant for the X Factor (p < 0.001).The feedback for the Leaders’ Debate was far more pos-itive with a TUp more than twice the TDn, whereas for theX Factor, TUp was similar to TDn. For TUp, we see a signif-icant difference between the algorithms, with the pos algo-rithm again performing lowest for both the X Factor and theLeaders’ Debate. Comparing algorithms to the controlalgorithm, it is the pos algorithm once more that demon-strates a significantly worse response for the X Factor TUpand TNet (p < 0.001), and TDn (p < 0.05). This pattern isalso present for TUp for the Leaders’ Debate (p < 0.05). Inboth experiments, a document in the pos stream was con-siderably less likely to receive thumbs up feedback than inthe neg or control stream. Also, the posneg algorithmperforms significantly worse than the control for TUp andTNet, although the effect size is smaller.The only significant differences in feedback according tothe between-subjects main effect was for participants in dif-ferent age groups for the X Factor study (p < 0.05). Thealgorithm ratings were higher for participants under the ageof 25 for both document-level and stream-level feedback.The patterns in sentiment are much more salient in termsof the X Factor than the Leaders’ Debate. We did howeverThumbs Up Thumbs DownXFpositive -0.31** 0.18**negative 0.3** -0.16**neutral 0 -0.11*GE11positive -0.08 0.11*negative 0.02 -0.04neutral 0.07 -0.08Table 4: Thumbs up and thumbs down feedback log odds perdocument sentiment typehave fewer participants for the debate and the event itselfwas shorter, so it is possible that some of our inconclusiveresults are subject to type II error.In modifying the sentiment in the stream we are possi-bly introducing negative effects other than upweighting anundesirable document sentiment type. For one, we are ob-scuring the true distribution of sentiment from the user. Weare also potentially limiting their exposure to documents ofother sentiment types. On reflection, these factors may havecontributed to the strong performance of the control al-gorithm.Document SentimentIn total there were 55,314 documents presented to users dur-ing the X Factor and 7,509 documents presented to usersduring the Leaders’ Debate.For the X Factor, negative documents were twice as likelyto receive thumbs up (p < 0.001), whereas positive docu-ments on the other hand were just half as likely to receivethumbs up (p < 0.001) (see Table 4). For the Leaders’Debate we observed no statistically significant sentiment-feeback dependencies. We saw significant thumbs down pat-terns for positive and negative documents for the X Factor(p < 0.001); positive documents were 52% more likely toreceive a thumbs down annotation than others, while neg-ative documents were 31% less likely. This is intuitivelyconsistent with the results for thumbs up annotations, al-though the effect size is smaller. Interestingly, this thumbsdown-positive document relationship is also observed for theLeaders’ Debate, where positive documents were 30% morelikely to receive thumbs down feedback (p < 0.05). We alsoobserved significant associations for neutral X Factor doc-uments, which were 28% more likely to receive a thumbsdown (p < 0.05).For X Factor feedback, thumbs up feedback was signifi-cantly less than expected where participants were (i) aged 25or older and (ii) male (p < 0.001). We observed the inversefor thumbs down and found this pattern to be most promi-nent in positive documents. An interesting observation fromthe debate is that those who were familiar with microblogsearch were 80% more likely to thumbs up a neutral docu-ment (p < 0.05), perhaps due to a higher level of trust placedin informative documents.Across both topics, we see positive documents are nega-tively received by participants and negative documents arepositively received, reinforcing what we see at algorithmlevel. Many of the documents classified as positive are thosewhere the sentiment is explicit, often a few words statingsupport for a topic entity, offering little in the way of con-tent. We observed that humorous and critical content tends tobe negative; perhaps this was valued highly by participants.Participant SentimentWe observed no significant between-subjects effect for anyof the prior participant sentiment categories with respectto different sentiment filtering algorithms for either over-all stream feedback or TNet. For the Leaders’ Debate, pos-itive participants were less likely to give thumbs down orthumbs up feedback for either positive (p < 0.05) or neg-ative (p < 0.001) documents. Indeed, positive participantswere more than three times less likely to thumbs down anegative document and only half as likely to thumbs downa positive document. Neutral participants on the other handwere more than 50% more likely to thumbs up a documentregardless of sentiment.The effect sizes observed for the X Factor were smaller,though in this case we saw a higher likelihood of positiveparticipants annotating positive documents as thumbs up,and a lower likelihood of positive participants annotatingpositive documents as thumbs down (p < 0.001). Negativeparticipants were less likely to thumbs up a positive docu-ment (p < 0.001) though other effects related to negativeparticipants were small, or not significant.The patterns for the X Factor and the Leaders Debate arequite different, although positive documents are consistentlyperceived the worst in each grouping. The larger effect ap-pears to be a difference in the way participants of differentprior sentiment approach the task, rather any effect relatedto content sentiment.ConclusionWe have described a system and methodology for examin-ing sentiment in real-time microblog search scenarios. Wetook two topics, a political debate and an entertainment tele-vision show, and conducted a series of laboratory user stud-ies. The largest effect we observed was for positive contentwhich consistently receives negative feedback. This is per-haps to do with participant dissatisfaction with positive con-tent, or perhaps the absence of other types of sentiment. Ourneg algorithm performs similarly to the control streamdespite having much less neutral and positive content. Wespeculate that negative content is valuable to the searcher.The control proves to be a strong baseline.We found that comparing feedback to participant priorsentiment reveals significant patterns. We conclude alsohowever, that these profile variables have a stronger as-sociation with task feedback in general, rather than anysentiment-specific aspect.Although the effects we see are mixed, it is clear that sen-timent in a real-time search system is a measurable quan-tity, that we can use sentiment to produce significant re-ponses from users, and that we can capture this responseeffectively with our experimental methods. This is a promis-ing result for automated sentiment analysis in real-time mi-croblog search.AcknowledgmentsThis work is supported by Science Foundation Ireland undergrant 07/CE/I1147.ReferencesBelkin, N., and Croft, W. 1992. Information filtering andinformation retrieval: two sides of the same coin? Commu-nications of the ACM 35(12):29–38.Bermingham, A., and Smeaton, A. F. 2011. On using Twit-ter to monitor political sentiment and predict election results.In SAAIP - Sentiment Analysis where AI meets Psychologyworkshop at the International Joint Conference on NaturalLanguage Processing (IJCNLP) November 13, 2011, Chi-ang Mai, Thailand. in press.Broder, A. 2002. A taxonomy of web search. In ACM Sigirforum, volume 36, 3–10. ACM.Churchill, A.; Liodakis, E.; and Ye, S. 2010. Twitter rele-vance filtering via joint bayes classifiers from user cluster-ing. CS 229 Final Project, Stanford University.Deller, R. A. 2011. Twittering on: Audience research andparticipation using twitter. Time (2009):216–245.Efron, M., and Golovchinksy, G. 2011. Estimation methodsfor ranking recent information. In SIGIR ’10: Proceedingsof the 34th Annual ACM Conference on Research and De-velopment in Information Retrieval. New York, NY, USA:ACM.Efron, M. 2011. Information search and retrieval in mi-croblogs. Journal of the American Society for InformationScience and Technology.Macdonald, C.; Santos, R. L. T.; Ounis, I.; and Soboroff, I.2010. Blog Track research at TREC. Sigir Forum 44.Massoudi, K.; Tsagkias, M.; de Rijke, M.; and Weerkamp,W. 2011. Incorporating query expansion and quality indica-tors in searching microblog posts. Advances in InformationRetrieval 362–367.Mishne, G., and de Rijke, M. 2006. A study of blog search.Advances in Information Retrieval 289–301.Mishne, G. A. 2007. Applied Text Analytics for Blogs. Ph.D.Dissertation, University of Amsterdam, Amsterdam.Ounis, I.; Macdonald, C.; and Soboroff, I. 2008. Overviewof the TREC-2008 Blog Track. In Proceedings of the 17thText REtrieval Conference (TREC 2008).Seki, Y.; Ku, L.-W.; Sun, L.; Chen, H.-H.; and Kando, N.2010. Overview of multilingual opinion analysis task atNTCIR-8. 209–220.Sriram, B.; Fuhry, D.; Demir, E.; Ferhatosmanoglu, H.; andDemirbas, M. 2010. Short text classification in Twitter toimprove information filtering. In Proceeding of the 33rd in-ternational ACM SIGIR conference on research and devel-opment in information retrieval, 841–842. ACM.Teevan, J.; Ramage, D.; and Morris, M. 2011. # Twit-terSearch: a comparison of microblog search and websearch. In Proceedings of the fourth ACM international con-ference on Web search and data mining, 35–44. ACM.",
      "id": 23091272,
      "identifiers": [
        {
          "identifier": "143912506",
          "type": "CORE_ID"
        },
        {
          "identifier": "386120791",
          "type": "CORE_ID"
        },
        {
          "identifier": "11310678",
          "type": "CORE_ID"
        },
        {
          "identifier": "147601859",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/14332",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:doras.dcu.ie:16867",
          "type": "OAI_ID"
        }
      ],
      "title": "An evaluation of the role of sentiment in second screen microblog search tasks",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:ojs.aaai.org:article/14332",
        "oai:doras.dcu.ie:16867"
      ],
      "publishedDate": "2012-05-20T01:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 16628879,
          "title": "A study of blog search.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "Mishne, G., and de Rijke, M. 2006. A study of blog search. Advances in Information Retrieval 289–301.",
          "cites": null
        },
        {
          "id": 16628847,
          "title": "A taxonomy of web search.",
          "authors": [],
          "date": "2002",
          "doi": null,
          "raw": "Broder, A. 2002. A taxonomy of web search. In ACM Sigir forum, volume 36, 3–10. ACM.",
          "cites": null
        },
        {
          "id": 16628882,
          "title": "AppliedText Analytics for Blogs.",
          "authors": [],
          "date": "2007",
          "doi": null,
          "raw": "Mishne,G. A. 2007. AppliedText Analytics for Blogs. Ph.D. Dissertation, University of Amsterdam, Amsterdam.",
          "cites": null
        },
        {
          "id": 16628869,
          "title": "Blog Track research at TREC. Sigir Forum 44.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "Macdonald, C.; Santos, R. L. T.; Ounis, I.; and Soboroff, I. 2010. Blog Track research at TREC. Sigir Forum 44.",
          "cites": null
        },
        {
          "id": 16628859,
          "title": "Estimation methods for ranking recent information.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Efron, M., and Golovchinksy,G. 2011. Estimation methods for ranking recent information. In SIGIR ’10: Proceedings of the 34th Annual ACM Conference on Research and Development in Information Retrieval. New York, NY, USA: ACM.",
          "cites": null
        },
        {
          "id": 16628874,
          "title": "Incorporatingquery expansion and quality indicators in searching microblog posts.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Massoudi, K.; Tsagkias, M.; de Rijke, M.; and Weerkamp, W. 2011. Incorporatingquery expansion and quality indicators in searching microblog posts. Advances in Information Retrieval 362–367.",
          "cites": null
        },
        {
          "id": 16628841,
          "title": "Information ﬁltering and information retrieval: two sides of the same coin?",
          "authors": [],
          "date": "1992",
          "doi": null,
          "raw": "Belkin, N., and Croft, W. 1992. Information ﬁltering and information retrieval: two sides of the same coin? Communications of the ACM 35(12):29–38.",
          "cites": null
        },
        {
          "id": 16628863,
          "title": "Information search and retrieval in microblogs.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Efron, M. 2011. Information search and retrieval in microblogs. Journal of the American Society for Information Science and Technology.",
          "cites": null
        },
        {
          "id": 16628844,
          "title": "On using Twittertomonitorpoliticalsentimentandpredictelectionresults.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Bermingham, A., and Smeaton, A. F. 2011. On using Twittertomonitorpoliticalsentimentandpredictelectionresults.",
          "cites": null
        },
        {
          "id": 16628892,
          "title": "Overview of multilingual opinion analysis task at NTCIR-8.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "Seki, Y.; Ku, L.-W.; Sun, L.; Chen, H.-H.; and Kando, N. 2010. Overview of multilingual opinion analysis task at NTCIR-8. 209–220.",
          "cites": null
        },
        {
          "id": 16628887,
          "title": "Overview of the TREC-2008 Blog Track.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Ounis, I.; Macdonald, C.; and Soboroff, I. 2008. Overview of the TREC-2008 Blog Track. In Proceedings of the 17th Text REtrieval Conference (TREC 2008).",
          "cites": null
        },
        {
          "id": 16628895,
          "title": "Short text classiﬁcation in Twitter to improve information ﬁltering.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "Sriram, B.; Fuhry, D.; Demir, E.; Ferhatosmanoglu, H.; and Demirbas, M. 2010. Short text classiﬁcation in Twitter to improve information ﬁltering. In Proceeding of the 33rd international ACM SIGIR conference on research and development in information retrieval, 841–842. ACM.",
          "cites": null
        },
        {
          "id": 16628851,
          "title": "Twitter relevance ﬁltering via joint bayes classiﬁers from user clustering.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "Churchill, A.; Liodakis, E.; and Ye, S. 2010. Twitter relevance ﬁltering via joint bayes classiﬁers from user clustering. CS 229 Final Project, Stanford University.",
          "cites": null
        },
        {
          "id": 16628855,
          "title": "Twittering on: Audience research and participation using twitter. Time",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Deller, R. A. 2011. Twittering on: Audience research and participation using twitter. Time (2009):216–245.",
          "cites": null
        },
        {
          "id": 16628900,
          "title": "TwitterSearch: a comparison of microblog search and web search.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Teevan, J.; Ramage, D.; and Morris, M. 2011. # TwitterSearch: a comparison of microblog search and web search. In Proceedingsof the fourth ACM internationalconference on Web search and data mining, 35–44. ACM.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/16867/1/ICWSM_2012_final_camera_ready.pdf"
      ],
      "updatedDate": "2022-07-14T04:40:31",
      "yearPublished": 2012,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11310678.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11310678"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11310678/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11310678/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/23091272"
        }
      ]
    },
    {
      "acceptedDate": "2017-07-31T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Agarwal A."
        },
        {
          "name": "Darwish K."
        },
        {
          "name": "Jiang L."
        },
        {
          "name": "Magdy W."
        },
        {
          "name": "Mozetič I."
        },
        {
          "name": "Ounis I."
        },
        {
          "name": "Vargas S."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Karin Sim",
        "Jana"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/267015448",
        "https://api.core.ac.uk/v3/outputs/96884603",
        "https://api.core.ac.uk/v3/outputs/296199261",
        "https://api.core.ac.uk/v3/outputs/192400109"
      ],
      "createdDate": "2017-10-20T09:28:28",
      "dataProviders": [
        {
          "id": 14433,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/14433",
          "logo": "https://api.core.ac.uk/data-providers/14433/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 8278,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/8278",
          "logo": "https://api.core.ac.uk/data-providers/8278/logo"
        },
        {
          "id": 42,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/42",
          "logo": "https://api.core.ac.uk/data-providers/42/logo"
        }
      ],
      "depositedDate": "2017-01-01T00:00:00",
      "abstract": "Global events such as terrorist attacks are commented upon in social media, such as Twitter, in different languages and from different parts of the world. Most prior studies have focused on monolingual sentiment analysis, and therefore excluded an extensive proportion of the Twitter userbase. In this paper, we perform a multilingual comparative sentiment analysis study on the terrorist attack in Paris, during November 2015. In particular, we look at targeted sentiment, investigating opinions on specific entities, not simply the general sentiment of each tweet. Given the potentially inflammatory and polarizing effect that these types of tweets may have on attitudes, we examine the sentiments expressed about different targets and explore whether disproportionate reaction was expressed about such targets across different languages. Specifically, we assess whether the sentiment for French speaking Twitter users during the Paris attack differs from English-speaking ones. We identify disproportionately negative attitudes in the English dataset over the French one towards some entities and, via a crowdsourcing experiment, illustrate that this also extends to forming an annotator bias",
      "documentType": "research",
      "doi": "10.1145/3110025.3110066",
      "downloadUrl": "https://core.ac.uk/download/96884603.pdf",
      "fieldOfStudy": null,
      "fullText": "  \n \n \n \n \nSmith, K. S., McCreadie, R., Macdonald, C. and Ounis, I. (2017) Analyzing \nDisproportionate Reaction via Comparative Multilingual Targeted Sentiment in \nTwitter. In: IEEE/ACM International Conference on Advances in Social Networks \nAnalysis and Mining (ASONAM '17), Sydney, Australia, 31 Jul - 03 Aug 2017, pp. \n317-320. ISBN 9781450349932. \n \n   \nThere may be differences between this version and the published version. You are \nadvised to consult the publisher’s version if you wish to cite from it. \n \n© Association for Computing Machinery 2017. This is the author's version of the \nwork. It is posted here for your personal use. Not for redistribution. The definitive \nVersion of Record was published in IEEE/ACM International Conference on \nAdvances in Social Networks Analysis and Mining (ASONAM '17), Sydney, \nAustralia, 31 Jul - 03 Aug 2017, pp. 317-320. ISBN 9781450349932, \nhttp://dx.doi.org/10.1145/3110025.3110066.   \n \n \nhttp://eprints.gla.ac.uk/148585/ \n     \n \n \n \n \n \n \nDeposited on: 22 September 2017 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nEnlighten – Research publications by members of the University of Glasgow \nhttp://eprints.gla.ac.uk \nAnalyzing Disproportionate Reaction via\nComparative Multilingual Targeted Sentiment in\nTwitter\nKarin Sim Smith, Richard McCreadie, Craig Macdonald, Iadh Ounis\nUniversity of Glasgow\nEmail: firstname.lastname@glasgow.ac.uk\nAbstract—Global events such as terrorist attacks are com-\nmented upon in social media, such as Twitter, in different lan-\nguages and from different parts of the world. Most prior studies\nhave focused on monolingual sentiment analysis, and therefore\nexcluded an extensive proportion of the Twitter userbase. In this\npaper, we perform a multilingual comparative sentiment analysis\nstudy on the terrorist attack in Paris, during November 2015. In\nparticular, we look at targeted sentiment, investigating opinions\non specific entities, not simply the general sentiment of each\ntweet. Given the potentially inflammatory and polarizing effect\nthat these types of tweets may have on attitudes, we examine the\nsentiments expressed about different targets and explore whether\ndisproportionate reaction was expressed about such targets across\ndifferent languages. Specifically, we assess whether the sentiment\nfor French speaking Twitter users during the Paris attack differs\nfrom English-speaking ones. We identify disproportionately neg-\native attitudes in the English dataset over the French one towards\nsome entities and, via a crowdsourcing experiment, illustrate that\nthis also extends to forming an annotator bias.\nI. INTRODUCTION\nWhen significant events occur, social media is often used as\nan outlet for people in different parts of the world to express\ntheir opinions, sentiments, as well as comment on that event.\nFor this reason, social media is a valuable resource to help\nunderstand how events are being perceived by different social\ngroups. However, most social media studies only analyse\ncontent in a single language (typically English) (Thelwall,\nBuckley, and Paltoglou, 2011; Vargas et al., 2016), and hence\nexclude a large proportion of the social media userbase.\nIn contrast, in this paper, we present a study of both\nEnglish and French language tweets posted following the\nterrorist attack that took place in Paris on the 20th November\n2015. English and French were the most frequent languages\ntweeted in following the attacks, with the largest amount of\ntweets being in English, followed by French. In particular, we\nanalyse how sentiment expressed about the targets involved in\nthe event on Twitter differs between users writing in different\nlanguages and explore the challenges in accurately identifying\nsuch sentiments. While there have been prior sentiment\nanalysis studies that aim to detect varying opinion following\nthe Paris attack (Magdy, Darwish, and Abokhodair, 2015),\nour work is different, as it both examines sentiment about\nparticular targets of interest, and more importantly provides\ninsights into how sentiment varies across geographical\nregions, as well as some implications of this variance.\nMore precisely, we analyse sentiment towards three different\ntargets for the 80 hours after the attack, namely French\nPresident Franc¸ois Hollande, Europe and Muslims. These\nwere chosen as being of significance for this event. We use\ncrowdsourcing over English and French tweet samples for\neach target to track public sentiment about those targets.\nInterestingly, based on the labels produced, we find a markedly\nhigher proportion of negative sentiment expressed towards\nthe targets by users posting in English than users posting in\nFrench, even though the attack in question took place in Paris.\nIndeed, if we assume that sentiment expressed by people local\nto the event is a reasonable baseline against which reactions\ncan be compared, we show that by contrast the reaction by\nEnglish-speaking regions to the event was disproportionately\nnegative. We also examine annotator bias of the crowdsourced\nworkers by comparing annotations from workers in different\nregions. We show that there are marked differences in the\nannotations produced from users in different regions when\nlabelling the same data.\nThe contributions of this work are two-fold: First, we show\nthat multilingual comparison of tweets allows for a more\ninformative analysis of wider global opinion for a major event\nthan a classical monolingual analysis. Indeed, our results high-\nlight how external reactions to a disaster can be significantly\nmore negative than local reactions; Second, we examine how\nannotator bias can affect the analysis of sentiment during an\nevent, showing that regional bias also affects (crowdsourced)\ntweet labelling. Such bias is an important factor to consider\nwhen using geographically-dispersed workers to label social\nmedia data.\nIn the next section, we survey related work before defining\nour task (Section III) and experimental setup (Section IV).\nWe then examine the Twitter user bias (Section V) and the\nannotator bias (Section VI), as well as discuss implications\nfor building automatic classifiers (Section VII). We summarize\nour conclusions in Section VIII.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy other wise, or republish, to post on servers or to\nredistribute to lists, requires prior specific permission and/or a fee. Request\npermissions from permissions@acm.org.\nII. RELATED WORK\nPrevious work on monolingual sentiment in Twitter has\nincluded analysis following important events (Thelwall,\nBuckley, and Paltoglou, 2011), finding negative sentiment\ngenerally exceeds positive sentiment, including for positive\nevents. Again in a monolingual setting Agarwal et al. (2011)\nused rich linguistic features in a tree kernel to improve Twitter\nsentiment detection. Vargas et al. (2016) as well as Jiang\net al. (2011) investigated targeted sentiment in monolingual\nsettings, but not as comparative multilingual analysis. There\nhas also been work on multilingual Twitter sentiment analysis\n(Narr, De Luca, and Albayrak, 2011; Tromp, 2012), although\nnot targeted towards specific entities, and in the case of the\nlatter, in a language-independent manner. In their multilingual\nstudy, Mozeticˇ, Grcˇar, and Smailovic´ (2016) compared human\nlabelling and classification models, hypothesizing that ‘the\ninter-annotator agreement approximates an upper bound for a\nclassifier performance’. In deeper monolingual analysis on the\npublic response in Twitter following this same attack, Magdy\net al. (2016) predict stance, particularly towards Muslims,\nbased on user profile. They use retweets and ‘likes’ as a\nbenchmark in researching emotional reaction (Magdy, Dar-\nwish, and Abokhodair, 2015). In contrast, we are interested in\nthe textual content, and the basis of our work is a multilingual\napproach, which is comparative and targeted in nature, as\nwell as being focused on one particular but important event.\nIII. TASK DEFINITION\nIn this paper, we analyse how sentiment expressed about\nan event on Twitter differs between users writing in different\nlanguages and explore the challenges in accurately identifying\nsuch sentiments. More precisely, for a tweet post p that is\npart of a larger discussion about a sensitive event e and\nthat also mentions a particular entity of interest (target) t,\nwe analyse whether that post p expresses sentiment (s ∈\n{negative, positive, neutral}) about its target t. To support\nthis analysis, we use crowdsourced workers to label the\nsentiments expressed within tweet samples in two languages\n(French and English) for a major event e (the 2015 Paris\nattacks). We answer two main research questions:\nRQ1: Do the sentiments expressed towards the targets differ\namong the French & English speaking Twitter users?\nRQ2: Do the sentiment labels about the targets differ among\nthe geographically diverse crowdsourcing workers?\nIV. EXPERIMENTAL SETUP\nDataset: The dataset we base our analysis on consists of\nTwitter tweets posted during Paris attack on 20th-23rd Nov\n2015, containing ‘#Paris’. This crawl contains tweets in a\nwide variety of languages. We filter on the language using\nthe ‘lang’ tag, which identifies the language via Twitter’s\nown language classifier1. According to this classifier, the most\ncommon language was English (1,232,100 tweets) followed by\nFrench (402,914 tweets).\n1Rather than the user’s self-defined language, which is less accurate.\nSentiment Targets: Manually analysing millions of tweets is\nnot feasible due to time/cost constraints. Hence, we choose\na small number of entities (targets) of interest to analyse\nin detail. In particular, we select French President Franc¸ois\nHollande, the European Union and Muslims as our tar-\ngets. We filter the above dataset to only include posts that\nmention these targets using separate [keywords] for each:\nFranc¸ois Hollande:[hollande]; European Union:[europe]; and\nMuslims:[muslim OR musulman]. We then divide this fil-\ntered set into six subsets based on the target and lan-\nguage: Hollande/English; Hollande/French; Europe/English;\nEurope/French; Muslim/English; Muslim/French.\nSampling Furthermore, to provide a detailed analysis, it is\ndesirable to have a diverse set of tweets to examine, both in\nterms of textual content and in terms of time (when during the\nevent each post was made). As such, we apply the following\nsampling strategy to the six tweet sets to create a diverse tweet\nsample for each. First, we divide the tweets from each set into\nhour batches based on their publication timestamps and index\neach hour using the Terrier open source IR platform (Ounis et\nal., 2006). Per hour, we rank the tweets using the keywords\nfor the associated target as the query. Inspired by (Kraaij\nand Spitters, 2003), we use a Gaussian function configured\nto promote sentences that are of approximately the length of\na normal English sentence2 for ranking. We select the top\n100 tweets from each hour to create the sample for each set.\nWe then remove near-duplicate tweets from each sample by\napplying a cosine similarity threshold τ over that sample in a\ngreedy time-ordered manner (τ= 0.7).\nCrowdsourcing To analyse how sentiment varies across\ntweets in different languages, we need to generate sentiment\nlabels for the tweets in our six samples. To achieve this,\nwe had crowdsourced workers manually annotate the tweets,\nusing the Crowdflower platform. As in earlier work on targeted\nsentiment labelling (Vargas et al., 2016), each tweet-target pair\nis given to three different workers. Each worker is asked to\nlabel the sentiment (negative, positive or neutral) expressed\nby the author of the tweet towards the target given. For the\nthree English tweet samples, only English-speaking users were\nallowed to participate in labelling those samples; similarly\nonly French-speaking users could label the three French\ntweet samples. To avoid a few users dominating the labelling\nprocess, the number of tweets a single worker could label\nwas limited to 200. Furthermore, to increase accuracy, worker\nquality was dynamically assessed against a gold standard set\nof 45 (French) or 48 (English) tweets, labelled by the authors,\nfluent in both languages. We disregarded the tweets from\nworkers whose accuracy dropped below 70%. To produce a\nsingle label for each tweet, we take the majority vote across\nthe three labels produced. We discard any tweets where there\nwas not majority agreement. The statistics of the six tweet\nsamples after labelling and discarding are provided in Table I.\nReproducibility: The tweet samples and crowdsourced labels\nused for evaluation are available as a free download at:\n2Mean/expectation was set to 25 and the standard deviation was set to 20.\nTABLE I\nRESULTS FOR MULTILINGUAL TARGETED SENTIMENT LABELLING ON TWITTER SAMPLES FOR ‘#PARIS’ BETWEEN THE 20TH TO THE 23RD OF\nNOVEMBER 2015. (EXCLUDING WHERE NO MAJORITY AGREEMENT)\nSource Tweet Sample tweets neutral negative positive\nParis All / French 1998 1521 (76%) 312 (16%) 165 (8%)\nParis Hollande / French 718 465(64.8%) 169 (23.5%) 84(11.7%)\nParis Europe / French 778 680 (87%) 70 (9%) 28 (4%)\nParis Muslim / French 513 387(75.4%) 73 (14.2%) 53(10.3%)\nParis All / English 1997 1199 (60%) 681 (34%) 118 (6%)\nParis Hollande / English 725 504 (70%) 163 (22%) 58 (8%)\nParis Europe / English 800 520 (65%) 257(32%) 23 (3%)\nParis Muslim / English 496 186 (37%) 273(55%) 38(8%)\nParis Muslim / English / GeoRestricted 466 226 (48%) 194(42%) 46(10%)\nhttp://dx.doi.org/10.5525/gla.researchdata.42\nV. TWITTER USER BIAS\nTable I reports the number and proportion of tweets from\neach of six tweet samples that were labelled as containing\neither neutral, negative or positive sentiment. As we can\nsee from Table I, there is a clear polarity over the various\ntargets. For example, for the target ‘Hollande’, the polarity\nbreakdown is similar across the two languages. There is a\nsimilar proportion of the French tweets that constitute negative\nsentiment (23.5%), as for the English (22%). The proportion\nof French tweets that are positive for this target is (11.7%).\nWhereas the English tweets analysed were less positive in their\njudgement of him, as indicated by the lower positive score\n(8%). However, what is particularly striking is the significant\ndiscrepancy between the amount of tweets labelled negative\nby the English speaking annotators for targets ‘Europe’ and\n‘Muslim’, compared to the French counterparts. For instance,\nthe French annotators labelled 14.2% of the tweets with\ntarget ‘musulman’ (‘muslim’) as negative, compared to 55%\nof the English annotators. The results for target ‘Europe’\nshow a similar trend, with 9% tweets labelled as negative\nby French annotators, and 32% were labelled as negative by\nEnglish annotators. Hence, to answer RQ1, there are marked\ndifferences in the sentiments expressed by Twitter users in\ndifferent geographical regions.\nThis result is unexpected, since those in Paris (and France\nmore generally) are the ones more directly affected by the\nattack. Indeed, if we consider the French reaction to be a\nreasonable baseline reaction to the terrorist attack, then by\ncontrast it makes the English (predominantly USA, UK and\nCanadian) response disproportionately negative.\nVI. ANNOTATOR BIAS\nIn the previous section, we showed that there was a large\ndifference between the proportion of English and French\ntweets that were labelled as positive and negative by crowd\nworkers. However, the workers themselves come from par-\nticular geographical regions. Hence, an interesting question\nis whether the crowd workers are also a source of bias. To\nexamine this, we first manually analyse a small subset of\ntweets. From this analysis, we observe a pattern, where tweets\nwere wrongly labelled as negative for one of the targets. For\ninstance, the following tweet was labelled negative for the\ngiven subject of ‘Muslim’, by the English speaking annotators:\n“Italian Muslims march to denounce Paris attacks: Muslims\nmarched through the streets of Rome to condemn religi...\nhttps://t.co/2Wl8sVvo0i”\nHowever, it can be considered positive (given that the instruc-\ntions were to label the sentiment of the author towards the\nsubject) or at least neutral, if considered as a statement of\nfact. Comparing with the French tweets, we find the following\nsimilar example, which was labelled as positive:\nRT @rtlinfo: La communaute´ musulmane condamne les attentats de\nParis.#RTLinfo19h https://t.co/uA7MyohZ9H3\nOn manual examination, we identified that over 10% of\nthese posts for the ‘Muslim’ target have wrongly been labelled\nas negative, when they should have been either neutral or\neven positive. The fact that they are labelled negative raises\nquestions about the biases of the crowd annotators. To ex-\nplore this in more detail, we perform an additional labelling\nexperiment. In particular, we restrict the geographical location\nof our annotators to prevent users from the UK, Australia,\nthe USA and Canada from participating, and then re-label the\nMuslim/English sample using a new pool of crowd workers,\nwhich we refer to as Muslim / English / GeoRestricted.\nThe last row in Table I shows the distribution of sentiment\nlabels from this additional annotation experiment. If we com-\npare the sentiment distribution of these sentiment annotations\nto the original sentiment annotations (the row above), we\nobserve that 72 (13%) fewer tweets were labelled as negative\n(again excluding items where annotator agreement was below\n67%). This indicates that the workers from the UK, Australia,\nthe USA and Canada, are more likely to label posts about\nMuslims as negative than workers in other regions. Of the 1599\nindividual labels, 1031 were USA-based workers, 88 Canada,\nand 480 UK. This is in line with findings of Darwish and\nMagdy (2015) on the source of anti-Muslim sentiment follow-\ning the attack, where they found that the largest amount of anti-\nMuslim sentiment following this attack was in fact in the USA.\nTo answer RQ2, we do indeed observe marked differences\nbetween the sentiment labels produced by crowdsourced work-\ners from different geographical regions. This is an important\nconsideration for future crowdsourced annotation experiments,\n3Manual Translation: The Muslim community condemn the attacks in Paris.\nTABLE II\nCLASSIFICATION WITH CROSSFOLD VALIDATION (10-FOLD).\nRELABELLED RESULTS INCLUDE REPLACED ‘MUSLIM’ SECTION ONLY.\nlanguage tweets precision recall F1\nFrench 2025 0.72 0.76 0.72\nEnglish 2033 0.62 0.65 0.63\nSubstitute relabelled Muslim set:\nEnglish 2014 0.59 0.63 0.59\nsince otherwise any conclusions drawn from such labels would\nalso be biased. Furthermore, there are implications when using\nsuch biased labels for classification, which we discuss below.\nVII. IMPLICATIONS FOR SUPERVISED CLASSIFICATION\nA common use for crowdsourced sentiment labels is as\ntraining for supervised classification approaches. Hence, in this\nsection, we examine how classification accuracy is affected by\nthe annotator bias we observed in the above section. For this\nexperiment, we aggregate all tweets from each language into\na single set and then train using a 10-fold cross validation. We\nextract n-gram features (1 ≤ n ≤ 5) to detect longer sequences\nwhich include the entity of the targeted sentiment. Table II\nreports the accuracy of a SVM (SGD) sentiment classifier\ntrained using scikit-learn, in terms of precision, recall and F1.\nFrom Table II, we see that when classifying the French\ntweets, the SVM classifier achieves 0.72 F1, which is a\ngood performance for Twitter (Agarwal et al., 2011; Jiang\net al., 2011). Interestingly, when classifying the English set,\nthe performance is lower (0.63 F1). The better scores for\nthe French tweets are biased by the stronger majority class.\nHowever, relating these results to our discussion on annotator\nbias in the previous section, one reason for the markedly lower\nperformance over the English tweets might be that annotator\nbias from a sub-set of the crowd workers has resulted in\ninconsistent training labels. To test this, we trained a second\nclassifier, where we replaced the Muslim/English sample in\nthe original dataset with the re-annotated Muslim / English /\nGeoRestricted version. Interestingly, as we see from Table II\nthe replacement of the labels for the Muslim target with the\nreannotated ones for this subset, results in a drop in F1 to\n0.59. This can be attributed to the fact that there is a more\neven label distribution, instead of a majority class. Also, there\nclearly is ambiguity in the English tweets, as the following\ntweet was labelled with 3 different labels:\nA small antidote to political vitriol. ”Muslim asks Parisians to hug\nhim if they trust him. Many do”. https://t.co/xXlzGglyJx\nThe increased ambiguity is clear from the lack of annotator\nagreement, which led to us then having to disregard more\ntweets for that target (496 to 466).\nHowever, while our results highlight the limitations of\nhuman annotators, it is worth noting that this is only one\ncause of classifier error. For instance, some of the misclassified\nnegative tweets are quite subtle, or nuanced, or instances where\nthe classifier cannot grasp the cynicism for instance:\n“UK Muslims Feel Backlash After Paris Attacks.Alway moaning Oh\nlook at how bad it is for us!!”\nThere are also tweets which are overtly racist, but where the\nclassifier would struggle to detect negativity, as it is too subtle:\n“I’ve been locked in a cupboard since the Paris attacks and am\nstarving to death. Anyone know a delivery service that doesn’t\nemploy muslims?”\nIndeed, detecting the negativity in these tweets may be simple\nfor humans, but requires more sophisticated classifier features.\nVIII. CONCLUSIONS\nIn this paper we illustrated the value of comparative mul-\ntilingual sentiment analysis as a tool to understand how\nsentiment about an event varies across geographical regions.\nThrough a crowdsourced user study, we showed that the\namount of negativity in the English tweets (34.39%), following\nthe Paris attacks of 2015 far exceeds that of the French\n(15.09%), despite the fact that the attack was on French soil.\nFurthermore, we examined how bias in crowd annotators can\naffect the analysis of sentiment during an event. Our results\nindicated that regional bias can have a strong influence when\ncrowdsourcing tweet sentiment labels. Indeed, we observed a\n14% reduction in the number of tweets that were labelled as\nnegative for the target ‘Muslims’ when we excluded workers\nfrom the USA, UK and Canada. This regional bias is an im-\nportant factor to consider when using geographically-dispersed\nworkers to label as social media data, particularly when the\nresultant labels are used as training for supervised classifiers.\nIX. ACKNOWLEDGEMENTS\nThis work was supported by the EC co-funded SUPER\n(FP7-606853) project.\nREFERENCES\nAgarwal, A.; Xie, B.; Vovsha, I.; Rambow, O.; and Passonneau, R.\nSentiment analysis of Twitter data. In Proceedings of LSM 2011.\nDarwish, K., and Magdy, W. Attitudes towards refugees in light of\nthe Paris attacks. Computing Research Repository Journal.\nJiang, L.; Yu, M.; Zhou, M.; Liu, X.; and Zhao, T. Target-dependent\nTwitter sentiment classification. In Proceedings of ACL-HLT 2011.\nKraaij, W. and Spitters, M. Language models for topic tracking.\nLanguage Modeling for Information Retrieval.\nMagdy, W.; Darwish, K.; Abokhodair, N.; Rahimi, A.; and Baldwin,\nT. #isisisnotislam or #deportallmuslims?: Predicting unspoken\nviews. In Proceedings of WebSci 2016.\nMagdy, W.; Darwish, K.; and Abokhodair, N. Quantifying public\nresponse towards Islam on Twitter after Paris attacks. Computing\nResearch Repository Journal.\nMozeticˇ, I.; Grcˇar, M.; and Smailovic´, J. Multilingual Twitter\nsentiment classification: The role of human annotators. Public\nLibrary of Science ONE Journal.\nNarr, S.; De Luca, E. W.; and Albayrak, S. Extracting semantic\nannotations from Twitter. In Proceedings of ESAIR 2011.\nOunis, I.; Amati, G.; Plachouras, V.; He, B.; Macdonald, C.; and\nLioma, C. Terrier: A high performance and scalable Information\nRetrieval platform. In Proceedings of OSIR 2006.\nThelwall, M.; Buckley, K.; and Paltoglou, G. 2011. Sentiment in\nTwitter events. American Society for Information Science and\nTechnology Journal.\nTromp, E. 2012. Multilingual Sentiment Analysis on Social Media.\nLAP Lambert Academic Publishing.\nVargas, S.; McCreadie, R.; Macdonald, C.; and Ounis, I. Comparing\noverall and targeted sentiments in social media during crises. In\nProceedings of ICWSM 2016.\n",
      "id": 44742767,
      "identifiers": [
        {
          "identifier": "2768696754",
          "type": "MAG_ID"
        },
        {
          "identifier": "267015448",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:researchdata.gla.ac.uk:429",
          "type": "OAI_ID"
        },
        {
          "identifier": "96884603",
          "type": "CORE_ID"
        },
        {
          "identifier": "192400109",
          "type": "CORE_ID"
        },
        {
          "identifier": "296199261",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:eprints.gla.ac.uk:148585",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.5525/gla.researchdata.429",
          "type": "DOI"
        },
        {
          "identifier": "10.1145/3110025.3110066",
          "type": "DOI"
        }
      ],
      "title": "Analyzing Disproportionate Reaction via Comparative Multilingual Targeted Sentiment in Twitter",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2768696754",
      "oaiIds": [
        "oai:researchdata.gla.ac.uk:429",
        "oai:eprints.gla.ac.uk:148585"
      ],
      "publishedDate": "2017-01-01T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://eprints.gla.ac.uk/148585/13/148585.pdf"
      ],
      "updatedDate": "2022-01-09T10:58:26",
      "yearPublished": 2017,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/96884603.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/96884603"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/96884603/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/96884603/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/44742767"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Darzi, A"
        },
        {
          "name": "Gohil, S"
        },
        {
          "name": "Vuik, S"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/157858839"
      ],
      "createdDate": "2018-06-04T19:12:53",
      "dataProviders": [
        {
          "id": 105,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/105",
          "logo": "https://api.core.ac.uk/data-providers/105/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "BACKGROUND: Twitter is a microblogging service where users can send and read short 140-character messages called \"tweets.\" There are several unstructured, free-text tweets relating to health care being shared on Twitter, which is becoming a popular area for health care research. Sentiment is a metric commonly used to investigate the positive or negative opinion within these messages. Exploring the methods used for sentiment analysis in Twitter health care research may allow us to better understand the options available for future research in this growing field. OBJECTIVE: The first objective of this study was to understand which tools would be available for sentiment analysis of Twitter health care research, by reviewing existing studies in this area and the methods they used. The second objective was to determine which method would work best in the health care settings, by analyzing how the methods were used to answer specific health care questions, their production, and how their accuracy was analyzed. METHODS: A review of the literature was conducted pertaining to Twitter and health care research, which used a quantitative method of sentiment analysis for the free-text messages (tweets). The study compared the types of tools used in each case and examined methods for tool production, tool training, and analysis of accuracy. RESULTS: A total of 12 papers studying the quantitative measurement of sentiment in the health care setting were found. More than half of these studies produced tools specifically for their research, 4 used open source tools available freely, and 2 used commercially available software. Moreover, 4 out of the 12 tools were trained using a smaller sample of the study's final data. The sentiment method was trained against, on an average, 0.45% (2816/627,024) of the total sample data. One of the 12 papers commented on the analysis of accuracy of the tool used. CONCLUSIONS: Multiple methods are used for sentiment analysis of tweets in the health care setting. These range from self-produced basic categorizations to more complex and expensive commercial software. The open source and commercial methods are developed on product reviews and generic social media messages. None of these methods have been extensively tested against a corpus of health care messages to check their accuracy. This study suggests that there is a need for an accurate and tested tool for sentiment analysis of tweets trained using a health care setting-specific corpus of manually annotated tweets first",
      "documentType": "research",
      "doi": "10.2196/publichealth.5789",
      "downloadUrl": "https://core.ac.uk/download/157858839.pdf",
      "fieldOfStudy": null,
      "fullText": "ReviewSentiment Analysis of Health Care Tweets: Review of the MethodsUsedSunir Gohil, BSc, MBBS; Sabine Vuik, MSc, PhD; Ara Darzi, MD, FRCSImperial College London, Department of Surgery and Cancer, London, United KingdomCorresponding Author:Sunir Gohil, BSc, MBBSImperial College LondonDepartment of Surgery and Cancer10th Floor QEQM Building, St Marys HospitalPraed St, LondonLondon, W2 1NYUnited KingdomPhone: 44 7715552952Fax: 44 1923854171Email: sunir.gohil06@imperial.ac.ukAbstractBackground: Twitter is a microblogging service where users can send and read short 140-character messages called “tweets.”There are several unstructured, free-text tweets relating to health care being shared on Twitter, which is becoming a popular areafor health care research. Sentiment is a metric commonly used to investigate the positive or negative opinion within these messages.Exploring the methods used for sentiment analysis in Twitter health care research may allow us to better understand the optionsavailable for future research in this growing field.Objective: The first objective of this study was to understand which tools would be available for sentiment analysis of Twitterhealth care research, by reviewing existing studies in this area and the methods they used. The second objective was to determinewhich method would work best in the health care settings, by analyzing how the methods were used to answer specific healthcare questions, their production, and how their accuracy was analyzed.Methods: A review of the literature was conducted pertaining to Twitter and health care research, which used a quantitativemethod of sentiment analysis for the free-text messages (tweets). The study compared the types of tools used in each case andexamined methods for tool production, tool training, and analysis of accuracy.Results: A total of 12 papers studying the quantitative measurement of sentiment in the health care setting were found. Morethan half of these studies produced tools specifically for their research, 4 used open source tools available freely, and 2 usedcommercially available software. Moreover, 4 out of the 12 tools were trained using a smaller sample of the study’s final data.The sentiment method was trained against, on an average, 0.45% (2816/627,024) of the total sample data. One of the 12 paperscommented on the analysis of accuracy of the tool used.Conclusions: Multiple methods are used for sentiment analysis of tweets in the health care setting. These range from self-producedbasic categorizations to more complex and expensive commercial software. The open source and commercial methods aredeveloped on product reviews and generic social media messages. None of these methods have been extensively tested againsta corpus of health care messages to check their accuracy. This study suggests that there is a need for an accurate and tested toolfor sentiment analysis of tweets trained using a health care setting–specific corpus of manually annotated tweets first.(JMIR Public Health Surveill 2018;4(2):e43)   doi:10.2196/publichealth.5789KEYWORDSTwitter; social mediaIntroductionToday’s doctors and patients take to online platforms such asblogs, social media, and websites to convey opinions on healthmatters [1]. Infodemiology is “the science of distribution anddeterminants of information in an electronic medium,specifically the Internet, or in a population, with the ultimateaim to inform public health and public policy” [2]. Data can beJMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.1http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderXcollected and analyzed from social media such as Twitter inreal time with the ability to survey public opinion (sentiment)toward a subject [3]. Bates and colleagues have described socialmedia as a “perfect storm” in regard to patient-centered healthcare, which is a valuable source of data for the public and healthorganizations [4]. Twitter is one such place, being easy to use,cheap, and accessible. Twitter is a mobile microblogging andsocial networking service. There are currently 955 millionregistered Twitter users who can share messages that containtext, video, photos, or links to external sources. One-third ofpeople with a social media profile use Twitter, with 75%accessing from a handheld device to convey an opinion [5,6].Sentiment analysis allows the content of free-text naturallanguage—that is, the words and symbols used in a message—tobe examined for the intensity of positive and negative opinionsand emotions. Sentiment analysis from social media is alreadya widely researched subject [7]. It is useful for businessmarketing to understand the public or consumer opinion towardtheir product [8]. Computerized software tools have beenproduced that automate the process of sentiment analysis,allowing large numbers of free-text comments to be processedinto quantitative sentiment scores quickly, for example, positiveor negative [7]. They are commonly based on text classifiers ormachine learning processes. These tend to be commerciallyorientated, expensive, and focused on gathering opinion on aspecific chosen product or service [9]. During the H1N1outbreak, Chew et al conducted a content analysis of tweets[10]. In this study, they measured sentiment in a qualitativecategorical way using content classifiers such as “humor” or“sarcasm.” Accurate and automated sentiment analysis ischallenging due to the subjectivity, complexity, and creativityof the language used [11].Sentiment analysis in the health care setting is not a newphenomenon. Using only manual annotation of health caretweets, it has been found that 40% of messages contain someform of sentiment (either positive or negative) [12]. A manualmethod has also been used in the analysis of suicide notes anddischarge summaries, where Cherry et al attempt to automatethe manual process using machine learning approaches [13-15].It was found that the manual classification of emotional textwas difficult and inconsistent [13]. Greater positive sentimentwithin discharge summaries was associated with significantlydecreased risk of readmission [14]. A study was also conductedmeasuring the sentiment of comments on the main NationalHealth Service (NHS) website (NHS choices) over a 2-yearperiod [16,17]. They found a strong agreement between thequantitative online ratings of health care providers and analysisof sentiment using their automated method.Sentiment analysis has made its way into the mainstreamanalysis of Twitter-based health care research. Twitter is apopular platform as it allows data to be collected easily usingtheir application programming interface. The limitations of othersocial media platforms such as Facebook are they do not allowsuch easy access to their data due to their varying privacypolicies. It is not as easy to collect data in an open and automatedway with other such media. The opinion of a tweet is foundwithin the text portion of the tweet. This is captured in anunstructured, nonstandardized, free-text form. Accuratelymeasuring the sentiment of a health care tweet represents anopportunity for understanding both the patient’s and health careprofessional’s opinion on a health subject [16]. Kent et al foundthat up to 40% of health care tweets contain some form ofsentiment [12]. A validated tool for sentiment analysis of healthcare messages on Twitter would allow for the assessment ofopinion on a mass scale [17]. Sentiment analysis in the medicalsetting offers a unique challenge as terms can have varyingusage and meanings, and requires complementarycontext-specific features with a domain-specific lexicon [18].The language used to convey sentiment in medicine is likely tobe different than that toward a product, as the boundary between“patient,” “consumer,” and “customer” is difficult to define andterms can have varying usage and meanings [11,19]. Therefore,the sentiments may be expressed differently in a health carecontext [18].To date, there has been no study looking at all the methods usedfor sentiment analysis on Twitter in the health care setting.Currently available sentiment analysis tools have not beendeveloped based on a health care setting. SentiStrength [20], apopular open source software was based on nonspecificmessages sent via MySpace [21]. Health care can be a verydifferent environment based on many aspects. Being a publicNational Health Service [19], the boundary between “patient,”“consumer,” and “customer” is difficult to define in health careTherefore, currently available sentiment analysis methods maynot be accurate.The aim of this study was to review the methods used to measuresentiment for Twitter-based health care studies. The firstobjective was to review what methods of sentiment analysishave been used and in which health care setting. The secondobjective was to explore to what extent the methods were trainedand validated for the study data, and if any justification for theirmethodology use was offered.MethodsIdentification and ScreeningIn May 2015, a computerized search of the literature wasconducted, following Preferred Reporting Items for SystemicReviews guidelines [22]. MEDLINE (OvidSP) and EMBASE(OvidSP) were searched using the terms. References werechecked from papers and reviews, and citations were checkedfrom included studies. The titles and abstracts were screenedfrom the retrieved search to identify relevant studies. Asupplementary hand search was carried out in September 2016in key journals. Studies had to include one of the followingsearch terms in the title, abstract, or keywords: “Twitter” orassociated terms “tweet” or “microblog” and “Sentiment” orassociated search terms “opinion” or “emoti” or “happi” or“Senti.” There were 3 inclusion criteria for the study. First, thestudy must have Twitter as its primary focus. The aim of thisreview was to explore research into the methods of sentimentanalysis on Twitter messages only. Second, the papers must berelating to a health care subject. This included all aspects ofhealth and health care delivery, health care research, policy, andorganizational and professional use. Finally, papers that used aJMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.2http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderXquantitative method to analyze both positive and negativesentiments of the messages, for example, “−1,” were included.Eligibility and InclusionThe studies were restricted to those published in English. Atotal of 69 full-text articles were assessed for eligibility. Ofthese, 15% (10/69) were rejected because they looked at socialmedia in general (not Twitter specifically), for example, the useof social media by surgical colleagues [23]. Moreover, 36%(25/69) were rejected because the study did not pertain to healthcare, for example, public perceptions of nonmedical use ofopioids [24]. Furthermore, 32% (22/69) papers were excludedbecause the sentiment analysis was either not measured, notquantitative or did not discuss positive and negative sentimentsspecifically, for example, characterizing sleep issues usingTwitter [25]. The criteria used to compare the methods in eachstudy looked at the method of tool production, in which settingit was used, and the method of testing the tool. For assessment,a comparison of the number of annotators used to manuallyannotate tweets, if any, and the level of agreement between themwas used. Furthermore, the proportion of tweets used to trainan algorithm compared with the final sample analyzed was alsoassessed.ResultsOverall ResultsIn total, 12 papers were found that satisfied all 3 inclusioncriteria (see Table 1 for overview). These were publishedbetween 2011 and 2016 with data collected from Twitterbetween 2006 and 2016. Moreover, 2 papers examined globaldata, 9 in the United States, and 1 in the United Kingdom.Comments from 2 papers suggest that on an average 46% (92/2)of health care tweets contain some form of sentiment, that is,not neutral [12,26]. Many studies conducted analysis on publichealth–related subjects (n=7). In addition, 3 papers examinedthe sentiment toward an aspect of disease: the disease itself(n=1), symptoms (n=1), or treatment (n=1). Finally, 2 papersstudied an emergency medical situation and a medicalconference.A total of 5 of the 12 studies conducted a manual sentimentanalysis of a sample of their data using annotators to train theirtool. One study used 13.58% (1000/7362) of their final datasample to train their developed method [34]. Three studies usedan average of 0.7% of their total dataset to train their tool(1.46%, 250/17,098; 0.55%, 2216/404,065; and 0.1%,250/198,499). One paper compared the accuracy of their chosenmethods with a manually annotated corpus of their data [30].Moreover, 2 papers from the group commented on justificationof the sentiment analysis tools used.There were 3 categories of sentiment analysis methods found(see Table 2), a tool specifically produced and trained for thatstudy data, open source tools, and commercially availablesoftware. This distinction was made based on the required levelof expertise in computer programming needed to implementthat method and if predefined lexicons were used. Toolsproduced specifically for the study required the most amountof programming knowledge as these sometimes required theuse of machine learning techniques to train a tool or rule-basedmethods. Alternatively, using commercially available softwarerequired the least knowledge as these are designed to be quickand easy to use. Half of the studies conducted quantitativesentiment analysis using an automated method developed bythe study group themselves using algorithms or machine learningtechniques. Moreover, 3 studies used commercially availablesentiment analysis products. The remaining 3 papers used opensource, freely available sentiment analysis software, whichrequired little programming experience. In addition, 1 studyfrom the open source and 1 from commercial method studiesused a method of manual training to tailor the tool for theirspecific study data [33].Table 1. Tools used for sentiment analysis.Type of methodSentiment towardSubject areaLocationYearAuthorOpen source25 Federal health agenciesPublic healthUnited States2012Bhattacharya et al [27]Commercial2011 Japanese earthquake and tsunamiEmergencymedicineUnited States2011Black et al [28]Produced for studyElectronic cigarettePublic healthUnited States2013-2014Cole-Lewis et al [29]Produced for studySentiment toward drug-related tweetsPublic healthUnited States2016Daniulaityte et al [30]Produced for studyTwitter activity at Kidney Week 2011Medical confer-enceUnited States2011Desai et al [31]CommercialHospital qualityPublic healthUnited King-dom2012Greaves et al [32]Open sourceHospital qualityPublic healthUnited States2015Hawkins et al [33]Produced for studyTobaccoPublic healthGlobal2012Myslin et al [34]Open sourcePalliative medicineDisease specificGlobal2015Nwosu et al [35]Open sourceMultiple sclerosis treatmentsDisease treatmentUnited States2006-2014Ramagopalan et al [26]Produced for studyTobaccoPublic healthUnited States2013Sofean and Smith [36]Produced for studyPainDisease symptomsUnited States2015Tighe et al [37]JMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.3http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderXTable 2. Sentiment tools based on type of tool: KNN: k-nearest-neighbors; N/A: not applicable; NB: Naïve Bayes; SVM; support vector machines.Manually annotated comparedwith total sample, n (%)Sample sizeManually annotatedsampleKappaAnnotatorsToolAuthor250 (1.46)17,098250.646Produced for study: ma-chine learning classifiersbased on 5 categories (NB,KNN, and SVM)Cole-Lewis et al [29]N/A  993N/AN/A  N/AProduced for study: rulebased using AFINN(Named after the author,Finn Arup Neilsen)Desai et al [31]N/AN/A3000.682Produced for study: logis-tic regression, NB, SVMDaniulaityte et al [30]1000 (13.58)73621000>.72Produced for study: ma-chine learning (NB, KNN,SVM)Myslin et al [34]N/AN/A500N/AN/A  Produced for study: 5-foldvalidation using supportvector machines (SVM’s)model using Waikato Envi-ronment for KnowledgeAnalysis toolkit toolkitSofean and Smith [36]N/A65,000N/AN/AN/AProduced for study: rulebased using AFINNTighe et al [37]N/A164,104N/AN/A3Open source: Sen-tiStrengthBhattacharya et al [27]2216 (0.55)404,0652216>.792+AmazonMechanicalTurkOpen source: machinelearning classifier usingPython library TextBlobHawkins et al [33]N/A60,037N/AN/AN/AOpen source: TwitteR Rpackage + Jeffrey Breen’ssentiment analysis codeRamagopalan et al [26]N/AN/AN/AN/AN/ACommercial: radian6Black et al [28]250 (0.13)198,499250N/AN/ACommercial: TheySayGreaves et al [32]N/A683,500N/AN/AN/AOpen source: TopsyProNwosu et al [35]A total of 5 studies commented on the number of annotatorsused for the manual classification of sentiment to train theirfinal tool (average=3 annotators, range 2-6). A single study useda method of outsourcing the task of manual classification tomultiple anonymous annotators via Amazon Mechanical Turk[38].Self-Produced Sentiment Analysis ToolsOf the 12 studies reviewed, 6 produced sentiment analysis toolswithin their own department, specifically designed for theirstudy using already defined algorithms. Liu describes thedifferent types of algorithms that can be used, and they producedifferent kinds of summaries [39,40]. Moreover, 2 differenttypes of algorithms were found to be used, a standard supervisedmachine learning algorithm and a classification method (suchas AFINN named after the author, Finn Arup Neilsen). Thesemethods produce their own classifier trained to detect polarityusing their original data. These may be different from the opensource tools, which use already pretrained classifiers in premadesoftware systems designed more toward an end user.A total of 3 papers used a similar method of sentiment viacategorization, all examining opinions toward smoking. Sofeanet al produced an automated sentiment tool based on identifying250 positive and 250 negative tweets from a smaller sample totrain their tool [36]. There was no further detail into theannotation and analysis process. A limitation to their tool wasthat it screened out emoticons (symbols used to express emotion)before producing a tool. This is a method often used by usersto convey emotion [39]. Myslin et al analyzed the sentimenttoward emerging tobacco products on 7362 tweets, whereCole-Lewis et al looked specifically at sentiment towardelectronic cigarettes on 17,098 tweets [29,34]. Neither of thestudies commented on why a self-produced solution was used.Tweets were broadly categorized into “positive,” “neutral,” or“negative” by the annotators. The intensity of the sentiment wasnot recorded. To find the relationship between the sentimentand subject, 3 machine learning algorithms were used, NaïveBayes, K-Nearest-Neighbor, and Support Vector Machine [41].An automated sentiment analysis tool was produced based onthe manual analysis of sentiment of a sample of tweets duringthe pilot phase of each study. This represented 13.58%(1000/7362) for Myslin. The study by Cole-Lewis used onlyJMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.4http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderX1.46% (250/17,098) of their total sample to train theiralgorithms. This represents a very small percentage of theirsample and may result in their method being less accurate thanintended. However, no comment is made by the study group towhy only this number was used.Desai et al used the AFINN (named after the author, Finn ArupNeilsen), to measure the sentiment of Twitter activity duringKidney Week 2011 from 993 tweets [31]. AFINN is a rule-basedapproach combined with statistical modeling to create a hybridapproach to sentiment classification [7]. This is based oncomparing a sample of data with a list of weights of positive ornegative keywords using the affective norms for English wordsdataset [42]. The AFINN consists of a list of manually labeledEnglish words that have been given an integer value between−5 (highly negative) to +5 (highly positive). A value is assignedfor each word in a tweet using the lexicon. The values areaveraged to calculate the sentiment score for the whole message.This method has been validated for use in microblogs such asTwitter [43]. Tighe et al used this method to assess the sentimentof tweets pertaining to pain, suggesting a rule-based classifierhas greater methodological advantage due to its deterministicresults compared with human annotators which can have poorinterannotator agreement with sentiment [37]. In addition, theysupplemented AFINN with the use of emoticon terminology toenhance the accuracy of the rule-based classifier [39,44]. Onestudy sought to compare different supervised machine learning(SML) techniques with each other, and to a rule-based opensource lexicon for drug-related tweets [30]. They found that byusing manually annotated tweets specifically from that subjectto train SML techniques was more accurate than a prepreparedlexicon due to the variation in language used. They also comparetypes of SML techniques to show that they all performed to asimilar level.Open Source Sentiment SoftwareOpen source software is a computer software that has its sourcecode made available to the public to modify [45]. The developersor copyright holders of the software give the rights to study anddistribute the software for any purpose for free. Moreover, 4papers used open source software for their sentiment analysis.None of these tools were initially produced using health caremessages. Ramagopalan et al investigated the opinions ofspecific multiple sclerosis treatments using 60,037 tweets [26].They used an open source sentiment analysis tool called packagetwitteR R [46] in combination with Jeffrey Breen’s sentimentanalysis code [47]. This software was developed for the analysisof consumer sentiment toward a product and compares thefrequency of positive or negative words against a predefinedlist. The overall sentiment score of each message is calculatedby subtracting the number of negative words from the numberof positive words. A sentiment score of >0 suggests that themessage has an overall positive opinion. Of their dataset, 52%of messages contained a non-neutral sentiment. This studyshowed that there was a statistically significant difference insentiment toward different types of multiple sclerosismedications. There was no comment on analysis of the toolitself or justification of its use.Bhattacharya et al used SentiStrength [20,48], a popular opensource software to analyze the sentiment of 164,104 tweets from25 Federal Health Agencies in the United States and their 130accounts. SentiStrength has been designed to measure thesentiment of short informal messages and has been widely usedfor Twitter analysis [49]. It was used in this case because itoutperforms other lexical classifiers [42]. No manual sentimentanalysis was conducted.SentiStrength was developed in 2009 to extract sentimentstrength from informal English text, giving a rating between −5and +5. The algorithm was developed on an initial set of 2600MySpace comments used for pilot testing. A set of 3 samegender (female) coders were used for initial testing and this wasoptimized by machine learning into its final version. It can detectpositive emotion with 60.6% accuracy and negative emotionwith 72.8% accuracy. SentiStrength outperforms a wide rangeof other machine learning approaches. SentiStrength has notyet been validated specifically for health care–based messages.Hawkins et al measured patient-perceived quality of care in UShospitals using Twitter [33]. Over 404,000 tweets were analyzedfor their sentiment and compared with established qualitymeasures over a 1-year period. Natural language processingwas used to measure the sentiment of the patient experiencetweets. This was based on a Python library TextBlob [50].TextBlob is trained from human annotated words commonlyfound in product reviews based on the Pattern Library [51]. Thesentiment score can range from −1 to +1, with a score of 0suggesting a tweet that is neutral. This was the first study thatadopted Amazon Mechanical Turk [38] to use multipleoutsourced anonymous curators to train their tool. They founda weak association between the positive sentiment toward ahospital and the readmission rate.Commercial SoftwareThere are numerous commercial software packages availableto analyze the sentiment of tweets. These range in pricedepending on the number of tweets or duration of use. In thisstudy, 2 papers were found using commercial software. Neithertool was developed with health care messages as its foundation,and no justification for their use is offered for either.The largest number of messages analyzed by Nwosu measuredthe sentiment of over 683,000 tweets based around palliativemedicine and end of life care [35]. Discussion about end of lifecan be difficult and sometimes missed [52]. TopsyPro was usedto measure the sentiment of tweets [53]. This software wascreated in 2015 as an Web based tool for Twitter analytics andsentiment analysis and is based on an annual subscription costingUS $12,000 per year per named user (for the “Pro” versionwhich enables more detailed analysis). There is no informationcurrently available on the methods used by Topsy Labs, Inc. onhow the sentiment analysis is conducted.Radian6 [54] is another piece of “listening” social mediasoftware to collect and analyze data. It has been previously usedto collect data during a medical conference, with analysisfocused on the major Twitter influencers [55]. The softwaredoes not require the user to have any programming knowledgeand is deigned to be easy to use. Black et al used this softwareJMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.5http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderXto analyze tweets based around public health emergencyresponse during the Japanese earthquake and tsunami in March2011. There was no comment on why this software was used.Radian6 can “listen” automatically to large-scale Twitterconversation based on specific keywords.A study conducted by Greaves et al was found looking athospital quality in the United Kingdom, and it measured thesentiment of over 198,000 tweets directed toward NHS hospitalsin 2012 [32]. The commercially available software used wasdeveloped by TheySay Ltd (Oxford, UK). TheySay is based oncompositional sentiment parsing, described by work fromMoilanen and Pulman, using 5 automated ways of naturallanguage processing [56]. For academic purposes, the softwarecosts roughly £350 for a similar volume of data to the mentionedstudy to be analyzed.DiscussionPrincipal FindingsOn average, 46% (92/2) of health-based tweets contain someform of positive or negative sentiment [12,26]. A relationshipbetween sentiment on Twitter and hospital statistics has alreadybeen proven [33]. It is important to conduct sentiment analysisfor health care tweets that is accurate and consistent. This studyhas found that there is a large disparity in the types of methodsused, from basic categorizations to seemingly sophisticated andexpensive commercially available software. Between the samesubject matter such as hospital quality, different sentimentanalysis methods have been used which makes it difficult tocompare the results between the two [32,33]. Chew et alconducted a content analysis of tweets during the 2009 H1N1outbreak and chose to use only a qualitative method forsentiment analysis of tweets, categorizing tweets based onemotive words, for example, “Humour” or “Concern” [10]. Onthe basis of complexity of implementation, 3 broad categoriesof methods have emerged: (1) self-produced methods usingalgorithms, (2) open source methods, and (3) commerciallyavailable software. Only 1 method in this study was producedwith health care language as its foundation using a corpus ofmanually annotated health care setting–specific tweets fortraining [30]. Many methods were based on tools trained onproduct reviews and nonspecific social media messages thatmay not be appropriate for use in the health care setting [20,57].The language used to convey sentiment in medicine is likely tobe different than that toward a product as the boundary between“patient,” “consumer,” and “customer” is difficult to define andterms can have varying usage and meanings [11,18,19].Health-related tweets represent a unique type of content, andtheir communication on Twitter carries special characteristicsas found in pain-related tweets [37].Most studies did not justify the reason for their selected method.Furthermore, there was no evidence of analysis of accuracy ofthe method before being used for the larger respective data.Researchers tend to assume a method selected will be accurate.Most self-produced methods train their tool using a very smallpercentage of their final dataset, in one case less than 2% [29].A formal process for checking the accuracy occurred in one ofthe author’s study that compared types of supervised machinelearning techniques. Software products and open source toolsbeing currently used tend to be designed originally to identifyopinions about products in the commercial setting rather thanbehaviors. This questions their accuracy when used in a medicalsetting.RecommendationsThis research shows that different approaches are used for thesentiment analysis of tweets in the health care setting. Theevidence suggests that there is a need for the production andanalysis of accuracy of a sentiment analysis tool trained usingsetting-specific health care tweets. Twitter is used globally, andhealth care can vary greatly depending on the setting. On thebasis of this study, such a tool would ideally be trained using ahealth care subject-specific corpus of labeled tweets to trainsupervised machine learning classifiers [30]. SemanticEvaluation Exercises (SemEval 2016) held in San Diego is anevent where programmers are tasked with producing a sentimentanalysis tool on a range of Twitter subjects such as a politicalcandidate or product, using a pre-annotated corpus. Thiscollaborative approach could be used to produce a moreadvanced and accurate tool for the health care setting usingsubject-specific lexicons and complementary health care–basedfeatures [11,18,58]. Furthermore, it could measure the intensityof sentiment using an aggregation of methods (eg, emoticons,natural language processing, and supervised machine learning),and it could check for accuracy against a slightly larger manuallyannotated dataset before being used on much larger samplesizes. This could allow future research in health care–basedtweets to accurately and consistently measure the sentiment ofsetting specific health care–based messages. Conflicts of InterestNone declared.References1. Afyouni S, Fetit AE, Arvanitis TN. #DigitalHealth: exploring users' perspectives through social media analysis. Stud HealthTechnol Inform 2015;213:243-246. [Medline: 26153005]2. Eysenbach G. Infodemiology and infoveillance: framework for an emerging set of public health informatics methods toanalyze search, communication and publication behavior on the Internet. J Med Internet Res 2009 Mar 27;11(1):e11 [FREEFull text] [doi: 10.2196/jmir.1157] [Medline: 19329408]3. Eysenbach G. Infodemiology and infoveillance tracking online health information and cyberbehavior for public health. AmJ Prev Med 2011 May;40(5 Suppl 2):S154-S158. [doi: 10.1016/j.amepre.2011.02.006] [Medline: 21521589]JMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.6http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderX4. Rozenblum R, Bates D. Patient-centred healthcare, social media and the internet: the perfect storm? BMJ Qual Saf 2013Feb 01;22(3):183-186. [doi: 10.1136/bmjqs-2012-001744]5. Ofcom. 2015. The communications market report URL: http://stakeholders.ofcom.org.uk/binaries/research/cmr/cmr15/icmr15/icmr_2015.pdf [WebCite Cache ID 6gjgoy6Ex]6. Lunden I. Techcrunch. 2013. Mobile twitterm+ (75%) access from handheld devices monthly, 65% of ad sales come frommobile URL: http://techcrunch.com/2013/10/03/mobile-twitter-161m-access-from-handheld-devices-each-month-65-of-ad-revenues-coming-from-mobile/ [accessed2016-04-13] [WebCite Cache ID 6gjgwD7Lv]7. Pang B, Lee L. Opinion mining and sentiment analysis. Foundations and trends in information retrieval 2008 Jul;2(1-2):1-35.8. Liu B, Zhang L. A survey of opinion mining and sentiment analysis. Mining Text Data 2012:415-463. [doi:10.1007/978-1-4614-3223-4]9. Nasukawa T. Sentiment analysis: capturing favorability using natural language processing. 2003 Jan 01 Presented at:Proceedings of the 2nd International Conference on Knowledge; October 23-25, 2003; Sanibel Island, FL, USA. [doi:10.1145/945645.945658]10. Chew C, Eysenbach G. Pandemics in the age of twitter: content analysis of Tweets during the 2009 H1N1 outbreak. PLoSOne 2010 Nov 29;5(11):e14118 [FREE Full text] [doi: 10.1371/journal.pone.0014118] [Medline: 21124761]11. Mohammad S. 9 – Sentiment analysis: detecting valence, emotions, and other affectual states from text. Emotion Measurement2016:201-237. [doi: 10.1016/B978-0-08-100508-8.00009-6]12. Kent E, Prestin A, Gaysynsky A, Galica K, Rinker R, Graff K, et al. “Obesity is the new major cause of cancer”: connectionsbetween obesity and cancer on facebook and twitter. J Canc Educ 2015 Apr 14;31(3):453-459. [doi:10.1007/s13187-015-0824-1]13. Danno K, Horio T. Sunburn cell: factors involved in its formation. Photochem Photobiol 1987 May;45(5):683-690. [Medline:3299408]14. Treves AJ, Carnaud C, Trainin N, Feldman M, Cohen IR. Enhancing T lymphocytes from tumor-bearing mice suppresshost resistance to a syngeneic tumor. Eur J Immunol 1974 Nov;4(11):722-727. [doi: 10.1002/eji.1830041104] [Medline:4547711]15. Yadav RN. Isocitrate dehydrogenase activity and its regulation by estradiol in tissues of rats of various ages. Cell BiochemFunct 1988 Jul;6(3):197-202. [doi: 10.1002/cbf.290060308] [Medline: 3409480]16. Greaves F, Ramirez-Cano D, Millett C, Darzi A, Donaldson L. Use of sentiment analysis for capturing patient experiencefrom free-text comments posted online. J Med Internet Res 2013 Nov 01;15(11):e239-e251 [FREE Full text] [doi:10.2196/jmir.2721] [Medline: 24184993]17. Alemi F, Torii M, Clementz L, Aron DC. Feasibility of real-time satisfaction surveys through automated analysis of patients'unstructured comments and sentiments. Qual Manag Health Care 2012;21(1):9-19. [doi: 10.1097/QMH.0b013e3182417fc4]18. Denecke K, Deng Y. Sentiment analysis in medical settings: new opportunities and challenges. Artif Intell Med 2015May;64(1):17-27. [doi: 10.1016/j.artmed.2015.03.006]19. Alemi F, Torii M, Clementz L, Aron DC. Feasibility of real-time satisfaction surveys through automated analysis of patients'unstructured comments and sentiments. Qual Manag Health Care 2012;21(1):9-19. [doi: 10.1097/QMH.0b013e3182417fc4][Medline: 22207014]20. Thelwall M, Buckley K, Paltoglou G, Cai D, Kappas A. Sentiment strength detection in short informal text. J Am Soc InfSci 2010 Dec 15;61(12):2544-2558. [doi: 10.1002/asi.21416]21. MySpace. 2016. Featured content on myspace URL: https://myspace.com/ [WebCite Cache ID 6gDjWFfGw]22. Moher D, Liberati A, Tetzlaff J, Altman DG, PRISMA Group. Preferred reporting items for systematic reviews andmeta-analyses: the PRISMA statement. Ann Intern Med 2009 Aug 18;151(4):264-9, W64. [Medline: 19622511]23. Loeb S, Bayne CE, Frey C, Davies BJ, Averch TD, Woo HH, American Urological Association Social Media Work Group.Use of social media in urology: data from the American Urological Association (AUA). BJU Int 2014 Jun;113(6):993-998[FREE Full text] [doi: 10.1111/bju.12586] [Medline: 24274744]24. Chan B, Lopez A, Sarkar U. The canary in the coal mine tweets: social media reveals public perceptions of non-medicaluse of opioids. PLoS One 2015;10(8):e0135072 [FREE Full text] [doi: 10.1371/journal.pone.0135072] [Medline: 26252774]25. de Chudnovsky A W. [The contemporary face and the prediction of growth]. Trib Odontol (B Aires) 1973;57(10):294-6passim. [Medline: 4526927]26. Adelman AG, Wigle ED, Ranganathan N, Webb GD, Kidd BS, Bigelow WG, et al. The clinical course in muscular subaorticstenosis. A retrospective and prospective study of 60 hemodynamically proved cases. Ann Intern Med 1972Oct;77(4):515-525. [Medline: 4264640]27. Bhattacharya S, Srinivasan P, Polgreen P. Engagement with health agencies on twitter. PLoS One 2014;9(11):e112235[FREE Full text] [doi: 10.1371/journal.pone.0112235] [Medline: 25379727]28. Silen W, Machen TE, Forte JG. Acid-base balance in amphibian gastric mucosa. Am J Physiol 1975 Sep;229(3):721-730.[doi: 10.1152/ajplegacy.1975.229.3.721] [Medline: 2015]JMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.7http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderX29. Cole-Lewis H, Varghese A, Sanders A, Schwarz M, Pugatch J, Augustson E. Assessing electronic cigarette-related tweetsfor sentiment and content using supervised machine learning. J Med Internet Res 2015 Aug 25;17(8):e208 [FREE Full text][doi: 10.2196/jmir.4392] [Medline: 26307512]30. Daniulaityte R, Chen L, Lamy FR, Carlson RG, Thirunarayan K, Sheth A. “When 'bad' is 'good'”: identifying personalcommunication and sentiment in drug-related tweets. JMIR Public Health Surveill 2016 Oct 24;2(2):e162 [FREE Full text][doi: 10.2196/publichealth.6327] [Medline: 27777215]31. Desai T, Shariff A, Shariff A, Kats M, Fang X, Christiano C, et al. Tweeting the meeting: an in-depth analysis of twitteractivity at kidney week 2011. PLoS One 2012;7(7):e40253 [FREE Full text] [doi: 10.1371/journal.pone.0040253] [Medline:22792254]32. Greaves F, Laverty AA, Cano DR, Moilanen K, Pulman S, Darzi A, et al. Tweets about hospital quality: a mixed methodsstudy. BMJ Qual Saf 2014 Oct;23(10):838-846 [FREE Full text] [doi: 10.1136/bmjqs-2014-002875] [Medline: 24748372]33. Hawkins J, Brownstein JS, Tuli G, Runels T, Broecker K, Nsoesie EO, et al. Measuring patient-perceived quality of carein US hospitals using Twitter. BMJ Qual Saf 2016 Dec;25(6):404-413 [FREE Full text] [doi: 10.1136/bmjqs-2015-004309][Medline: 26464518]34. Myslín M, Zhu SH, Chapman W, Conway M. Using twitter to examine smoking behavior and perceptions of emergingtobacco products. J Med Internet Res 2013 Aug 29;15(8):e174 [FREE Full text] [doi: 10.2196/jmir.2534] [Medline:23989137]35. Nwosu A, Debattista M, Rooney C, Mason S. Social media and palliative medicine: a retrospective 2-year analysis of globaltwitter data to evaluate the use of technology to communicate about issues at the end of life. BMJ Support Palliat Care 2015Jun;5(2):207-212. [doi: 10.1136/bmjspcare-2014-000701] [Medline: 25183713]36. Sofean M, Smith M. Sentiment analysis on smoking in social networks. Stud Health Technol Inform 2013;192:1118.[Medline: 23920892]37. Tighe PJ, Goldsmith RC, Gravenstein M, Bernard HR, Fillingim RB. The painful tweet: text, sentiment, and communitystructure analyses of tweets pertaining to pain. J Med Internet Res 2015 Apr 02;17(4):e84 [FREE Full text] [doi:10.2196/jmir.3769] [Medline: 25843553]38. Amazon. Amazon. 2015. Amazon mechanical turk - welcome URL: https://www.mturk.com/ [WebCite Cache ID 6gDjkM7J3]39. Liu K, Li W, Guo M. Emoticon smoothed language models for twitter sentiment analysis. 2012 Presented at: AAAI'12Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence; July 22-26, 2012; Toronto, Ontario, Canadap. 1678-1684.40. Liu S, Yamada M, Collier N, Sugiyama M. Change-point detection in time-series data by relative density-ratio estimation.Neural Netw 2013 Jul;43:72-83. [doi: 10.1016/j.neunet.2013.01.012] [Medline: 23500502]41. Franklin J. The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelligencer 2008Nov 12;27(2):83-85. [doi: 10.1007/BF02985802]42. Nielsen F. arxiv. 2011. A new ANEW: evaluation of a word list for sentiment analysis in microblogs URL: https://arxiv.org/abs/1103.2903 [WebCite Cache ID 6yE38xA6F]43. Kouloumpis E. Twitter sentiment analysis: the good the bad and the OMG!. 2011 Presented at: Proceedings of the FifthInternational AAAI Conference on Weblogs and Social Media; July 17-21, 2011; Barcelona, Catalonia, Spain p. 538-541.44. Davidov D, Tsur O, Rappoport A. Enhanced sentiment learning using Twitter hashtags and smileys. 2010 Presented at:COLING '10 Proceedings of the 23rd International Conference on Computational Linguistics: Posters; August 23-27, 2010;Beijing, China.45. Opensource. 2015. What is open source? URL: https://opensource.com/resources/what-open-source [WebCite Cache ID6gjjUVOPo]46. Soranaka K, Matsushita S. Relationship between emotional words and emoticons in tweets. 2012 Presented at: 2012Conference on Technologies and Applications of Artificial Intelligence (TAAI); November 16-18, 2012; Tainan, Taiwan.[doi: 10.1109/TAAI.2012.30]47. Wordpress. 2011. slides from my R tutorial on Twitter text mining #rstats URL: https://jeffreybreen.wordpress.com/2011/07/04/twitter-text-mining-r-slides/ [accessed 2016-04-13] [WebCite Cache ID 6gjqcREzI]48. Sentistrength. 2015. SentiStrength - sentiment strength detection in short texts - sentiment analysis, opinion mining URL:http://sentistrength.wlv.ac.uk/ [WebCite Cache ID 6gDjqyjzI]49. Thelwall M, Buckley K, Paltoglou G. Sentiment in twitter events. J Am Soc Inf Sci 2010 Dec 06;62(2):406-418. [doi:10.1002/asi.21462]50. Loria S. TextBlob. TextBlob: simplified text processing URL: https://textblob.readthedocs.org/en/dev/ [accessed 2016-04-13][WebCite Cache ID 6gjqofstl]51. Computational Linguistics & Psycholinguistics Research Center (CLiPS). 2015. pattern.en URL: http://www.clips.ua.ac.be/pages/pattern-en [WebCite Cache ID 6gDjvzIX2]52. Mori M, Shimizu C, Ogawa A, Okusaka T, Yoshida S, Morita T. A national survey to systematically identify factorsassociated with oncologists' attitudes toward end-of-life discussions: what determines timing of end-of-life discussions?Oncologist 2015 Nov;20(11):1304-1311 [FREE Full text] [doi: 10.1634/theoncologist.2015-0147] [Medline: 26446232]JMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.8http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderX53. Hortanoticias. 2015. Twitter analytics by Topsy. Search hashtags, sentiment & trends URL: https://www.hortanoticias.com/wp-content/uploads/2015/03/Twitter-Analytics-by-Topsy-HortaTuits.pdf [accessed 2016-03-23] [WebCite Cache ID6gDk08ji2]54. Salesforce. 2015. Social studio migration URL: https://www.salesforce.com/form/marketingcloud/social-studio-migration.jsp [accessed 2016-04-13] [WebCite Cache ID 6gjdAk1IS]55. Ferguson C, Inglis SC, Newton PJ, Cripps PJ, Macdonald PS, Davidson PM. Social media: A tool to spread information:a case study analysis of twitter conversation at the cardiac society of Australia & New Zealand 61st annual scientific meeting2013. Collegian 2014 Jun;21(2):89-93. [doi: 10.1016/j.colegn.2014.03.002] [Medline: 25109206]56. Moilanen K, Pulman S. Sentiment composition. 2007 Presented at: Proceedings of Recent Advances in Natural LanguageProcessing (RANLP 2007); September 27-29, 2007; Borovets, Bulgaria p. 378-382.57. Hawkins C, Duszak R, Rawson JV. Social media in radiology: early trends in twitter microblogging at radiology's largestinternational meeting. J Am Coll Radiol 2014 Apr;11(4):387-390. [doi: 10.1016/j.jacr.2013.07.015] [Medline: 24139963]58. Mohammad S, Turney P. Crowdsourcing a word-emotion association lexicon. Comput Intell 2013 Aug;29(3):436-465.[doi: 10.1111/j.1467-8640.2012.00460.x]AbbreviationsKNN: k-nearest-neighborsNB: Naïve BayesNHS: National Health ServiceSML: Supervised Machine LearningSVM: support vector machinesEdited by G Eysenbach; submitted 18.04.16; peer-reviewed by S Mohammad, R Hilscher, T Hernández, M Larsen, H Singh; commentsto author 25.07.16; revised version received 31.10.16; accepted 14.03.17; published 23.04.18Please cite as:Gohil S, Vuik S, Darzi ASentiment Analysis of Health Care Tweets: Review of the Methods UsedJMIR Public Health Surveill 2018;4(2):e43URL: http://publichealth.jmir.org/2018/2/e43/ doi:10.2196/publichealth.5789PMID:29685871©Sunir Gohil, Sabine Vuik, Ara Darzi. Originally published in JMIR Public Health and Surveillance (http://publichealth.jmir.org),23.04.2018. This is an open-access article distributed under the terms of the Creative Commons Attribution License(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,provided the original work, first published in JMIR Public Health and Surveillance, is properly cited. The complete bibliographicinformation, a link to the original publication on http://publichealth.jmir.org, as well as this copyright and license informationmust be included.JMIR Public Health Surveill 2018 | vol. 4 | iss. 2 | e43 | p.9http://publichealth.jmir.org/2018/2/e43/(page number not for citation purposes)Gohil et alJMIR PUBLIC HEALTH AND SURVEILLANCEXSL•FORenderX",
      "id": 34441454,
      "identifiers": [
        {
          "identifier": "oai:spiral.imperial.ac.uk:10044/1/59674",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.2196/publichealth.5789",
          "type": "DOI"
        },
        {
          "identifier": "157858839",
          "type": "CORE_ID"
        }
      ],
      "title": "Sentiment analysis of health care tweets: review of the methods used.",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:spiral.imperial.ac.uk:10044/1/59674"
      ],
      "publishedDate": "2017-03-14T00:00:00",
      "publisher": "'JMIR Publications Inc.'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://spiral.imperial.ac.uk/bitstream/10044/1/59674/6/cdb279b4ca1fba5ac133ad634b235189.pdf"
      ],
      "updatedDate": "2022-06-28T12:30:56",
      "yearPublished": 2017,
      "journals": [
        {
          "title": "JMIR Public Health and Surveillance",
          "identifiers": [
            "2369-2960",
            "issn:2369-2960"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/157858839.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/157858839"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/157858839/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/157858839/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/34441454"
        }
      ]
    },
    {
      "acceptedDate": "2015-12-15T00:00:00",
      "arxivId": "1509.01599",
      "authors": [
        {
          "name": "Bhatia, Parminder"
        },
        {
          "name": "Eisenstein, Jacob"
        },
        {
          "name": "Ji, Yangfeng"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Parminder",
        "Lluís",
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/103415870",
        "https://api.core.ac.uk/v3/outputs/210378713"
      ],
      "createdDate": "2015-09-24T01:55:33",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2015-01-01T00:00:00",
      "abstract": "Discourse structure is the hidden link between surface features and\ndocument-level properties, such as sentiment polarity. We show that the\ndiscourse analyses produced by Rhetorical Structure Theory (RST) parsers can\nimprove document-level sentiment analysis, via composition of local information\nup the discourse tree. First, we show that reweighting discourse units\naccording to their position in a dependency representation of the rhetorical\nstructure can yield substantial improvements on lexicon-based sentiment\nanalysis. Next, we present a recursive neural network over the RST structure,\nwhich offers significant improvements over classification-based methods.Comment: Published at Empirical Methods in Natural Language Processing (EMNLP\n  2015",
      "documentType": "research",
      "doi": "10.18653/v1/d15-1263",
      "downloadUrl": "http://arxiv.org/abs/1509.01599",
      "fieldOfStudy": null,
      "fullText": "Better Document-level Sentiment Analysis from RST Discourse Parsing∗\nParminder Bhatia and Yangfeng Ji and Jacob Eisenstein\nSchool of Interactive Computing\nGeorgia Institute of Technology\nAtlanta, GA 30308\nparminder.bhatia243@gmail.com, {jiyfeng,jacobe}@gatech.edu\nAbstract\nDiscourse structure is the hidden link be-\ntween surface features and document-level\nproperties, such as sentiment polarity. We\nshow that the discourse analyses produced\nby Rhetorical Structure Theory (RST)\nparsers can improve document-level senti-\nment analysis, via composition of local in-\nformation up the discourse tree. First, we\nshow that reweighting discourse units ac-\ncording to their position in a dependency\nrepresentation of the rhetorical structure\ncan yield substantial improvements on\nlexicon-based sentiment analysis. Next,\nwe present a recursive neural network\nover the RST structure, which offers sig-\nnificant improvements over classification-\nbased methods.\n1 Introduction\nSentiment analysis and opinion mining are among\nthe most widely-used applications of language\ntechnology, impacting both industry and a vari-\nety of other academic disciplines (Feldman, 2013;\nLiu, 2012; Pang and Lee, 2008). Yet senti-\nment analysis is still dominated by bag-of-words\napproaches, and attempts to include additional\nlinguistic context typically stop at the sentence\nlevel (Socher et al., 2013). Since document-level\nopinion mining inherently involves multi-sentence\ntexts, it seems that analysis of document-level\nstructure should have a role to play.\nA classic example of the potential relevance of\ndiscourse to sentiment analysis is shown in Fig-\nure 1. In this review of the film The Last Samu-\nrai, the positive sentiment words far outnumber\nthe\n::::::::\nnegative\n:::::::::\nsentiment words. But the discourse\nstructure — indicated here with Rhetorical Struc-\nture Theory (RST; Mann and Thompson, 1988) —\n∗Code is available at https://github.com/\nparry2403/R2N2\nR\nCONCESSION\n\t\nJUSTIFY\n1A\nCONJUNCTION\n\t\nELABORATION\n1B 1C\n1D\n\t\nJUSTIFY\n1E CONJUNCTION\n1F 1G\n1H\n[It could have been a great movie]1A [It does have\nbeautiful scenery,]1B [some of the best since Lord of\nthe Rings.]1C [The acting is well done,]1D [and I really\nliked the son of the leader of the Samurai.]1E [He was\na likable chap,]1F [and I\n::::\nhated to see him die.]1G [But,\nother than all that, this movie is\n::::::\nnothing more than hid-\nden\n:::::\nrip-offs.]1H\nFigure 1: Example adapted from Voll and Taboada\n(2007).\nclearly favors the final sentence, whose polarity\nis negative. This example is illustrative in more\nthan one way: it was originally identified by Voll\nand Taboada (2007), who found that manually-\nannotated RST parse trees improved lexicon-\nbased sentiment analysis, but that automatically-\ngenerated parses from the SPADE parser (Soricut\nand Marcu, 2003), which was then state-of-the-art,\ndid not.\nSince this time, RST discourse parsing has im-\nproved considerably, with the best systems now\nyielding 5-10% greater raw accuracy than SPADE,\ndepending on the metric. The time is therefore\nright to reconsider the effectiveness of RST for\ndocument-level sentiment analysis. In this pa-\nper, we present two different ways of combin-\ning RST discourse parses with sentiment analy-\nsis. The methods are both relatively simple, and\nar\nX\niv\n:1\n50\n9.\n01\n59\n9v\n2 \n [c\ns.C\nL]\n  1\n1 S\nep\n 20\n15\ncan be used in combination with an “off the shelf”\ndiscourse parser. We consider the following two\narchitectures:\n• Reweighting the contribution of each dis-\ncourse unit, based on its position in a\ndependency-like representation of the dis-\ncourse structure. Such weights can be de-\nfined using a simple function, or learned from\na small of data.\n• Recursively propagating sentiment up\nthrough the RST parse, in an architecture in-\nspired by recursive neural networks (Smolen-\nsky, 1990; Socher et al., 2011).\nBoth architectures can be used in combination\nwith either a lexicon-based sentiment analyzer, or\na trained classifier. Indeed, for users whose start-\ning point is a lexicon-based approach, a simple\nRST-based reweighting function can offer signif-\nicant improvements. For those who are willing\nto train a sentiment classifier, the recursive model\nyields further gains.\n2 Background\n2.1 Rhetorical Structure Theory\nRST is a compositional model of discourse struc-\nture, in which elementary discourse units (EDUs)\nare combined intro progressively larger discourse\nunits, ultimately covering the entire document.\nDiscourse relations may involve a nucleus and a\nsatellite, or they may be multinuclear. In the ex-\nample in Figure 1, the unit 1C is the satellite of\na relationship with its nucleus 1B; together they\nform a larger discourse unit, which is involved in\na multinuclear CONJUNCTION relation.\nThe nuclearity structure of RST trees suggests\na natural approach to evaluating the importance\nof segments of text: satellites tend to be less\nimportant, and nucleii tend to be more impor-\ntant (Marcu, 1999). This idea has been leveraged\nextensively in document summarization (Gerani\net al., 2014; Uzeˆda et al., 2010; Yoshida et\nal., 2014), and was the inspiration for Voll and\nTaboada (2007), who examined intra-sentential re-\nlations, eliminating all words except those in the\ntop-most nucleus within each sentence. More re-\ncent work focuses on reweighting each discourse\nunit depending on the relations in which it partic-\nipates (Heerschop et al., 2011; Hogenboom et al.,\n2015). We consider such an approach, and com-\npare it with a compositional method, in which sen-\ntiment polarity is propagated up the discourse tree.\nMarcu (1997) provides the seminal work on\nautomatic RST parsing, but there has been a re-\ncent spike of interest in this task, with contempo-\nrary approaches employing discriminative learn-\ning (Hernault et al., 2010), rich features (Feng\nand Hirst, 2012), structured prediction (Joty et al.,\n2015), and representation learning (Ji and Eisen-\nstein, 2014; Li et al., 2014). With many strong\nsystems to choose from, we employ the publicly-\navailable DPLP parser (Ji and Eisenstein, 2014),1.\nTo our knowledge, this system currently gives the\nbest F-measure on relation identification, the most\ndifficult subtask of RST parsing. DPLP is a shift-\nreduce parser (Sagae, 2009), and its time complex-\nity is linear in the length of the document.\n2.2 Sentiment analysis\nThere is a huge literature on sentiment analy-\nsis (Pang and Lee, 2008; Liu, 2012), with partic-\nular interest in determining the overall sentiment\npolarity (positive or negative) of a document. Bag-\nof-words models are widely used for this task, as\nthey offer accuracy that is often very competitive\nwith more complex approaches. Given labeled\ndata, supervised learning can be applied to obtain\nsentiment weights for each word. However, the\neffectiveness of supervised sentiment analysis de-\npends on having training data in the same domain\nas the target, and this is not always possible. More-\nover, in social science applications, the desired\nlabels may not correspond directly to positive or\nnegative sentiment, but may focus on other cat-\negories, such as politeness (Danescu-Niculescu-\nMizil et al., 2013), narrative frames (Jurafsky et\nal., 2014), or a multidimensional spectrum of emo-\ntions (Kim et al., 2012). In these cases, labeled\ndocuments may not be available, so users of-\nten employ a simpler method: counting matches\nagainst lists of words associated with each cate-\ngory. Such lists may be built manually from intro-\nspection, as in LIWC (Tausczik and Pennebaker,\n2010) and the General Inquirer (Stone, 1966). Al-\nternatively, they may be induced by bootstrapping\nfrom a seed set of words (Hatzivassiloglou and\nMcKeown, 1997; Taboada et al., 2011). While\nlexicon-based methods may be less accurate than\nsupervised classifiers, they are easier to apply to\n1https://github.com/jiyfeng/DPLP\n1H\n1A\n1B\n1C\n1D 1E\n1F 1G\nFigure 2: Dependency-based discourse tree repre-\nsentation of the discourse in Figure 1\nnew domains and problem settings. Our proposed\napproach can be used in combination with either\nmethod for sentiment analysis, and in principle,\ncould be directly applied to other document-level\ncategories, such as politeness.\n2.3 Datasets\nWe evaluate on two review datasets. In both cases,\nthe goal is to correctly classify the opinion po-\nlarity as positive or negative. The first dataset\nis comprised of 2000 movie reviews, gathered by\nPang and Lee (2004). We perform ten-fold cross-\nvalidation on this data. The second dataset is\nlarger, consisting of 50,000 movie reviews, gath-\nered by Socher et al. (2013), with a predefined\n50/50 split into training and test sets. Documents\nare scored on a 1-10 scale, and we treat scores\n≤ 4 as negative,≥ 7 as positive, and ignore scores\nof 5-6 as neutral — although in principle nothing\nprevents extension of our approaches to more than\ntwo sentiment classes.\n3 Discourse depth reweighting\nOur first approach to incorporating discourse in-\nformation into sentiment analysis is based on\nquantifying the importance of each unit of text in\nterms of its discourse depth. To do this, we em-\nploy the dependency-based discourse tree (DEP-\nDT) formulation from prior work on summariza-\ntion (Hirao et al., 2013). The DEP-DT formal-\nism converts the constituent-like RST tree into\na directed graph over elementary discourse units\n(EDUs), in a process that is a close analogue of the\ntransformation of a headed syntactic constituent\nparse to a syntactic dependency graph (Ku¨bler et\nal., 2009).\nThe DEP-DT representation of the discourse in\nFigure 1 in shown in Figure 2. The graph is con-\nstructed by propagating “head” information up the\nRST tree; if the elementary discourse unit ei is the\nsatellite in a discourse relation headed by ej , then\nthere is an edge from ej to ei. Thus, the “depth”\nof each EDU is the number of times in which it is\nembedded in the satellite of a discourse relation.\nThe exact algorithm for constructing DEP-DTs is\ngiven by Hirao et al. (2013).\nGiven this representation, we construct a simple\nlinear function for weighting the contribution of\nthe EDU at depth di:\nλi = max(0.5, 1− di/6). (1)\nThus, at di = 0, we have λi = 1, and at di ≥ 3, we\nhave λi = 0.5. Now assume each elementary dis-\ncourse unit contributes a prediction ψi = θ>wi,\nwhere wi is the bag-of-words vector, and θ is a\nvector of weights, which may be either learned or\nspecified by a sentiment lexicon. Then the overall\nprediction for a document is given by,\nΨ =\n∑\ni\nλi(θ\n>wi) = θ>(\n∑\ni\nλiwi). (2)\nEvaluation We apply this approach in combi-\nnation with both lexicon-based and classification-\nbased sentiment analysis. We use the lexicon of\nWilson et al. (2005), and set θj = 1 for words\nmarked “positive”, and θj = −1 for words marked\nnegative. For classification-based analysis, we set\nθ equal to the weights obtained by training a logis-\ntic regression classifier, tuning the regularization\ncoefficient on held-out data.\nResults are shown in Table 1. As seen in\nthe comparison between lines B1 and D1, dis-\ncourse depth weighting offers substantial improve-\nments over the bag-of-words approach for lexicon-\nbased sentiment analysis, with raw improvements\nof 4−5%. Given the simplicity of this approach —\nwhich requires only a sentiment lexicon and a dis-\ncourse parser — we strongly recommend the ap-\nplication of discourse depth weighting for lexicon-\nbased sentiment analysis at the document level.\nHowever, the improvements for the classification-\nbased models are considerably smaller, less than\n1% in both datasets.\n4 Rhetorical Recursive Neural Networks\nDiscourse-depth reweighting offers significant im-\nprovements for lexicon-based sentiment analy-\nsis, but the improvements over the more accurate\nclassification-based method are meager. We there-\nfore turn to a data-driven approach for combining\nsentiment analysis with rhetorical structure theory,\nbased on recursive neural networks (Socher et al.,\nPang & Lee Socher et al.\nBaselines\nB1. Lexicon 68.3 74.9\nB2. Classifier 82.4 81.5\nDiscourse depth weighting\nD1. Lexicon 72.6 78.9\nD2. Classifier 82.9 82.0\nRhetorical recursive neural network\nR1. No relations 83.4 85.5\nR2. With relations 84.1 85.6\nTable 1: Sentiment classification accuracies on\ntwo movie review datasets (Pang and Lee, 2004;\nSocher et al., 2013), described in Section 2.3.\n2011). The idea is simple: recursively propagate\nsentiment scores up the RST tree, until the root of\nthe document is reached. For nucleus-satellite dis-\ncourse relations, we have:\nΨi = tanh(K\n(ri)\nn Ψn(i) +K\n(ri)\ns Ψs(i)), (3)\nwhere i indexes a discourse unit composed from\nrelation ri, n(i) indicates its nucleus, and s(i) in-\ndicates its satellite. Returning to the example in\nFigure 1, the sentiment score for the discourse unit\nobtained by combining 1B and 1C is obtained\nfrom tanh(K(elaboration)n Ψ1B + K\n(elaboration)\ns Ψ1C).\nSimilarly, for multinuclear relations, we have,\nΨi = tanh(\n∑\nj∈n(i)\nK(ri)n Ψj). (4)\nIn the base case, each elementary discourse unit’s\nsentiment is constructed from its bag-of-words,\nΨi = θ\n>wi. Because the structure of each doc-\nument is different, the network architecture varies\nin each example; nonetheless, the parameters can\nbe reused across all instances.\nThis approach, which we call a Rhetorical Re-\ncursive Neural Network (R2N2), is reminiscent of\nthe compositional model proposed by Socher et al.\n(2013), where composition is over the constituents\nof the syntactic parse of a sentence, rather than\nthe units of a discourse. However, a crucial differ-\nence is that in R2N2s, the elements Ψ and K are\nscalars: we do not attempt to learn a latent dis-\ntributed representation of the sub-document units.\nThis is because discourse units typically comprise\nmultiple words, so that accurate analysis of the\nsentiment for elementary discourse units is not so\ndifficult as accurate analysis of individual words.\nThe scores for individual discourse units can be\ncomputed from a bag-of-words classifier, or, in fu-\nture work, from a more complex model such as a\nrecursive or recurrent neural network.\nWhile this neural network structure captures\nthe idea of compositionality over the RST tree,\nthe most deeply embedded discourse units can be\nheavily down-weighted by the recursive compo-\nsition (assuming Ks < Kn): in the most ex-\ntreme case of a right-branching or left-branching\nstructure, the recursive operator may be appliedN\ntimes to the most deeply embedded EDU. In con-\ntrast, discourse depth reweighting applies a uni-\nform weight of 0.5 to all discourse units with depth\n≥ 3. In the spirit of this approach, we add an addi-\ntional component to the network architecture, cap-\nturing the bag-of-words for the entire document.\nThus, at the root node we have:\nΨdoc = γθ\n>(\n∑\ni\nwi) + Ψrst-root, (5)\nwith Ψrst-root defined recursively from Equations 3\nand 4, θ indicating the vector of per-word weights,\nand the scalar γ controlling the tradeoff between\nthese two components.\nLearning R2N2 is trained by backpropagating\nfrom a hinge loss objective; assuming yt ∈\n{−1, 1} for each document t, we have the loss\nLt = (1 − ytΨdoc,t)+. From this loss, we\nuse backpropagation through structure to obtain\ngradients on the parameters (Goller and Kuch-\nler, 1996). Training is performed using stochas-\ntic gradient descent. For simplicity, we fol-\nlow Zirn et al. (2011) and focus on the dis-\ntinction between contrastive and non-contrastive\nrelations. The set of contrastive relations in-\ncludes CONTRAST, COMPARISON, ANTITHE-\nSIS, ANTITHESIS-E, CONSEQUENCE-S, CON-\nCESSION, and PROBLEM-SOLUTION.\nEvaluation Results for this approach are shown\nin lines R1 and R2 of Table 1. Even without dis-\ntinguishing between discourse relations, we get an\nimprovement of more than 3% accuracy on the\nStanford data, and 0.5% on the smaller Pang &\nLee dataset. Adding sensitivity to discourse rela-\ntions (distinguishingK(r) for contrastive and non-\ncontrastive relations) offers further improvements\non the Pang & Lee data, outperforming the base-\nline classifier (D2) by 1.3%.\nThe accuracy of discourse relation detection is\nonly 60% for even the best systems (Ji and Eisen-\nstein, 2014), which may help to explain why re-\nlations do not offer a more substantial boost. An\nanonymous reviewer recommended evaluating on\ngold RST parse trees to determine the extent to\nwhich improvements in RST parsing might trans-\nfer to downstream document analysis. Such an\nevaluation would seem to require a large corpus of\ntexts with both gold RST parse trees and sentiment\npolarity labels; the SFU Review Corpus (Taboada,\n2008) of 30 review texts offers a starting point, but\nis probably too small to train a competitive senti-\nment analysis system.\n5 Related Work\nSection 2 mentions some especially relevant prior\nwork. Other efforts to incorporate RST into\nsentiment analysis have often focused on intra-\nsentential discourse relations (Heerschop et al.,\n2011; Zhou et al., 2011; Chenlo et al., 2014),\nrather than relations over the entire document.\nWang et al. (2012) address sentiment analysis in\nChinese. Lacking a discourse parser, they focus\non explicit connectives, using a strategy that is re-\nlated to our discourse depth reweighting. Wang\nand Wu (2013) use manually-annotated discourse\nparses in combination with a sentiment lexicon,\nwhich is automatically updated based on the dis-\ncourse structure. Zirn et al. (2011) use an RST\nparser in a Markov Logic Network, with the goal\nof making polarity predictions at the sub-sentence\nlevel, rather than improving document-level pre-\ndiction. None of the prior work considers the sort\nof recurrent compositional model presented here.\nAn alternative to RST is to incorporate “shal-\nlow” discourse structure, such as the relations\nfrom the Penn Discourse Treebank (PDTB).\nPDTB relations were shown to improve sentence-\nlevel sentiment analysis by Somasundaran et al.\n(2009), and were incorporated in a model of sen-\ntiment flow by Wachsmuth et al. (2014). PDTB\nrelations are often signaled with explicit discourse\nconnectives, and these may be used as a fea-\nture (Trivedi and Eisenstein, 2013; Lazaridou et\nal., 2013) or as posterior constraints (Yang and\nCardie, 2014). This prior work on discourse rela-\ntions within sentences and between adjacent sen-\ntences can be viewed as complementary to our fo-\ncus on higher-level discourse relations across the\nentire document.\nThere are unfortunately few possibilities for\ndirect comparison of our approach against prior\nwork. Heerschop et al. (2011) and Wachsmuth et\nal. (2014) also employ the Pang and Lee (2004)\ndataset, but neither of their results are directly\ncomparable: Heerschop et al. (2011) exclude doc-\numents that SPADE fails to parse, and Wachsmuth\net al. (2014) evaluates only on individual sentences\nrather than entire documents. The only possi-\nble direct comparison is with very recent work\nfrom Hogenboom et al. (2015), who employ a\nweighting scheme that is similar to the approach\ndescribed in Section 3. They evaluate on the Pang\nand Lee data, and consider only lexicon-based\nsentiment analysis, obtaining document-level ac-\ncuracies between 65% (for the baseline) and 72%\n(for their best discourse-augmented system). Ta-\nble 1 shows that fully supervised methods give\nmuch stronger performance on this dataset, with\naccuracies more than 10% higher.\n6 Conclusion\nSentiment polarity analysis has typically relied\non a “preponderance of evidence” strategy, hop-\ning that the words or sentences representing the\noverall polarity will outweigh those representing\ncounterpoints or rhetorical concessions. How-\never, with the availability of off-the-shelf RST dis-\ncourse parsers, it is now easy to include document-\nlevel structure in sentiment analysis. We show\nthat a simple reweighting approach offers robust\nadvantages in lexicon-based sentiment analysis,\nand that a recursive neural network can substan-\ntially outperform a bag-of-words classifier. Future\nwork will focus on combining models of discourse\nstructure with richer models at the sentence level.\nAcknowledgments Thanks to the anonymous review-\ners for their helpful suggestions on how to improve the paper.\nThe following members of the Georgia Tech Computational\nLinguistics Laboratory offered feedback throughout the re-\nsearch process: Naman Goyal, Vinodh Krishan, Umashanthi\nPavalanathan, Ana Smith, Yijie Wang, and Yi Yang. Several\nclass projects in Georgia Tech CS 4650/7650 took alternative\napproaches to the application of discourse parsing to senti-\nment analysis, which was informative to this work; thanks\nparticularly to Julia Cochran, Rohit Pathak, Pavan Kumar\nRamnath, and Bharadwaj Tanikella. This research was sup-\nported by a Google Faculty Research Award, by the National\nInstitutes of Health under award number R01GM112697-01,\nand by the Air Force Office of Scientific Research. The con-\ntent is solely the responsibility of the authors and does not\nnecessarily represent the official views of these sponsors.\nReferences\nJose´ M Chenlo, Alexander Hogenboom, and David E\nLosada. 2014. Rhetorical structure theory for po-\nlarity estimation: An experimental study. Data &\nKnowledge Engineering.\nCristian Danescu-Niculescu-Mizil, Moritz Sudhof,\nDan Jurafsky, Jure Leskovec, and Christopher Potts.\n2013. A computational approach to politeness with\napplication to social factors. In Proceedings of the\nAssociation for Computational Linguistics (ACL),\npages 250–259, Sophia, Bulgaria.\nRonen Feldman. 2013. Techniques and applica-\ntions for sentiment analysis. Communications of the\nACM, 56(4):82–89.\nVanessa Wei Feng and Graeme Hirst. 2012. Text-level\nDiscourse Parsing with Rich Linguistic Features. In\nProceedings of the Association for Computational\nLinguistics (ACL), Jeju, Korea.\nShima Gerani, Yashar Mehdad, Giuseppe Carenini,\nRaymond T Ng, and Bita Nejat. 2014. Abstractive\nsummarization of product reviews using discourse\nstructure. In Proceedings of the Association for\nComputational Linguistics (ACL), Baltimore, MD.\nChristoph Goller and Andreas Kuchler. 1996. Learn-\ning task-dependent distributed representations by\nbackpropagation through structure. In Neural Net-\nworks, IEEE International Conference on, pages\n347–352. IEEE.\nVasileios Hatzivassiloglou and Kathleen R. McKeown.\n1997. Predicting the semantic orientation of adjec-\ntives. In Proceedings of the Association for Compu-\ntational Linguistics (ACL), pages 174–181, Madrid,\nSpain.\nBas Heerschop, Frank Goossen, Alexander Hogen-\nboom, Flavius Frasincar, Uzay Kaymak, and Fran-\nciska de Jong. 2011. Polarity analysis of texts using\ndiscourse structure. In Proceedings of the 20th ACM\ninternational conference on Information and knowl-\nedge management, pages 1061–1070. ACM.\nHugo Hernault, Helmut Prendinger, David A. duVerle,\nand Mitsuru Ishizuka. 2010. HILDA: A Discourse\nParser Using Support Vector Machine Classification.\nDialogue and Discourse, 1(3):1–33.\nTsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,\nNorihito Yasuda, and Masaaki Nagata. 2013.\nSingle-document summarization as a tree knapsack\nproblem. In Proceedings of Empirical Methods\nfor Natural Language Processing (EMNLP), pages\n1515–1520.\nAlexander Hogenboom, Flavius Frasincar, Franciska\nde Jong, and Uzay Kaymak. 2015. Using rhetorical\nstructure in sentiment analysis. Communications of\nthe ACM, 58(7):69–77.\nYangfeng Ji and Jacob Eisenstein. 2014. Represen-\ntation learning for text-level discourse parsing. In\nProceedings of the Association for Computational\nLinguistics (ACL), Baltimore, MD.\nShafiq Joty, Giuseppe Carenini, and Raymond Ng.\n2015. CODRA: A novel discriminative framework\nfor rhetorical analysis. Computational Linguistics,\n41(3).\nDan Jurafsky, Victor Chahuneau, Bryan R Routledge,\nand Noah A Smith. 2014. Narrative framing of con-\nsumer sentiment in online restaurant reviews. First\nMonday, 19(4).\nSuin Kim, JinYeong Bak, and Alice Haeyun Oh. 2012.\nDo you feel what i feel? social aspects of emotions\nin twitter conversations. In Proceedings of the In-\nternational Conference on Web and Social Media\n(ICWSM), Menlo Park, California. AAAI Publica-\ntions.\nSandra Ku¨bler, Ryan McDonald, and Joakim Nivre.\n2009. Dependency parsing. Synthesis Lectures on\nHuman Language Technologies, 1(1):1–127.\nAngeliki Lazaridou, Ivan Titov, and Caroline\nSporleder. 2013. A bayesian model for joint\nunsupervised induction of sentiment, aspect and\ndiscourse representations. In Proceedings of the\nAssociation for Computational Linguistics (ACL),\npages 1630–1639, Sophia, Bulgaria.\nJiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recur-\nsive deep models for discourse parsing. In Proceed-\nings of Empirical Methods for Natural Language\nProcessing (EMNLP).\nBing Liu. 2012. Sentiment analysis and opinion min-\ning. Synthesis Lectures on Human Language Tech-\nnologies, 5(1):1–167.\nWilliam Mann. 1984. Discourse structures for text\ngeneration. In Proceedings of the 10th International\nConference on Computational Linguistics and 22nd\nannual meeting on Association for Computational\nLinguistics, pages 367–375. Association for Com-\nputational Linguistics.\nDaniel Marcu. 1997. The rhetorical parsing of natu-\nral language texts. In Proceedings of the European\nChapter of the Association for Computational Lin-\nguistics (EACL), pages 96–103.\nDaniel Marcu. 1999. The automatic construction of\nlarge-scale corpora for summarization research. In\nProceedings of the 22nd annual international ACM\nSIGIR conference on Research and development in\ninformation retrieval, pages 137–144. ACM.\nBo Pang and Lillian Lee. 2004. A sentimental edu-\ncation: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In Proceed-\nings of the Association for Computational Linguis-\ntics (ACL), pages 271–278.\nBo Pang and Lillian Lee. 2008. Opinion mining and\nsentiment analysis. Foundations and trends in infor-\nmation retrieval, 2(1-2):1–135.\nKenji Sagae. 2009. Analysis of Discourse Struc-\nture with Syntactic Dependencies and Data-Driven\nShift-Reduce Parsing. In Proceedings of the 11th\nInternational Conference on Parsing Technologies\n(IWPT’09), pages 81–84, Paris, France, October.\nAssociation for Computational Linguistics.\nPaul Smolensky. 1990. Tensor product variable bind-\ning and the representation of symbolic structures\nin connectionist systems. Artificial intelligence,\n46(1):159–216.\nRichard Socher, Cliff C. Lin, Andrew Y. Ng, and\nChristopher D. Manning. 2011. Parsing Natural\nScenes and Natural Language with Recursive Neu-\nral Networks. In Proceedings of the International\nConference on Machine Learning (ICML), Seattle,\nWA.\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of Empirical Methods for\nNatural Language Processing (EMNLP).\nSwapna Somasundaran, Galileo Namata, Janyce\nWiebe, and Lise Getoor. 2009. Supervised and\nunsupervised methods in employing discourse rela-\ntions for improving opinion polarity classification.\nIn Proceedings of Empirical Methods for Natural\nLanguage Processing (EMNLP), Singapore.\nRadu Soricut and Daniel Marcu. 2003. Sentence\nlevel discourse parsing using syntactic and lexical\ninformation. In Proceedings of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL), pages 149–156.\nPhilip J. Stone. 1966. The General Inquirer: A\nComputer Approach to Content Analysis. The MIT\nPress.\nMaite Taboada, Julian Brooke, Milan Tofiloski, Kim-\nberly Voll, and Manfred Stede. 2011. Lexicon-\nbased methods for sentiment analysis. Computa-\ntional linguistics, 37(2):267–307.\nMaite Taboada. 2008. SFU review cor-\npus. http://www.sfu.ca/˜mtaboada/\nresearch/SFU_Review_Corpus.html.\nYla R Tausczik and James W Pennebaker. 2010. The\npsychological meaning of words: LIWC and com-\nputerized text analysis methods. Journal of Lan-\nguage and Social Psychology, 29(1):24–54.\nRakshit Trivedi and Jacob Eisenstein. 2013. Discourse\nconnectors for latent subjectivity in sentiment anal-\nysis. In Proceedings of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL), pages 808–813, Stroudsburg, Pennsylva-\nnia. Association for Computational Linguistics.\nVinı´cius Rodrigues Uzeˆda, Thiago Alexan-\ndre Salgueiro Pardo, and Maria Das Grac¸as Volpe\nNunes. 2010. A comprehensive comparative eval-\nuation of rst-based summarization methods. ACM\nTransactions on Speech and Language Processing\n(TSLP), 6(4):4.\nKimberly Voll and Maite Taboada. 2007. Not all\nwords are created equal: Extracting semantic orien-\ntation as a function of adjective relevance. In Pro-\nceedings of Australian Conference on Artificial In-\ntelligence.\nHenning Wachsmuth, Martin Trenkmann, Benno Stein,\nand Gregor Engels. 2014. Modeling review argu-\nmentation for robust sentiment analysis. In Proceed-\nings of the International Conference on Computa-\ntional Linguistics (COLING).\nFei Wang and Yunfang Wu. 2013. Exploiting hierar-\nchical discourse structure for review sentiment anal-\nysis. In Proceedings of the Conference on Asian\nLanguage Processing (IALP), pages 121–124.\nFei Wang, Yunfang Wu, and Likun Qiu. 2012. Ex-\nploiting discourse relations for sentiment analysis.\nIn Proceedings of the International Conference on\nComputational Linguistics (COLING), pages 1311–\n1320, Mumbai, India.\nTheresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005. Recognizing contextual polarity in phrase-\nlevel sentiment analysis. In Proceedings of Em-\npirical Methods for Natural Language Processing\n(EMNLP), pages 347–354.\nBishan Yang and Claire Cardie. 2014. Context-aware\nlearning for sentence-level sentiment analysis with\nposterior regularization. In Proceedings of the As-\nsociation for Computational Linguistics (ACL), Bal-\ntimore, MD.\nYasuhisa Yoshida, Jun Suzuki, Tsutomu Hirao, and\nMasaaki Nagata. 2014. Dependency-based Dis-\ncourse Parser for Single-Document Summarization.\nIn Proceedings of Empirical Methods for Natural\nLanguage Processing (EMNLP).\nLanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei,\nand Kam-Fai Wong. 2011. Unsupervised dis-\ncovery of discourse relations for eliminating intra-\nsentence polarity ambiguities. In Proceedings of\nEmpirical Methods for Natural Language Process-\ning (EMNLP), pages 162–171.\nCa¨cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,\nand Michael Strube. 2011. Fine-grained sentiment\nanalysis with structural features. In Proceedings of\nthe International Joint Conference on Natural Lan-\nguage Processing (IJCNLP), pages 336–344, Chi-\nang Mai, Thailand.\n",
      "id": 18091911,
      "identifiers": [
        {
          "identifier": "29568473",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.18653/v1/d15-1263",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1509.01599",
          "type": "OAI_ID"
        },
        {
          "identifier": "103415870",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.697.234",
          "type": "OAI_ID"
        },
        {
          "identifier": "210378713",
          "type": "CORE_ID"
        },
        {
          "identifier": "1509.01599",
          "type": "ARXIV_ID"
        }
      ],
      "title": "Better Document-level Sentiment Analysis from RST Discourse Parsing",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:citeseerx.psu:10.1.1.697.234",
        "oai:arxiv.org:1509.01599"
      ],
      "publishedDate": "2015-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://aclweb.org/anthology/D/D15/D15-1263.pdf",
        "http://arxiv.org/abs/1509.01599"
      ],
      "updatedDate": "2021-08-03T15:53:17",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1509.01599"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/18091911"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1808.07733",
      "authors": [
        {
          "name": "Iyyer, Mohit"
        },
        {
          "name": "Jyothi, Preethi"
        },
        {
          "name": "Krishna, Kalpesh"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Ellen",
        "Kalpesh"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/470621779"
      ],
      "createdDate": "2018-09-08T08:15:02",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "We analyze the performance of different sentiment classification models on\nsyntactically complex inputs like A-but-B sentences. The first contribution of\nthis analysis addresses reproducible research: to meaningfully compare\ndifferent models, their accuracies must be averaged over far more random seeds\nthan what has traditionally been reported. With proper averaging in place, we\nnotice that the distillation model described in arXiv:1603.06318v4 [cs.LG],\nwhich incorporates explicit logic rules for sentiment classification, is\nineffective. In contrast, using contextualized ELMo embeddings\n(arXiv:1802.05365v2 [cs.CL]) instead of logic rules yields significantly better\nperformance. Additionally, we provide analysis and visualizations that\ndemonstrate ELMo's ability to implicitly learn logic rules. Finally, a\ncrowdsourced analysis reveals how ELMo outperforms baseline models even on\nsentences with ambiguous sentiment labels.Comment: EMNLP 2018 Camera Read",
      "documentType": "research",
      "doi": "10.18653/v1/d18-1505",
      "downloadUrl": "http://arxiv.org/abs/1808.07733",
      "fieldOfStudy": null,
      "fullText": "Revisiting the Importance of Encoding Logic Rules\nin Sentiment Classification\nKalpesh Krishna♠♣ Preethi Jyothi♠\nIndian Institute of Technology, Bombay♠\nUniversity of Massachusetts, Amherst♣\n{kalpesh,miyyer}@cs.umass.edu\npjyothi@cse.iitb.ac.in\nMohit Iyyer♣\nAbstract\nWe analyze the performance of different sen-\ntiment classification models on syntactically-\ncomplex inputs like A-but-B sentences. The\nfirst contribution of this analysis addresses re-\nproducible research: to meaningfully compare\ndifferent models, their accuracies must be av-\neraged over far more random seeds than what\nhas traditionally been reported. With proper\naveraging in place, we notice that the distil-\nlation model described in Hu et al. (2016),\nwhich incorporates explicit logic rules for sen-\ntiment classification, is ineffective. In contrast,\nusing contextualized ELMo embeddings (Pe-\nters et al., 2018a) instead of logic rules yields\nsignificantly better performance. Additionally,\nwe provide analysis and visualizations that\ndemonstrate ELMo’s ability to implicitly learn\nlogic rules. Finally, a crowdsourced analysis\nreveals how ELMo outperforms baseline mod-\nels even on sentences with ambiguous senti-\nment labels.\n1 Introduction\nIn this paper, we explore the effectiveness of meth-\nods designed to improve sentiment classification\n(positive vs. negative) of sentences that con-\ntain complex syntactic structures. While simple\nbag-of-words or lexicon-based methods (Pang and\nLee, 2005; Wang and Manning, 2012; Iyyer et al.,\n2015) achieve good performance on this task, they\nare unequipped to deal with syntactic structures\nthat affect sentiment, such as contrastive conjunc-\ntions (i.e., sentences of the form “A-but-B”) or\nnegations. Neural models that explicitly encode\nword order (Kim, 2014), syntax (Socher et al.,\n2013; Tai et al., 2015) and semantic features (Li\net al., 2017) have been proposed with the aim\nof improving performance on these more compli-\ncated sentences. Recently, Hu et al. (2016) in-\ncorporate logical rules into a neural model and\nshow that these rules increase the model’s accu-\nracy on sentences containing contrastive conjunc-\ntions, while Peters et al. (2018a) demonstrate in-\ncreased overall accuracy on sentiment analysis by\ninitializing a model with representations from a\nlanguage model trained on millions of sentences.\nIn this work, we carry out an in-depth study\nof the effectiveness of the techniques in Hu et al.\n(2016) and Peters et al. (2018a) for sentiment clas-\nsification of complex sentences. Part of our con-\ntribution is to identify an important gap in the\nmethodology used in Hu et al. (2016) for perfor-\nmance measurement, which is addressed by av-\neraging the experiments over several executions.\nWith the averaging in place, we obtain three key\nfindings: (1) the improvements in Hu et al. (2016)\ncan almost entirely be attributed to just one of\ntheir two proposed mechanisms and are also less\npronounced than previously reported; (2) contex-\ntualized word embeddings (Peters et al., 2018a)\nincorporate the “A-but-B” rules more effectively\nwithout explicitly programming for them; and (3)\nan analysis using crowdsourcing reveals a big-\nger picture where the errors in the automated sys-\ntems have a striking correlation with the inherent\nsentiment-ambiguity in the data.\n2 Logic Rules in Sentiment Classification\nHere we briefly review background from Hu et al.\n(2016) to provide a foundation for our reanalysis\nin the next section. We focus on a logic rule for\nsentences containing an “A-but-B” structure (the\nonly rule for which Hu et al. (2016) provide exper-\nimental results). Intuitively, the logic rule for such\nsentences is that the sentiment associated with the\nwhole sentence should be the same as the senti-\nment associated with phrase “B”.1\n1The rule is vacuously true if the sentence does not have\nthis structure.\nar\nX\niv\n:1\n80\n8.\n07\n73\n3v\n1 \n [c\ns.C\nL]\n  2\n3 A\nug\n 20\n18\nMore formally, let pθ(y|x) denote the proba-\nbility assigned to the label y ∈ {+,−} for an\ninput x by the baseline model using parameters\nθ. A logic rule is (softly) encoded as a variable\nrθ(x, y) ∈ [0, 1] indicating how well labeling x\nwith y satisfies the rule. For the case of A-but-B\nsentences, rθ(x, y) = pθ(y|B) if x has the struc-\nture A-but-B (and 1 otherwise). Next, we discuss\nthe two techniques from Hu et al. (2016) for in-\ncorporating rules into models: projection, which\ndirectly alters a trained model, and distillation,\nwhich progressively adjusts the loss function dur-\ning training.\nProjection. The first technique is to project a\ntrained model into a rule-regularized subspace, in\na fashion similar to Ganchev et al. (2010). More\nprecisely, a given model pθ is projected to a model\nqθ defined by the optimum value of q in the fol-\nlowing optimization problem:2\nmin\nq,ξ≥0\nKL(q(X,Y )||pθ(X,Y )) + C\n∑\nx∈X\nξx\ns.t. (1− Ey←q(·|x)[rθ(x, y)]) ≤ ξx\nHere q(X,Y ) denotes the distribution of (x, y)\nwhen x is drawn uniformly from the set X and\ny is drawn according to q(·|x).\nIterative Rule Knowledge Distillation. The\nsecond technique is to transfer the domain knowl-\nedge encoded in the logic rules into a neural\nnetwork’s parameters. Following Hinton et al.\n(2015), a “student” model pθ can learn from\nthe “teacher” model qθ, by using a loss function\npiH(pθ, Ptrue)+ (1− pi)H(pθ, qθ) during training,\nwhere Ptrue denotes the distribution implied by\nthe ground truth, H(·, ·) denotes the cross-entropy\nfunction, and pi is a hyperparameter. Hu et al.\n(2016) computes qθ after every gradient update\nby projecting the current pθ, as described above.\nNote that both mechanisms can be combined: Af-\nter fully training pθ using the iterative distillation\nprocess above, the projection step can be applied\none more time to obtain qθ which is then used as\nthe trained model.\nDataset. All of our experiments (as well as those\nin Hu et al. (2016)) use the SST2 dataset, a\n2The formulation in Hu et al. (2016) includes another hy-\nperparameter λ per rule, to control its relative importance;\nwhen there is only one rule, as in our case, this parameter can\nbe absorbed into C.\nbinarized subset of the popular Stanford Senti-\nment Treebank (SST) (Socher et al., 2013). The\ndataset includes phrase-level labels in addition\nto sentence-level labels (see Table 1 for detailed\nstatistics); following Hu et al. (2016), we use both\ntypes of labels for the comparisons in Section 3.2.\nIn all other experiments, we use only sentence-\nlevel labels, and our baseline model for all exper-\niments is the CNN architecture from Kim (2014).\n3 A Reanalysis\nIn this section we reanalyze the effectiveness of\nthe techniques of Hu et al. (2016) and find that\nmost of the performance gain is due to projection\nand not knowledge distillation. The discrepancy\nwith the original analysis can be attributed to the\nrelatively small dataset and the resulting variance\nacross random initializations. We start by analyz-\ning the baseline CNN by Kim (2014) to point out\nthe need for an averaged analysis.\n0.0\n0.2\n0.4\n0.6\n83.47 85.64 86.16 86.49 87.20\nAc\ncu\nra\ncy\n (%\n)\nNumber of epochs of training\nFigure 1: Variation in models trained on SST-2 (sentence-\nonly). Accuracies of 100 randomly initialized models are\nplotted against the number of epochs of training (in gray),\nalong with their average accuracies (in red, with 95% confi-\ndence interval error bars). The inset density plot shows the\ndistribution of accuracies when trained with early stopping.\n3.1 Importance of Averaging\nWe run the baseline CNN by Kim (2014) across\n100 random seeds, training on sentence-level la-\nNumber of Phrases Train Dev Test\nInstances 76961 6920 872 1821\nA-but-B 3.5% 11.1% 11.5% 11.5%\nNegations 2.0% 17.5% 18.3% 17.2%\nDiscourse 5.0% 24.6% 26.0% 24.5%\nTable 1: Statistics of SST2 dataset. Here “Discourse” in-\ncludes both A-but-B and negation sentences. The mean length\nof sentences is in terms of the word count.\nReported Test Accuracy  \n(Hu et al., 2016) Averaged Test Accuracy  Averaged A-but-B accuracy  \nno-distill distill no-distill distill no-distill distill\nno-project 87.2 88.8 87.66 87.97 80.25 82.17\nproject 87.9 89.3 88.73 88.77 89.56 89.13\n+1.6\n+1.4\n+0.7 +0.5\n+0.29\n+0.04\n+1.07 +0.80\n+1.92\n-0.43\n+9.31 +6.96\nFigure 2: Comparison of the accuracy improvements reported in Hu et al. (2016) and those obtained by averaging over 100\nrandom seeds. The last two columns show the (averaged) accuracy improvements for A-but-B style sentences. All models use\nthe publicly available implementation of Hu et al. (2016) trained on phrase-level SST2 data.\nbels. We observe a large amount of variation from\nrun-to-run, which is unsurprising given the small\ndataset size. The inset density plot in Figure 1\nshows the range of accuracies (83.47 to 87.20)\nalong with 25, 50 and 75 percentiles.3 The figure\nalso shows how the variance persists even after the\naverage converges: the accuracies of 100 models\ntrained for 20 epochs each are plotted in gray, and\ntheir average is shown in red.\nWe conclude that, to be reproducible, only av-\neraged accuracies should be reported in this task\nand dataset. This mirrors the conclusion from a\ndetailed analysis by Reimers and Gurevych (2017)\nin the context of named entity recognition.\n3.2 Performance of Hu et al. (2016)\nWe carry out an averaged analysis of the publicly\navailable implementation4 of Hu et al. (2016).\nOur analysis reveals that the reported performance\nof their two mechanisms (projection and distil-\nlation) is in fact affected by the high variability\nacross random seeds. Our more robust averaged\nanalysis yields a somewhat different conclusion of\ntheir effectiveness.\nIn Figure 2, the first two columns show the re-\nported accuracies in Hu et al. (2016) for models\ntrained with and without distillation (correspond-\ning to using values pi = 1 and pi = 0.95t in the\ntth epoch, respectively). The two rows show the\nresults for models with and without a final projec-\ntion into the rule-regularized space. We keep our\nhyper-parameters identical to Hu et al. (2016).5\nThe baseline system (no-project, no-distill) is\nidentical to the system of Kim (2014). All the sys-\ntems are trained on the phrase-level SST2 dataset\n3We use early stopping based on validation performance\nfor all models in the density plot.\n4https://github.com/ZhitingHu/logicnn/\n5In particular, C = 6 for projection.\nwith early stopping on the development set. The\nnumber inside each arrow indicates the improve-\nment in accuracy by adding either the projection\nor the distillation component to the training al-\ngorithm. Note that the reported figures suggest\nthat while both components help in improving ac-\ncuracy, the distillation component is much more\nhelpful than the projection component.\nThe next two columns, which show the re-\nsults of repeating the above analysis after averag-\ning over 100 random seeds, contradict this claim.\nThe averaged figures show lower overall accuracy\nincreases, and, more importantly, they attribute\nthese improvements almost entirely to the projec-\ntion component rather than the distillation com-\nponent. To confirm this result, we repeat our av-\neraged analysis restricted to only “A-but-B” sen-\ntences targeted by the rule (shown in the last two\ncolumns). We again observe that the effect of pro-\njection is pronounced, while distillation offers lit-\ntle or no advantage in comparison.\n4 Contextualized Word Embeddings\nTraditional context-independent word embed-\ndings like word2vec (Mikolov et al., 2013) or\nGloVe (Pennington et al., 2014) are fixed vec-\ntors for every word in the vocabulary. In contrast,\ncontextualized embeddings are dynamic represen-\ntations, dependent on the current context of the\nword. We hypothesize that contextualized word\nembeddings might inherently capture these logic\nrules due to increasing the effective context size\nfor the CNN layer in Kim (2014). Following the\nrecent success of ELMo (Peters et al., 2018a) in\nsentiment analysis, we utilize the TensorFlow Hub\nimplementation of ELMo6 and feed these contex-\ntualized embeddings into our CNN model. We\n6https://tfhub.dev/google/elmo/1\nfine-tune the ELMo LSTM weights along with the\nCNN weights on the downstream CNN task. As in\nSection 3, we check performance with and without\nthe final projection into the rule-regularized space.\nWe present our results in Table 2.\nSwitching to ELMo word embeddings improves\nperformance by 2.9 percentage points on an aver-\nage, corresponding to about 53 test sentences. Of\nthese, about 32 sentences (60% of the improve-\nment) correspond to A-but-B and negation style\nsentences, which is substantial when considering\nthat only 24.5% of test sentences include these dis-\ncourse relations (Table 1). As further evidence that\nELMo helps on these specific constructions, the\nnon-ELMo baseline model (no-project, no-distill)\ngets 255 sentences wrong in the test corpus on av-\nerage, only 89 (34.8%) of which are A-but-B style\nor negations.\nStatistical Significance: Using a two-sided\nKolmogorov-Smirnov statistic (Massey Jr, 1951)\nwith α = 0.001 for the results in Table 2, we find\nthat ELMo and projection each yield statistically\nsignificant improvements, but distillation does not.\nAlso, with ELMo, projection is not significant.\nSpecific comparisons have been added in the Ap-\npendix, in Table A3.\nKL Divergence Analysis: We observe no sig-\nnificant gains by projecting a trained ELMo model\ninto an A-but-B rule-regularized space, unlike the\nother models. We confirm that ELMo’s predic-\ntions are much closer to the A-but-B rule’s man-\nifold than those of the other models by computing\nKL(qθ||pθ) where pθ and qθ are the original and\nprojected distributions: Averaged across all A-but-\nB sentences and 100 seeds, this gives 0.27, 0.26\nand 0.13 for the Kim (2014), Hu et al. (2016)\nwith distillation and ELMo systems respectively.\nIntra-sentence Similarity: To understand the\ninformation captured by ELMo embeddings for\nA-but-B sentences, we measure the cosine simi-\nlarity between the word vectors of every pair of\nwords within the A-but-B sentence (Peters et al.,\n2018b). We compare the intra-sentence similar-\nity for fine-tuned word2vec embeddings (base-\nline), ELMo embeddings without fine-tuning and\nfinally fine-tuned ELMo embeddings in Figure 3.\nIn the fine-tuned ELMo embeddings, we notice\nthe words within the A and within the B part of\nthe A-but-B sentence share the same part of the\nvector space. This pattern is less visible in the\nModel Test but but or neg\nno-distill no-project 85.98 78.69 80.13\nno-distill project 86.54 83.40 -\ndistill 7 no-project 86.11 79.04 -\ndistill project 86.62 83.32 -\nELMo no-project 88.89 86.51 87.24\nELMo project 88.96 87.20 -\nTable 2: Average performance (across 100 seeds) of ELMo\non the SST2 task. We show performance on A-but-B sen-\ntences (“but”), negations (“neg”).\nELMo embeddings without fine-tuning and absent\nin the word2vec embeddings. This observation\nis indicative of ELMo’s ability to learn specific\nrules for A-but-B sentences in sentiment classifica-\ntion. More intra-sentence similarity heatmaps for\nA-but-B sentences are in Figure A1.\n5 Crowdsourced Experiments\nWe conduct a crowdsourced analysis that reveals\nthat SST2 data has significant levels of ambiguity\neven for human labelers. We discover that ELMo’s\nperformance improvements over the baseline are\nrobust across varying levels of ambiguity, whereas\nthe advantage of Hu et al. (2016) is reversed in\nsentences of low ambiguity (restricting to A-but-B\nstyle sentences).\nOur crowdsourced experiment was conducted\non Figure Eight.8 Nine workers scored the senti-\nment of each A-but-B and negation sentence in the\ntest SST2 split as 0 (negative), 0.5 (neutral) or 1\n(positive). (SST originally had three crowdwork-\ners choose a sentiment rating from 1 to 25 for ev-\nery phrase.) More details regarding the crowd ex-\nperiment’s parameters have been provided in Ap-\npendix A.\nWe average the scores across all users for each\nsentence. Sentences with a score in the range\n(x, 1] are marked as positive (where x ∈ [0.5, 1)),\nsentences in [0, 1 − x) marked as negative, and\nsentences in [1 − x, x] are marked as neutral.\nFor instance, “flat , but with a revelatory perfor-\nmance by michelle williams” (score=0.56) is neu-\ntral when x = 0.6.9 We present statistics of\nour dataset10 in Table 3. Inter-annotator agree-\n7Trained on sentences and not phrase-level labels for a fair\ncomparison with baseline and ELMo, unlike Section 3.2.\n8 https://www.figure-eight.com/\n9More examples of neutral sentences have been provided\nin the Appendix in Table A1, as well as a few “flipped” sen-\ntences receiving an average score opposite to their SST2 label\n(Table A2).\n10The dataset along with source code can be found in\nth\ner\ne\nar\ne\nslo\nw\nan\nd\nre\npe\ntit\niv\ne\npa\nrts\n, bu\nt\nit ha\ns\nju\nst\nen\nou\ngh\nsp\nice\nto ke\nep\nit in\nte\nre\nst\nin\ng\nthere\nare\nslow\nand\nrepetitive\nparts\n,\nbut\nit\nhas\njust\nenough\nspice\nto\nkeep\nit\ninteresting 0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nth\ner\ne\nar\ne\nslo\nw\nan\nd\nre\npe\ntit\niv\ne\npa\nrts\n, bu\nt\nit ha\ns\nju\nst\nen\nou\ngh\nsp\nice\nto ke\nep\nit in\nte\nre\nst\nin\ng\nthere\nare\nslow\nand\nrepetitive\nparts\n,\nbut\nit\nhas\njust\nenough\nspice\nto\nkeep\nit\ninteresting\n0.2\n0.3\n0.4\n0.5\n0.6\nth\ner\ne\nar\ne\nslo\nw\nan\nd\nre\npe\ntit\niv\ne\npa\nrts\n, bu\nt\nit ha\ns\nju\nst\nen\nou\ngh\nsp\nice\nto ke\nep\nit in\nte\nre\nst\nin\ng\nthere\nare\nslow\nand\nrepetitive\nparts\n,\nbut\nit\nhas\njust\nenough\nspice\nto\nkeep\nit\ninteresting\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFigure 3: Heat map showing the cosine similarity between pairs of word vectors within a single sentence. The left figure has\nfine-tuned word2vec embeddings. The middle figure contains the original ELMo embeddings without any fine-tuning. The\nright figure contains fine-tuned ELMo embeddings. For better visualization, the cosine similarity between identical words has\nbeen set equal to the minimum value in the heat map.\nment was computed using Fleiss’ Kappa (κ). As\nexpected, inter-annotator agreement is higher for\nhigher thresholds (less ambiguous sentences). Ac-\ncording to Landis and Koch (1977), κ ∈ (0.2, 0.4]\ncorresponds to “fair agreement”, whereas κ ∈\n(0.4, 0.6] corresponds to “moderate agreement”.\nWe next compute the accuracy of our model\nfor each threshold by removing the correspond-\ning neutral sentences. Higher thresholds corre-\nspond to sets of less ambiguous sentences. Table 3\nshows that ELMo’s performance gains in Table 2\nextends across all thresholds. In Figure 4 we com-\npare all the models on the A-but-B sentences in this\nset. Across all thresholds, we notice trends similar\nto previous sections: 1) ELMo performs the best\namong all models on A-but-B style sentences, and\nprojection results in only a slight improvement; 2)\nmodels in Hu et al. (2016) (with and without distil-\nlation) benefit considerably from projection; but 3)\ndistillation offers little improvement (with or with-\nout projection). Also, as the ambiguity threshold\nincreases, we see decreasing gains from projection\non all models. In fact, beyond the 0.85 threshold,\nprojection degrades the average performance, in-\ndicating that projection is useful for more ambigu-\nous sentences.\n6 Conclusion\nWe present an analysis comparing techniques for\nincorporating logic rules into sentiment classifi-\ncation systems. Our analysis included a meta-\nstudy highlighting the issue of stochasticity in\nperformance across runs and the inherent ambi-\nguity in the sentiment classification task itself,\nwhich was tackled using an averaged analysis and\nhttps://github.com/martiansideofthemoon/\nlogic-rules-sentiment.\nThreshold 0.50 0.66 0.75 0.90\nNeutral Sentiment 10 70 95 234\nFlipped Sentiment 15 4 2 0\nFleiss’ Kappa (κ) 0.38 0.42 0.44 0.58\nno-distill, no-project 81.32 83.54 84.54 87.55\nELMo, no-project 87.56 90.00 91.31 93.14\nTable 3: Number of sentences in the crowdsourced study\n(447 sentences) which got marked as neutral and which got\nthe opposite of their labels in the SST2 dataset, using vari-\nous thresholds. Inter-annotator agreement is computed using\nFleiss’ Kappa. Average accuracies of the baseline and ELMo\n(over 100 seeds) on non-neutral sentences are also shown.\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nthreshold\n80\n82\n84\n86\n88\n90\n92\n94\n96\nte\nst\n p\ne\nrf\no\nrm\na\nn\nce\nno-distill, no-project\nno-distill, project\ndistill, no-project\ndistill, project\nELMo, no-project\nELMo, project\nFigure 4: Average performance on the A-but-B part of the\ncrowd-sourced dataset (210 sentences, 100 seeds)). For each\nthreshold, only non-neutral sentences are used for evaluation.\na crowdsourced experiment identifying ambigu-\nous sentences. We present evidence that a re-\ncently proposed contextualized word embedding\nmodel (ELMo) (Peters et al., 2018a) implicitly\nlearns logic rules for sentiment classification of\ncomplex sentences like A-but-B sentences. Future\nwork includes a fine-grained quantitative study of\nELMo word vectors for logically complex sen-\ntences along the lines of Peters et al. (2018b).\nReferences\nKuzman Ganchev, Jennifer Gillenwater, Ben Taskar,\net al. 2010. Posterior regularization for structured\nlatent variable models. Journal of Machine Learn-\ning Research, 11(Jul):2001–2049.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. NIPS\nDeep Learning and Representation Learning Work-\nshop.\nZhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard\nHovy, and Eric Xing. 2016. Harnessing deep neural\nnetworks with logic rules. In Association for Com-\nputational Linguistics (ACL).\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daume´ III. 2015. Deep unordered compo-\nsition rivals syntactic methods for text classification.\nIn Association for Computational Linguistics (ACL).\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Empirical Methods in\nNatural Language Processing (EMNLP).\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nBiometrics, pages 159–174.\nShen Li, Zhe Zhao, Tao Liu, Renfen Hu, and Xi-\naoyong Du. 2017. Initializing convolutional filters\nwith semantic features for text classification. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nFrank J Massey Jr. 1951. The Kolmogorov-Smirnov\ntest for goodness of fit. Journal of the American sta-\ntistical Association, 46(253):68–78.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nBo Pang and Lillian Lee. 2005. Seeing stars: Ex-\nploiting class relationships for sentiment categoriza-\ntion with respect to rating scales. In Association for\nComputational Linguistics (ACL).\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Empirical Methods in Natural\nLanguage Processing (EMNLP).\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In North American Association for\nComputational Linguistics (NAACL).\nMatthew E. Peters, Mark Neumann, Wen tau Yih, and\nLuke Zettlemoyer. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP).\nNils Reimers and Iryna Gurevych. 2017. Reporting\nscore distributions makes a difference: Performance\nstudy of LSTM-networks for sequence tagging. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP).\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Association for Computational Linguis-\ntics (ACL).\nS. I. Wang and C. Manning. 2012. Baselines and bi-\ngrams: Simple, good sentiment and text classifica-\ntion. In Association for Computational Linguistics\n(ACL).\nAppendix\nA Crowdsourcing Details\nCrowd workers residing in five English-speaking\ncountries (United States, United Kingdom, New\nZealand, Australia and Canada) were hired. Each\ncrowd worker had a Level 2 or higher rating on\nFigure Eight, which corresponds to a “group of\nmore experienced, higher accuracy contributors”.\nEach contributor had to pass a test questionnaire\nto be eligible to take part in the experiment. Test\nquestions were also hidden throughout the task\nand untrusted contributions were removed from\nthe final dataset. For greater quality control, an\nupper limit of 75 judgments per contributor was\nenforced.\nCrowd workers were paid a total of $1 for 50 judg-\nments. An internal unpaid workforce (including\nthe first and second author of the paper) of 7 con-\ntributors was used to speed up data collection.\n# Judgments Average Sentence\nPositive Negative Neutral\n1 1 7 0.50 the fight scenes are fun , but it grows tedious\n3 2 4 0.56\nit ’s not exactly a gourmet meal but the fare is fair ,\neven coming from the drive thru\n2 3 4 0.44 propelled not by characters but by caricatures\n4 2 3 0.61\nnot everything works , but the average is higher than\nin mary and most other recent comedies\nTable A1: Examples of neutral sentences for a threshold of 0.66\n# Judgments Average Original Sentence\nPositive Negative Neutral\n1 5 3 0.28 Positive\nde niro and mcdormand give solid perfor-\nmances , but their screen time is sabotaged by\nthe story ’s inability to create interest\n6 0 3 0.83 Negative\nson of the bride may be a good half hour too\nlong but comes replete with a flattering sense\nof mystery and quietness\n0 5 4 0.22 Positive\nwasabi is slight fare indeed , with the entire\nproject having the feel of something tossed\noff quickly ( like one of hubert ’s punches )\n, but it should go down smoothly enough with\npopcorn\nTable A2: Examples of flipped sentiment sentences, for a threshold of 0.66\nModel 1 vs Model 2 Significant\ndistill no-project distill project Yes\nno-distill no-project no-distill project Yes\nELMo no-project ELMo project No\nno-distill no-project distill no-project No\nno-distill project distill project No\nno-distill no-project ELMo no-project Yes\ndistill no-project ELMo no-project Yes\nno-distill project ELMo project Yes\ndistill project ELMo project Yes\nTable A3: Statistical significance using a two-sided Kolmogorov-Smirnov statistic (Massey Jr, 1951) with α = 0.001.\nal\nl\nen\nds\nwe\nll\n, so\nrt\nof , bu\nt\nth\ne\nfre\nnz\nie\nd\nco\nm\nic\nm\nom\nen\nts\nne\nve\nr\ncli\nck\nall\nends\nwell\n,\nsort\nof\n,\nbut\nthe\nfrenzied\ncomic\nmoments\nnever\nclick 0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nal\nl\nen\nds\nwe\nll\n, so\nrt\nof , bu\nt\nth\ne\nfre\nnz\nie\nd\nco\nm\nic\nm\nom\nen\nts\nne\nve\nr\ncli\nck\nall\nends\nwell\n,\nsort\nof\n,\nbut\nthe\nfrenzied\ncomic\nmoments\nnever\nclick 0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nal\nl\nen\nds\nwe\nll\n, so\nrt\nof , bu\nt\nth\ne\nfre\nnz\nie\nd\nco\nm\nic\nm\nom\nen\nts\nne\nve\nr\ncli\nck\nall\nends\nwell\n,\nsort\nof\n,\nbut\nthe\nfrenzied\ncomic\nmoments\nnever\nclick\n0.1\n0.2\n0.3\n0.4\n0.5\nm\nar\nisa\nto\nm\nei\nis go\nod\n, bu\nt\nju\nst\na ki\nss\nis ju\nst\na m\nes\ns\nmarisa\ntomei\nis\ngood\n,\nbut\njust\na\nkiss\nis\njust\na\nmess 0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nm\nar\nisa\nto\nm\nei\nis go\nod\n, bu\nt\nju\nst\na ki\nss\nis ju\nst\na m\nes\ns\nmarisa\ntomei\nis\ngood\n,\nbut\njust\na\nkiss\nis\njust\na\nmess\n0.2\n0.3\n0.4\n0.5\n0.6\nm\nar\nisa\nto\nm\nei\nis go\nod\n, bu\nt\nju\nst\na ki\nss\nis ju\nst\na m\nes\ns\nmarisa\ntomei\nis\ngood\n,\nbut\njust\na\nkiss\nis\njust\na\nmess 0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nth\ne\nirw\nin\ns\nem\ner\nge\nun\nsc\nat\nhe\nd\n, bu\nt\nth\ne\nfic\ntio\nna\nl\nfo\not\nag\ne\nis un\nco\nnv\nin\ncin\ng\nan\nd\ncr\nim\nin\nal\nly\nba\ndl\ny\nac\nte\nd\nthe\nirwins\nemerge\nunscathed\n,\nbut\nthe\nfictional\nfootage\nis\nunconvincing\nand\ncriminally\nbadly\nacted\n0.1\n0.0\n0.1\n0.2\n0.3\nth\ne\nirw\nin\ns\nem\ner\nge\nun\nsc\nat\nhe\nd\n, bu\nt\nth\ne\nfic\ntio\nna\nl\nfo\not\nag\ne\nis un\nco\nnv\nin\ncin\ng\nan\nd\ncr\nim\nin\nal\nly\nba\ndl\ny\nac\nte\nd\nthe\nirwins\nemerge\nunscathed\n,\nbut\nthe\nfictional\nfootage\nis\nunconvincing\nand\ncriminally\nbadly\nacted 0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nth\ne\nirw\nin\ns\nem\ner\nge\nun\nsc\nat\nhe\nd\n, bu\nt\nth\ne\nfic\ntio\nna\nl\nfo\not\nag\ne\nis un\nco\nnv\nin\ncin\ng\nan\nd\ncr\nim\nin\nal\nly\nba\ndl\ny\nac\nte\nd\nthe\nirwins\nemerge\nunscathed\n,\nbut\nthe\nfictional\nfootage\nis\nunconvincing\nand\ncriminally\nbadly\nacted\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure A1: Heat map showing the cosine similarity between pairs of word vectors within a single sentence. The leftmost\ncolumn has word2vec (Mikolov et al., 2013) embeddings, fine-tuned on the downstream task (SST2). The middle column\ncontains the original ELMo embeddings (Peters et al., 2018a) without any fine-tuning. The representations from the three layers\n(token layer and two LSTM layers) have been averaged. The rightmost column contains ELMo embeddings fine-tuned on the\ndownstream task. For better visualization, the cosine similarity between identical words has been set equal to the minimum\nvalue in the map.\n",
      "id": 52337954,
      "identifiers": [
        {
          "identifier": "oai:arxiv.org:1808.07733",
          "type": "OAI_ID"
        },
        {
          "identifier": "470621779",
          "type": "CORE_ID"
        },
        {
          "identifier": "1808.07733",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "10.18653/v1/d18-1505",
          "type": "DOI"
        },
        {
          "identifier": "160786395",
          "type": "CORE_ID"
        }
      ],
      "title": "Revisiting the Importance of Encoding Logic Rules in Sentiment\n  Classification",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1808.07733"
      ],
      "publishedDate": "2018-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1808.07733"
      ],
      "updatedDate": "2021-08-10T17:40:03",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1808.07733"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/52337954"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "McDonald, Ryan"
        },
        {
          "name": "Täckström, Oscar"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/11435101",
        "https://api.core.ac.uk/v3/outputs/81027342",
        "https://api.core.ac.uk/v3/outputs/301005424",
        "https://api.core.ac.uk/v3/outputs/479759868"
      ],
      "createdDate": "2013-07-10T16:48:15",
      "dataProviders": [
        {
          "id": 21159,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/21159",
          "logo": "https://api.core.ac.uk/data-providers/21159/logo"
        },
        {
          "id": 905,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/905",
          "logo": "https://api.core.ac.uk/data-providers/905/logo"
        },
        {
          "id": 362,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/362",
          "logo": "https://api.core.ac.uk/data-providers/362/logo"
        },
        {
          "id": 10762,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/10762",
          "logo": "https://api.core.ac.uk/data-providers/10762/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "We derive two variants of a semi-supervised model for fine-grained sentiment analysis. Both models leverage abundant natural supervision in the form of review ratings, as well as a small amount of manually crafted sentence labels, to learn sentence-level sentiment classifiers. The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart. This allows for highly efficient estimation and inference algorithms with rich feature definitions. We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/11435101.pdf",
      "fieldOfStudy": null,
      "fullText": "Semi-supervised latent variable models for sentence-level sentiment analysis\nOscar Ta¨ckstro¨m\nSICS, Kista / Uppsala University, Uppsala\noscar@sics.se\nRyan McDonald\nGoogle, Inc., New York\nryanmcd@google.com\nAbstract\nWe derive two variants of a semi-supervised\nmodel for fine-grained sentiment analysis.\nBoth models leverage abundant natural super-\nvision in the form of review ratings, as well as\na small amount of manually crafted sentence\nlabels, to learn sentence-level sentiment clas-\nsifiers. The proposed model is a fusion of a\nfully supervised structured conditional model\nand its partially supervised counterpart. This\nallows for highly efficient estimation and infer-\nence algorithms with rich feature definitions.\nWe describe the two variants as well as their\ncomponent models and verify experimentally\nthat both variants give significantly improved\nresults for sentence-level sentiment analysis\ncompared to all baselines.\n1 Sentence-level sentiment analysis\nIn this paper, we demonstrate how combining\ncoarse-grained and fine-grained supervision bene-\nfits sentence-level sentiment analysis – an important\ntask in the field of opinion classification and retrieval\n(Pang and Lee, 2008). Typical supervised learning ap-\nproaches to sentence-level sentiment analysis rely on\nsentence-level supervision. While such fine-grained\nsupervision rarely exist naturally, and thus requires\nlabor intensive manual annotation effort (Wiebe et\nal., 2005), coarse-grained supervision is naturally\nabundant in the form of online review ratings. This\ncoarse-grained supervision is, of course, less infor-\nmative compared to fine-grained supervision, how-\never, by combining a small amount of sentence-level\nsupervision with a large amount of document-level\nsupervision, we are able to substantially improve on\nthe sentence-level classification task. Our work com-\nbines two strands of research: models for sentiment\nanalysis that take document structure into account;\nand models that use latent variables to learn unob-\nserved phenomena from that which can be observed.\nExploiting document structure for sentiment anal-\nysis has attracted research attention since the early\nwork of Pang and Lee (2004), who performed min-\nimal cuts in a sentence graph to select subjective\nsentences. McDonald et al. (2007) later showed that\njointly learning fine-grained (sentence) and coarse-\ngrained (document) sentiment improves predictions\nat both levels. More recently, Yessenalina et al.\n(2010) described how sentence-level latent variables\ncan be used to improve document-level prediction\nand Nakagawa et al. (2010) used latent variables over\nsyntactic dependency trees to improve sentence-level\nprediction, using only labeled sentences for training.\nIn a similar vein, Sauper et al. (2010) integrated gen-\nerative content structure models with discriminative\nmodels for multi-aspect sentiment summarization\nand ranking. These approaches all rely on the avail-\nability of fine-grained annotations, but Ta¨ckstro¨m\nand McDonald (2011) showed that latent variables\ncan be used to learn fine-grained sentiment using only\ncoarse-grained supervision. While this model was\nshown to beat a set of natural baselines with quite a\nwide margin, it has its shortcomings. Most notably,\ndue to the loose constraints provided by the coarse\nsupervision, it tends to only predict the two dominant\nfine-grained sentiment categories well for each docu-\nment sentiment category, so that almost all sentences\nin positive documents are deemed positive or neutral,\nand vice versa for negative documents. As a way of\novercoming these shortcomings, we propose to fuse\na coarsely supervised model with a fully supervised\nmodel.\nBelow, we describe two ways of achieving such\na combined model in the framework of structured\nconditional latent variable models. Contrary to (gen-\nerative) topic models (Mei et al., 2007; Titov and\na) yd\n· · · ysi−1 ysi ysi+1 · · ·\n· · · si−1 si si+1 · · ·\nb) yd\n· · · ysi−1 ysi ysi+1 · · ·\n· · · si−1 si si+1 · · ·\nFigure 1: a) Factor graph of the fully observed graphical model. b) Factor graph of the corresponding latent variable\nmodel. During training, shaded nodes are observed, while non-shaded nodes are unobserved. The input sentences si are\nalways observed. Note that there are no factors connecting the document node, yd, with the input nodes, s, so that the\nsentence-level variables, ys, in effect form a bottleneck between the document sentiment and the input sentences.\nMcDonald, 2008; Lin and He, 2009), structured con-\nditional models can handle rich and overlapping fea-\ntures and allow for exact inference and simple gradi-\nent based estimation. The former models are largely\northogonal to the one we propose in this work and\ncombining their merits might be fruitful. As shown\nby Sauper et al. (2010), it is possible to fuse gener-\native document structure models and task specific\nstructured conditional models. While we do model\ndocument structure in terms of sentiment transitions,\nwe do not model topical structure. An interesting\navenue for future work would be to extend the model\nof Sauper et al. (2010) to take coarse-grained task-\nspecific supervision into account, while modeling\nfine-grained task-specific aspects with latent vari-\nables.\nNote also that the proposed approach is orthogonal\nto semi-supervised and unsupervised induction of\ncontext independent (prior polarity) lexicons (Turney,\n2002; Kim and Hovy, 2004; Esuli and Sebastiani,\n2009; Rao and Ravichandran, 2009; Velikovich et al.,\n2010). The output of such models could readily be\nincorporated as features in the proposed model.\n1.1 Preliminaries\nLet d be a document consisting of n sentences, s =\n(si)\nn\ni=1, with a document–sentence-sequence pair de-\nnoted d = (d, s). Let yd = (yd,ys) denote random\nvariables1 – the document level sentiment, yd, and the\nsequence of sentence level sentiment, ys = (ysi )\nn\ni=1.\n1We are abusing notation throughout by using the same sym-\nbols to refer to random variables and their particular assignments.\nIn what follows, we assume that we have access to\ntwo training sets: a small set of fully labeled in-\nstances, DF = {(dj ,ydj )}mfj=1, and a large set of\ncoarsely labeled instances DC = {(dj , ydj )}mf+mcj=mf+1.\nFurthermore, we assume that yd and all ysi take val-\nues in {POS, NEG, NEU}.\nWe focus on structured conditional models in the\nexponential family, with the standard parametrization\npθ(y\nd,ys|s) = exp\n{\n〈φ(yd,ys, s), θ〉 −Aθ(s)\n}\n,\nwhere θ ∈ <n is a parameter vector, φ(·) ∈ <n is a\nvector valued feature function that factors according\nto the graph structure outlined in Figure 1, and Aθ\nis the log-partition function. This class of models is\nknown as conditional random fields (CRFs) (Lafferty\net al., 2001), when all variables are observed, and as\nhidden conditional random fields (HCRFs) (Quattoni\net al., 2007), when only a subset of the variables are\nobserved.\n1.2 The fully supervised fine-to-coarse model\nMcDonald et al. (2007) introduced a fully super-\nvised model in which predictions of coarse-grained\n(document) and fine-grained (sentence) sentiment are\nlearned and inferred jointly. They showed that learn-\ning both levels jointly improved performance at both\nlevels, compared to learning each level individually,\nas well as to using a cascaded model in which the\npredictions at one level are used as input to the other.\nFigure 1a outlines the factor graph of the corre-\nsponding conditional random field.2 The parameters,\nθF , of this model can be estimated from the set of\nfully labeled data, DF , by maximizing the joint con-\nditional likelihood function\nLF (θF ) =\nmf∑\nj=1\nlog pθF (y\nd\nj ,y\ns\nj |sj)−\n‖θF ‖2\n2σ2F\n,\nwhere σ2F is the variance of the Normal(0, σ\n2\nF ) prior.\nNote that LF is a concave function and consequently\nits unique maximum can be found by gradient based\noptimization techniques.\n1.3 Latent variables for coarse supervision\nRecently, Ta¨ckstro¨m and McDonald (2011) showed\nthat fine-grained sentiment can be learned from\ncoarse-grained supervision alone. Specifically, they\nused a HCRF model with the same structure as that\nin Figure 1a, but with sentence labels treated as la-\ntent variables. The factor graph corresponding to this\nmodel is outlined in Figure 1b.\nThe fully supervised model might benefit from fac-\ntors that directly connect the document variable, yd,\nwith the inputs s. However, as argued by Ta¨ckstro¨m\nand McDonald (2011), when only document-level\nsupervision is available, the document variable, yd,\nshould be independent of the input, s, conditioned\non the latent variables, ys. This prohibits the model\nfrom bypassing the latent variables, which is crucial,\nsince we seek to improve the sentence-level predic-\ntions, rather than the document-level predictions.\nThe parameters, θC , of this model can be esti-\nmated from the set of coarsely labeled data, DC , by\nmaximizing the marginalized conditional likelihood\nfunction\nLC(θC) =\nmf+mc∑\nj=mf+1\nlog\n∑\nys\npθC (y\nd\nj ,y\ns|sj)−‖θC‖\n2\n2σ2C\n,\nwhere the marginalization is over all possible se-\nquences of latent sentence label assignments ys.\nDue to the introduction of latent variables, the\nmarginal likelihood function is non-concave and thus\nthere are no guarantees of global optimality, how-\never, we can still use a gradient based optimization\ntechnique to find a local maximum.\n2Figure 1a differs slightly from the model employed by Mc-\nDonald et al. (2007), where they had factors connecting the\ndocument label yd with each input si as well.\n2 Combining coarse and full supervision\nThe fully supervised and the partially supervised\nmodels both have their merits. The former requires\nan expensive and laborious process of manual an-\nnotation, while the latter can be used with readily\navailable document labels, such as review star rat-\nings. The latter, however, has its shortcomings in\nthat the coarse-grained sentiment signal is less infor-\nmative compared to a fine-grained signal. Thus, in\norder to get the best of both worlds, we would like to\ncombine the merits of both of these models.\n2.1 A cascaded model\nA straightforward way of fusing the two models is\nby means of a cascaded model in which the predic-\ntions of the partially supervised model, trained by\nmaximizing LC(θC) are used to derive additional\nfeatures for the fully supervised model, trained by\nmaximizing LF (θF ).\nAlthough more complex representations are pos-\nsible, we generate meta-features for each sentence\nbased solely on operations on the estimated distribu-\ntions, pθC (y\nd, ysi |s). Specifically, we encode the fol-\nlowing probability distributions as discrete features\nby uniform bucketing, with bucket width 0.1: the\njoint distribution, pθC (y\nd, ysi |s); the marginal docu-\nment distribution, pθC (y\nd|s); and the marginal sen-\ntence distribution, pθC (y\ns\ni |s). We also encode the\nargmax of these distributions, as well as the pair-\nwise combinations of the derived features.\nThe upshot of this cascaded approach is that it is\nvery simple to implement and efficient to train. The\ndownside is that only the partially supervised model\ninfluences the fully supervised model; there is no\nreciprocal influence between the models. Given the\nnon-concavity of LC(θC), such influence could be\nbeneficial.\n2.2 Interpolating likelihood functions\nA more flexible way of fusing the two models is to\ninterpolate their likelihood functions, thereby allow-\ning for both coarse and joint supervision of the same\nmodel. Such a combination can be achieved by con-\nstraining the parameters so that θI = θF = θC and\ntaking the mean of the likelihood functions LF and\nLC , appropriately weighted by a hyper-parameter λ.\nThe result is the interpolated likelihood function\nLI(θI) = λLF (θI) + (1− λ)LC(θI) .\nA simple, yet efficient, way of optimizing this ob-\njective function is to use stochastic gradient ascent\nwith learning rate η. At each step we select a fully\nlabeled instance, (dj ,ydj ) ∈ DF , with probability λ\nand a coarsely labeled instance, (dj , ydj ) ∈ DC , with\nprobability (1− λ). We then update the parameters,\nθI , according to the gradients ∂LF and ∂LC , respec-\ntively. In principle we could use different learning\nrates ηF and ηC as well as different prior variances\nσ2F and σ\n2\nC , but in what follows we set them equal.\nSince we are interpolating conditional models, we\nneed at least partial observations of each instance.\nMethods for blending discriminative and generative\nmodels (Lasserre et al., 2006; Suzuki et al., 2007;\nAgarwal and Daume´, 2009; Sauper et al., 2010),\nwould enable incorporation of completely unlabeled\ndata as well. It is straightforward to extend the pro-\nposed model along these lines, however, in practice\ncoarsely labeled sentiment data is so abundant on\nthe web (e.g., rated consumer reviews) that incorpo-\nrating completely unlabeled data seems superfluous.\nFurthermore, using conditional models with shared\nparameters throughout allows for rich overlapping\nfeatures, while maintaining simple and efficient in-\nference and estimation.\n3 Experiments\nFor the following experiments, we used the same data\nset and a comparable experimental setup to that of\nTa¨ckstro¨m and McDonald (2011).3 We compare the\ntwo proposed hybrid models (Cascaded and Interpo-\nlated) to the fully supervised model of McDonald et\nal. (2007) (FineToCoarse) as well as to the soft vari-\nant of the coarsely supervised model of Ta¨ckstro¨m\nand McDonald (2011) (Coarse).\nThe learning rate was fixed to η = 0.001, while\nwe tuned the prior variances, σ2, and the number of\nepochs for each model. When sampling according to\nλ during optimization of LI(θI), we cycle through\nDF and DC deterministically, but shuffle these sets\nbetween epochs. Due to time constraints, we fixed the\ninterpolation factor to λ = 0.1, but tuning this could\n3The annotated test data can be downloaded from\nhttp://www.sics.se/people/oscar/datasets.\npotentially improve the results of the interpolated\nmodel. For the same reason we allowed a maximum\nof 30 epochs, for all models, while Ta¨ckstro¨m and\nMcDonald (2011) report a maximum of 75 epochs.\nTo assess the impact of fully labeled versus\ncoarsely labeled data, we took stratified samples with-\nout replacement, of sizes 60, 120, and 240 reviews,\nfrom the fully labeled folds and of sizes 15,000 and\n143,580 reviews from the coarsely labeled data. On\naverage each review consists of ten sentences. We\nperformed 5-fold stratified cross-validation over the\nlabeled data, while using stratified samples for the\ncoarsely labeled data. Statistical significance was as-\nsessed by a hierachical bootstrap of 95% confidence\nintervals, using the technique described by Davison\nand Hinkley (1997).\n3.1 Results and analysis\nTable 1 lists sentence-level accuracy along with 95%\nconfidence interval for all tested models. We first\nnote that the interpolated model dominates all other\nmodels in terms of accuracy. While the cascaded\nmodel requires both large amounts of fully labeled\nand coarsely labeled data, the interpolated model\nis able to take advantage of both types of data on\nits own and jointly. Still, by comparing the fully\nsupervised and the coarsely supervised models, the\nsuperior impact of fully labeled over coarsely labeled\ndata is evident. As can be seen in Figure 2, when\nall data is used, the cascaded model outperforms the\ninterpolated model for some recall values, and vice\nversa, while both models dominate the supervised\napproach for the full range of recall values.\nAs discussed earlier, and confirmed by Table 2,\nthe coarse-grained model only performs well on the\npredominant sentence-level categories for each docu-\nment category. The supervised model handles nega-\ntive and neutral sentences well, but performs poorly\non positive sentences even in positive documents.\nThe interpolated model, while still better at capturing\nthe predominant category, does a better job overall.\nThese results are with a maximum of 30 training\niterations. Preliminary experiments with a maximum\nof 75 iterations indicate that all models gain from\nmore iterations; this seems to be especially true for\nthe supervised model and for the cascaded model\nwith less amount of course-grained data.\n|DC | = 15,000 |DC | = 143,580\n|DF | = 60 |DF | = 120 |DF | = 240 |DF | = 60 |DF | = 120 |DF | = 240\nFineToCoarse 49.3 (-1.3, 1.4) 53.4 (-1.8, 1.7) 54.6 (-3.6, 3.8) 49.3 (-1.3, 1.4) 53.4 (-1.8, 1.7) 54.6 (-3.6, 3.8)\nCoarse 49.6 (-1.5, 1.8) 49.6 (-1.5, 1.8) 49.6 (-1.5, 1.8) 53.5 (-1.2, 1.4) 53.5 (-1.2, 1.4) 53.5 (-1.2, 1.4)\nCascaded 39.7 (-6.8, 5.7) 45.4 (-3.1, 2.9) 42.6 (-6.5, 6.5) 55.6 (-2.9, 2.7) 55.0 (-3.2, 3.4) 56.8 (-3.8, 3.6)\nInterpolated 54.3 (-1.4, 1.4) 55.0 (-1.7, 1.6) 57.5 (-4.1, 5.2) 56.0 (-2.4, 2.1) 54.5 (-2.9, 2.8) 59.1 (-2.8, 3.4)\nTable 1: Sentence level results for varying numbers of fully labeled (DF ) and coarsely labeled (DC) reviews. Bold:\nsignificantly better than the FineToCoarse model according to a hierarchical bootstrapped confidence interval, p < 0.05.\n0 10 20 30 40 50 60 70 80 90 100\nRecall\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nP\nre\nci\nsi\non\nPOS sentences\nFineToCoarse\nCascaded\nInterpolated\n0 10 20 30 40 50 60 70 80 90 100\nRecall\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nP\nre\nci\nsi\non\nNEG sentences\nFineToCoarse\nCascaded\nInterpolated\nFigure 2: Interpolated POS / NEG sentence-level precision-recall curves with |DC | = 143,580 and |DF | = 240.\nPOS docs. NEG docs. NEU docs.\nFineToCoarse 35 / 11 / 59 33 / 76 / 42 29 / 63 / 55\nCoarse 70 / 14 / 43 11 / 71 / 34 43 / 47 / 53\nCascaded 43 / 17 / 61 0 / 75 / 49 10 / 64 / 50\nInterpolated 73 / 16 / 51 42 / 72 / 48 54 / 52 / 57\nTable 2: POS / NEG / NEU sentence-level F1-scores per\ndocument category (|DC | = 143,580 and |DF | = 240).\n4 Conclusions\nLearning fine-grained classification tasks in a fully su-\npervised manner does not scale well due to the lack of\nnaturally occurring supervision. We instead proposed\nto combine coarse-grained supervision, which is natu-\nrally abundant but less informative, with fine-grained\nsupervision, which is scarce but more informative.\nTo this end, we introduced two simple, yet effective,\nmethods of combining fully labeled and coarsely\nlabeled data for sentence-level sentiment analysis.\nFirst, a cascaded approach where a coarsely super-\nvised model is used to generate features for a fully\nsupervised model. Second, an interpolated model\nthat directly optimizes a combination of joint and\nmarginal likelihood functions. Both proposed mod-\nels are structured conditional models that allow for\nrich overlapping features, while maintaining highly\nefficient exact inference and robust estimation prop-\nerties. Empirically, the interpolated model is superior\nto the other investigated models, but with sufficient\namounts of coarsely labeled and fully labeled data,\nthe cascaded approach is competitive.\nAcknowledgments\nThe first author acknowledges the support of the\nSwedish National Graduate School of Language\nTechnology (GSLT). The authors would also like to\nthank Fernando Pereira and Bob Carpenter for early\ndiscussions on using HCRFs in sentiment analysis.\nReferences\nArvind Agarwal and Hal Daume´. 2009. Exponential\nfamily hybrid semi-supervised learning. In Proceed-\nings of the International Jont conference on Artifical\nIntelligence (IJCAI).\nAnthony C. Davison and David V. Hinkley. 1997. Boot-\nstrap Methods and Their Applications. Cambridge Se-\nries in Statistical and Probabilistic Mathematics. Cam-\nbridge University Press, Cambridge, UK.\nAndrea Esuli and Fabrizio Sebastiani. 2009. SentiWord-\nNet: A publicly available lexical resource for opinion\nmining. In Proceedings of the Language Resource and\nEvaluation Conference (LREC).\nSoo-Min Kim and Eduard Hovy. 2004. Determining\nthe sentiment of opinions. In Proceedings of the In-\nternational Conference on Computational Linguistics\n(COLING).\nJohn Lafferty, Andrew McCallum, and Fernando Pereira.\n2001. Conditional random fields: Probabilistic models\nfor segmenting and labeling sequence data. In Pro-\nceedings of the International Conference on Machine\nLearning (ICML).\nJulia A. Lasserre, Christopher M. Bishop, and Thomas P.\nMinka. 2006. Principled hybrids of generative and\ndiscriminative models. In Proceedings of the IEEE\nComputer Society Conference on Computer Vision and\nPattern Recognition (CVPR).\nChenghua Lin and Yulan He. 2009. Joint sentiment/topic\nmodel for sentiment analysis. In Proceeding of the Con-\nference on Information and Knowledge Management\n(CIKM).\nRyan McDonald, Kerry Hannan, Tyler Neylon, Mike\nWells, and Jeff Reynar. 2007. Structured models for\nfine-to-coarse sentiment analysis. In Proceedings of\nthe Annual Conference of the Association for Computa-\ntional Linguistics (ACL).\nQ. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007.\nTopic sentiment mixture: modeling facets and opin-\nions in weblogs. In Proceedings of the International\nConference on World Wide Web (WWW).\nTetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.\n2010. Dependency Tree-based Sentiment Classification\nusing CRFs with Hidden Variables. In Proceedings of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL).\nBo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. In Proceedings of the Associ-\nation for Computational Linguistics (ACL).\nBo Pang and Lillian Lee. 2008. Opinion mining and\nsentiment analysis. Now Publishers.\nAriadna Quattoni, Sybor Wang, Louis-Philippe Morency,\nMichael Collins, and Trevor Darrell. 2007. Hidden\nconditional random fields. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence.\nDelip Rao and Deepak Ravichandran. 2009. Semi-\nsupervised polarity lexicon induction. In Proceedings\nof the European Chapter of the Association for Compu-\ntational Linguistics (EACL).\nChristina Sauper, Aria Haghighi, and Regina Barzilay.\n2010. Incorporating content structure into text analy-\nsis applications. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nJun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.\nSemi-supervised structured output learning based on\na hybrid generative and discriminative approach. In\nPorceedings of the Conference on Emipirical Methods\nin Natural Language Processing (EMNLP).\nOscar Ta¨ckstro¨m and Ryan McDonald. 2011. Discov-\nering fine-grained sentiment with latent variable struc-\ntured prediction models. In Proceedings of the Euro-\npean Conference on Information Retrieval (ECIR).\nIvan Titov and Ryan McDonald. 2008. Modeling online\nreviews with multi-grain topic models. In Proceedings\nof the Annual World Wide Web Conference (WWW).\nPeter Turney. 2002. Thumbs up or thumbs down? Senti-\nment orientation applied to unsupervised classification\nof reviews. In Proceedings of the Annual Conference of\nthe Association for Computational Linguistics (ACL).\nLeonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-\nnan, and Ryan McDonald. 2010. The viability of\nweb-derived polarity lexicons. In Proceedings of the\nNorth American Chapter of the Association for Compu-\ntational Linguistics (NAACL).\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating expressions of opinions and emotions in\nlanguage. In Language Resources and Evaluation\n(LREC).\nAinur Yessenalina, Yisong Yue, and Claire Cardie. 2010.\nMulti-level structured models for document-level senti-\nment classification. In Proceedings of the Conference\non Empirical Methods in Natural Language Processing\n(EMNLP).\n",
      "id": 4838488,
      "identifiers": [
        {
          "identifier": "oai:diva.org:ri-23853",
          "type": "OAI_ID"
        },
        {
          "identifier": "11435101",
          "type": "CORE_ID"
        },
        {
          "identifier": "301005424",
          "type": "CORE_ID"
        },
        {
          "identifier": "479759868",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:generic.eprints.org:4157/core10762",
          "type": "OAI_ID"
        },
        {
          "identifier": "81027342",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:generic.eprints.org:4157/core362",
          "type": "OAI_ID"
        }
      ],
      "title": "Semi-supervised latent variable models for sentence-level sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:generic.eprints.org:4157/core362",
        "oai:generic.eprints.org:4157/core10762",
        "oai:diva.org:ri-23853"
      ],
      "publishedDate": "2011-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 1645045,
          "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.",
          "authors": [],
          "date": "2004",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645055,
          "title": "Annotating expressions of opinions and emotions in language.",
          "authors": [],
          "date": "2005",
          "doi": "10.1007/s10579-005-7880-9",
          "raw": null,
          "cites": null
        },
        {
          "id": 1645036,
          "title": "Bootstrap Methods and Their Applications.",
          "authors": [],
          "date": "1997",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645039,
          "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data.",
          "authors": [],
          "date": "2001",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645044,
          "title": "Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645038,
          "title": "Determining the sentiment of opinions.",
          "authors": [],
          "date": "2004",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645051,
          "title": "Discovering fine-grained sentiment with latent variable structured prediction models.",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645035,
          "title": "Exponential family hybrid semi-supervised learning.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645047,
          "title": "Hidden conditional random fields.",
          "authors": [],
          "date": "2007",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645049,
          "title": "Incorporating content structure into text analysis applications.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645041,
          "title": "Joint sentiment/topic model for sentiment analysis.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645052,
          "title": "Modeling online reviews with multi-grain topic models.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645056,
          "title": "Multi-level structured models for document-level sentiment classification.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645046,
          "title": "Opinion mining and sentiment analysis.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645040,
          "title": "Principled hybrids of generative and discriminative models.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645050,
          "title": "Semi-supervised structured output learning based on a hybrid generative and discriminative approach.",
          "authors": [],
          "date": "2007",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645048,
          "title": "Semisupervised polarity lexicon induction.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645037,
          "title": "SentiWordNet: A publicly available lexical resource for opinion mining.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645042,
          "title": "Structured models for fine-to-coarse sentiment analysis.",
          "authors": [],
          "date": "2007",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645054,
          "title": "The viability of web-derived polarity lexicons.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645053,
          "title": "Thumbs up or thumbs down? Sentiment orientation applied to unsupervised classification of reviews.",
          "authors": [],
          "date": "2002",
          "doi": null,
          "raw": null,
          "cites": null
        },
        {
          "id": 1645043,
          "title": "Topic sentiment mixture: modeling facets and opinions in weblogs.",
          "authors": [],
          "date": "2007",
          "doi": null,
          "raw": null,
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://soda.swedishict.se/4157/1/ssl-sentiment-acl2011.pdf",
        "http://soda.swedish-ict.se/4157/1/ssl-sentiment-acl2011.pdf"
      ],
      "updatedDate": "2022-02-25T19:08:29",
      "yearPublished": 2011,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11435101.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11435101"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11435101/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11435101/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4838488"
        }
      ]
    },
    {
      "acceptedDate": "2016-08-13T00:00:00",
      "arxivId": "1712.01794",
      "authors": [
        {
          "name": "Kiritchenko, Svetlana"
        },
        {
          "name": "Mohammad, Saif M."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/84762590",
        "https://api.core.ac.uk/v3/outputs/194416345"
      ],
      "createdDate": "2017-08-13T08:15:18",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 962,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/962",
          "logo": "https://api.core.ac.uk/data-providers/962/logo"
        }
      ],
      "depositedDate": "2016-01-01T00:00:00",
      "abstract": "Negators, modals, and degree adverbs can significantly affect the sentiment\nof the words they modify. Often, their impact is modeled with simple\nheuristics; although, recent work has shown that such heuristics do not capture\nthe true sentiment of multi-word phrases. We created a dataset of phrases that\ninclude various negators, modals, and degree adverbs, as well as their\ncombinations. Both the phrases and their constituent content words were\nannotated with real-valued scores of sentiment association. Using phrasal terms\nin the created dataset, we analyze the impact of individual modifiers and the\naverage effect of the groups of modifiers on overall sentiment. We find that\nthe effect of modifiers varies substantially among the members of the same\ngroup. Furthermore, each individual modifier can affect sentiment words in\ndifferent ways. Therefore, solutions based on statistical learning seem more\npromising than fixed hand-crafted rules on the task of automatic sentiment\nprediction.Comment: In Proceedings of the 7th Workshop on Computational Approaches to\n  Subjectivity, Sentiment and Social Media Analysis (WASSA), San Diego,\n  California, 201",
      "documentType": "research",
      "doi": "10.18653/v1/w16-0410",
      "downloadUrl": "http://arxiv.org/abs/1712.01794",
      "fieldOfStudy": null,
      "fullText": "The Effect of Negators, Modals, and Degree Adverbs\non Sentiment Composition\nSvetlana Kiritchenko and Saif M. Mohammad\nNational Research Council Canada\n{svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.ca\nAbstract\nNegators, modals, and degree adverbs can sig-\nnificantly affect the sentiment of the words\nthey modify. Often, their impact is modeled\nwith simple heuristics; although, recent work\nhas shown that such heuristics do not capture\nthe true sentiment of multi-word phrases. We\ncreated a dataset of phrases that include vari-\nous negators, modals, and degree adverbs, as\nwell as their combinations. Both the phrases\nand their constituent content words were an-\nnotated with real-valued scores of sentiment\nassociation. Using phrasal terms in the created\ndataset, we analyze the impact of individual\nmodifiers and the average effect of the groups\nof modifiers on overall sentiment. We find\nthat the effect of modifiers varies substantially\namong the members of the same group. Fur-\nthermore, each individual modifier can affect\nsentiment words in different ways. Therefore,\nsolutions based on statistical learning seem\nmore promising than fixed hand-crafted rules\non the task of automatic sentiment prediction.\n1 Introduction\nSentiment associations are commonly captured\nin sentiment lexicons—lists of associated word–\nsentiment pairs (optionally with a score indicating\nthe degree of association). They are mostly used\nin sentiment analysis (Pontiki et al., 2014; Rosen-\nthal et al., 2015), but are also beneficial in stance\ndetection (Mohammad et al., 2016a; Mohammad et\nal., 2016b), (Hartner, 2013; Kleres, 2011; Moham-\nmad, 2012), detecting personality traits (Grijalva et\nal., 2015; Mohammad and Kiritchenko, 2015), and\nother applications.\nManually created sentiment lexicons are espe-\ncially useful because they tend to be more accurate\nthan automatically generated ones; they can be used\nto automatically generate large high-coverage lexi-\ncons (Tang et al., 2014; Esuli and Sebastiani, 2006);\nthey can be used to evaluate different methods of\nautomatically creating sentiment lexicons; and they\ncan be used for linguistic analysis such as examining\nhow modifiers (negators, modals, degree adverbs,\netc.) impact overall sentiment. However, most exist-\ning manually created sentiment lexicons tend to pro-\nvide only lists of positive and negative words with\nvery coarse levels of sentiment (Stone et al., 1966;\nWilson et al., 2005; Mohammad and Turney, 2013).\nThe coarse-grained distinctions may be less useful in\ndownstream applications than having access to fine-\ngrained (real-valued) sentiment association scores.\nManually created sentiment lexicons usually in-\nclude only single words. Yet, the sentiment of a\nphrase can differ markedly from the sentiment of\nits constituent words. Sentiment composition is the\ndetermining of sentiment of a multi-word linguistic\nunit, such as a phrase or a sentence, from its con-\nstituents. Lexicons that include sentiment associa-\ntions for phrases as well as for their constituents are\nuseful in studying sentiment composition. We refer\nto them as sentiment composition lexicons (SCLs).\nWe created a sentiment composition lexicon for\nphrases formed with negators (such as no and can-\nnot), modals (such as would have been and could),\ndegree adverbs (such as quite and less), and their\ncombinations. Both the phrases and their constituent\ncontent words were manually annotated with real-\nvalued scores of sentiment association using a tech-\nar\nX\niv\n:1\n71\n2.\n01\n79\n4v\n1 \n [c\ns.C\nL]\n  5\n D\nec\n 20\n17\nnique known as Best–Worst Scaling, which provides\nreliable annotations. We refer to the resulting lexi-\ncon as Sentiment Composition Lexicon for Negators,\nModals, and Degree Adverbs (SCL-NMA). The lexi-\ncon is also known as SemEval-2016 General English\nSentiment Modifiers Lexicon.1\nWe calculate the minimum difference in senti-\nment scores of two terms that is perceptible to na-\ntive speakers of a language. For sentiment scores\nbetween -1 and 1, we show that the perceptible dif-\nference is about 0.07 for English speakers. Know-\ning the least perceptible difference helps interpret\nthe impact of sentiment composition. For example,\nwe can determine whether a modifier significantly\nimpacts the sentiment of the word it composes with\nby calculating the difference in sentiment scores be-\ntween the combined phrase and the constituent, and\nchecking whether this difference is greater than the\nleast perceptible difference.\nWe use the phrasal terms in the created lexicon to\nanalyze the impact of common modifiers on the sen-\ntiment of the terms they modify. We measure the ef-\nfect of individual modifiers as well as the average ef-\nfect of the groups of modifiers on overall sentiment.\nWe show that the sentiment of a negated expression\n(such as not w) on the [-1,1] scale is on average\n0.926 points less than the sentiment of the modified\nterm w, if the w is positive. However, the sentiment\nof the negated expression is on average 0.791 points\nhigher than w, if the w is negative. Similar analy-\nsis for modals and degree adverbs shows that they\nimpact sentiment less dramatically than negators.\nFurthermore, the impact of modifiers substantially\nvaries even within a group, e.g., the average change\nin sentiment score brought by the negator ‘will not\nbe’ is 0.41 larger than the change introduced by the\nnegator ‘never’. Likewise, each individual modifier\ncan affect sentiment words in different ways. As a\nresult, in automatic sentiment prediction solutions\nbased on statistical learning seem more promising\nthan fixed hand-crafted rules.\n1This lexicon was first introduced in (Kiritchenko and Mo-\nhammad, 2016a) where we investigated the applicability and re-\nliability of the Best–Worst Scaling annotation technique in cap-\nturing word–sentiment associations. In this paper, we provide\nfurther details on the creation of the lexicon and present anal-\nysis of how negators, modals, and degree adverbs impact the\nsentiment of the words they modify.\nIn related work (not described here), we also cre-\nated a sentiment composition lexicon for another\nchallenging category of phrases—phrases that in-\nclude at least one positive word and at least one neg-\native word (Kiritchenko and Mohammad, 2016b).\nWe call such phrases opposing polarity phrases.\nBoth lexicons have been used as official test sets in\nSemEval-2016 Task 7 ‘Determining Sentiment In-\ntensity of English and Arabic Phrases’ (Kiritchenko\net al., 2016).2 The lexicons are made freely available\nto the research community.3\n2 Related Work\nSentiment Lexicons: There exist a number of man-\nually created lexicons that provide lists of positive\nand negative words, for example, General Inquirer\n(Stone et al., 1966), Hu and Liu Lexicon (Hu and\nLiu, 2004), and NRC Emotion Lexicon (Mohammad\nand Turney, 2013). Only a few manually created\nlexicons provide real-valued scores of sentiment as-\nsociation (Bradley and Lang, 1999; Warriner et al.,\n2013; Dodds et al., 2011). None of these lexicons,\nhowever, contain multi-word phrases. Manually cre-\nated sentiment lexicons can be used to automati-\ncally generate larger sentiment lexicons using semi-\nsupervised techniques (Esuli and Sebastiani, 2006;\nTurney and Littman, 2003; Mohammad et al., 2013;\nDe Melo and Bansal, 2013; Tang et al., 2014). (See\nMohammad (2016) for a survey on manually created\nand automatically generated affect resources.)\nAutomatically generated lexicons often have real-\nvalued sentiment association scores, are larger in\nscale, and can easily be collected for a specific do-\nmain; therefore, they were found to be more bene-\nficial in downstream applications, such as sentence-\nlevel sentiment prediction (Kiritchenko et al., 2014).\nHowever, any analysis of the relationship between\nthe sentiment of a phrase and its constituents is less\nreliable when made from an automatically generated\nresource as opposed to when made from a manu-\nally created resource (as automatically generated re-\nsources are less accurate). In this work, we pro-\nvide an extensive analysis of the impact of differ-\nent modifiers on sentiment based on reliable fine-\ngrained manual annotations.\n2http://alt.qcri.org/semeval2016/task7/\n3http://www.saifmohammad.com/WebPages/SCL.html\nContextual Valence Shifters: Negators, modals,\nand degree adverbs impact the sentiment of the word\nor phrase they modify and are commonly referred\nto as contextual valence shifters (Polanyi and Zae-\nnen, 2004; Kennedy and Inkpen, 2005; Jia et al.,\n2009; Wiegand et al., 2010; Lapponi et al., 2012).\nConventionally, the impact of contextual valence\nshifters is captured by simple heuristics. For exam-\nple, negation is often handled by reversing the polar-\nities of the sentiment words in the scope of negation\n(Polanyi and Zaenen, 2004; Kennedy and Inkpen,\n2005; Choi and Cardie, 2008) or by shifting the sen-\ntiment score of a term in a negated context towards\nthe opposite polarity by a fixed amount (Taboada\net al., 2011). However, such heuristics do not ade-\nquately capture the true sentiment of multi-word ex-\npressions (Zhu et al., 2014). Liu and Seneff (2009)\nrelax the assumption of a fixed shifting margin and\nestimate these margins for each modifier separately\nfrom data. Kiritchenko et al. (2014), on the other\nhand, estimate the impact of negation on each indi-\nvidual sentiment word through a corpus-based sta-\ntistical method. Ruppenhofer et al. (2015) automat-\nically rank English adverbs by their intensifying or\ndiminishing effect on adjectives using ratings meta-\ndata from product reviews.\nAnnotation techniques: A widely used method of\nannotation for obtaining numerical scores is the rat-\ning scale method—where one is asked to rate an\nitem on a five-, ten-, or hundred-point scale. While\neasy to understand, rating items on a scale is not nat-\nural for people. It is hard for annotators to remain\nconsistent when annotating a large number of items.\nAlso, respondents often use just a limited part of\nthe scale reducing the discrimination among items\n(Cohen, 2003). To obtain reliable annotations, the\nrating scale methods require a high number of re-\nsponses, typically 15 to 20 (Warriner et al., 2013;\nGraham et al., 2015). A more natural annotation task\nfor humans is to compare items (e.g., whether one\nword is more positive than the other). Most com-\nmonly, the items are compared in pairs (Thurstone,\n1927; David, 1963). In this work, we use Best–Worst\nScaling—a technique that exploits the comparative\napproach to annotation while keeping the number of\nrequired annotations small (Section 3.2). It has been\nshown to produce reliable annotations of terms by\nsentiment (Kiritchenko and Mohammad, 2016a).\nTerm Sentiment\nscore\nfavor 0.653\nwould be very easy 0.431\ndid not harm 0.194\nincreasingly difficult -0.583\nsevere -0.833\nTable 1: Example entries with real-valued sentiment scores\nfrom SCL-NMA.\n3 Creating SCL-NMA\nWe now describe the term selection process and\nthe Best–Worst Scaling annotation technique used\nto create the Sentiment Composition Lexicon for\nNegators, Modals, and Degree Adverbs. Table 1\nshows a few example entries from the lexicon. We\nalso describe how we calculated the minimum dif-\nference in sentiment scores of two terms that is per-\nceptible to native speakers of a language.\n3.1 Term Selection\nGeneral Inquirer (Stone et al., 1966) provides a\nlist of 1,621 positive and negative words from Os-\ngood’s seminal study on word meaning (Osgood et\nal., 1957). These are words commonly used in ev-\neryday English. We include all of these words. In\naddition, we include 1,586 high-frequency phrases\nformed by the Osgood words in combination with\nsimple negators such as no, don’t, and never, modals\nsuch as can, might, and should, or degree adverbs\nsuch as very and fairly.4 The eligible adverbs are\nchosen manually from adverbs frequently occurring\nin the British National Corpus (BNC)5. Each phrase\nincludes at least one modal, one negator, or one ad-\nverb; a phrase can include several modifiers (e.g.,\nwould be very happy). The modifiers and the phrases\nare chosen in such a way that the full set includes\nseveral phrases for each Osgood sentiment word and\nincludes several phrases for each modifier. In total,\nsixty-four different (single or multi-word) modifiers\nare selected. The final list contains 3,207 terms.\n4The complete lists of negators, modals, and de-\ngree adverbs used to create this dataset are available at\nhttp://www.saifmohammad.com/WebPages/SCL.html#NMA.\n5The British National Corpus, version 3 (BNC XML\nEdition). 2007. Distributed by Oxford University\nComputing Services on behalf of the BNC Consortium.\nhttp://www.natcorp.ox.ac.uk/\n3.2 Best–Worst Scaling\nBest–Worst Scaling (BWS), also sometimes referred\nto as Maximum Difference Scaling (MaxDiff), is\nan annotation scheme that exploits the comparative\napproach to annotation (Louviere and Woodworth,\n1990; Cohen, 2003; Louviere et al., 2015). Annota-\ntors are given four items (4-tuple) and asked which\nitem is the Best (highest in terms of the property of\ninterest) and which is the Worst (least in terms of the\nproperty of interest). These annotations can then be\nconverted into real-valued scores and also a ranking\nof items as per their association with the property\nof interest through a simple counting procedure: For\neach item, its score is calculated as the percentage of\ntimes the item was chosen as the Best minus the per-\ncentage of times the item was chosen as the Worst\n(Orme, 2009; Flynn and Marley, 2014). The scores\nrange from -1 to 1. Further details on Best–Worst\nScaling and its application to the task of sentiment\nannotation can be found in (Kiritchenko and Mo-\nhammad, 2016a).\n3.3 Annotation process\nThe complete list of 3,207 terms was randomly sam-\npled (with replacement) to create 6,414 (2 x 3,207)\n4-tuples that satisfy the following criteria:\n1. no two 4-tuples have the same four terms;\n2. no two terms within a 4-tuple are identical;\n3. each term in the term list appears approxi-\nmately in the same number of 4-tuples;\n4. each pair of terms appears approximately in the\nsame number of 4-tuples.\nNext, the set of 4-tuples was annotated through a\ncrowdsourcing platform, CrowdFlower. The anno-\ntators were presented with four terms (single words\nand multi-word phrases) at a time, and asked which\nterm is the most positive (or least negative) and\nwhich is the most negative (or least positive).6 Each\n4-tuple was annotated by ten respondents. We deter-\nmined accuracy of every annotator on a small set of\ncheck questions labeled by the authors of this paper.\nWe discarded all annotations provided by an anno-\ntator if their accuracy on these check questions was\nless than 70%.\n6The full set of instructions to annotators is available at\nhttp://www.saifmohammad.com/WebPages/SCL.html#NMA.\n3.4 Quality of Annotations\nLet majority answer refer to the option chosen most\noften for a question. 80% of the responses to the\nBest–Worst questions matched the majority answer.\nWe also tested the reliability of the aggregated\nscores by randomly dividing the sets of ten re-\nsponses to each question into two halves and com-\nparing the rankings obtained from these two groups\nof responses. The Spearman rank correlation coef-\nficient between the two sets of rankings was found\nto be 0.98. (The Pearson correlation coefficient be-\ntween the two sets of sentiment scores was also\n0.98.) Thus, even though annotators might disagree\nabout answers to individual questions, the aggre-\ngated scores produced by applying the counting pro-\ncedure on the Best–Worst annotations are remark-\nably reliable at ranking terms by sentiment.\n3.5 Least Perceptible Difference in Sentiment\nIn psychophysics, there is a notion of least percep-\ntible difference (aka just-noticeable difference)—the\namount by which something that can be measured\n(e.g., weight or sound intensity) needs to be changed\nin order for the difference to be noticeable by a hu-\nman (Fechner, 1966). Analogously, we can measure\nthe least perceptible difference in sentiment. If two\nwords have close to identical sentiment associations,\nthen it is expected that native speakers will choose\neach of the words about the same number of times\nwhen forced to pick a word that is more positive.\nHowever, as the difference in sentiment starts get-\nting larger, the frequency with which the two terms\nare chosen as most positive begins to diverge. At one\npoint, the frequencies diverge so much that we can\nsay with high confidence that the two terms do not\nhave the same sentiment associations. The average\nof this minimum difference in sentiment score is the\nleast perceptible difference for sentiment.\nTo calculate the least perceptible difference, we\nfirst build a plot of the relationship between ’differ-\nence in the sentiment scores between two terms’ and\n‘agreement among annotators’ when asked which\nterm is more positive. For each term pair w1 and w2\nsuch that d = score(w1 )− score(w2 )≥ 0, we count\nthe number of Best–Worst annotations from which\nwe can infer that w1 is more positive than w2 and\ndivide this number by the total number of annota-\nFigure 1: Human agreement on annotating term w1 as\nmore positive than term w2 for pairs with difference in\nscores d = score(w1 ) - score(w2 ). The x-axis represents\nd. The y-axis plots the avg. percentage of human an-\nnotations that judge term w1 as more positive than term\nw2 (thick line) and the corresponding 99.9%-confidence\nlower bound (thin blue line).\ntions from which we can infer either that w1 is more\npositive than w2 or that w2 is more positive than w1.\n(We can infer that w1 is more positive than w2 if in\na 4-tuple that has both w1 and w2 the annotator se-\nlected w1 as the most positive or w2 as the least pos-\nitive. The case for w2 being more positive than w1\nis similar.) This ratio is the human agreement for w1\nbeing more positive than w2. To get more reliable\nestimates, we average the human agreement for all\npairs of terms whose sentiment differs by d ± 0.01.\nFigure 1 shows the resulting average human agree-\nment. The thin blue line in the Figure depicts the\n99.9%-confidence lower bounds on the agreement.\nThe least perceptible difference is the point starting\nat which the lower bound consistently exceeds 50%\nthreshold (i.e., the point starting at which we observe\nwith 99.9% confidence that the human agreement is\nhigher than chance). The least perceptible differ-\nence when calculated from SCL-NMA is 0.069. In\nthe next section, we use the least perceptible differ-\nence to determine whether a modifier significantly\nimpacts the sentiment of the word it composes with.\n4 Impact of Negators, Modals, and Degree\nAdverbs on Sentiment\nSCL-NMA contains many phrases formed by differ-\nent types of modifiers—negators, modals, and de-\ngree adverbs. Thus, this lexicon is a good resource\nfor studying the impact of these types of modifiers\non sentiment. In the following, we compare the sen-\ntiment score of single-word term w with the sen-\ntiment score for phrase mod w, where mod is a\nmodifier from a particular group (negator, modal,\nor degree adverb). Table 2 shows the average ef-\nfect of different modifier groups on sentiment. The\ncolumns show the average change in sentiment score\nbetweenw and mod w, the number of pairs (ofw and\nmod w) used to determine the average, the number\nof phrases mod w whose sentiment score is greater\n(↑) or less (↓) than the score of w by at least 0.069\n(the least perceptible difference). Since the impact\nof modifiers can be different depending on the sen-\ntiment of the modified word w, we present separate\nanalyses for when w is positive and when w is neg-\native. For the analysis in this section only, a word is\nconsidered positive if it has a sentiment score greater\nthan or equal to 0.3, and considered negative if its\nsentiment score is less than or equal to -0.3.7\nObserve that the most change in sentiment is\ncaused by negation; it consistently decreases the\nscores of positive words, and increases the scores\nof negative words. The average score difference is\nsubstantial for both positive words (0.926 points)\nand negative words (0.791 points). Modals also\ntend to decrease the scores of positive words, and\nincrease the scores of negative words, though to a\nmuch smaller extent than negators. As with nega-\ntors, modals affect positive words more strongly\nthan they do negative words. Degree adverbs show\nless consistency than negators and modals; they can\nboth heighten or lower the sentiment of a word.\nMoreover, the same adverb can behave differently\nwith different words from the same sentiment group\n(positive or negative). Therefore, we report the av-\nerage absolute differences in scores for this modifier\ngroup. These average differences are substantially\nsmaller than the ones reported for modals and nega-\ntors; the effect of degree adverbs is minor. Besides,\nin contrast to modals and negators, for a large per-\ncentage of degree adverb phrases (for 35% of the\npositive-word phrases and for 37% of the negative-\nword phrases), the sentiment scores do not differ\nfrom the scores for the corresponding single words\nby the least perceptible difference (0.069 points). In\nthe subsections below, we further examine the im-\n7This threshold is somewhat arbitrary, and is chosen to dis-\ncard neutral terms from the analysis, whose sentiment tends not\nto change much with these modifiers.\nTable 2: The impact of different modifier groups on sentiment. ‘Avg. diff.’ is the average difference between the score of mod w\nand w. ‘# pairs‘ is the number of pairs (of w and mod w) used to determine the average. ‘# score ↑ (↓)‘ indicates the number of\nphrases for which score(mod w) is greater (less) than score(w) by at least 0.069 (the perceptible difference).\nModifier Group On positive words On negative words\nAvg. diff. # pairs # score ↑ # score ↓ Avg. diff. # pairs # score ↑ # score ↓\nnegators -0.926 265 1 264 0.791 71 71 0\nmodals -0.317 258 9 231 0.238 72 54 8\ndegree adverbs (abs. diff.) 0.201 435 106 212 0.166 163 42 68\nFigure 2: The impact of negators on sentiment. The x-\naxis is score(w), the sentiment score of a term w; the\ny-axis is score(mod w), the sentiment score of a term\nw preceded by a negator. Each dot corresponds to one\nphrase mod w. The black line shows an average effect of\nthe negators group. The dashed red line shows the revers-\ning polarity hypothesis score(mod w)= −score(w).\npact of each modifier category on the sentiment of its\nscope. Also, we provide rankings of different nega-\ntors, modals, and degree adverbs as per the average\nchange in sentiment score between w and mod w.\nThis would allow linguists and other researchers to\nbetter understand the behavior of different modifiers.\n4.1 Negation\nThere exist two common approaches to incorpo-\nrate the impact of negation in automatic systems:\n(1) reversing polarity hypothesis, where the senti-\nment score of a word ‘score(w)’ is replaced with\n‘−score(w)’; and (2) shifting hypothesis, where the\nsentiment score of a word ‘score(w)’ is reduced by a\nfixed amount: ‘score(w)−sign(score(w))× b’. We\nwill show that neither hypothesis accurately captures\nthe impact of negation. We will also present an anal-\nysis of the overall impact of negation and the impact\nof individual negators (aka negation triggers).\nIn our dataset, the negators are formed by ‘no’\nnegation words like no, not, never, and nothing in\ncombination with auxiliary and modal verbs. Fig-\nure 2 shows the overall impact of negation on sen-\ntiment of single words. Each dot in this figure cor-\nresponds to one negated phrase ‘negator w’. The\nx-axis corresponds to score(w) (the sentiment score\nof a word w); the y-axis is score(mod w) (the sen-\ntiment score of a word w preceded by a negator).\nThe black line shows an average effect of negation.\nThe dashed red line shows the reversing polarity\nhypothesis: score(mod w)= −score(w). Observe\nthat on average negators tend to substantially down-\nshift the sentiment of positive words turning them\ninto negative expressions. On the other hand, the\nscores of negative terms increase, but to a smaller\nextent than the scores of positive words. Words\nwith high absolute sentiment values tend to expe-\nrience the greatest shift. This is true for both posi-\ntive and negative words. However, this shift is sub-\nstantially smaller than is proposed by the revers-\ning polarity hypothesis. Overall, the reversing po-\nlarity hypothesis fit is rather poor. The shifting\nhypothesis does not explain the data either. An-\nother observation is that words with similar senti-\nment scores can form negated phrases with very dif-\nferent sentiment scores (appearing as columns of\ndots in the graph). This is mostly due to the ef-\nfect of different negators. However, the same nega-\ntor can sometimes have different effect on words\nwith similar sentiment. For example, the three\nwords easy, good, and better all have similar sen-\ntiment scores: score(easy) = 0.598, score(good) =\n0.556, score(better) = 0.486. Yet, the correspond-\ning negated phrases formed with the same nega-\ntor never range from negative (score(never good) =\n− 0.542), to slightly negative (score(never easy) =\n− 0.112), to positive (score(never better) = 0.666).\nTable 3: The impact of negators on sentiment.\nModifier Avg. diff. # pairs # score ↑ # score ↓\nOn positive words\nwill not be -1.066 9 0 9\ncannot -1.030 12 0 12\ndid not -0.978 13 0 13\nnot very -0.961 14 0 14\nnot -0.959 45 0 45\nno -0.948 29 0 29\nwas no -0.939 14 0 14\nwill not -0.935 11 0 11\nwas not -0.928 29 0 29\nhave no -0.917 8 0 8\ndoes not -0.907 13 0 13\ncould not -0.893 12 0 12\nwould not -0.869 11 0 11\nhad no -0.862 7 0 7\nwould not be -0.848 15 0 15\nmay not -0.758 6 0 6\nnothing -0.755 6 0 6\nnever -0.650 7 1 6\nOn negative words\nwill not 0.878 5 5 0\ndoes not 0.823 5 5 0\nwas not 0.786 10 10 0\nno 0.768 8 8 0\nnot 0.735 14 14 0\nNext, we investigate the effect of individual nega-\ntors. Table 3 shows the impact of negation triggered\nby different negators. The majority of the negation\ntriggers have a large effect on both positive and neg-\native words; the absolute difference in scores be-\ntween a negated phrase and the corresponding sen-\ntiment word is 0.8-1.0 points on positive words and\n0.7-0.9 points on negative words. The greatest shift\nin sentiment on positive words was observed for the\nmodifier will not be, and on negative words for mod-\nifier will not. The weakest effect is caused by may\nnot, nothing, and never. Verb tenses seem not to\naffect the behavior of negators significantly. For ex-\nample, the average change in sentiment caused by\nnot and by was not differs only by 0.03-0.05 points.\nThe modal verbs will and can form strong nega-\ntion phrases will not, will not be, and cannot that\nshowed the most change in sentiment. Other modal\nverbs, such as could, would, and may, form negation\nphrases with smaller effect on sentiment.\nFigure 3: The impact of modals on sentiment. The x-axis\nis score(w), the sentiment score of a term w; the y-axis is\nscore(mod w), the sentiment score of a term w preceded by\na modal verb. Each dot corresponds to one phrase mod w. The\nblack line shows an average effect of the modals group. The\ndashed red line shows the function score(mod w) = score(w).\n4.2 Modals\nIn our dataset, the modal modifiers are formed by\nmodal verbs can, could, should, would, may, might,\nand must in combination with auxiliary verbs. Fig-\nure 3 demonstrates the overall impact of modals on\nsentiment. One can observe that on average modals\nhave a smoothing effect on sentiment: they make\nnegative words less negative and positive words less\npositive. Words with high absolute sentiment val-\nues tend to experience the greatest shift; though, this\nshift is still quite small (around 0.4 points).\nThe effect of individual modal modifiers on pos-\nitive and negative words is shown in Table 4. The\nmost influential modal modifier is would have been.\nIt consistently downshifts sentiment by a signifi-\ncant margin (about 0.5 points). Modifiers involving\nmodals could, and might also affect sentiment in a\nconsistent and noticeable way for both positive and\nnegative words. Modals can and would form mod-\nifiers that have the smallest effect on sentiment of\npositive and negative words (with the exception of\nthe modifier would have been).\n4.3 Degree Adverbs\nAs mentioned earlier, the average differences in sen-\ntiment caused by degree adverbs are quite small;\nmany differences are negligible. Furthermore, these\nmodifiers are less consistent than negators and\nmodals; there are many degree adverbs that increase\nTable 4: The impact of modals on sentiment.\nModifier Avg. diff. # pairs # score ↑ # score ↓\nOn positive words\nwould have been -0.491 12 0 12\ncould -0.390 16 1 15\nmight -0.387 14 0 14\nmay -0.384 14 1 13\nshould be -0.365 22 0 22\ncould be -0.342 14 0 13\nmust -0.338 12 0 11\nshould -0.314 14 0 12\nmay be -0.300 20 2 18\nmight be -0.298 14 2 10\nmust be -0.287 19 0 17\nwould -0.284 16 0 15\ncan be -0.283 18 2 15\nwould be -0.261 29 0 25\ncan -0.208 16 1 13\nwould be very -0.186 8 0 6\nOn negative words\ncould 0.351 5 5 0\ncould be 0.268 8 7 1\nmight be 0.256 5 5 0\ncan be 0.249 6 4 0\nwould be 0.224 12 7 2\ncan 0.200 5 4 0\nmay be 0.169 8 5 2\nthe sentiment intensity of some words from one class\n(positive or negative) and decrease the sentiment in-\ntensity of other words from the same class. For ex-\nample, certainly heightens the sentiment intensity\nof positive word important (by about 0.21 points),\nbut lowers the sentiment intensity of another positive\nword hope (by about 0.31 points). We found that the\nonly degree adverb in our set that affects sentiment\nto a large extent (0.835 points) is less; it consistently\nand significantly decreases the sentiment intensity of\npositive words. In fact, it acts as negator and reduces\nthe sentiment intensity of positive words to a degree\nsimilar to that of negators. There are a few other\nmodifiers that consistently reduce the sentiment in-\ntensity of positive words by a significant amount:\nwas too, too, probably, fairly, and relatively. Only\none intensifier, highly, consistently and significantly\nincrease the sentiment of positive words. The sen-\ntiment of negative words is significantly lowered by\nintensifiers extremely and very very.\n4.4 Interactive Visualization\nAs part of this project, we created an interactive vi-\nsualization for SCL-NMA.8 The visualization has\nseveral components that allow to investigate the ef-\nfect of sentiment modifiers on individual words as\nwell as to inspect the complete set in one scatter plot.\nThe groups of modifiers are color-coded for ease of\nexploration. The full information for a phrase, in-\ncluding the sentiment scores of the phrase and its\nconstituent content word, can be viewed by hover-\ning over the point in the graph with the mouse. The\nscatter plot can be filtered to show phrases that in-\nclude only a particular type of the modifiers (nega-\ntors, modals, or degree adverbs). All the compo-\nnents are linked together so that by clicking on a\npoint in one component one can highlight or filter\nthe corresponding points shown in the other compo-\nnents. We hope that the users will find this visual-\nization very helpful in exploring aspects of the data\nthey are interested in.\n5 Conclusions\nWe created a real-valued sentiment lexicon of\nphrases that include a variety of common sentiment\nmodifiers such as negators, modals, and degree ad-\nverbs. Both phrases and their constituent content\nwords are annotated manually using the Best–Worst\nScaling technique. We showed that the obtained an-\nnotations are reliable—re-doing the annotation with\ndifferent sets of annotators produces a very similar\nranking of terms by sentiment. We use the annota-\ntions for the phrases to present an extensive analy-\nsis of how negators, modals, and degree adverbs im-\npact the sentiment of other words in their scope. We\ndemonstrate that these modifiers affect sentiment in\ncomplex ways so that their effect cannot be easily\nmodeled with simple heuristics. In particular, we\nobserve that the effect of a modifier is often deter-\nmined not only by the type of the modifier (whether\nit is a negator, modal, or degree adverb) but also by\nthe modifier word and the content word themselves.\nThe created lexicon is made freely available to the\nresearch community to foster further research, es-\npecially towards automatic methods for sentiment\ncomposition and towards a better understanding of\nhow sentiment is composed in the human brain.\n8www.saifmohammad.com/WebPages/SCL.html#NMA\nReferences\nMargaret M Bradley and Peter J Lang. 1999. Affective\nnorms for English words (ANEW): Instruction manual\nand affective ratings. Technical report, The Center for\nResearch in Psychophysiology, University of Florida.\nYejin Choi and Claire Cardie. 2008. Learning with com-\npositional semantics as structural inference for subsen-\ntential sentiment analysis. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 793–801.\nSteven H. Cohen. 2003. Maximum difference scaling:\nImproved measures of importance and preference for\nsegmentation. Sawtooth Software, Inc.\nHerbert Aron David. 1963. The method of paired com-\nparisons. Hafner Publishing Company, New York.\nGerard De Melo and Mohit Bansal. 2013. Good,\ngreat, excellent: Global inference of semantic inten-\nsities. Transactions of the Association for Computa-\ntional Linguistics, 1:279–290.\nPeter Sheridan Dodds, Kameron Decker Harris, Isabel M.\nKloumann, Catherine A. Bliss, and Christopher M.\nDanforth. 2011. Temporal patterns of happiness and\ninformation in a global social network: Hedonomet-\nrics and Twitter. PloS One, 6(12):e26752.\nAndrea Esuli and Fabrizio Sebastiani. 2006. SENTI-\nWORDNET: A publicly available lexical resource for\nopinion mining. In Proceedings of the 5th Confer-\nence on Language Resources and Evaluation (LREC),\npages 417–422.\nGustav Fechner. 1966. Elements of psychophysics. Vol.\nI. New York: Holt, Rinehart and Winston.\nT. N. Flynn and A. A. J. Marley. 2014. Best-worst scal-\ning: theory and methods. In Stephane Hess and An-\ndrew Daly, editors, Handbook of Choice Modelling,\npages 178–201. Edward Elgar Publishing.\nYvette Graham, Nitika Mathur, and Timothy Baldwin.\n2015. Accurate evaluation of segment-level machine\ntranslation metrics. In Proceedings of the Annual Con-\nference of the North American Chapter of the ACL\n(NAACL), pages 1183–1191.\nEmily Grijalva, Daniel A. Newman, Louis Tay, M. Brent\nDonnellan, P.D. Harms, Richard W. Robins, and Taiyi\nYan. 2015. Gender differences in narcissism: A meta-\nanalytic review. Psychological bulletin, 141(2):261–\n310.\nMarcus Hartner. 2013. The lingering after-effects in the\nreader’s mind – an investigation into the affective di-\nmension of literary reading. Journal of Literary The-\nory Online.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the 10th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining (KDD), pages 168–\n177, New York, NY, USA.\nLifeng Jia, Clement Yu, and Weiyi Meng. 2009. The\neffect of negation on sentiment analysis and retrieval\neffectiveness. In Proceedings of the 18th ACM Con-\nference on Information and Knowledge Management\n(CIKM), pages 1827–1830, New York, NY, USA.\nAlistair Kennedy and Diana Inkpen. 2005. Sentiment\nclassification of movie and product reviews using con-\ntextual valence shifters. In Proceedings of the Work-\nshop on the Analysis of Informal and Formal Infor-\nmation Exchange during Negotiations (FINEXIN), Ot-\ntawa, Ontario, Canada.\nSvetlana Kiritchenko and Saif M. Mohammad. 2016a.\nCapturing reliable fine-grained sentiment associations\nby crowdsourcing and best–worst scaling. In Pro-\nceedings of The 15th Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL), San Diego, California.\nSvetlana Kiritchenko and Saif M. Mohammad. 2016b.\nSentiment composition of words with opposing polar-\nities. In Proceedings of The 15th Annual Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL), San Diego, California.\nSvetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-\nhammad. 2014. Sentiment analysis of short infor-\nmal texts. Journal of Artificial Intelligence Research,\n50:723–762.\nSvetlana Kiritchenko, Saif M. Mohammad, and Moham-\nmad Salameh. 2016. SemEval-2016 Task 7: De-\ntermining sentiment intensity of english and arabic\nphrases. In Proceedings of the International Work-\nshop on Semantic Evaluation (SemEval), San Diego,\nCalifornia, June.\nJochen Kleres. 2011. Emotions and narrative analysis:\nA methodological approach. Journal for the Theory of\nSocial Behaviour, 41(2):182–202.\nEmanuele Lapponi, Jonathon Read, and Lilja Ovrelid.\n2012. Representing and resolving negation for senti-\nment analysis. In Proceedings of the 12th IEEE In-\nternational Conference on Data Mining Workshops,\npages 687–692.\nJingjing Liu and Stephanie Seneff. 2009. Review senti-\nment scoring via a parse-and-paraphrase paradigm. In\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing, pages 161–169.\nJordan J. Louviere and George G. Woodworth. 1990.\nBest-worst analysis. Working Paper. Department of\nMarketing and Economic Analysis, University of Al-\nberta.\nJordan J. Louviere, Terry N. Flynn, and A. A. J. Marley.\n2015. Best-Worst Scaling: Theory, Methods and Ap-\nplications. Cambridge University Press.\nSaif M. Mohammad and Svetlana Kiritchenko. 2015.\nUsing hashtags to capture fine emotion categories from\ntweets. Computational Intelligence, 31(2):301–326.\nSaif M. Mohammad and Peter D. Turney. 2013. Crowd-\nsourcing a word–emotion association lexicon. Com-\nputational Intelligence, 29(3):436–465.\nSaif M. Mohammad, Svetlana Kiritchenko, and Xiaodan\nZhu. 2013. NRC-Canada: Building the state-of-the-\nart in sentiment analysis of tweets. In Proceedings\nof the International Workshop on Semantic Evaluation\n(SemEval), Atlanta, Georgia, USA, June.\nSaif M. Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016a. A\ndataset for detecting stance in tweets. In Proceed-\nings of 10th edition of the the Language Resources and\nEvaluation Conference (LREC), Portorozˇ, Slovenia.\nSaif M. Mohammad, Parinaz Sobhani, and Svetlana Kir-\nitchenko. 2016b. Stance and sentiment in tweets. Spe-\ncial Section of the ACM Transactions on Internet Tech-\nnology on Argumentation in Social Media, Submitted.\nSaif M Mohammad. 2012. From once upon a time\nto happily ever after: Tracking emotions in mail and\nbooks. Decision Support Systems, 53(4):730–741.\nSaif M. Mohammad. 2016. Sentiment analysis: Detect-\ning valence, emotions, and other affectual states from\ntext. In Herb Meiselman, editor, Emotion Measure-\nment. Elsevier.\nBryan Orme. 2009. Maxdiff analysis: Simple counting,\nindividual-level logit, and HB. Sawtooth Software,\nInc.\nCharles E Osgood, George J Suci, and Percy Tannen-\nbaum. 1957. The measurement of meaning. Univer-\nsity of Illinois Press.\nLivia Polanyi and Annie Zaenen. 2004. Contextual va-\nlence shifters. In Proceedings of the Exploring At-\ntitude and Affect in Text: Theories and Applications\n(AAAI Spring Symposium Series).\nMaria Pontiki, Harris Papageorgiou, Dimitrios Galanis,\nIon Androutsopoulos, John Pavlopoulos, and Suresh\nManandhar. 2014. SemEval-2014 Task 4: Aspect\nbased sentiment analysis. In Proceedings of the 8th\nInternational Workshop on Semantic Evaluation (Se-\nmEval), Dublin, Ireland.\nSara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,\nSaif Mohammad, Alan Ritter, and Veselin Stoyanov.\n2015. SemEval-2015 task 10: Sentiment analysis in\nTwitter. In Proceedings of the 9th International Work-\nshop on Semantic Evaluation (SemEval), pages 450–\n462, Denver, Colorado.\nJosef Ruppenhofer, Jasper Brandes, Petra Steiner, and\nMichael Wiegand. 2015. Ordering adverbs by their\nscaling effect on adjective intensity. In Proceedings\nof Recent Advances in Natural Language Processing\n(RANLP), pages 545–554, Hissar, Bulgaria.\nPhilip Stone, Dexter C. Dunphy, Marshall S. Smith,\nDaniel M. Ogilvie, and associates. 1966. The General\nInquirer: A Computer Approach to Content Analysis.\nThe MIT Press.\nMaite Taboada, Julian Brooke, Milan Tofiloski, Kimberly\nVoll, and Manfred Stede. 2011. Lexicon-based meth-\nods for sentiment analysis. Computational Linguis-\ntics, 37(2):267–307.\nDuyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting\nLiu. 2014. Building large-scale Twitter-specific senti-\nment lexicon: A representation learning approach. In\nProceedings of the International Conference on Com-\nputational Linguistics (COLING), pages 172–182.\nLouis L. Thurstone. 1927. A law of comparative judg-\nment. Psychological review, 34(4):273.\nPeter Turney and Michael L Littman. 2003. Measuring\npraise and criticism: Inference of semantic orientation\nfrom association. ACM Transactions on Information\nSystems, 21(4).\nAmy Beth Warriner, Victor Kuperman, and Marc Brys-\nbaert. 2013. Norms of valence, arousal, and domi-\nnance for 13,915 English lemmas. Behavior Research\nMethods, 45(4):1191–1207.\nMichael Wiegand, Alexandra Balahur, Benjamin Roth,\nDietrich Klakow, and Andre´s Montoyo. 2010. A sur-\nvey on the role of negation in sentiment analysis. In\nProceedings of the Workshop on Negation and Spec-\nulation in Natural Language Processing (NeSp-NLP),\npages 60–68, Stroudsburg, PA, USA.\nTheresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005. Recognizing contextual polarity in phrase-level\nsentiment analysis. In Proceedings of the Conference\non Human Language Technology and Empirical Meth-\nods in Natural Language Processing, pages 347–354,\nStroudsburg, PA, USA.\nXiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svet-\nlana Kiritchenko. 2014. An empirical study on the\neffect of negation words on sentiment. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics, pages 304–313, Balti-\nmore, Maryland, June.\n",
      "id": 45957600,
      "identifiers": [
        {
          "identifier": "oai:arxiv.org:1712.01794",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.18653/v1/w16-0410",
          "type": "DOI"
        },
        {
          "identifier": "oai:cisti-icist.nrc-cnrc.ca:cistinparc:23001908",
          "type": "OAI_ID"
        },
        {
          "identifier": "info:doi/10.18653%2fv1%2fw16-0410",
          "type": "OAI_ID"
        },
        {
          "identifier": "194416345",
          "type": "CORE_ID"
        },
        {
          "identifier": "84762590",
          "type": "CORE_ID"
        },
        {
          "identifier": "1712.01794",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "141535865",
          "type": "CORE_ID"
        }
      ],
      "title": "The Effect of Negators, Modals, and Degree Adverbs on Sentiment\n  Composition",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1712.01794",
        "oai:cisti-icist.nrc-cnrc.ca:cistinparc:23001908",
        "info:doi/10.18653%2fv1%2fw16-0410"
      ],
      "publishedDate": "2016-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1712.01794"
      ],
      "updatedDate": "2024-02-29T00:17:20",
      "yearPublished": 2016,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1712.01794"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/45957600"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Chen, Jiajun, M.S. in Statistics"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Lin, Lizhen, Ph. D.",
        "Keitt, Timothy"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/211337086"
      ],
      "createdDate": "2019-07-09T05:16:33",
      "dataProviders": [
        {
          "id": 222,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/222",
          "logo": "https://api.core.ac.uk/data-providers/222/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "textThe purpose of sentiment analysis is to determine the attitude of a writer or a speaker with respect to some topic or his feeling in a document. Thanks to the rise of social media, nowadays there are numerous data generated by users.  Mining and categorizing these data will not only bring profits for companies, but also benefit the nation. Sentiment analysis not only enables business decision makers to better understand customers' behaviors, but also allows customers to know how the public feel about a product before purchasing. On the other hand, the aggregation of emotions will effectively measure the public response toward an event or news. For example, the level of distress and sadness will increase significantly after terror attacks or natural disaster. In our project, we are going to build a search engine that allows users to check the sentiment of his query. Some of previous researches on classifying sentiment of messages on micro-blogging services like Twitter have tried to solve this problem but they have ignored neutral tweets, which will result in problematic results (12). Our sentiment analysis will also be based on tweets collected from twitter, since twitter can offer sufficient and real-time corpora for analysis. We will preprocess each tweet in the training set and label it as positive, negative or neutral. As we use words in the tweet as the feature for our model, different features will be used. We will show that accuracy achieved by different machine learning algorithms (Naïve Bayes, Maximum Entropy) can be improved with a feature vector obtained by using bigrams (5). In our practice, we find that Naive Bayes has better performance than Maximum Entropy.Statistic",
      "documentType": "research",
      "doi": "10.15781/t2ss51",
      "downloadUrl": "https://core.ac.uk/download/211337086.pdf",
      "fieldOfStudy": null,
      "fullText": "         Copyright by Jiajun Chen 2015   The Report Committee for Jiajun Chen Certifies that this is the approved version of the following report: Search Engine for Twitter Sentiment Analysis APPROVED BY SUPERVISING COMMITTEE: Lizhen Lin Timothy Keitt Supervisor: Search Engine for Twitter Sentiment Analysis by Jiajun Chen, B.ECO. Report Presented to the Faculty of the Graduate School of The University of Texas at Austin in Partial Fulfillment  of the Requirements for the Degree of  Master of Science in Statistics The University of Texas at Austin May 2015 iv Abstract Search Engine for Twitter Sentiment Analysis Jiajun Chen, M.S.STAT. The University of Texas at Austin, 2015 Supervisor:  Lizhen Lin The purpose of sentiment analysis is to determine the attitude of a writer or a speaker with respect to some topic or his feeling in a document. Thanks to the rise of social media, nowadays there are numerous data generated by users.  Mining and categorizing these data will not only bring profits for companies, but also benefit the nation. Sentiment analysis not only enables business decision makers to better understand customers’ behaviors, but also allows customers to know how the public feel about a product before purchasing. On the other hand, the aggregation of emotions will effectively measure the public response toward an event or news. For example, the level of distress and sadness will increase significantly after terror attacks or natural disaster. In our project, we are going to build a search engine that allows users to check the sentiment of his query. Some of previous researches on classifying sentiment of messages on micro-blogging services like Twitter have tried to solve this problem but they have ignored neutral tweets, which will result in problematic results [12]. Our sentiment analysis will also be based on tweets collected from twitter, since twitter can offer sufficient and real-time corpora for analysis. We will preprocess each tweet in the training set and label it as positive, negative or neutral. As we use words in the tweet as the feature for our model, different features will be used. We will show that accuracy achieved by different machine learning algorithms (Naïve Bayes, Maximum Entropy) can be improved with a feature vector obtained by using bigrams [5]. In our practice, we find that Naive Bayes has better performance than Maximum Entropy. v Table of Contents List of Tables  ...................................................................................................... viiList of Figures  ................................................................................................... .viiiIntroduction ..............................................................................................................1Define the sentiment .......................................................................................1Related Work ...........................................................................................................3Methodology ............................................................................................................4Collect Data for Training ................................................................................4Preprocess .......................................................................................................4     Feature Reduction ...................................................................................4     Calculate Setiment Score ........................................................................5     Store and Index Data in the Database ..............................................................5      Sentiment Analysis ..........................................................................................5     Feature Extraction ...................................................................................6    Machine Learing Algorithms ..................................................................6 Naïve Bayes ....................................................................................6   Maximum Entropy ..........................................................................7     Vector Space Modeling ....................................................................................8    Architecture .............................................................................................9     Relevance Ranking ........................................................................................10    User Interface Design ....................................................................................11Experiment .............................................................................................................13Results ....................................................................................................................14Classifier Performance ..................................................................................14Search Engine Demo .....................................................................................14vi Conclusion .............................................................................................................18Future Work ...........................................................................................................19References ..............................................................................................................20 vii List of Tables Table 1:\t   Example Tweets ......................................................................................1\t  Table 2:\t   Training data and test data ....................................................................13 Table 3:\t   Effects of Feature Selection ..................................................................13 Table 4:\t   Accuracy of classifier ...........................................................................14\t   viii List of Figures Figure 1:\t   Expression for Positive, Neutral and Negative .....................................1\t  Figure 2:\t   Procedure of Our Twitter Search Engine ..............................................2 Figure 3:\t   Workflow of Sentiment Analysis ..........................................................6\t  Figure 4:\t   Sample\t  of\t  Inverted\t  Index\t  Construction .............................................9\t  Figure 5:\t   The\t  Search\t  Engine\t  Home\t  Page .........................................................15\t  Figure 6:\t   The sentiment analysis of \"glad\" with 50% sentiment portion ...........15\t  Figure 7:\t   The sentiment analysis of \"sad\" with 50% sentiment portion .............16\t  Figure 8:\t   The sentiment analysis of \"black friday\" with 70% sentiment portion17\t  \t      1 Keyword Twitter, Sentiment Analysis, Search Engine    INTRODUCTION Twitter is a popular micro-blogging service where people can post status messages (tweets). Such tweets may show users’ opinion or sentiment toward a certain topic. A large amount of researches have been made in the area of sentiment classification, and most of them focus on large pieces of text, for example, reviews [5]. However, Tweets are different from reviews in several aspects: (1) The length of tweets is limited to 140 characters, and there are usually more words in a review;  (2) Tweets contain a variety of topics while reviews are always tailored to a specific topic; (3) The words in tweets are more informal. Misspellings and slang are more likely to appear in tweets; (4) Reviews usually represent summarized thoughts of people, but tweets are more casual in contents. In general, tweets are not as thoughtfully composed as reviews. Nevertheless tweets could be a great source for analyzing sentiment. Twitter users are from different background, which enable us to collect a large amount of users from various social and interest group. Besides, tweets with different topics are generated every day and the tweets posted can grow dramatically, which makes it possible to collect sufficient training data for our model.    Define the Sentiment In our assumption, we define sentiment to be a personal feeling which is defined to be positive or negative. However, when a tweet shows no subjective emotion, it should be considered as neutral. Table 1 shows some examples of the definition of sentiment and Figure 1 shows the expression of these three types of sentiment.  Sentiment Tweets Positive What a wonderful day! Neutral I just arrived at the building Negative History exam studying? Urh… Table 1:  Example Tweets   Figure 1. Expression for Positive, Neutral and Negative  2 Picture source: http://blog.qburst.com/2012/08/sentiment-analysis-using-natural-language-processing/ Structure of Twitter Search Engine   Figure 2. Procedure of Our Twitter Search Engine  The procedure of building the Twitter Sentiment Search Engine involves several steps, and they can be listed as follows:   1. Collect Data For Training  2. Preprocess Data 3. Store and Index Data in the Database 4. Sentiment Analysis 5. Vector Space Modeling 6. Relevance Ranking 7. User Interface Design  Figure 2 shows the structure of the procedure for making our Twitter Search Engine. Each step mentioned above will be discussed in detail in the following sections.        3 RELATED WORK A large amount of research has been made in sentiment analysis. Pang et al [5] have analyzed the performance of different classifiers on movie reviews. The work of them has served as a baseline model, and many researchers have used the techniques mentioned in their paper across different domains. Yang et al [4] use web-blogs to construct corpora for sentiment analysis and use emotion icons assigned to blog posts as indicators of users’ mood [6]. Researchers also analyzed the brand impact of micro-blogging [4]. However, their sentiment analysis is based on polarity analysis and neutral class is not considered. In twitter sentiment analysis, we often label a tweet as either positive or negative. Some researchers consider that the objective sentences of the text are less informative so they ignore those sentences and focus on subjective statement in order to improve the binary classification [10]. However, some suggest that three categories (positive, negative, neutral) should be identified, and the introduction of neutral category can improve the overall performance [12].  Go et al [1] has tried different machine learning algorithms to train classifiers for sentiment analysis. Their training data are based on tweets. The methods they use are quite similar to the techniques used by Pang and Lee et al[5 on movie review. Unlike previous researchers, they take neutral class into account. It turns out that the best result is obtained by Naïve Bayes and they can get up to 81% accuracy on their test data.                      4 METHODOLOGY Collect Data For Training As mentioned above, our sentiment analysis will be based on tweets downloaded from twitter. Using tweets has two advantages. First, tweets can be easily obtained using twitter stream API, and they usually are in JSON format, which will be convenient for us to extract the data we need. Besides, tweets obtained from the twitter API are up-to-date. This enables us to find out some interesting results like how people feel about the recent events.  To extract a large number of tweets, the Twitter API is very helpful [1], especially the Twitter Streaming APIs, which give us low latency access to Twitter’s global stream of tweet data. Twitter has three streaming options available. They are public stream, user stream, and site stream. However, the user stream and the site stream are really limited to some particular purposes because the user stream only contains the data corresponding with a single user’s view of Twitter, and the site stream is just a multi-user version of the user stream. Therefore, the public stream that contains public data flowing through Twitter is more suitable for our project. In the public streams, Twitter provides two kinds of endpoints. One is a filter that returns public statuses by matching one or more filter predicates. The other is a sample that returns a small random sample of all public statuses. Generally, the sample endpoint gives us an overall picture, but it only has 1% of the tweets, so if there are 3000 relevant tweets, we only get 30 of them using the sample endpoint. With the filter endpoint, we can feed in a set of keywords and then get more relevant data.  It is worth mentioning that, to extract and analyze large amounts of tweets using the Streaming API, not only an authentication for extracting tweets is required by Twitter, but also we need to fully build a database (such as SQL) server to store all those tweets. In our project, 100,000 tweets are collected and stored in the database.  Preprocess Data Before training our data for sentiment analysis, we need to clean them. Tweets contain lots of informal words. For instance, “They are reeeeeeeeeeeeeally awesome!!!”.  In this sentence, there are repeated character ‘e’ in the word ‘really’, people will show their excitement by saying so. However such casual language will be difficult to analyze. To clean data, we will typically deal with stop words, emoticon, repeated characters and URL links in tweets. This step is also called feature reduction [2].  3.2.1 Feature Reduction As we will use words in the tweet as features, we need to reduce features by excluding stop words and emoticon, substituting URL link, Username and repeated character. In general, feature reduction can be divided into two steps: exclusion and substitution.   5 Exclusion Stop words are words filtered out before or after processing of natural language data [13]. Words like ‘is’, ‘and’, ‘a’ belong to stop words. In our stop words list, there are 423 words. Stop words are used frequently but they are not indicative of sentiment. Besides, as we need to calculate the sentiment score of each tweet based on keyword, emoticons like ‘:)’, ‘:(’ will be stripped off also. Go et al also find that emoticons will have a negative impact on the performance of machine learning classifiers [13].   Substitution We also need to substitute some components in tweets to make the calculation of sentiment score faster. In tweets, URL link and @username appear frequently. However, just like stop words they do not provide any valuable information about sentiment. We will substitute url links with URL and @username with USER. Besides, as mentioned above, some informal words contain repeated characters and we should also substitute these words with their proper form. One thing worth mentioning is that the word ‘USER’ and ‘URL’ will be added to our stop words list after the substitution step is done.  3.2.2 Calculate Sentiment Score Once tweets are all preprocessed, we can calculate the sentiment score for each tweet. We will label a tweet as sentiment if the sentiment score is greater than 0, and negative if the score is less than 0. We will also include neutral class in our model, so tweet will be labeled as neutral if the sentiment score is 0. Besides, the sentiment score will also be useful for our relevance ranking. The way we calculate the sentiment score is based on AFINN-111 [8]. It is a list of English words rated for valence with an integer ranging from -5 to 5. The words in the AFINN-111 have been manually labeled by Finn in 2009-2011. AFINN-111 is the newest version available and it contains 2477 words and phrases.  To get the sentiment score, we will scan through every processed word in a tweet and sum up the score of those words that are in the AFINN-111.  Store and Index Data in the Database This part is quite straightforward. We simply store our 100,000 processed tweets in the MySQL database and index them based on their order. For each record, the first column will always be the sentiment score, and the second column is the tweet.  Sentiment Analysis This is the most challenging and important part of our project.  Figure 3 below show the workflow chart of sentiment analysis. As shown in the workflow chart, before training our classifier model, we need to extract features of the tweet. We will use unigrams and bigrams as the features of our model and machine learning algorithms like Naïve Bayes and Maximum Entropy will be implemented to train our model.  6   Figure 3. Workflow of Sentiment Analysis Picture source: http://nltk.googlecode.com/  3.4.1 Feature Extraction Unigram Extracting unigrams from a tweet is the easiest way to get a feature. After tweets are processed, we simply parse a tweet into words, and every single word is a unigram.   Bigram We will also use bigrams to parse the tweet. The advantage of bigrams over unigrams is that bigrams are more eligible to handle negated phrases like “not well”, “never like”. If parsing such phrases with unigrams, we can’t accurately get their sentiment. In our experiment, we show that accuracy is significantly improved when bigrams are used.  3.4.2 Machine Learning Algorithms 3.4.2.1 Naive Bayes The Naïve Bayes Algorithm is used widely for document classification. The method is named so because it is based on Bayes’ theorem, which assumes that each word in the document is statistically independent with the next word [7]. At first glance, such assumption does not seem to be correct. For instance, “England” is more likely to follow the word “Queen [of]” than other words. Though such assumption is naïve, studying words separately may yield good results [1].  The reason Naïve Bayes works well is that it de-correlates the frequency of a word seen in a document from its statistical importance. To understand this, think of the word “a”. This word is so common that it can be found in many languages. Assuming that you have an English corpora training set, the probability that ‘a’ appears in the data set can be up to 100%. However, we cannot conclude that documents containing ‘a’ are English. By using Bayes’ theorem, we now convert the probability that ‘a’ appears in English documents (which can be 100%) to probability that the document is English given ‘a’ is in it (which  7 may be 50% or less).  In this way, common words found everywhere are assigned weaker weight and unique words that appear in the document will be assigned stronger weight. This makes sense because we are not interested in common words such as ‘this’, ‘the’, ‘can’, etc.  The assumption of Naïve Bayes is simple and the algorithm is easy to implement. This is the advantage of using Naïve Bayes to analyze sentiment. The formula of Naïve Bayes in our model can be written as  P(s | T) = P(s) * P(T | s) /P(T)   Where s denotes sentiment, T is for tweets. However, one problem of this formula is that P(T) can be zero if there are no related tweets found given the query. Though such case is rare, but if it happens, we cannot use Naïve Bayes in our model. One way to fix this is Laplace Smoothing, in which we add one to each count [1].  3.4.2.2 Maximum Entropy  Maximum Entropy (MaxEnt) is another useful method for Text Classification and Sentiment Analysis. MaxEnt is based on the maximum principle entropy. The idea behind MaxEnt is that when nothing is known, the probability distribution should be as uniform as possible, that is, have maximal entropy [10]. The purpose of choosing the most ‘uniform’ model is that we want to minimize the risk of prediction. When we want to make prediction, the safest way is to assume the distribution of what we are uncertain is uniform and the prediction should satisfy the given constraint. For instance, when we roll a dice, if we are told that we have 1/3 probability to get the number 4, intuitively, we will guess the probability of the other number is 2/15 respectively. Here, the given constraint is the probability of 4 appears, which is 1/3, and the situation that we are uncertain is the probability of other five number appears, which is (1-1/3)/5 =2/15.  Unlike Naïve Bayes, MaxEnt does not make any assumption of the independence of its features. This is particularly meaningful for documentation classification. Therefore, features like bigrams and phrases can be added to MaxEnt without worrying about overlapping features. This is one of advantage of Maximum Entropy against Naïve Bayes.  The formula for Maximum Entropy can be written as:   8   Where λi is a weight vector, which measures the significance of feature in the classification, x is the tweet in our model and y is the class. The larger the value of weight is, the higher the probability that feature x should be labeled as class y.   To build a training model with maximum entropy, I use the python NLTK package, which is developed by Stanford NLP group. Stochastic gradient descent is applied to train the weight.  In theory, Maximum Entropy handles feature overlap better than Naïve Bayes does, so Maximum Entropy is supposed to performs better. However, Naïve Bayes can still perform well on lots of problems [10], which is also consistent with our result. We will discuss it further in our Experiment section.  Vector Space Modeling  The Vector Space model is a simple but effective text retrieval model based on linear algebra [11]. Compared with the Boolean model of information retrieval that only returns the binary relevant documents, the Vector Space model can computer the degrees of similarity between queries and documents, therefore allowing for the terms to be ranked and partially matched. The Vector Space model uses algebraic vectors as identifiers in order to represent user queries and text documents. The vectors are then used to filter, index, and rank for the information retrieval system.  On the other hand, the disadvantages of using the Vector Space model are as follows: • Search keywords must precisely match document (false positive matching) • Documents with similar context but different term vocabulary won't be associated (false negative matching) • The order in which terms appear in the document is lost in the vector space  To implement the relevancy rankings, we adopted the term frequency-inverse document frequency model (TF-IDF), which is a classic and well-known vector space model proposed by Salton, Wong, and Yang [14]. Generally, documents are represented as vectors. In each vector, each term takes one dimension. Each dimension is weighted by term frequency and inverse document frequency. Specifically, if a term appears multiple  9 times in the document, the TF-IDF model will reward a high weight to that dimension. On the other hand, if a term is frequent in the collection, the TF-IDF model will offer a low weight to that dimension.  3.5.1 Architecture Step 1: Construct the term-document incidence matrix The construction of the term-document incidence matrix lists the documents as the columns and term as the rows of the matrix. The intersection of the row and column will be equal to the value 1 if the term occurs in the document and 0 if the term does not.  After the construction of the term-document incidence matrix, each term (row) can be assigned a Boolean value that can be used for the user's search.  Step 2: Construct an inverted index, with frequency information a) Collect the documents to be indexed b) Tokenize the text, turning each document into a list of tokens c) Do linguistic preprocessing, producing a list of normalized tokens, which are the indexing terms. d) Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings  Figure 4 shows a sample of an inverted index construction. Specifically, for example, the term “be” was mentioned 6 times in 4 different documents - Document 1mentioned it once, Document 2 mentioned it twice, Document 3mentioned it once, and Document 4 mentioned it twice.   Figure 4. Sample of Inverted Index Construction  Step 3: Calculate the inverse document frequency (IDF) for each term The inverse document frequency is  idft = log10 ( N / dft ).  10  Where N refers to the total number of documents and dft refers to the number of documents containing term t. The calculation of the IDF allows for the document vectors to be taken column wise. Once the document vectors are defined, we can calculate the cosine similarity for each term in order to filter, index, and rank the terms for an information retrieval system.  Step 4: Calculate the TF-IDF weight The TF-IDF weight of a term is the product of its TF weight and its IDF weight.  wt,d = tft,d * idft  Where tft is the frequency of term t in document d (how many times term t occurs in document d) and idft is the inverse document frequency of term t defined for the collection of documents (how many documents have term t).  Step 5: Calculate the cosine similarity score between query and candidate documents In order to calculate the cosine similarity, the following processes must be completed: a) Compute the vector magnitude or length for each corresponding document vector by taking the square root of sum of squared of individual terms in the document vector. b) Identify candidate documents for incoming query c) Compute the vector magnitude or length of the query vector    Cosine similarity is calculated by multiplying corresponding terms of documents (dot product) and normalization is done by dividing dot product with product of magnitudes.  Step 6: Sort the candidate documents base on the cosine similarity scores  Relevance Ranking Generally, the purpose of ranking documents is to return the most important information (e.g., the most relevant document with respect to the query) to the user at the boldest position of the web page. Therefore, back to our Twitter sentiment search engine, ranking each tweet is not only based on the score of the cosine similarity, but also take the sentiment score into account. To balance and combine the cosine similarity score of a tweet with its sentiment score, we introduce the sentiment proportion to our search engine. Particularly, we calculate the overall ranking score for each tweet using the following equation.   11  OS = p * |SS| + (1 - p) * CSS (0 ≤ p ≤ 1)  Here, OS means the overall ranking score of a tweet; p refers to the sentiment proportion; SS means the tweet’s sentiment score; CSS means the tweet’s cosine similarity score. However, this intuitive but sensible ranking method raises two problems.  Problem 1: How to set the value of the sentiment proportion It is difficult and unreasonable for us to set a fixed value to the sentiment proportion. To personalize our search engine, we design a drop-down list to let users feel free to select the sentiment proportion value from 0% to 100%. Therefore, for examples, if users are concerned only with the sentiment of the tweets and want to see the strongest sentimental tweet at first, they can select 100% sentiment proportion when searching queries; if users only care about the relevance of the tweets with respect to the query and hope to see the most relevant tweet at first, they can set the sentiment proportion to 0%. In summary, this solution allows users to balance the sentiment scores and the relevance scores of the tweets by themselves for the corresponding results they desire.  Problem 2: The sentiment scores and the cosine similarity scores are in different scales Actually, because the relevance of a tweet to the query is calculated by the cosine, the cosine similarity score, or called relevance score, is always between 0 and 1. However, the sentiment score is based on the sentiment dictionary - each word has a specific sentiment value in the dictionary from -5 (most negative sentiment) to +5 (most positive sentiment), so if a tweet is able to have infinite words, the sentiment score will be on a scale of -∞ to +∞. Without identical scales, we cannot balance the cosine similarity score with the sentiment score. That is because the sentiment score is so significant that it will dominate the overall ranking score since it can be much larger than the cosine similarity score. To solve this problem, we design a method to normalize the sentiment scores and make them on the scale of -1 to +1. Specifically, after finding all the candidate tweets that include at least one keyword in the query, the search engine will try to obtain the maximum absolute value of the sentiment score among those candidate tweets. Then divide every candidate tweet’s sentiment score by that maximum absolute value. Therefore, at this point, every sentiment score is on the scale of -1 to +1.  User Interface Design We use PHP to create the Twitter sentiment search engine. The feature search algorithm takes regular expressions to parse user queries. According to the user input, our search engine constructs a list of relevant tweets. Specifically, users type one or more keywords to query which tweets include at least one of the input keywords. On the other hand, users also need to select a sentiment proportion from the drop-down list when they search a query, so the search engine will show the tweet with highest overall ranking score at the top of the list and the tweet with lowest overall ranking score at the bottom of the list. As mentioned before, the overall ranking score is calculated based on the cosine similarity score of each tweet to the query, the corresponding normalized sentiment score, and the  12 sentiment proportion that users set. For each tweet in the list, if it expresses a positive sentiment (the sentiment score is greater than zero), this tweet will be displayed in a green frame with a green background; if it expresses a negative sentiment (the sentiment score is lower than zero), this tweet will be displayed in a red frame with a red background; if it expresses a neutral sentiment (the sentiment score is equal to zero), this tweet will be displayed in a grey frame with a white background. From the data structure perspective, we use arrays to record the query vector and the candidate tweet vectors. The time complexity of constructing a list of ranked tweets is dominated by the computation of the similarity between the query vector and each candidate tweet vector, and hence is equal to: O(Number of candidate tweets * Number of terms per tweet).  According to the user input, our search engine also constructs a sentiment analysis summary for that input. Inspired by Welbourne et al. [15], we count the number of positive sentiment tweets, negative sentiment tweets, and neutral sentiment tweets in the database, and then use Google Line Chart and Google Pie Chart to represent those numbers and their percentages.                 13 EXPERIMENT We crawl 100,000 tweets using the twitter stream API. The API has a parameter that specifies what kind of language to retrieve the tweets in. In our project, we just focus on tweet in English. We set up our URL parameter in the API as follows: ‘http://stream.twitter.com/1.1/statuses/sample.json?language=en&result_type=recent’. This URL shows that we collect tweets using the twitter stream API 1.1 version, and the raw tweet obtained will be in JSON format. The language parameter is equal to ‘en’ and result_type is recent. That means we will only extract tweets in English that are newly posted.  The detail of our training data and test data is shown in table 2 below. The training dataset consists of 21657 tweets, in which 9812 are labeled as positive and 8628 are negative. The number of neutral tweets is much less than others. 5000 tweets are randomly chosen from our 100,000 tweets that are not used for training classifier.    Dataset Positive Neutral Negative Total Training 9812 3217 8628 21657 Test Randomly Chosen 5000 Table 2. Training Data and Test Data  We know that not all words in a tweet offer valuable information for sentiment analysis. Extracting all the features without filtering will make the computation and analysis run quite slowly. To avoid such an issue, we need to make the feature vector sufficiently small. As mentioned previously, feature reduction can take this role. Table 3 below shows the effects of feature reduction in our training dataset. The number of feature turns out to be down to 9.1% of the original number of features. With such a “small number” of features, we can train our classifier efficiently.  After Feature Reduction Step Number of Features Percentage ------- 281687 100% Substitution 112191 39.8% Exclusion 25877 9.19% Final 25877 9.19% Table 3. Effects of Feature Reduction     14 RESULTS 5.1 Classifier Performance After training our classifier with Naïve Bayes and Max Entropy, we compare their accuracy given the 5000 test data. The result of the comparison is shown in table 4. It is not surprising that training classifier with bigrams perform better than that with unigrams. As bigrams use two consecutive words in a tweet as feature, it can deal with double negation and negated phrases. Another interesting of our finding is that Naïve Bayes always performs better than Maximum Entropy in our experiment.  Unlike Naïve Bayes, Maximum Entropy makes no assumption about the independence between words, so it should perform better than Naïve Bayes. Our result is consistent with what Nigam et al [10] proposed in 1999.  We think two possible reasons may explain this situation. For one thing, inter-correlation among predictor variables is not as strong as we think. Thus the assumption of independence made by Naïve Bayes may have limited impact on the performance. On the other hand, Zhang [9] proposed that Naïve Bayes is good not only when features are independent, but also when dependencies of features from each other are similar between features. In our randomly chosen test data, we have striped off emoticons, stop words and substituted repeated character. After feature reduction, dependencies in our feature vector could be quite similar especially when we only focus on plain text.  Features  Naïve Bayes Max Entropy Unigrams 63.42% 58.31% Bigrams 76.78% 69.02% Table 4.  Accuracy of Classifiers  5.2 Search Engine Demo In this section, we demonstrate our proposed Twitter sentiment search engine. Specifically, we focus on the retrieval of tweets based on two different queries: query with a known sentiment and query with an unknown sentiment.  Figure 5 shows the interface of our Twitter sentiment search engine. The input form consists of three parts: a one-line input field for text input, a drop-down list for sentiment proportion selection, and a Search button for submitting the form. Users can type a query in the input field, select a sentiment proportion from the drop-down list, and then click the Search button to retrieve tweets.   15  Figure 5. Twitter Sentiment Search Home Page  Figure 6 shows the result page according to the query “glad” with 50% sentiment proportion. The search engine returned 137 tweets that include the term “glad”. As everyone knows, the term “glad” should be a positive word, so the sentiment analysis summary shows that 91% of the results expressed the positive sentiment. Also, the background color of the top tweets is green, which means the sentiment of those tweets is positive. In addition, in each frame of a tweet, we can see some details of the tweet, such as the user who posted the tweet, the content of the tweet, the rank, the total score, the cosine similarity score, the sentiment score, and the posted date.   Figure 6.  Query is “glad” with 50% Sentiment Proportion   16 Figure 7 shows the result page according to the query “sad” with 50% sentiment proportion. Though the term “sad” is a known negative word, there is still a certain amount of results (15%) that expressed a positive sentiment. That probably means people sometimes use negative words for joking.    Figure 7. Result Page for the Query “sad” with 50% Sentiment Proportion  Figure 8 shows the result page according to the query “black friday” with 70% sentiment proportion. It is hard to imagine the sentiment of the query “black friday” before searching. Then the result page provided some information: among the 1133 relevant tweets, 44% of them (499 tweets) expressed the neutral sentiment, 39% of them (437 tweets) expressed the negative sentiment, and 17% of them (197 tweets) expressed the positive sentiment. This can be interpreted that when we mention “black friday”, most of people would have either neutral or negative sentiment.  17  Figure 8. Query is “black friday” with 70% Sentiment Proportion                         18 CONCLUSION Micro-blogging nowadays became one of the major types of the communication [3]. The information contained in tweets makes them an attractive source of data for opinion mining and sentiment analysis. In our project, we build a sentiment-based search engine that enables users to check the sentiment of his query. We train our model by using two different features: unigrams and bigrams. Instead of using unigrams, which uses each word in the tweet as a feature, we find that bigrams will result in better performance. As bigrams usually contain more information about the feature than unigrams.   Besides, in our experiment, Naïve Bayes performs better than Maximum Entropy. For either feature (unigrams and bigrams) used in our training model, the Naïve Bayes classifier always achieves higher accuracy than Maximum Entropy. This is consistent with what Nigam et al found [10].  The term frequency-inverse document frequency model is a classic and well-known vector space model. In our Twitter sentiment search engine, the performance of this model is sensible. However, the TF-IDF model is a very old model. It may be not robust enough to fulfill the requirement of today’s search engines. Moreover, the TF-IDF model is used to rank documents that have certain long content (e.g. blog and news). It may not be good enough to deal with short document (such as tweets) rankings. Therefore, in the future, we will try to find a more robust and powerful ranking algorithm to judge the relevance of a tweet to the user’s query.  Letting users select the sentiment proportion is a convenient and personalized method to balance sentiment weight and cosine similarity weight. However, sometimes, users are not quite sure what sentiment proportion they should set. They would prefer the search engine to help them select the best sentiment proportion. Therefore, for that point, maybe we should introduce some machine learning technologies to our search engine. In addition, the equation for the overall ranking score is constructed based on our intuition. We still need to find a more effective method to balance and combine the relevance score of a tweet with its sentiment score.            19 FUTURE WORK The overall performance of our classifier is not satisfactory. We think in the future we can improve our model in several aspects.  Larger Dataset   One possible reason for the unsatisfactory performance of our model is that our training data only contains 21657 tweets. Larger dataset will cover a wider range of words appearing in tweets, which results in better feature vector.   Feature Extraction   Among the n-grams features, unigrams and bigrams are the simplest feature we can obtain from corpora. However, if we try more complex feature such as trigram, our model may achieve higher accuracy. One potential flaw of using complex feature is that sometimes the length of the tweet may be less than three words. Therefore trigrams will be of no use. However, we can avoid such problem by only collecting tweets whose length is long enough for analysis.  Emoticons    Emotions are not included in the feature vectors. In fact, emoticons can also be a valuable source for sentiment analysis.  Internationalization We only collect English tweets in our project. However, there are lots of twitter users from non-English speaking countries. Sentiment analysis applicable in other languages.  Besides, our work could be more promising if our sentiment analysis is based on real-time data rather than static data.  Real-time Data   Even though we have 100,000 tweets stored in the database, it is still likely that a user may fail to check sentiment of his query. The number of tweets increases dramatically fast and many new words and topics are generated each day. Storing a limited number of static tweets in the database will not meet users’ need. One way to solve such a problem is to use twitter search API. By twitter search API, we can instantly retrieve sufficient real time tweets for analysis. Analyze the sentiment in a dynamic way may also lead us to detect sentiment trend for an event.          20 REFERENCES [1] Alec Go, Lei Huang and Richa Bhayani. 2009. Twitter Sentiment Analysis. Final Projects from CS224N for Spring 2008/2009 at The Stanford Natural Language Processing Group [2] Alec Go, Richa Bhayani, Lei Huang. Twitter Sentiment Classification using Distant Supervision. Technical report, Stanford Digital Library Technologies Project, 2009. [3] Alexander Pak, Patrick Paroubek. 2010. Twitter as a Corpus for Sentiment Analysis and Opinion Mining. In Proceeding of the Seventh conference on International Language Resources and Evaluation. [4] B. Jansen, M. Zhang, K. Sobel, A. Chowdury. The Commerical Impact of Social Mediating Technologies: Micro-blogging as Online Word-of-Mouth Branding, 2009. [5] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86, 2002. [6] Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi Chen. 2007. Emotion classification using web blog corpora. In WI ’07: Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence, pages 275–278, Washington, DC, USA. IEEE Computer Society. [7] Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008 [8] Finn Årup Nielsen, A new ANEW: Evaluation of a word list for sentiment analysis in microblogs, Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings: 93-98. 2011 May.   [9] Harry Zhang, “The Optimality of Naive Bayes”.  In FLAIRS Conference (2004)  [10] K. Nigam, J. Lafferty, and A. Mccallum. Using maximum entropy for text classification. In IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61–67, 1999. [11] Lee, Dik L., Huei Chuang, and Kent Seamons. \"Document ranking and the vector-space model.\" Software, IEEE 14.2 (1997): 67-75. [12] Moshe Koppel and Jonathan Schler. 2005. The Importance of Neutral Examples for Learning Sentiment. Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations (FINEXIN) [13] Rajaraman, A.; Ullman, J. D. (2011). \"Mining of Massive Datasets\". pp. 1–17. doi:10.1017/CBO9781139058452.002 [14] Salton, Gerard, Anita Wong, and Chung-Shu Yang. \"A vector space model for automatic indexing.\" Communications of the ACM 18.11 (1975): 613-620. [15] Welbourne, Evan, et al. \"Building the internet of things using RFID: the RFID ecosystem experience.\" Internet Computing, IEEE 13.3 (2009): 48-55. ",
      "id": 33899585,
      "identifiers": [
        {
          "identifier": "10.15781/t2ss51",
          "type": "DOI"
        },
        {
          "identifier": "oai:repositories.lib.utexas.edu:2152/32489",
          "type": "OAI_ID"
        },
        {
          "identifier": "211337086",
          "type": "CORE_ID"
        }
      ],
      "title": "Search engine For Twitter sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:repositories.lib.utexas.edu:2152/32489"
      ],
      "publishedDate": "2015-11-16T18:06:35",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://repositories.lib.utexas.edu/bitstream/2152/32489/1/CHEN-MASTERSREPORT-2015.pdf"
      ],
      "updatedDate": "2023-12-13T03:53:50",
      "yearPublished": 2015,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/211337086.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/211337086"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/211337086/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/211337086/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/33899585"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1904.02839",
      "authors": [
        {
          "name": "Augenstein, Isabelle"
        },
        {
          "name": "Cotterell, Ryan"
        },
        {
          "name": "Hoyle, Alexander"
        },
        {
          "name": "Wallach, Hanna"
        },
        {
          "name": "Wolf-Sonkin, Lawrence"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/322819664"
      ],
      "createdDate": "2019-06-02T02:00:56",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 1238,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1238",
          "logo": "https://api.core.ac.uk/data-providers/1238/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "When assigning quantitative labels to a dataset, different methodologies may\nrely on different scales. In particular, when assigning polarities to words in\na sentiment lexicon, annotators may use binary, categorical, or continuous\nlabels. Naturally, it is of interest to unify these labels from disparate\nscales to both achieve maximal coverage over words and to create a single, more\nrobust sentiment lexicon while retaining scale coherence. We introduce a\ngenerative model of sentiment lexica to combine disparate scales into a common\nlatent representation. We realize this model with a novel multi-view\nvariational autoencoder (VAE), called SentiVAE. We evaluate our approach via a\ndownstream text classification task involving nine English-Language sentiment\nanalysis datasets; our representation outperforms six individual sentiment\nlexica, as well as a straightforward combination thereof.Comment: To appear in NAACL-HLT 201",
      "documentType": "research",
      "doi": "10.18653/v1/n19-1065",
      "downloadUrl": "https://core.ac.uk/download/322819664.pdf",
      "fieldOfStudy": null,
      "fullText": "Combining Sentiment Lexica with a\nMulti-View Variational Autoencoder\nAlexander Hoyle@ Lawrence Wolf-SonkinS Hanna WallachZ\nRyan CotterellH Isabelle AugensteinP\n@University College London, London, UK\nSDepartment of Computer Science, Johns Hopkins University, Baltimore, USA\nZMicrosoft Research, New York City, USA\nHDepartment of Computer Science and Technology, University of Cambridge, Cambridge, UK\nPDepartment of Computer Science, University of Copenhagen, Copenhagen, Denmark\nalexander.hoyle.17@ucl.ac.uk, lawrencews@jhu.edu\nhanna@dirichlet.net, rdc42@cam.ac.uk, genstein@di.ku.dk\nAbstract\nWhen assigning quantitative labels to a\ndataset, different methodologies may rely on\ndifferent scales. In particular, when assigning\npolarities to words in a sentiment lexicon,\nannotators may use binary, categorical, or\ncontinuous labels. Naturally, it is of interest to\nunify these labels from disparate scales to both\nachieve maximal coverage over words and to\ncreate a single, more robust sentiment lexicon\nwhile retaining scale coherence. We introduce\na generative model of sentiment lexica to\ncombine disparate scales into a common latent\nrepresentation. We realize this model with\na novel multi-view variational autoencoder\n(VAE), called SentiVAE. We evaluate our\napproach via a downstream text classification\ntask involving nine English-Language sen-\ntiment analysis datasets; our representation\noutperforms six individual sentiment lexica, as\nwell as a straightforward combination thereof.\n1 Introduction\nSentiment lexica provide an easy way to automat-\nically label texts with polarity values, and are also\nfrequently transformed into features for supervised\nmodels, including neural networks (Palogiannidi\net al., 2016; Ma et al., 2018). Indeed, given their\nutility, a veritable cottage industry has emerged\nfocusing on the design of sentiment lexica. In prac-\ntice, using any single lexicon, unless specifically\nand carefully designed for the particular domain\nof interest, has several downsides. For example,\nany lexicon will typically have low coverage\ncompared to the language’s entire vocabulary,\nand may have misspecified labels for the domain.\nIn many cases, it may therefore be desirable to\ncombine multiple sentiment lexica into a single\nrepresentation. Indeed, some research on unifying\nFigure 1: A depiction of the “encoder” portion of Sen-\ntiVAE. The word peppy has polarity values of 0.65 and\npos in the SenticNet and Hu-Liu lexica, respectively.\nThese values are “encoded” into two three-dimensional\nvectors, which are then summed and added to (1, 1, 1)\n(not shown) to form the parameters of a Dirichlet over\nthe latent representation of the word’s polarity value.\nsuch lexica has emerged (Emerson and Declerck,\n2014; Altrabsheh et al., 2017), borrowing ideas\nfrom crowdsourcing (Raykar et al., 2010; Hovy\net al., 2013). However, this is a non-trivial task,\nbecause lexica can use binary, categorical, or\ncontinuous scales to quantify polarity—in addition\nto different interpretations for each—and thus\ncannot easily be combined. In Fig. 1, we show an\nexample of the same word labeled using different\nlexica to illustrate the nature of the challenge.\nTo combine sentiment lexica with disparate\nscales, we introduce SentiVAE, a novel multi-\nview variant of the variational autoencoder (VAE)\n(Kingma and Welling, 2014). SentiVAE, visualized\nas a graphical model in Fig. 2, differs from the orig-\ninal VAE in two ways: (i) it uses a Dirichlet latent\nvariable (rather than a Gaussian) for each word in\nthe combined vocabulary, and (ii) it has multiple\nemission distributions—one for each lexicon. Be-\ncause the latent variables are shared across the lex-\nar\nX\niv\n:1\n90\n4.\n02\n83\n9v\n1 \n [c\ns.C\nL]\n  5\n A\npr\n 20\n19\nLexicon Source N Dom\nSentiWordNet WordNet 14107 [−1, 1]2\nMPQA Newswire 4397 {0, 1}\nSenticNet — 100000 [−1, 1]\nHu-Liu Product reviews 6790 {0, 1}\nGI — 4206 {0, 1}\nVADER Social media 7489 {0, . . . , 8}10\nTable 1: Descriptive statistics for the sentiment lexica.\nN : vocabulary size. Dom: Domain of polarity values.\nica, we are able to derive a common latent represen-\ntation of the words’ polarities. The resulting model\nis spiritually related to a multi-view learning ap-\nproach (Sun, 2013), where each view corresponds\nto a different lexicon. Experimentally, we use\nSentiVAE to combine six commonly used English-\nlanguage sentiment lexica with disparate scales.\nWe evaluate the resulting representation via\na text classification task involving nine English-\nlanguage sentiment analysis datasets. For each\ndataset, we transform each text into an average\npolarity value using either our representation, one\nof the six commonly used sentiment lexica, or a\nstraightforward combination thereof. We then train\na classifier to predict the overall sentiment of each\ntext from its average polarity value. We find that\nour representation outperforms the individual lex-\nica, as well as the straightforward combination for\nsome datasets. Our representation is particularly\nefficacious for datasets from domains that are not\nwell-supported by standard sentiment lexica.1\nThe existing research that is most closely re-\nlated to our work is SentiMerge (Emerson and De-\nclerck, 2014), a Bayesian approach for aligning\nsentiment lexica with different continuous scales.\nSentiMerge consists of two steps: (i) aligning the\nlexica via rescaling, and (ii) combining the rescaled\nlexica using a Gaussian distribution. The authors\nperform token-level evaluation using a single senti-\nment analysis dataset where each token is labeled\nwith its contextually dependent sentiment. Because\nSentiMerge can only combine lexica with continu-\nous scales, we do not include it in our evaluation.\n2 Sentiment Lexica and Scales\nWe use the following commonly used English-\nlanguage sentiment lexica: SentiWordNet (Bac-\ncianella et al., 2010), MPQA (Wilson et al., 2005),\nSenticNet 5 (Cambria et al., 2014), Hu-Liu (Hu and\n1Our representation and code are available at https://\ngithub.com/ahoho/SentiVAE.\nLiu, 2004), GI (Stone et al., 1962), and VADER\n(Hutto and Gilbert, 2014). Descriptive statistics for\neach lexicon are shown in Tab. 1. Each word in\nSentiWordNet is labeled with two real values, each\nin the interval [0, 1], corresponding to the strength\nof positive and negative sentiment (e.g., the label\n(0 0) is neutral, while the label (1 0) is maximally\npositive). Each word in VADER is labeled by ten\ndifferent human evaluators, with each evaluator pro-\nviding a polarity value on a nine-point scale (where\nthe midpoint is neutral), yielding a 10-dimensional\nlabel. MPQA, Hu-Liu, and GI all use binary scales.\nLastly, each word in SenticNet is labeled with a\nreal value in the interval [−1, 1], where 0 is neutral.\n3 SentiVAE\nWe first describe a figurative generative process for\na single sentiment lexicon d ∈ D, where D is a set\nof sentiment lexica. Imagine there is a true (latent)\npolarity value zw associated with each word w\nin the lexicon’s vocabulary. When the lexicon’s\ncreator labels that word according to their chosen\nscale (e.g., thumbs-up or thumbs-down, a real\nvalue in the interval [0, 1]), they deterministically\ntransform this true value to their chosen scale\nvia a function f( · ; θd).2 Sometimes, noise is\nintroduced during this labeling process, corrupting\nthe label as it leaves the ethereal realm and\nproducing the (observed) polarity label xwd . They\nthen add this potentially noisy label to the lexicon.\nGiven a lexicon of observed polarity labels, the\nlatent polarity values can be inferred using a VAE.\nThe original VAE posits a generative model of ob-\nserved data X and latent variables Z: P (X ,Z) =\nP (X | Z)P (Z). Inference of Z then proceeds by\napproximating the (intractable) posterior P (Z | X )\nwith a Gaussian distribution, factorized over the in-\ndividual latent variables. A parameterized encoder\nfunction compresses X into Z , while a parameter-\nized decoder function reconstructs X from Z .\nSentiVAE extends the original VAE model to\ncombine multiple lexica with disparate scales, pro-\nducing a common latent representation of the polar-\nity value for each word in the combined vocabulary.\nGenerative process. Given a set of sentiment\nlexica D with a combined vocabulary W , Senti-\nVAE posits a common latent representation zw of\nthe polarity value for each word w ∈ W , where zw\nis a three-dimensional categorical distribution over\n2Parameterized by lexicon-specific weights θd.\nαw\nzw\nρwd θd\nxwd\nw ∈ W d ∈ D\nFigure 2: Generative model for SentiVAE.\nthe sentiments positive, negative, and neutral.\nThe generative process starts by drawing each\nlatent polarity value zw from a three-dimensional\nDirichlet prior, parameterized by αw = (1, 1, 1):\nzw ∼ Dir(αw). (1)\nIf the word is uncontroversial,3 we spur this\nprior somewhat using the number of lexica in\nwhich the word appears c(w). Specifically, we\nadd c(w) to the parameter for the sentiment\nassociated with that word in the lexica, e.g.,\nαSUPERB = (1 + c(SUPERB), 1, 1). This has the ef-\nfect of regularizing the inferred latent polarity value\ntoward the desired distribution over sentiments.\nHaving generated zw, the process proceeds by\n“decoding” zw into each lexicon’s chosen scale.\nFirst, for each lexicon d ∈ D, zw is determinis-\ntically transformed via neural network f( · ; θd)\nwith a single 32-dimensional hidden layer,\nparameterized by lexicon-specific weights θd:\nρwd = f(z\nw;θd). (2)\nThe transformed value ρwd is then used to generate\nthe (observed) polarity label xwd for that lexicon:\nxwd ∼ Pd(xwd | ρwd ). (3)\nThe dimensionality of ρwd and the emission distribu-\ntion Pd are lexicon-specific. For SentiWordNet, Pd\n3We say that a word is uncontroversial if there is strong\nagreement across the sentiment lexica in which it appears.\nEven without this spurring, the inferred latent representation\ntypically separates into the three sentiment classes, but perfor-\nmance on our text classification task is somewhat diminished.\nDataset Source N Classes\nIMDB Movies 25000 2\nYelp Product reviews 100000 5 / 3\nSemEval Twitter 7668 3\nMultiDom Product reviews 6500 2\nACL Scientific reviews 248 5 / 3\nICLR Scientific reviews 2166 10 / 3\nTable 2: Descriptive statistics for the training portions\nof the sentiment analysis datasets. N : number of texts.\nis a two-dimensional Gaussian with mean ρwd and\na diagonal covariance matrix equal to 0.01I; for\nVADER, Pd consists of ten nine-dimensional cate-\ngorical distributions, collectively parameterized by\nρwd ; for MPQA, Hu-Liu, and GI, Pd is a Bernoulli\ndistribution, parameterized by ρwd ; and for Sen-\nticNet, Pd is a univariate Gaussian with mean and\nvariance each an element in a two-dimensional ρwd .\nInference. Inference involves forming the pos-\nterior distribution over the latent polarity values\nZ given the observed polarity labels X . Because\ncomputing the normalizing constant P (X ) is in-\ntractable, we instead approximate the posterior\nwith a family of distributions Qλ(Z), indexed by\nvariational parameters λ. Specifically, we use\nQλ(Z) =\n∏\nw∈W\nQβw(z\nw) =\n∏\nw∈W\nDir(βw). (4)\nTo construct βw, we first define a neural net-\nwork g(·; φd), with a single 32-dimensional hid-\nden layer, which “encodes” xwd into a three-\ndimensional vector. The output of this neural net-\nwork is then transformed via a softmax as follows:\nωwd = softmax\n(\ng(xwd ; φd)\n)\n(5)\nβw = 1 +\n∑\nd∈D\nωwd . (6)\nThe intuition behind βw can be understood by\nappealing to the “pseudocount” interpretation of\nDirichlet parameters. Each lexicon contributes ex-\nactly one pseudocount, divided among positive,\nnegative, and neutral, to what would otherwise be\na symmetric, uniform Dirichlet distribution. As a\nconsequence of this construction, words that ap-\npear in more lexica will have more concentrated\nDirichlets. Intuitively, this property is appealing.\nWe optimize the resulting ELBO objective (Blei\net al., 2017) with respect to the variational parame-\nters via stochastic variational inference (Hoffman\nIMDB 2C Yelp 5C Yelp 3C SemEval 3C MultiDom 2C ACL 5C ACL 3C ICLR 10C ICLR 3C\nSentiVAE EQ[zw] 72.7 49.8 57.5 46.0 70.8 66.7 73.3 92.6 87.0\nSentiVAE βw 73.4 49.7 59.4 52.2 74.7 73.3 80.0 92.6 86.5\nSentiWordNet 63.4 36.0 47.6 32.2 62.0 60.0 53.3 89.1 83.5\nMPQA 65.4 44.0 53.0 29.9 67.4 60.0 53.3 89.1 83.5\nSenticNet 60.5 38.4 43.4 37.2 62.3 60.0 53.3 89.1 83.9\nHu-Liu 67.2 46.6 56.4 31.5 69.4 60.0 53.3 89.1 83.5\nGI 58.4 40.7 47.9 31.3 61.6 60.0 53.3 89.1 83.5\nVADER 71.7 46.8 59.3 38.5 73.5 66.7 66.7 94.3 86.1\nCombined 75.6 51.0 64.1 50.6 75.4 66.7 66.7 93.9 86.1\nTable 3: Classification accuracies for our representation, six lexica, and a straightforward combination thereof.\net al., 2013) using Adam (Kingma and Ba, 2015)\nin the Pyro framework (Bingham et al., 2018). The\nstandard reparameterization trick used in the origi-\nnal VAE does not apply to models with Dirichlet-\ndistributed latent variables, so we use the general-\nized reparameterization trick of Ruiz et al. (2016).\n4 Experiments and Results\nTo evaluate our approach, we first use SentiVAE\nto combine the six lexica described in §2. For\neach word w in the combined vocabulary, we ob-\ntain an estimate of zw by taking the mean of\nQβw(z\nw) = Dir(βw)—i.e., by normalizing βw.\nWe compare this representation to using βw di-\nrectly, because βw contains information about Sen-\ntiVAE’s certainty about the word’s latent polar-\nity value. We evaluate our common latent rep-\nresentation via a text classification task involving\nnine English-language sentiment analysis datasets:\nIMDB (Maas et al., 2011), Yelp (Zhang et al.,\n2015), SemEval 2017 Task 4 (SemEval, Rosen-\nthal et al. (2017)), multi-domain sentiment analysis\n(MultiDom, Blitzer et al. (2007)), and PeerRead\n(Kang et al., 2018) with splits ACL 2017 and ICLR\n2017 (Kang et al., 2018). Each dataset consists of\nmultiple texts (e.g., tweets, articles), each labeled\nwith an overall sentiment (e.g., positive). Descrip-\ntive statistics for each dataset are shown in Tab. 2.\nFor the datasets with more than three sentiment la-\nbels, we consider two versions—the original and a\nversion with only three (bucketed) sentiment labels.\nFor each dataset, we transform each text into an\naverage polarity value using either our represen-\ntation, one of the six lexica,4 or a straightforward\ncombination thereof, where the polarity value for\n4We bucket the upper four and lower four points of\nVADER’s nine-point scale, to yield a three-point scale. With-\nout this bucketing, our representation outperforms VADER\non four of the nine datasets. We do not bucket VADER when\nusing it in SentiVAE or in the straightforward combination.\neach word in the (combined) vocabulary is a 16-\ndimensional vector that consists of a concatenation\nof polarity values. (Unlike SentiVAE, this concate-\nnation does not yield a single sentiment lexicon\nthat retains scale coherence, while achieving maxi-\nmal coverage over words.) Specifically, we replace\neach token with its corresponding polarity value,\nand then average the these values (Go et al., 2009;\nO¨zdemir and Bergler, 2015; Kiritchenko et al.,\n2014). We then use the training portion of the\ndataset to learn a logistic regression classifier to\npredict the overall sentiment of each text from its\naverage polarity value. Finally, we use the testing\nportion to compute the accuracy of the classifier.\nResults. The results in Tab. 3 show that our rep-\nresentation using βw outperforms the individual\nlexica for all but one dataset, and that our repre-\nsentation using the mean of Qβw(zw) outperforms\nthem for six datasets. This is likely because Senti-\nVAE has a richer representation of sentiment than\nany individual lexicon, and it has greater coverage\nover words (see Tab. 4). The results in Tab. 5 sup-\nport the former reason: even when we limit the\nwords in our representation to match those in an\nindividual lexicon, our representation still outper-\nforms the individual lexicon. Unsurprisingly, our\nrepresentation especially outperforms lexica with\nunidimensional scales. We also find that our rep-\nresentation outperforms the straightforward com-\nbination for datasets from domains that are not\nwell supported by the individual lexica (see Tabs. 1\nand 2 for lexicon and dataset sources, respectively).\nBy combining lexica from different domains, our\nrepresentation captures a general notion of senti-\nment that is not tailored to any specific domain.\n5 Conclusion\nWe introduced a generative model of sentiment\nlexica to combine disparate scales into a common\nIMDB SemEval Multi ICLR\nSentiVAE 70 64 81 71\nSentiWordNet 15 14 24 16\nMPQA 10 7 18 9\nSenticNet 40 39 53 45\nHu-Liu 7 5 13 5\nGI 8 7 15 6\nVADER 7 6 13 5\nTable 4: Coverage over words (percentage) by lexicon\nfor the training portions of four of the nine datasets.\nIMDB 2C SemEval 3C\nSV Lex SV Lex\nSentiVAE 74.7 – 72.4 –\nSentiWordNet 70.6 63.4 67.4 55.1\nMPQA 73.5 66.6 62.6 51.8\nSenticNet 74.4 60.9 72.1 59.5\nHu-Liu 73.6 68.4 59.1 51.1\nGI 71.4 59.3 63.8 54.0\nVADER 73.6 73.1 60.9 58.7\nTable 5: Classification accuracies for a 10% validation\nportion of two of the datasets. The first row, labeled\nSentiVAE, contains the classification accuracy for our\nrepresentation using βw. Subsequent (lexicon-specific)\nrows compare our representation (SV), restricted to the\nvocabulary of that lexicon, to the lexicon itself (Lex).\nlatent representation, and realized this model with\na novel multi-view variational autoencoder, called\nSentiVAE. We then used SentiVAE to combine six\ncommonly used English-language sentiment lex-\nica with binary, categorical, and continuous scales.\nVia a downstream text classification task involving\nnine English-language sentiment analysis datasets,\nwe found that our representation outperforms the\nindividual lexica, as well as a straightforward com-\nbination thereof. We also found that our represen-\ntation is particularly efficacious for datasets from\ndomains that are not well-supported by standard\nsentiment lexica. Finally, we note that our approach\nis more general than SentiMerge (Emerson and De-\nclerck, 2014). While SentiMerge can only combine\nsentiment lexica with continuous scales, SentiVAE\nis designed to combine lexica with disparate scales.\n6 Acknowledgements\nWe would like to thank to Adam Forbes for the de-\nsign of Fig. 1. We further acknowledge the support\nof the NVIDIA Corporation with the donation of\nthe Titan Xp GPU used to conduct this research.\nReferences\nNabeela Altrabsheh, Mazen El-Masri, and Hanady\nMansour. 2017. Combining Sentiment Lexicons of\nArabic Terms. In AMCIS. Association for Informa-\ntion Systems.\nStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-\ntiani. 2010. SentiWordNet 3.0: An Enhanced Lex-\nical Resource for Sentiment Analysis and Opinion\nMining. 10(2010):2200–2204.\nEli Bingham, Jonathan P. Chen, Martin Jankowiak,\nFritz Obermeyer, Neeraj Pradhan, Theofanis Kar-\naletsos, Rohit Singh, Paul A. Szerlip, Paul Horsfall,\nand Noah D. Goodman. 2018. Pyro: Deep Universal\nProbabilistic Programming. CoRR, abs/1810.09538.\nDavid M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.\n2017. Variational inference: A review for statisti-\ncians. Journal of the American Statistical Associa-\ntion, 112:859–877.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, Boom-boxes and Blenders:\nDomain Adaptation for Sentiment Classification. In\nProceedings of the 45th Annual Meeting of the As-\nsociation of Computational Linguistics, pages 440–\n447, Prague, Czech Republic. Association for Com-\nputational Linguistics.\nErik Cambria, Daniel Olsher, and Dheeraj Rajagopal.\n2014. SenticNet 3: A Common and Common-\nsense Knowledge Base for Cognition-driven Sen-\ntiment Analysis. In Proceedings of the Twenty-\nEighth AAAI Conference on Artificial Intelligence,\nAAAI’14, pages 1515–1521. AAAI Press.\nGuy Emerson and Thierry Declerck. 2014. Sen-\ntiMerge: Combining Sentiment Lexicons in a\nBayesian Framework. In Proceedings of Work-\nshop on Lexical and Grammatical Resources for\nLanguage Processing, pages 30–38. Association for\nComputational Linguistics and Dublin City Univer-\nsity.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twitter\nSentiment Classification using Distant Supervision.\nProcessing, pages 1–6.\nMatthew D. Hoffman, David M. Blei, Chong Wang,\nand John William Paisley. 2013. Stochastic varia-\ntional inference. Journal of Machine Learning Re-\nsearch, 14(1):1303–1347.\nDirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,\nand Eduard H. Hovy. 2013. Learning Whom to\nTrust with MACE. In Proceedings of the 2013 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1120–1130. Association\nfor Computational Linguistics.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the Tenth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pages 168–177.\nACM.\nC. J. Hutto and Eric Gilbert. 2014. VADER: A Parsi-\nmonious Rule-Based Model for Sentiment Analysis\nof Social Media Text. In Eighth International Con-\nference on Weblogs and Social Media (ICWSM-14).\nDongyeop Kang, Waleed Ammar, Bhavana Dalvi,\nMadeleine van Zuylen, Sebastian Kohlmeier, Ed-\nuard H. Hovy, and Roy Schwartz. 2018. A Dataset\nof Peer Reviews (PeerRead): Collection, Insights\nand NLP Applications. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 1647–1661. Association for Computational\nLinguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nDiederik P. Kingma and Max Welling. 2014. Auto-\nEncoding Variational Bayes. In Proceedings of the\nSecond International Conference on Learning Rep-\nresentations (ICLR).\nSvetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-\nhammad. 2014. Sentiment Analysis of Short Infor-\nmal Texts. Journal of Machine Learning Research,\n50:723–762.\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Tar-\ngeted Aspect-Based Sentiment Analysis via Embed-\nding Commonsense Knowledge into an Attentive\nLSTM. In AAAI, pages 5876–5883. AAAI Press.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher Potts.\n2011. Learning Word Vectors for Sentiment Analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nCanberk O¨zdemir and Sabine Bergler. 2015. A Com-\nparative Study of Different Sentiment Lexica for\nSentiment Analysis of Tweets. In Proceedings of the\nInternational Conference Recent Advances in Natu-\nral Language Processing, pages 488–496. INCOMA\nLtd. Shoumen, BULGARIA.\nElisavet Palogiannidi, Athanasia Kolovou, Fenia\nChristopoulou, Filippos Kokkinos, Elias Iosif, Niko-\nlaos Malandrakis, Haris Papageorgiou, Shrikanth\nNarayanan, and Alexandros Potamianos. 2016.\nTweester at SemEval-2016 Task 4: Sentiment Anal-\nysis in Twitter Using Semantic-Affective Model\nAdaptation. In Proceedings of the 10th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2016), pages 155–163. The Association for Com-\nputer Linguistics.\nVikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-\nardo Hermosillo Valadez, Charles Florin, Luca Bo-\ngoni, and Linda Moy. 2010. Learning From Crowds.\nJournal of Machine Learning Research, 11:1297–\n1322.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 Task 4: Sentiment Analysis in Twit-\nter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages 502–518, Vancouver, Canada. Association for\nComputational Linguistics.\nFrancisco R. Ruiz, Michalis K. Titsias, and David M.\nBlei. 2016. The Generalized Reparameterization\nGradient. In Advances in Neural Information Pro-\ncessing Systems, pages 460–468.\nPhilip J. Stone, Robert F. Bales, J. Zvi Namenwirth,\nand Daniel M. Ogilvie. 1962. The general inquirer:\nA computer system for content analysis and retrieval\nbased on the sentence as a unit of information. Be-\nhavioral Science, 7(4):484–498.\nShiliang Sun. 2013. A Survey on Multi-view Learning.\nNeural Computing and Applications, 23(7-8):2031–\n2038.\nTheresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005. Recognizing Contextual Polarity in Phrase-\nLevel Sentiment Analysis. In Proceedings of Hu-\nman Language Technology Conference and Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 347–354, Vancouver, British\nColumbia, Canada. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassification. In Advances in Neural Information\nProcessing Systems, pages 649–657.\n",
      "id": 58966289,
      "identifiers": [
        {
          "identifier": "oai:pure.atira.dk:publications/908a0dbc-3c85-4d18-a8a7-a08b6d256453",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1904.02839",
          "type": "OAI_ID"
        },
        {
          "identifier": "1904.02839",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "10.18653/v1/n19-1065",
          "type": "DOI"
        },
        {
          "identifier": "200821605",
          "type": "CORE_ID"
        },
        {
          "identifier": "322819664",
          "type": "CORE_ID"
        }
      ],
      "title": "Combining Sentiment Lexica with a Multi-View Variational Autoencoder",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:pure.atira.dk:publications/908a0dbc-3c85-4d18-a8a7-a08b6d256453",
        "oai:arxiv.org:1904.02839"
      ],
      "publishedDate": "2019-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://curis.ku.dk/ws/files/240629212/OA_Combining_Sentiment_Lexica.pdf",
        "http://arxiv.org/abs/1904.02839"
      ],
      "updatedDate": "2022-08-25T03:53:34",
      "yearPublished": 2019,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/322819664.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/322819664"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/322819664/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/322819664/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/58966289"
        }
      ]
    },
    {
      "acceptedDate": "2018-02-06T00:00:00",
      "arxivId": "1712.00732",
      "authors": [
        {
          "name": "Guo, Minyi"
        },
        {
          "name": "Hou, Min"
        },
        {
          "name": "Liu, Qi"
        },
        {
          "name": "Wang, Hongwei"
        },
        {
          "name": "Xie, Xing"
        },
        {
          "name": "Zhang, Fuzheng"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/466610688"
      ],
      "createdDate": "2017-12-15T09:10:02",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2018-01-01T00:00:00",
      "abstract": "In online social networks people often express attitudes towards others,\nwhich forms massive sentiment links among users. Predicting the sign of\nsentiment links is a fundamental task in many areas such as personal\nadvertising and public opinion analysis. Previous works mainly focus on textual\nsentiment classification, however, text information can only disclose the \"tip\nof the iceberg\" about users' true opinions, of which the most are unobserved\nbut implied by other sources of information such as social relation and users'\nprofile. To address this problem, in this paper we investigate how to predict\npossibly existing sentiment links in the presence of heterogeneous information.\nFirst, due to the lack of explicit sentiment links in mainstream social\nnetworks, we establish a labeled heterogeneous sentiment dataset which consists\nof users' sentiment relation, social relation and profile knowledge by\nentity-level sentiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework\nto extract users' latent representations from heterogeneous networks and\npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep\nautoencoders to map each user into a low-dimension feature space while\npreserving the network structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommendation in two\nreal-world datasets. The experimental results also prove the efficacy of SHINE\nin cold start scenario.Comment: The 11th ACM International Conference on Web Search and Data Mining\n  (WSDM 2018",
      "documentType": "research",
      "doi": "10.1145/3159652.3159666",
      "downloadUrl": "http://arxiv.org/abs/1712.00732",
      "fieldOfStudy": null,
      "fullText": "SHINE: Signed Heterogeneous Information Network Embedding\nfor Sentiment Link Prediction\nHongwei Wang∗\nShanghai Jiao Tong University\nShanghai, China\nwanghongwei55@gmail.com\nFuzheng Zhang\nMicrosoft Research Asia\nBeijing, China\nfuzzhang@microsoft.com\nMin Hou\nUniversity of Science and Technology\nof China, Hefei, Anhui, China\nhmhoumin@gmail.com\nXing Xie\nMicrosoft Research Asia\nBeijing, China\nxingx@microsoft.com\nMinyi Guo†\nShanghai Jiao Tong University\nShanghai, China\nguo-my@cs.sjtu.edu.cn\nQi Liu\nUniversity of Science and Technology\nof China, Hefei, Anhui, China\nqiliuql@ustc.edu.cn\nABSTRACT\nIn online social networks people often express attitudes towards\nothers, which forms massive sentiment links among users. Predict-\ning the sign of sentiment links is a fundamental task in many areas\nsuch as personal advertising and public opinion analysis. Previous\nworks mainly focus on textual sentiment classification, however,\ntext information can only disclose the “tip of the iceberg” about\nusers’ true opinions, of which the most are unobserved but implied\nby other sources of information such as social relation and users’\nprofile. To address this problem, in this paper we investigate how\nto predict possibly existing sentiment links in the presence of het-\nerogeneous information. First, due to the lack of explicit sentiment\nlinks in mainstream social networks, we establish a labeled het-\nerogeneous sentiment dataset which consists of users’ sentiment\nrelation, social relation and profile knowledge by entity-level sen-\ntiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding\n(SHINE) framework to extract users’ latent representations from\nheterogeneous networks and predict the sign of unobserved sen-\ntiment links. SHINE utilizes multiple deep autoencoders to map\neach user into a low-dimension feature space while preserving the\nnetwork structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommen-\ndation in two real-world datasets. The experimental results also\nprove the efficacy of SHINE in cold start scenario.\nACM Reference Format:\nHongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, and Qi\nLiu. 2018. SHINE: Signed Heterogeneous Information Network Embedding\nfor Sentiment Link Prediction. In Proceedings of the 11th ACM International\nConference on Web Search and Data Mining (WSDM’18). ACM, New York,\nNY, USA, 9 pages. https://doi.org/10.1145/3159652.3159666\n1 INTRODUCTION\nThe past decade has witnessed the proliferation of online social\nnetworks such as Facebook, Twitter and Weibo. In these social\nnetwork sites, people often share feelings and express attitudes\ntowards others, e.g., friends, movie stars or politicians, which forms\n∗This work is done while H. Wang and M. Hou are visiting Microsoft Research Asia.\n†M. Guo is the corresponding author.\nWSDM’18, February 5–9, 2018, Marina Del Rey, CA, USA\n2018. ACM ISBN 978-1-4503-5581-0/18/02. . . $15.00\nhttps://doi.org/10.1145/3159652.3159666\nsentiment links among these users. Different from explicit social\nlinks indicating friend or follow relationship, sentiment links are\nimplied by the semantic content posted by users, and involve dif-\nferent types: positive sentiment links express like, trust or support\nattitudes, while negative sentiment links signify dislike or disap-\nproval of others. For example, a tweet saying “Vote Trump!” shows\na positive sentiment link from the poster to Donald Trump, and\n“Trump is mad...” indicates the opposite case.\nFor a given sentiment link, we define its sign to be positive\nor negative depending on whether its related content expresses\na positive or negative attitude from the generator of the link to\nthe recipient [14], and all such sentiment links form a new net-\nwork topology called sentiment network. Previous work [6, 11, 15]\nmainly focuses on sentiment classification based on the concrete\ncontent posted by users. However, they cannot detect the existence\nof sentiment links without any prior content information, which\ngreatly limits the number of possible sentiment links that could be\nfound. For example, if a user does not post any word concerning\nTrump, it is impossible for traditional sentiment classifiers to ex-\ntract the user’s attitude towards him because “one cannot make\nbricks without straw”. Therefore, a fundamental question is, can\nwe predict the sign of a given sentiment link without observing its\nrelated content? The solution to this problem will benefit a great\nmany online services such as personalized advertising, new friends\nrecommendation, public opinion analysis, opinion polls, etc.\nDespite the great importance, there is little prior work concern-\ning predicting the sign of sentiment links among users in social\nnetworks. The challenges are two-fold. On the one hand, lack of\nexplicit sentiment labels makes it difficult to determine the polarity\nof existing and potential sentiment links. On the other hand, the\ncomplexity of sentiment generation and the sparsity of sentiment\nlinks make it hard for algorithms to achieve desirable performance.\nRecently, several studies [12, 14, 31, 35] propose methods to solve\nthe problem of predicting signed links. However, they rely heavily\non manually designed features and cannot work well in real-world\nscenarios. Another promising approach called network embedding\n[8, 17, 23, 26], which automatically learns features of users in net-\nwork, seems plausible to solve the task. However, they can only\napply to networks with positive-weighted (i.e., unsigned) and single-\ntype (i.e., homogeneous) edges, which limits their power in the task\nof practical sentiment link prediction.\nar\nX\niv\n:1\n71\n2.\n00\n73\n2v\n1 \n [s\ntat\n.M\nL]\n  3\n D\nec\n 20\n17\nAlice\nFemale\nCalifornia\n…\nBob\nMale\nTexas\n…\nDonald Trump\nMale\nPolitician\nAmerican\n1946\n…\nEmma Watson\nFemale\nActress\nBritish\n1990\n…\nAlbert Einstein\nMale\nScientist\nGerman\n1879\n…\npositive sentiment link\nnegative sentiment link\nsocial link\nFig. 1: Illustration of a snippet of heterogeneous networks\nwith sentiment, social relationship and user profile.\nBased on the above facts, in this paper we investigate the prob-\nlem of predicting sentiment links in absence of sentiment related\ncontent in online social networks. Our work is two-step. First, con-\nsidering the lack of labeled data, we establish a labeled sentiment\ndataset fromWeibo, one of the most popular social network sites in\nChina. We leverage state-of-the-art entity-level sentiment extrac-\ntion method to calculate the sentiment of the poster towards the\ncelebrity in each tweet. Besides, to handle the sparsity problem, we\ncollect two additional types of side information: social relationship\namong users and profile knowledge of users and celebrities. Our\nchoices are enlightened by [27] and [34], respectively, in which\n[27] demonstrates that the structural information of social net-\nworks can greatly affect users’ preference towards online items,\nand [34] proves that information from knowledge base could boost\nthe performance of recommendation. The heterogeneous informa-\ntion networks are illustrated in Fig. 1.\nTo explore more possible sentiment links from the network, in\nthe second step, we propose a novel end-to-end framework termed\nas Signed Heterogeneous Information Network Embedding (SHINE).\nGreatly different from existing network embedding approaches,\nSHINE is able to learn user representation and predict sentiment\nfrom signed heterogeneous networks. Specifically, SHINE adopts\nmultiple deep autoencoders [20], a type of deep-learning-based\nembedding technique, to extract users’ highly nonlinear represen-\ntations from the sentiment network, social network and profile\nnetwork, respectively. The learned three types of user represen-\ntations are subsequently fused together by specific aggregation\nfunction for further sentiment prediction. In addition to the adapt-\nability to signed heterogeneous networks, the superiority of SHINE\nalso lies in its end-to-end prediction technology and high flexibil-\nity of adding or removing modules of side information (i.e., social\nrelationship and profile knowledge), which is discuss in Section 5.\nWe conduct extensive experiments on two real-world datasets.\nThe results show that SHINE achieves substantial gains compared\nwith baselines. Specifically, SHINE outperforms other strong base-\nlines by 8.8% to 16.8% in the task of link prediction on Accuracy,\nand by 17.2% to 219.4% in the task of node recommendation on\nRecall@100 for positive nodes. The results also prove that SHINE\nis able to utilize the side information efficiently, and maintains a\ndecent performance in cold start scenario.\n2 RELATEDWORK\n2.1 Signed Link Prediction\nOur problem of predicting positive and negative sentiment links\nconnects to a large body of work on signed social networks, in-\ncluding trust propagation [9], spectral analysis [13], and social me-\ndia mining [22]. For the link prediction problem in signed graphs,\nLeskovec. et al. [14] adopt signed triads as features for prediction\nbased on structural balance theory. Ye et al. [31] utilize transfer\nlearning to leverage edge sign information from source network\nand improve prediction accuracy in target network. Tang et al. de-\nsign NeLP framework [21] which exploits positive links in social\nmedia to predict negative links. The difference between the above\nwork and ours is that we construct a labeled dataset by entity-level\nsentiment extraction method, as there is no explicit signed links in\nmainstream online social networks. Besides, we use state-of-the-art\ndeep learning approach to learn the representation of links.\n2.2 Network Embedding\nThere is a long history of work on network embedding. Earlier\nworks such as IsoMap [24] and Laplacian Eigenmap [1] first con-\nstruct the affinity graph of data using the feature vectors and then\nembed the affinity graph into a low-dimension space. Recently,\nDeepWalk [17] deploys random walk to learning representations\nof social network. LINE [23] proposes objective functions that pre-\nserve both local and global network structures for network embed-\nding. Node2vec [8] designs a biased randomwalk procedure to learn\na mapping of nodes that maximizes the likelihood of preserving\nnetwork neighborhoods of nodes. SDNE [26] uses autoencoder to\ncapture first-order and second-order network structures and learn\nuser representation. However, these methods can only address un-\nsigned and homogeneous networks. Additionally, several studies\nfocus on representation learning in the scenario of heterogeneous\nnetwork [3, 32], attributed network [10], or signed network [29, 33].\nHowever, these methods are specialized in only one particular type\nof networks, which is not applicable to the problem of sentiment\nprediction in real-world signed and heterogeneous networks.\n3 DATASET ESTABLISHMENT\nIn this section we introduce the process of collecting data from\nonline social networks, and discuss the details of how to extract\nsentiment towards celebrities from tweets.\n3.1 Data Collection\n3.1.1 Weibo Tweets. We select Weibo1 as the online social net-\nworks studied in this work. Weibo is one of the most popular social\nnetwork sites in China which is akin to a hybrid of Facebook and\nTwitter. We collected 2.99 billion tweets on Weibo from August 14,\n2009 to May 23, 2014 as raw dataset. To filter out useful data which\ncontains sentiment towards celebrities, we first apply Jieba2, the\nmost popular Chinese text segmentation tool, to tag the part of\nspeech (POS) of each word for each tweet. Then we select those\ntweets containing words with POS tagging as “person name” which\nexist in our established celebrity database (detailed in Section 3.1.4).\n1http://weibo.com\n2https://github.com/fxsjy/jieba\nTable 1: Statistics of Weibo sentiment datasets. “celebrities\nv.” means the celebrities owning verified accounts onWeibo.\n# users 12,814 # social links 71,268\n# celebrities 1,723 # tweets 126,380\n# celebrities v. 706 # pos. tweets 108,906\n# ordinary users 11,091 # neg. tweets 17,474\nAfter getting the set of candidate tweets, for each tweet we calculate\nits sentiment value (-1 to +1) towards the mentioned celebrities,\nand select those tweets with high absolute sentiment values. The\nfinal dataset consists of a set of triples (a,b, s), where a is the user\nwho posts the tweet, b is the certain celebrity mentioned in the\ntweet, and s ∈ {+1,−1} is the sentiment polarity of user a towards\nuser b. The method of calculating sentiment values is detailed in\nSection 3.2.\n3.1.2 Social Relation. In addition to the sentiment dataset, we\nalso collect the social relation among users fromWeibo. The dataset\nof social relation consists of tuples (a,b), where a is the follower\nand b is the followee.\n3.1.3 Profile of Ordinary Users. The profile of ordinary users\nare collected from Weibo. For each ordinary user, we extract two of\nhis attributes, gender and location, as his profile information. The\nattribute values are represented as one-hot vectors.\n3.1.4 Profile of Celebrities. We use Microsoft Satori3 knowledge\nbase to extract profile of celebrities. First, we traverse the knowledge\nbase and select terms with object type as “person”. Then we filter\nout popular celebrities with high edit frequency in knowledge base\nand high appearance frequency in Weibo tweets. For each of these\n“hot” celebrities, we extract 9 attributes as his profile information:\nplace of birth, date of birth, ethnicity, nationality, specialization, gen-\nder, height, weight, and astrological sign. Values of these attributes\nare discretized so that every celebrity’s attribute values can be ex-\npressed as one-hot vectors. Furthermore, we remove celebrities\nwith ambiguous names as well as other noises.\n3.2 Sentiment Extraction\nTo extract users’ sentiment towards celebrities in tweets, we first\ngenerate a sentiment lexicon consisting of words and their sentiment\norientation (SO) scores. To achieve this, we manually construct a\nemoticon-sentiment mapping file and map each tweet to positive\nor negative class according to the label of emoticon appeared in\nthe tweet. For example, “I love Kobe! [kiss]” is mapped to positive\nclass if the key-value pair ([kiss], positive) exists in the emoticon-\nsentiment mapping file. Note that the class of emoticon cannot\nbe directly regarded as the sentiment towards celebrities since we\nfound a large number of mismatch cases, e.g., “Miss you Taylor Swift\n[cry][cry]”. Afterwards, for each word (segmented by Jieba) with\noccurrence frequency from 2,000 to 10,000,000 in the raw tweets\ndatasets, similar to [2], we calculate its SO score as\nSO(word) = PMI (word,pos) − PMI (word,neд), (1)\nwhere PMI is the point-wise mutual information [25] defined as\nPMI (x ,y) = log p(x,y)p(x )p(y) , pos and neд are the tweets of positive and\n3http://searchengineland.com/library/bing/bing-satori\n+1\n+1\n+1\n？\n-1\n-1\n(a) Sentiment network (b) Social network\ngender\nnationality\nspecialization\n(c) Profile network\nFig. 2: Illustration of the three studied networks.\nnegative class, respectively. SO scores are subsequently normalized\nto [−1, 1].\nAfter getting the lexicon, we use SentiCircle [19] to calculate\nsentiment towards celebrities in each tweet. Given a piece of tweet\nas well as the mentioned celebrity, we represent the contextual\nsemantics of the celebrity as a polar coordinate space, where the\ncelebrity is situated in the origin and other terms in the tweet are\nscattered around. Specifically, for celebrity term c , the coordinate\nof term ti is (ri ,θi ), where ri is the inverse of distance between\nc and ti in syntax dependence graph generated by LTP [4], and\nθi = SO(ti ) · π . The overall sentiment towards the celebrity c is,\ntherefore, approximated as the geometric center of all terms ci .\nWe take the projection of the geometric center on y-axis as final\nsentiment value towards the celebrity.\nTo validate the effectiveness of sentiment extraction, we ran-\ndomly select 1,000 tweets (500 positive and 500 negative tagged\nby our method) in Weibo sentiment dataset, and manually label\neach one of them. The result shows that the precision is 95.2% for\npositive class and 91.0% for negative class, which we believe is\naccurate enough for subsequent experiments. The basic statistics\nof Weibo sentiment datasets is presented in Table 1.\n4 PROBLEM FORMULATION\nIn this section we formulate the problem of predicting sentiment\nlinks in heterogeneous information networks. For better illustration,\nwe split the original heterogeneous network into the following three\nsingle-type networks:\nSentiment network. The directed sentiment network is denoted\nas Gs = (V , S), where V = {1, ..., |V |} represents the set of users\n(either ordinary users or celebrities) and S = {si j | i ∈ V , j ∈ V }\nrepresents sentiment links among users. Each si j can take the value\nof +1, −1 or 0, representing that user i holds a positive, negative,\nor unobserved sentiment towards user j, respectively.\nSocial network. The directed social network is denoted asGr =\n(V ,R), where R = {ri j | i ∈ V , j ∈ V } represents social links among\nusers. Each ri j can take the value of 1 or 0, representing that user i\nfollows user j or not in the social network.\nProfile network. We denote A = {A1, ...,A |A |} the set of\nuser’s attributes, and akl ∈ Ak the l-th possible value of attribute\nAk . We take the union of all possible values of attributes and renum-\nber them as U =\n⋃\nAk = {aj | j = 1, ...,\n∑\nk |Ak |}. Then the undi-\nrected bipartite profile network can be denoted as Gp = (V ,U , P),\nwhere P = {pi j | i ∈ V ,aj ∈ U } represents profile links between\nusers and attribute values. Each pi j can take the value of 1 or 0,\nrepresenting that user i possesses attribute value j or not.\nBob\nTrump\n“Vote Trump!”\nBob\n… … ……\nsentiment embedding\nsentiment autoencoder\nsentiment embedding\n… … ……\nsocial embedding\nsocial autoencoder\n… … ……\nprofile embedding\nprofile autoencoder\nheterogeneous \nembedding\nheterogeneous \nembedding\naggregation\naggregation\n+1\nsocial embedding\nprofile embedding\n𝑓(∙,∙)\nҧ𝑠\npredicted \nsentiment\n+1\ntarget\ntraining\nsentiment \nextraction\ntweet\nsentiment link\nsentiment network\nprofile network\nsocial network\nFig. 3: Framework of the end-to-end SHINE model. To clearly demonstrate the model, we only show the encoder part of all\nthe three autoencoders and leave out the decoder part in this figure.\nThe three networks are illustrated in Fig. 2.\nSentiment links prediction. We define the problem of pre-\ndicting sentiment links in heterogeneous information networks as\nfollows: Given the sentiment network Gs , social network Gr and\nprofile networkGp , we aim to predict the sentiment of unobserved\nlinks between users in Gs .\n5 SIGNED HETEROGENEOUS INFORMATION\nNETWORK EMBEDDING\nIn this section we introduce the proposed SHINE model. We first\nshow the whole framework of SHINE. Then we present the details\nof the SHINE model, including how to extract user representation\njointly from the three networks as well as the learning algorithm.\nAt last we give some discussions on the model.\n5.1 Framework\nIn this paper we propose an end-to-end SHINE model to predict\nsentiment links. The framework of SHINE is shown in Fig. 3. In\ngeneral, the whole framework consists of three major components:\nsentiment extraction and heterogeneous networks construction\n(the left part), user representation extraction (the middle part), as\nwell as representation aggregation and sentiment prediction (the\nright part). For each tweet mentioning a specific celebrity, we first\ncalculate the associated sentiment (discussed in Section 3.1), and\nrepresent the user and the celebrity in this sentiment link by us-\ning their neighborhood information from the three constructed\nnetworks (introduced in Section 4). We then design three distinct\nautoencoders to extract short and dense embeddings from origi-\nnal sparse neighborhood-based representation respectively, and\naggregate these three kinds of embeddings into final heterogeneous\nembedding. The predicted sentiment can thus be calculated by ap-\nplying specific similarity measurement function (e.g., inner product\nor logistic regression) to the two heterogeneous embeddings, and\nthe whole model can be trained based on the predicted sentiment\nand the target (i.e., the ground truth obtained in sentiment extrac-\ntion step). In the following subsections we will introduce SHINE\nmodel in detail.\n5.2 Sentiment Network Embedding\nGiven the sentiment graph Gs = (V , S), for each user i ∈ V , we\ndefine its sentiment adjacency vector xi = {si j | j ∈ V } ∪ {sji | j ∈\nV }. Note that xi fully contains the global incoming and outgoing\nsentiment information of user i . However, it is impractical to take xi\ndirectly as the sentiment representation of user i , as the adjacency\nvector is too long and sparse for further processing. Recently, a lot of\nnetwork embedding models [8, 17, 23, 26] are proposed, which aim\nto learn low-dimension representations of vertices while preserving\nthe network structure. Among those models, deep autoencoder is\nproved to be one of state-of-the-art solutions, as it is able to capture\nhighly nonlinear network structure by using deep models [26].\nIn general, autoencoder [20] is an unsupervised neural network\nmodel of codings aiming to learn a representation of a set of data.\nAutoencoder consists of two parts, the encoder and the decoder,\nwhich contains multiple nonlinear functions (layers) for mapping\nthe input data to representation space and reconstructing original\ninput from representation, respectively. In our SHINE model, we\npropose to use autoencoders for efficiently user representation\nlearning.\nFig. 4 illustrates the autoencoder for sentiment network embed-\nding. As shown in Fig. 4, the sentiment autoencoder maps each\nuser to a low-dimension latent representation space and recover\noriginal information from latent representation by using multiple\nfully-connected layers. Given the input xi , the hidden representa-\ntions for each layer are\nxki = σ\n(\nWks x\nk−1\ni + b\nk\ns\n)\n, k = 1, 2, ...,Ks , (2)\nwhere Wks and bks are weight and bias parameters of layer k in the\nsentiment autoencoder, respectively, σ (·) is the nonlinear activation\nfunction, Ks is the number of layers of sentiment autoencoder, and\nx0i = xi . For simplicity, we denote x\n′\ni = x\nKs\ni the reconstruction of\nxi .\nThe basic goal of the autoencoder is to minimize the reconstruc-\ntion loss between input and output representations. Similar to [26],\nin SHINE model the reconstruction loss term of sentiment autoen-\ncoder is defined as\nLs =\n∑\ni ∈V\n\r\r(xi − x′i ) ⊙ li \r\r22 , (3)\nsentiment \nnetwork\n…\n…\n… …… …\nsentiment \nadjacency \nvector\n… …\nhidden layers hidden layers\nreconstructed \nsentiment \nadjacency vector\nsentiment \nembedding\n𝑊𝑠\n1 𝑊𝑠\n2 𝑊𝑠\n3 𝑊𝑠\n6𝑊𝑠\n5𝑊𝑠\n4\n𝐱𝑖\n1\n𝐱𝑖\n𝐱𝑖\n2\n𝐱𝑖\n3\n𝐱𝑖\n4 𝐱𝑖\n5\n𝐱𝑖\n6\nuser 𝑖\nFig. 4: Illustration of a 6-layer autoencoder for sentiment\nnetwork embedding.\nwhere ⊙ denotes theHadamard product, and li = (li,1, li,2, ..., li,2 |V |)\nis the sentiment reconstruction weight vector in which\nli, j =\n{\nα > 1, i f si j = ±1;\n1, i f si j = 0.\n(4)\nThe meaning of the above loss term lies in that we impose more\npenalty to the reconstruction error of the non-zero elements than\nthat of zero elements in input xi, as a non-zero si j carries more\nexplicit sentiment information than an implicit zero si j . Note that\nthe sentiment embedding of user i can be obtained from the layer\nKs/2 in the sentiment autoencoder, and we denote x̂i = xKs /2i the\nsentiment embedding of user i for simplicity.\n5.3 Social Network Embedding\nSimilar to previous sentiment network embedding, we apply au-\ntoencoder to extract user representation from the social network.\nGiven the social network Gr = (V ,R), for each user i ∈ V , we\ndefine its social adjacency vector yi = {ri j | j ∈ V } ∪ {r ji | j ∈ V },\nwhich fully contains the structural information of user i in the\nsocial network. The hidden representations of each layer in the\nsocial autoencoder are\nyki = σ\n(\nWkr y\nk−1\ni + b\nk\nr\n)\n, k = 1, 2, ...,Kr , (5)\nwhere the meaning of notations are similar to those in Eq. (2).\nWe also denote y′i = y\nKr\ni the reconstruction of yi . Similarly, the\nreconstruction loss term of social autoencoder is\nLr =\n∑\ni ∈V\n\r\r(yi − y′i ) ⊙ mi \r\r22 , (6)\nwhere mi = (mi,1,mi,2, ...,mi,2 |V |) is the social reconstruction\nweight vector in which if ri j = 1,mi, j = α > 1, elsemi, j = 1. The\nsocial embedding of user i is denoted as ŷi = yKr /2i .\n5.4 Profile Network Embedding\nThe profile networkGp = (V ,U , P) is an undirected bipartite graph\nwhich consists of two disjoint sets of users and attribute values.\nFor each user i ∈ V , its profile adjacency vector is defined as\nzi = {pi j | j ∈ U }. User i’s hidden representations of each layer in\nthe profile autoencoder are\nzki = σ\n(\nWkp z\nk−1\ni + b\nk\np\n)\n, k = 1, 2, ...,Kp , (7)\nwhere the meaning of notations are similar to those in Eq. (2). We\nalso use the notation z′i to denote the reconstruction of zi . Therefore,\nthe reconstruction loss term of profile autoencoder is\nLp =\n∑\ni ∈V\n\r\r(zi − z′i ) ⊙ ni \r\r22 , (8)\nwhere ni is the profile reconstruction weight vector defined sim-\nilarly to mi in the previous subsection. The profile embedding of\nuser i is denoted as ẑi = z\nKp/2\ni .\n5.5 Representation Aggregation and Sentiment\nPrediction\nOnce we obtain the sentiment embedding x̂i , social embedding ŷi ,\nand profile embedding ẑi of user i , we can aggregate these embed-\ndings into final heterogeneous embedding ei by specific aggregation\nfunctionд(·, ·, ·). We list some of the available aggregation functions\nas follows:\n• Summation [34], i.e., ei = x̂i + ŷi + ẑi ;\n• Max pooling [28], i.e., ei = element-wise-max (̂xi , ŷi , ẑi );\n• Concatenation [23], i.e., ei = ⟨̂xi , ŷi , ẑi ⟩.\nFinally, given two users i and j as well as their heterogeneous\nembedding ei and ej , the predicted sentiment s¯i j can be calculated\nas s¯i j = f (i, j), where f (·, ·) is specific similarity measurement\nfunction. For example:\n• Inner product [3, 5], i.e., s¯i j = eTi ej + b, where b is a trainable\nbias parameter;\n• Euclidean distance [26], i.e., s¯i j = −∥ei − ej ∥2 +b, where b is a\ntrainable bias parameter;\n• Logistic regression [17], i.e., s¯i j = WT⟨ei , ej ⟩+b, whereW and\nb are trainable weights and bias parameters.\nWe will study the choices of f and д in the experimental part.\n5.6 Optimization\nThe complete objective function of SHINE model is as follows:\nL =\n∑\ni ∈V\n\r\r(xi − x′i ) ⊙ li \r\r22 + λ1∑i ∈V \r\r(yi − y′i ) ⊙ mi \r\r22\n+ λ2\n∑\ni ∈V\n\r\r(zi − z′i ) ⊙ ni \r\r22 + λ3∑si j=±1 ( f (ei , ej ) − si j )2\n+ λ4Lr eд ,\n(9)\nwhere λ1, λ2, λ3 and λ4 are balancing parameters. The first three\nterms in Eq. (9) are the reconstruction loss terms of sentiment au-\ntoencoder, social autoencoder, and profile autoencoder, respectively.\nThe fourth term in Eq. (9) is the supervised loss term for penaliz-\ning the divergence between predicted sentiment and ground truth.\nThe last term in Eq. (9) is the regularization term that prevents\nover-fitting, i.e.,\nLr eд =\nKs∑\nk=1\n\r\rWks \r\r22 + Kr∑\nk=1\n\r\rWkr \r\r22 + Kp∑\nk=1\n\r\rWkp \r\r22 + \r\rf \r\r22, (10)\nwhere Wks , Wkr , Wkp are the weight parameters of layer k in the\nsentiment autoencoder, social autoencoder, and profile autoencoder,\nrespectively, and ∥ f ∥22 is the regularization penalty for similarity\nmeasurement function f (·, ·) (if appropriate).\nWe employ the AdaGrad [7] algorithm to minimize the objective\nfunctions in Eq. (9). In each iteration, we randomly select a batch\nof sentiment links from training dataset and compute the gradient\nof the objective function with respect to each trainable parameter\nrespectively. Then we update each trainable parameter according\nto the AdaGrad algorithm till convergence.\n5.7 Discussions\n5.7.1 Asymmetry. Many real-world networks are directed, which\nimplies that for two nodes i and j in the network, edges (i, j) and\n(j, i) may coexist and their values are not necessarily identical. A\nfew recent studies have focused on this asymmetry issue [16, 36]. In\nthis work, whether the basic SHINE model can characterize asym-\nmetry depends on the choice of similarity measurement function f .\nSpecifically, SHINE is capable of dealing with the direction of a link\nif and only if f (i, j) , f (j, i) (e.g., logistic regression). However (and\nfortunately), even if we choose a symmetric function (e.g., inner\nproduct or Euclidean distance) as f , we can still easily extend the\nbasic SHINE model to asymmetry-aware version by setting two\ndistinct sets of autoencoders to extract representation of source\nnode and target node respectively. From this point of view, in basic\nSHINE model the parameters of autoencoders are actually shared\nfor source node and target node to alleviate over-fitting, and we\ncan choose to explicitly distinguish the two sets of autoencoders\nfor asymmetry reasons.\n5.7.2 Cold start problem. A practical issue for network embed-\nding is how to learn representations for newly arrived node, which\nis the cold start problem. Almost all existing models cannot work\nwell in cold start scenario because they only use the information\nfrom the target network (e.g., sentiment network in this paper),\nwhich is not applicable for the newly arrived node who has little in-\nteraction with the existing target network. However, SHINE is free\nof the cold start problem, as it makes full use of side information\nand incorporate it naturally into the target network when learning\nuser representations. We will further study the performance of\nSHINE in cold start scenario in the experiment part.\n5.7.3 Flexibility. It is worth noticing that SHINE is also a frame-\nwork with high flexibility. For any other new available side infor-\nmation of users (e.g., users’ browsing history), we can easily design\na new parallel processing component and “plug” it in the original\nSHINE framework to assist learning representation. Contrarily, we\ncan also “pull out” social autoencoder or profile autoencoder from\nSHINE framework if such side information is unavailable. Besides,\nthe flexibility of SHINE also lies in that one can choose different\naggregation functions д and similarity measurement functions f ,\nas discussed in Section 5.5.\n6 EXPERIMENTS\nIn this section, we evaluate the performance of our proposed SHINE\non real-world datasets. We first introduce the datasets, baselines,\nand parameter settings for experiments, then present the experi-\nmental results of SHINE and baselines.\n6.1 Datasets\nTo comprehensively demonstrate the effectiveness of SHINE frame-\nwork, we use the following two datasets for experiments:\n• Weibo-STC: Our proposed Weibo Sentiment Towards Celebri-\nties dataset consists of three heterogeneous networks with\n12,814 users, 126,380 tweets, 71,268 social links and 37,689\nprofile values, of which the detail is presented in Section 3.\n• Wiki-RfA: Wikipedia Requests for Adminship [30] is a signed\nnetwork with 10,835 nodes and 159,388 edges, corresponding\nto votes cast by Wikipedia uses in election for promoting\nindividuals to the role of administrator. A signed link indicates\na positive or negative vote by one user on the promotion\nof another. Note that Wiki-RfA does not contain any side\ninformation of nodes, therefore, this dataset is used to validate\nthe efficacy of the basic sentiment autoencoder in SHINE.\n6.2 Baselines\nWe use the following five methods as baselines, in which the first\nthree are network embedding methods, FxG is a signed link predic-\ntion approach, and LIBFM is a generic classification model. Note\nthat the first three methods are not directly applicable to signed\nheterogeneous networks, so we use them to learn user representa-\ntions from positive and negative part of each network respectively,\nand concatenate them to form the final embeddings. For FxG on\nWeibo-STC dataset, we only use the sentiment network as input\nbecause the FxG model cannot utilize the side information of nodes.\n• LINE: Large-scale Information Network Embedding [23] de-\nfines loss functions to preserve the first-order and second-\norder proximity and learns representations of vertices.\n• Node2vec: Node2vec [8] designs a biased random walk proce-\ndure to learn a mapping of nodes that maximizes the likelihood\nof preserving network neighborhoods of nodes.\n• SDNE: Structural Deep Network Embedding [26] is a semi-\nsupervised network embedding model using autoencoder to\ncapture local and global structure of target networks.\n• FxG: Fairness and Goodness [12] predicts the weights of edges\nin weighted signed networks by introducing two measures of\nnode behavior: goodness (i.e., how much the node is liked by\nother nodes) and fairness (i.e., how fair the node is in rating\nother nodes’ likeability).\n• LIBFM: LIBFM [18] is a state-of-the-art feature based factor-\nization model. In this paper, we use the concatenated one-hot\nvectors of users in three networks as input to feed LIBFM.\n6.3 Parameter Setttings\nWe design a 4-layer autoencoder in SHINE for each network, in\nwhich the hidden layer is with 1,000 units and the embedding layer\nis with 100 units. Deeper architectures cannot further improve the\nperformance but incur heavier computational overhead according\nto our experimental results. We choose concatenation as the aggre-\ngation function д and inner product as the similarity measurement\nfunction f . Besides, we set the reconstruction weight of non-zero\nelements α = 10, the balancing parameters λ1 = 1, λ2 = 1, λ3 = 20,\nand λ4 = 0.01 for SHINE. We will study the sensitivity of these\nparameters in Section 6.6. For LINE, we concatenate the first-order\nand second-order representations to form the final 100-dimension\nembeddings for each node, and the total number of samples is 100\nmillion. For node2vec, the number of embedding dimension is set\nas 100. For SDNE, the reconstruction weight of non-zero elements\nis 10 and the weight of first-order term is 0.05. For LIBFM, the\ndimensionality of the factorization machine is set as {1, 1, 0} and\n0 5 10\n \n SHINE LINE node2vec SDNE FxG LIBFM\n0.2 0.4 0.6 0.8 1.0\n0.6\n0.7\n0.8\n0.9\nPercentage of training set\nAc\ncu\nra\ncy\n \n \n(a) Accuracy on Weibo-STC\n0.2 0.4 0.6 0.8 1.00.6\n0.7\n0.8\n0.9\nPercentage of training set\nM\nic\nro\n−F\n1\n \n \n(b) Micro-F1 on Weibo-STC\n0.2 0.4 0.6 0.8 1.00.65\n0.70\n0.75\n0.80\n0.85\nPercentage of training set\nAc\ncu\nra\ncy\n \n \n(c) Accuracy on Wiki-RfA\n0.2 0.4 0.6 0.8 1.00.65\n0.70\n0.75\n0.80\n0.85\n0.90\nPercentage of training set\nM\nic\nro\n−F\n1\n \n \n(d) Micro-F1 on Wiki-RfA\nFig. 5: Accuracy and micro-F1 on Weibo-STC and Wiki-RfA for link prediction.\nwe use SGD method for training with learning rate of 0.5 and 200\niterations. Other parameters in these baselines are set as default.\nIn the following subsections, we conduct experiments on two\ntasks: link prediction and node recommendation.\n6.4 Link Prediction\nIn link prediction setting, our task is to predict the sign of an unob-\nserved link between two given nodes. As the existing links in the\noriginal network are known and can serve as the ground truth, we\nrandomly hide 20% of links in the sentiment network and select a\nbalanced test set (i.e., the number of positive links is the same as\nnegative links) out of them, while use the remaining network to\ntrain SHINE as well as all baselines. We use Accuracy and Micro-F1\nas the evaluation metrics in link prediction task. For a more fine-\ngrained analysis, we compare the performance while varying the\npercentage of training set from 10% to 100%. The result is presented\nin Fig. 5, from which we have the following observations:\n• Fig. 5 shows that our methods SHINE achieves significant im-\nprovements in Accuracy and Micro-F1 over the baselines in\nboth datasets. Specifically, in Weibo-STC, SHINE outperforms\nLINE, node2vec, and SDNE by 13.8%, 16.2%, and 8.78% respec-\ntively on Accuracy, and achieves 15.5%, 17.6%, 9.71% gains\nrespectively on Micro-F1.\n• Among the three state-of-the-art network embeddingmethods,\nSDNE performs best while LINE and node2vec show relatively\npoor performance. Note that SDNE also uses autoencoder to\nlearning the embedding of nodes, which proves the superiority\nof autoencoder in extracting highly nonlinear representations\nof networks from a side.\n• FxG performs much better in Wiki-RfA than in Weibo-STC.\nThis is probably due to the following two reasons: 1) Unlike\nother methods, FxG cannot utilize the side information in\nWeibo-STC dataset. 2) Weibo-STC is sparser than Wiki-RfA,\nwhich is unfavorable to the computing of goodness and fair-\nness of nodes in FxG model.\n• Although LIBFM is not specially designed for network-structured\ndata, it still achieves fine performance compared with other\nnetwork embedding methods. However, during experiments\nwe find that LIBFM is unstable and prone to parameters tuning.\nThis can also be validated by the fluctuating curves of LIBFM\nin Fig. 5c and Fig. 5d.\nTo compare the performance of SHINE and baselines in cold\nstart scenario, we construct a test set of newly arrived users for\nTable 2: Comparison of models in terms of Accuracy and\nMicro-F1 on Weibo-STC in cold start scenario.\nModel Accuracy Micro-F1all users new users all users new users\nSHINE 0.855 0.834 0.881 0.858\nLINE 0.751 0.664 0.763 0.739\nnode2vec 0.736 0.653 0.749 0.667\nSDNE 0.786 0.667 0.803 0.751\nFxG 0.732 0.601 0.765 0.652\nLIBFM 0.748 0.639 0.802 0.746\nWeibo-STC, in which the associated ordinary user of each senti-\nment link dose not appear in the training set. We report Accuracy\nand Micro-F1 for all users and new users in Table 2. From the results\nin Table 2 it is evident that SHINE can still maintain a decent perfor-\nmance in the cold start scenario, as it fully exploits the information\nfrom social network and profile network to compensate for the\nlack of sentiment links. By comparison, the performance of other\nbaselines degrades significantly in cold start scenario. Specifically,\nthe Accuracy decreases by 2.46% for SHINE and by 11.58%, 11.28%,\n15.14%, 17.90%, 14.57% respectively for LINE, node2vec, SDNE, FxG\nand LIBFM, which proves that SHINE are more capable of effec-\ntively transferring knowledge among heterogeneous information\nnetworks, especially in cold start scenario.\n6.5 Node Recommendation\nIn addition to link prediction, we also conduct experiments on node\nrecommendation, in which for each user we aim to recommend a set\nof users who have not been explicitly expressed attitude to but may\nbe liked by the user. The performance of node recommendation can\nreveal the quality of learned representations as well. Specifically, for\neach user, we calculate his sentiment score toward all other users,\nand selectK users with largest sentiment score for recommendation.\nFor completeness, we recommend not only the nodes that a user\nmay like but also the nodes that he may dislike. Therefore, we use\npositive and negative Precision@K and Recall@K respectively for\nevaluation in corresponding experimental scenarios. The results\nare shown in Fig. 6, which provides us the following observations:\n• The curve of SHINE is almost consistently above the curves\nof baselines, which proves that SHINE can better learn the\nrepresentations of heterogeneous networks and perform rec-\nommendation than baselines.\n0 5 10\n \n SHINE LINE node2vec SDNE FxG LIBFM\n20 40 60 80 1000\n0.05\n0.1\nK\npo\ns.\n P\nre\ncis\nio\nn@\nK\n(a) pos. Precision@K on Weibo-STC\n20 40 60 80 1000\n0.02\n0.04\n0.06\n0.08\nK\nn\ne\ng.\n P\nre\ncis\nio\nn@\nK\n(b) neg. Precision@K on Weibo-STC\n20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\nK\npo\ns.\n R\nec\nal\nl@\nK\n(c) pos. Recall@K on Weibo-STC\n20 40 60 80 1000\n0.1\n0.2\n0.3\n0.4\nK\nn\ne\ng.\n R\nec\nal\nl@\nK\n(d) neg. Recall@K on Weibo-STC\n20 40 60 80 1000\n0.02\n0.04\n0.06\n0.08\nK\npo\ns.\n P\nre\ncis\nio\nn@\nK\n(e) pos. Precision@K on Wiki-RfA\n20 40 60 80 1000\n0.02\n0.04\nK\nn\ne\ng.\n P\nre\ncis\nio\nn@\nK\n(f) neg. Precision@K on Wiki-RfA\n20 40 60 80 1000\n0.05\n0.10\n0.15\n0.20\n0.25\nK\npo\ns.\n R\nec\nal\nl@\nK\n(g) pos. Recall@K on Wiki-RfA\n20 40 60 80 1000\n0.1\n0.2\n0.3\nK\nn\ne\ng.\n R\nec\nal\nl@\nK\n(h) neg. Recall@K on Wiki-RfA\nFig. 6: Positive and negative Precision@K and Recall@K on Weibo-STC and Wiki-RfA for node recommendation.\nTable 3: Accuracy on Weibo-STC w.r.t. the combinations of\nsimilarity measurement function and aggregate function.\nf\nд\nSummation Max pooling Concatenation\nInner product 0.802 0.761 0.855\nEuclidean distance 0.788 0.779 0.837\nLogistic regression 0.816 0.782 0.842\n• Negative precision is low than positive precision while nega-\ntive recall is higher than positive recall for most methods. This\nis because negative links are far fewer than positive links in\nboth datasets, which makes it easier to cover more negative\nlinks in the recommendation set.\n• In general, the results of precision and recall on Weibo-STC is\nbetter than Wiki-RfA, which is in accordance with the results\nin link prediction. The reason lies in that Weibo-STC provides\nmore side information which can greatly improve the quality\nof learned user representations.\n6.6 Parameters Sensitivity\nSHINE involves a number of hyper-parameters. In this subsection\nwe examine how the different choices of parameters affect the\nAccuracy of SHINE onWeibo-STC dataset. Except for the parameter\nbeing tested, all other parameters are set as default.\nSimilaritymeasurement function f and aggregation func-\ntion д.We first investigate how the similarity measurement func-\ntion f and aggregation function д affect the performance by testing\non all combinations of f and д, and present the results in Table 3.\nIt is clear that the combination of inner product and concatenation\nachieves the best Accuracy, while max pooling performs worst,\nwhich is probably due to the reason that concatenation preserves\nmore information out of the three types of embeddings than sum-\nmation and max pooling during embedding aggregation. It should\nalso be noted that there is no absolute advantage of all the three f\nfunctions according to the results in Table 3.\nDimension of embedding layer and reconstructionweight\nof non-zero elements α . We also show how the dimension of\nembedding layer in the three autoencoders of SHINE and the hyper-\nparameterα affect the performance in Fig. 7a.We have the following\ntwo observations: 1) The performance is initially improved with\nthe increase of dimension, because more bits in embedding layer\ncan encode more useful information. However, the performance\ndrops when the dimension further increases, as too large number\nof dimensions may introduce noises which mislead the subsequent\nprediction. 2) α controls the reconstruction weight of non-zero\nelements in autoencoders. When α is too small (e.g., α = 1), SHINE\nwill reconstruct the zero and non-zero elements without much\ndiscrimination, which deteriorates the performance because non-\nzero elements are more informative than zero ones. However, the\nperformance will decrease if α gets too large (e.g., α = 30), because\nlarge α will lead SHINE to totally ignore the dissimilarity (i.e., zero\nelements) among users.\nBalancing parameters λ1, λ2, and λ3. λ1, λ2, and λ3 balance\nthe loss terms of the objective function in Eq. (9). We treat λ1 and\nλ2 as binary parameters and vary the value of λ3 to study the per-\nformance of SHINE. Note that whether λ1 or λ2 equals 1 indicates\nthat whether we use the additional social information or profile\ninformation in link prediction. Therefore, the study of λ1 and λ2\ncan also be seen as to validate the effectiveness of social network\nembedding module and profile network embedding module. The\nresult is presented in Fig. 7b, from which we can conclude that:\n1) The curve of λ1 = 1, λ2 = 0 and λ1 = 0, λ2 = 1 are both above\nthe curve of λ1 = 0, λ2 = 0, which demonstrates the significant\ngain by incorporating the social information and profile informa-\ntion (especially the latter) into the sentiment network. Moreover,\ncombining both additional information can further improve the\nperformance. 2) Increasing the value of λ3 can greatly boost the\naccuracy, as SHINE will concentrate more on the prediction er-\nror rather than the reconstruction error. However, similar to other\nhyper-parameters, too large λ3 is not satisfactory since it breaks\nthe trade-off among loss terms in objective function.\n10 50 100 200 500\n0.65\n0.70\n0.75\n0.80\n0.85\nDimension of embedding layer\nAc\ncu\nra\ncy\n \n \nα = 1\nα = 10\nα = 20\nα = 30\n(a) dim. of embedding layer and α\n0.1 1 5 10 20 300.70\n0.75\n0.80\n0.85\nλ3\nAc\ncu\nra\ncy\n \n \nλ1=0, λ2=0\nλ1=1, λ2=0\nλ1=0, λ2=1\nλ1=1, λ2=1\n(b) λ1, λ2, and λ3\nFig. 7: Parameter sensitivity w.r.t. the dimension of embed-\nding layers, α , λ1, λ2, and λ3.\n7 CONCLUSIONS\nIn this paper we study the problem of predicting sentiment links in\nabsence of sentiment related content in online social networks. We\nfirst establish a labeled, heterogeneous, and entity-level sentiment\ndataset from Weibo due to the lack of explicit sentiment links. To\nefficiently learn from these heterogeneous networks, we propose\nSigned Heterogeneous Information Network Embedding (SHINE),\na deep-learning-based network embedding framework to extract\nusers’ highly nonlinear representations while preserving the struc-\nture of original networks. We conduct extensive experiments to\nevaluate the performance of SHINE. Experimental results prove\nthe competitiveness of SHINE against several strong baselines and\ndemonstrate the effectiveness of usage of social relation and profile\ninformation, especially in cold start scenario.\nACKNOWLEDGMENTS\nWe thank our anonymous reviewers for their feedback and sug-\ngestions. This work was partially sponsored by the National Basic\nResearch 973 Program of China under Grant 2015CB352403.\nREFERENCES\n[1] Mikhail Belkin and Partha Niyogi. 2001. Laplacian eigenmaps and spectral\ntechniques for embedding and clustering. In NIPS, Vol. 14. 585–591.\n[2] Felipe Bravo-Marquez, Eibe Frank, and Bernhard Pfahringer. 2015. Positive,\nnegative, or neutral: Learning an expanded opinion lexicon from emoticon-\nannotated tweets. In IJCAI 2015, Vol. 2015. AAAI Press, 1229–1235.\n[3] Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and\nThomas S Huang. 2015. Heterogeneous network embedding via deep archi-\ntectures. In Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM, 119–128.\n[4] Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technol-\nogy platform. In Proceedings of the 23rd International Conference on Computational\nLinguistics: Demonstrations. Association for Computational Linguistics, 13–16.\n[5] Xin Dong, Lei Yu, Zhonghuo Wu, Yuxia Sun, Lingfeng Yuan, and Fangxi Zhang.\n2017. A Hybrid Collaborative Filtering Model with Deep Structure for Recom-\nmender Systems. In Thirty-First AAAI Conference on Artificial Intelligence.\n[6] Cícero Nogueira Dos Santos and Maira Gatti. 2014. Deep Convolutional Neural\nNetworks for Sentiment Analysis of Short Texts.. In COLING. 69–78.\n[7] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods\nfor online learning and stochastic optimization. Journal of Machine Learning\nResearch 12, Jul (2011), 2121–2159.\n[8] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for\nnetworks. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM, 855–864.\n[9] Ramanthan Guha, Ravi Kumar, Prabhakar Raghavan, and Andrew Tomkins.\n2004. Propagation of trust and distrust. In Proceedings of the 13th international\nconference on World Wide Web. ACM, 403–412.\n[10] Xiao Huang, Jundong Li, and Xia Hu. 2017. Label informed attributed network\nembedding. In Proceedings of the Tenth ACM International Conference on Web\nSearch and Data Mining. ACM, 731–739.\n[11] Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014.\nNRC-Canada-2014: Detecting aspects and sentiment in customer reviews. In\nProceedings of the 8th International Workshop on Semantic Evaluation (SemEval\n2014). 437–442.\n[12] Srijan Kumar, Francesca Spezzano, VS Subrahmanian, and Christos Faloutsos.\n2016. Edge weight prediction in weighted signed networks. In Data Mining\n(ICDM), 2016 IEEE 16th International Conference on. IEEE, 221–230.\n[13] Jérôme Kunegis, Stephan Schmidt, Andreas Lommatzsch, Jürgen Lerner,\nErnesto W De Luca, and Sahin Albayrak. 2010. Spectral analysis of signed\ngraphs for clustering, prediction and visualization. In Proceedings of the 2010\nSIAM International Conference on Data Mining. SIAM, 559–570.\n[14] Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg. 2010. Predicting pos-\nitive and negative links in online social networks. In Proceedings of the 19th\ninternational conference on World wide web. ACM, 641–650.\n[15] Thien Hai Nguyen and Kiyoaki Shirai. 2015. PhraseRNN: Phrase Recursive Neural\nNetwork for Aspect-based Sentiment Analysis.. In EMNLP. 2509–2514.\n[16] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, andWenwu Zhu. 2016. Asymmet-\nric transitivity preserving graph embedding. In Proc. of ACM SIGKDD. 1105–1114.\n[17] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning\nof social representations. In Proceedings of the 20th ACM SIGKDD international\nconference on Knowledge discovery and data mining. ACM, 701–710.\n[18] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on\nIntelligent Systems and Technology (TIST) 3, 3 (2012), 57.\n[19] Hassan Saif. 2015. Semantic Sentiment Analysis of Microblogs. Ph.D. Dissertation.\nThe Open University.\n[20] Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. International\nJournal of Approximate Reasoning 50, 7 (2009), 969–978.\n[21] Jiliang Tang, Shiyu Chang, Charu Aggarwal, and Huan Liu. 2015. Negative\nlink prediction in social media. In Proceedings of the Eighth ACM International\nConference on Web Search and Data Mining. ACM, 87–96.\n[22] Jiliang Tang, Yi Chang, Charu Aggarwal, and Huan Liu. 2016. A survey of signed\nnetwork mining in social media. ACM Computing Surveys (CSUR) 49, 3 (2016),\n42.\n[23] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.\n2015. Line: Large-scale information network embedding. In Proceedings of the\n24th International Conference on World Wide Web. ACM, 1067–1077.\n[24] Joshua B Tenenbaum, Vin De Silva, and John C Langford. 2000. A global geometric\nframework for nonlinear dimensionality reduction. science 290, 5500 (2000), 2319–\n2323.\n[25] Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied\nto unsupervised classification of reviews. In Proceedings of the 40th annual meet-\ning on association for computational linguistics. Association for Computational\nLinguistics, 417–424.\n[26] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network em-\nbedding. In Proceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM, 1225–1234.\n[27] Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, and Minyi Guo. 2017.\nJoint-Topic-Semantic-aware Social Recommendation for Online Voting. In Pro-\nceedings of the 26th ACM International Conference on Conference on Information\nand Knowledge Management. ACM, 347–356.\n[28] Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi\nCheng. 2015. Learning hierarchical representation model for nextbasket rec-\nommendation. In Proceedings of the 38th International ACM SIGIR conference on\nResearch and Development in Information Retrieval. ACM, 403–412.\n[29] Suhang Wang, Jiliang Tang, Charu Aggarwal, Yi Chang, and Huan Liu. 2017.\nSigned network embedding in social media. In Proceedings of the 2017 SIAM\nInternational Conference on Data Mining. SIAM, 327–335.\n[30] Robert West, Hristo S Paskov, Jure Leskovec, and Christopher Potts. 2014. Ex-\nploiting social network structure for person-to-person sentiment analysis. arXiv\npreprint arXiv:1409.2450 (2014).\n[31] Jihang Ye, Hong Cheng, Zhe Zhu, and Minghua Chen. 2013. Predicting positive\nand negative links in signed social networks by transfer learning. In Proceedings\nof the 22nd international conference on World Wide Web. ACM, 1477–1488.\n[32] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandel-\nwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation:\nA heterogeneous information network approach. In Proceedings of the 7th ACM\ninternational conference on Web search and data mining. ACM, 283–292.\n[33] Shuhan Yuan, Xintao Wu, and Yang Xiang. 2017. SNE: Signed Network Em-\nbedding. In Pacific-Asia Conference on Knowledge Discovery and Data Mining.\nSpringer, 183–195.\n[34] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.\n2016. Collaborative knowledge base embedding for recommender systems. In\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. ACM, 353–362.\n[35] Quan Zheng and David B Skillicorn. 2015. Spectral embedding of signed networks.\nIn Proceedings of the 2015 SIAM International Conference on Data Mining. SIAM,\n55–63.\n[36] Chang Zhou, Yuqiong Liu, Xiaofei Liu, Zhongyi Liu, and Jun Gao. 2017. Scalable\nGraph Embedding for Asymmetric Proximity.. In AAAI. 2942–2948.\n",
      "id": 45956798,
      "identifiers": [
        {
          "identifier": "2772021946",
          "type": "MAG_ID"
        },
        {
          "identifier": "10.1145/3159652.3159666",
          "type": "DOI"
        },
        {
          "identifier": "141534803",
          "type": "CORE_ID"
        },
        {
          "identifier": "466610688",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1712.00732",
          "type": "OAI_ID"
        },
        {
          "identifier": "1712.00732",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "info:doi/10.1145%2f3159652.3159666",
          "type": "OAI_ID"
        }
      ],
      "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment\n  Link Prediction",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2772021946",
      "oaiIds": [
        "info:doi/10.1145%2f3159652.3159666",
        "oai:arxiv.org:1712.00732"
      ],
      "publishedDate": "2017-12-03T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1712.00732"
      ],
      "updatedDate": "2024-02-23T20:47:25",
      "yearPublished": 2017,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1712.00732"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/45956798"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1403.6067",
      "authors": [
        {
          "name": "Gao, Huiji"
        },
        {
          "name": "Mahmud, Jalal"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/386121020",
        "https://api.core.ac.uk/v3/outputs/100481049"
      ],
      "createdDate": "2014-10-24T19:25:15",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 11965,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11965",
          "logo": "https://api.core.ac.uk/data-providers/11965/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Twitter has been increasingly used for spreading messages about campaigns.\nSuch campaigns try to gain followers through their Twitter accounts, influence\nthe followers and spread messages through them. In this paper, we explore the\nrelationship between followers sentiment towards the campaign topic and their\nrate of retweeting of messages generated by the campaign. Our analysis with\nfollowers of multiple social-media campaigns found statistical significant\ncorrelations between such sentiment and retweeting rate. Based on our analysis,\nwe have conducted an online intervention study among the followers of different\nsocial-media campaigns. Our study shows that targeting followers based on their\nsentiment towards the campaign can give higher retweet rate than a number of\nother baseline approaches",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/1403.6067",
      "fieldOfStudy": null,
      "fullText": "  \nWhy Do You Spread This Message?  \nUnderstanding Users Sentiment in Social Media Campaigns \nJalal Mahmud\n1\n and Huiji Gao\n2\n \n1IBM Research - Almaden       2Arizona State University \njumahmud@us.ibm.com                        Huiji.Gao@asu.edu \n \n \n \nAbstract \nTwitter has been increasingly used for spreading messages \nabout campaigns. Such campaigns try to gain followers \nthrough their Twitter accounts, influence the followers and \nspread messages through them. In this paper, we explore the \nrelationship between followers’ sentiment towards the cam-\npaign topic and their rate of retweeting of messages gener-\nated by the campaign.  Our analysis with followers of mul-\ntiple social-media campaigns found statistical significant \ncorrelations between such sentiment and retweeting rate.  \nBased on our analysis, we have conducted an online inter-\nvention study among the followers of different social-media \ncampaigns. Our study shows that targeting followers based \non their sentiment towards the campaign can give higher re-\ntweet rate than a number of other baseline approaches.  \n \nIntroduction   \nRecent years have seen a rapid growth in micro-blogging \nand the rise of popular micro-blogging services such as \nTwitter. With the growing usage of such micro-blogging \nservices, a wide number of social-media campaigns rang-\ning from politics and government to social issues also exist \nin Twitter. Such campaigns maintain Twitter accounts, \nwant to gain a large number of followers through their \nTwitter accounts, influence the followers and spread mes-\nsages through them. However, in reality, all followers may \nnot be equally engaged with the campaign (Chen et al. \n2012). Chen et al. found that users’ activity level, prior \ntopic interest, prior interpersonal relation and geographical \nlocation is correlated with their level of engagement \n(measured by retweets and hashtag usage) with Occupy \nWall Street\n1\n campaign (Chen et al. 2012).  \n    Motivated by their findings, we explore whether follow-\ners’ sentiment towards the campaign topic is correlated \nwith their engagement level with the campaign, where en-\ngagement is measured as the rate of retweeting of the mes-\n                                                 \n \n1\n https://twitter.com/OccupyWallSt \nsages generated by the campaign. Furthermore, we investi-\ngate whether such retweet rate can be predicted from their \nsentiment towards the campaign topic.  \n    Our research is also inspired by an established theoreti-\ncal framework in psychological and marketing research on \nattitudes and attitude models, where attitude is defined as a \nunified concept containing three aspects: “feelings”, “be-\nliefs”, and “actions” (Schiffman  et al. 2010). According to \nthe framework, beliefs are acquired on attitude object (e.g., \na topic or product), which in turns influences the feelings \non the object and the actions with regard to the attitude ob-\nject. Since user’s  belief is hard to observe from social me-\ndia data, we focused on understanding relation between \nfeelings (sentiment) towards a campaign topic and actions \nwhich result from such feelings (sentiment).  We hypothe-\nsize that user’s sentiment towards social media campaign \nhas positive effects on their actions (e.g., retweet) related \nto the campaign, and validate this hypothesis by this re-\nsearch. We have also conducted an online intervention \nstudy to understand the effect of sentiment on increasing \nretweet rates of campaign relevant messages.  \n Our study shows that targeting followers of a campaign \nbased on their higher positive sentiment towards the cam-\npaign gives higher retweeting rate than a number of base-\nlines, such as random targeting and topic based targeting. \nThus, a campaign can be more effective by sending target-\ned messages to followers with stronger sentiment towards \nthe campaign topic.  \nDataset \nFor the sake of concreteness, we limit our exploration on \nthe campaign topic “fracking”2, and analyze data from \nTwitter accounts of several campaigns either supporting or  \n                                                 \n2 fracking or hydrolic fracturing is the process of extracting natural gas \nfrom shale rock layers. This is very controversial due to its potential im-\npact on energy and environment \n \nSentiment Tweet \nPositive The science & economics of #fracking says \nyes to fracking http://ow.ly/ll1J4 \nNegative #Fracking wastewater threatens to drown \nOhio: http://t.co/ft768gkW \nTable 2.  Example tweets related to fracking \nopposing “fracking”. We manually selected 4 Twitter cam-\npaigns related to “fracking” on July 31, 2013. Table 1 \nshows the data-statistics of the campaigns. We crawled \ntheir tweets from last one month and identified tweets \nwhich contained keywords or hashtags related to fracking, \nsuch as “fracking”, “shale”, “#fracking”, “#shale”, “oil”. \nWe denoted these tweets as topic relevant tweets. We also \nmanually inspected those tweets and found that they were \nindeed topically relevant. Table 2 shows few such tweets \nwith their sentiment. The rest of the tweets of each cam-\npaign in last one month are denoted as general tweets. \nThen, we crawled followers of each campaign and their \ntweets from last one month. These tweets are denoted as \nrecent tweets. However, we excluded followers who had \nprotected accounts or who had posted fewer than 100 \ntweets. We also crawled historical tweets (200 max.) of \nthose followers before last one month (tweets until July 1, \n2013), and denoted such tweets as historical tweets.  \nSentiment and Retweet Behavior \nOur goal is to investigate whether followers’ sentiment to-\nwards the campaign topic has any correlation with how \nthey engage with the campaign. However, we do not have \nground truth sentiment for such followers. Hence, we in-\nferred their sentiment from historical tweets. \n \nSentiment Prediction Model \nMotivated by prior works on sentiment analysis (Barbosa \net al. 2010, Davidov et al. 2010), we developed a simple \ncontent-based sentiment prediction model which predicts \nthe sentiment of a Twitter user towards “fracking”. We \nfirst created a labeled dataset for sentiment prediction from \nTwitter. We used Twitter’s streaming API from January, \n2013 to March, 2013 for tweets containing keywords or \nhashtags related to fracking (e.g., “fracking”, “shale”, \n“#fracking”, “#shale”, “oil”). In total, we collected about \n1.68 million tweets. We identified retweets from our data, \nand computed how many times each tweet was retweeted.  \n We selected the tweets which were retweeted at least \n100 times, and manually analyzed them for ground-truth \ncreation. In particular, we found 163 such tweets which we \nlabeled as either positive or negative towards “fracking”. \n22 tweets were positive (pro-fracking tweets), and 141 \ntweets were negative (anti-fracking tweets). From our data, \nwe identified all users who had retweeted those tweets. \nThere were 5384 such users in total:  1562 users retweeted \npro-fracking tweets, and 3822 users retweeted anti-\nfracking tweets. We considered the users who retweeted \npro-fracking tweets as positive towards “fracking”, and \nthose who retweeted anti-fracking tweets as negative to-\nwards “fracking”. This is based on the traditional assump-\ntion that retweet is an act of endorsement of the original \ntweet (Boyd et al. 2010). From the 5384 users (1562 posi-\ntive, 3822 negative), we randomly sampled 1000 positive \n(pro-fracking) users and another 1000 negative (anti-\nfracking) users. Then, we used Twitter’s REST API to \ncrawl the historical tweets (200 max.) of those users before \nthe retweet. Furthermore, we used Twitter's streaming API \nto  randomly sample another 1000 users who were as-\nsumed to have neutral sentiment towards “fracking”. We \nalso crawled their historical (200 max.) tweets. Thus, our \nfinal dataset for sentiment prediction model contained \n3000 users: 1000 positive, 1000 negative and 1000 neutral \ntowards “fracking”.   \n From the above data, we constructed classification-\nbased model with three categories (positive, negative, neu-\ntral). These are sentiment polarity for users' sentiment pre-\ndiction. Our model used unigrams computed from histori-\ncal tweets as features.  We tried a number of different sta-\ntistical models from WEKA, a widely used machine learn-\ning toolkit. SVM-based models outperformed others, and \nachieved ~92% accuracy (0.92 recall, 0.93 precision, 0.925 \nF-measure) under 10-fold cross-validation.  \n \nAccuracy of Sentiment Prediction for Campaign \nFollowers \nWe conducted a study where participants labeled the sen-\ntiment (positive, negative, neutral) of the followers by \nlooking at their historical tweets (tweets until July 1, \n2013). Then, we compared this manually labeled sentiment \nwith inferred sentiment by our algorithm and computed ac-\ncuracy. We randomly selected 50 followers from each \ncampaign (200 followers in total) and collected their his-\ntorical tweets. We recruited 100 participants from Crowd-\nFlower (http://crowdflower.com/), a popular crowd sourc-\ning platform. Each of them labeled 4 followers, and each of \nthe follower was labeled by two participants. Participants \nwere in agreement for 126 followers, and we used those as \nground-truth to compute the accuracy of our sentiment \nprediction model. We applied our sentiment prediction \nmodel to infer the sentiment of those 126 followers. Our \nmodel's inferred sentiment matched manually labeled sen-\ntiment for 110 users which corresponds to 87.3% accuracy. \nTwitter account \nname of the cam-\npaign \nType \n(pro/anti \nfracking) \n# of fol-\nlowers as \nof July \n31, 2013 \n# of  Tweets \nposted as of \nJuly 31, \n2013  \nshalebiz pro 2776 6940 \nEnergy From Shale pro 3965 4826 \nFracking News anti 1219 3062 \nFrackfree America anti 1042 6133 \nTable 1. Social Media Campaigns in our dataset \nTable 4. Regression results over 10 fold cross-validation \nTwitter account \nname of the \ncampaign \nMean Absolute Error (MAE) \nPredicting  \ntopical msg  \nPredicting  \ngeneral msg  \nPred. \nfrom \nSent. \npolarity  \nPred. \nfrom \nSent. \nstrength  \nPred. \nfrom \nSent.  \npolarity  \nPred. \nfrom \nSent. \nstrength  \nshalebiz 0.32 0.25 0.35 0.26 \nEnergy From \nShale \n0.30 0.24 0.32 0.28 \nFracking News 0.28 0.25 0.33 0.27 \nFrackfree  \nAmerica \n0.27 0.23 0.3 0.25 \n \nCorrelation of Sentiment with Retweets \nWe applied the sentiment prediction model to infer the sen-\ntiment of followers of each campaign from their historical \ntweets (tweets until July 1, 2013). Then, we investigated \nwhether followers' predicted sentiment towards fracking \nhas any correlation with their actions (e.g., retweets). To-\nwards that, we analyzed the recent tweets of each follower, \nand computed the number of times they retweeted any top-\nic relevant tweets of the campaign. We divided that num-\nber by the total number of topic relevant tweets of the \ncampaign. The resultant ratio is named as retweet-rate-of-\ntopic-related-campaign-msg. \n We do a similar analysis for other tweets generated from \nthe campaign and computed the ratio: retweet-rate-of-\ngeneral-campaign-msg. Since some followers can be more \nactive in retweeing than others, we divided each of these \nratios by the total number of retweets of each follower over \nlast one month. This resulted  normalized retweet rates \nwhich we used for our analysis. \n   Next, we conducted a pearson correlation analysis be-\ntween sentiment and retweet rates.  For such analysis, we \nconverted predicted sentiment to numeric scores. For fol-\nlowers of pro-fracking campaigns, the numeric scores are \nas follows: 1 for positive, 0 for neutral and -1 for negative. \nFor followers of anti-fracking campaigns, we do the oppo-\nsite (1 for negative, 0 for neutral and -1 for positive). Our \nsentiment prediction model also returned a probability es-\ntimate associated with sentiment prediction. We multiplied \nthis probability with the numeric score for predicted senti-\nment to obtain sentiment strength of each follower. Then, \nwe also investigated the correlation between sentiment \nstrength and retweet rates.   \n As shown in Table 3, we found positive correlations be-\ntween sentiment polarity, and retweet rates for all cam-\npaigns. This indicates that followers who express similar \nsentiment of the campaign are also more likely to retweet \nmessages generated by the campaign. For retweeting topic \nrelevant campaign tweets, such correlations are statistically \nsignificant for all campaigns. However, for retweeting \ngeneral campaign tweets, correlations are not statistically \nsignificant. Table 3 also shows the correlations between \nsentiment strength and retweeting rates. Statistical signifi-\ncant correlations are observed in each case. We also see a \nhigher correlation value for sentiment strength, which indi-\ncates its effectiveness to impact followers’ retweets.   \n \nPrediction Model \nWe also built predictive models of retweeting a campaign’s \nmessage based on the sentiment of the followers. We per-\nformed both regression analysis and a classification study \nusing WEKA, a widely used machine learning toolkit.  For \nboth cases, we tried two settings - predicting from senti-\nment polarity, and predicting from sentiment strength.  \n For regression analysis, sentiment polarity (first setting) \nand sentiment strength (second setting) are used as the val-\nue of the independent variable in the regression model. Re-\ntweet rates are used as dependent variable in the model. \nWe tried a number of regression approaches (e.g., logistic \nregression, SVM regression) and performed 10-fold cross \nvalidation. Logistic regression performed the best, and the \nresults in terms of mean absolute error (MAE) are shown \nin Table 4. We find lower prediction errors when predict-\ning from sentiment strength. In particular, retweeting topi-\nTwitter account \nname of the \ncampaign \nArea Under ROC Curve (AUC) \nPredicting  \ntopical msg  \nPredicting  \ngeneral msg  \nPred. \nfrom \nSent.  \npolarity  \nPred. \nfrom \nSent. \nstrength  \nPred. \nfrom \nSent. \npolarity  \nPred. \nfrom \nSent. \nstrength  \nshalebiz 0.67 0.74 0.63 0.70 \nEnergy From \nShale \n0.69 0.76 0.67 0.72 \nFracking News 0.70 0.77 0.66 0.73 \nFrackfree Amer-\nica \n0.73 0.80 0.70 0.74 \nTable 5. Classification results over 10 fold cross-validation \nTwitter account \nname of the \ncampaign \nCorrelation Analysis \nnormalized-\nretweet-rate-of-\ntopic-related-\ncampaign-msg \nnormalized-\nretweet-rate-of-\ngeneral-campaign-\nmsg  \nCorr. \nwith \nSent.  \npolarity  \nCorr. \nwith \nSent. \nstrength  \nCorr. \nwith \nSent.  \npolarity  \nCorr. \nwith \nSent. \nstrength  \nShalebiz 0.65* 0.7* 0.6 0.62* \nEnergy From \nShale \n0.71* 0.73* 0.64 0.65* \nFracking News 0.8* 0.82* 0.77 0.8* \nFrackfree  \nAmerica \n0.6* 0.63* 0.5 0.55* \nTable 3. Pearson correlation between sentiment and norma-\nlized retweet rates (* means significant, p < 0.05) \ncal messages can be predicted within 23%-25% MAE and \ngeneral messages can be predicted within 25%-28% MAE.  \n For classification study, we used supervised binary ma-\nchine learning algorithms to classify followers with above \nmedian levels of retweet rates. We experimented with a \nnumber of classifiers from WEKA, including naive Bayes,  \nSVM, J48, Random Forest. SVM slightly outperformed the \nrest. Table 5 shows the classification result in terms of \nAUC under 10-fold cross validation. We see that classify-\ning high (above median) or low (below median) retweeters \nfrom sentiment can be done quite accurately.  \nTable 6.  Example intervention messages     \nSentiment Example Intervention Message \nPositive \"@zas Plz RT “Fracking saves us money; frack-\ning creates jobs\"”   (pro-fracking) \n \nNegative \"To anyone speaking of the economic ”benefits” \nof fracking: what use is that money if your food \nand water are full of poison. Plz RT\"(anti-\nfracking) \nOnline Engagement Study \nWe conducted an engagement study among followers with \ndifferent sentiment. In our study, we sent campaign rele-\nvant messages to such followers. We first created 6 ac-\ncounts on Twitter, 3 for sending pro-fracking and another 3 \nfor sending anti-fracking messages.  We maintained a pool \nof 10 pro-fracking messages and 10 anti-fracking messag-\nes. Each such message asked the recipient to retweet the \nmessage.  \n We ranked the positive sentiment followers of pro-\nfracking campaigns according to their sentiment strength \ntowards the “fracking” topic and selected top-500 (senti-\nment-ranked-top-followers) from the ranked list. From the \nremaining pro-fracking followers, we randomly selected \n500 followers and denoted them as  followers. We obtained \nanother set of 500 users from random sampling from Twit-\nter stream. Finally, we looked for Twitter stream for users \nwho mentioned the term “fracking” and obtained another \nset of 500 users from such matched users (topic-relevant).  \nFrom our pro-fracking accounts, we sent pro-fracking \nmessages to each group of users. Thus, 500 messages were \nsent for each target group. In each such message sending, \nwe randomly selected a message from the pool of 10 pro-\nfracking messages. We waited about a week, and recorded \nhow many messages were retweeted in each case. Then, \nwe computed the retweet rates (defined as the ratio of the \nnumber of messages retweeted and the total number of \nmessages sent) obtained in each case.  \n We do a similar study for anti-fracking followers. Thus, \nwe sent anti-fracking messages to sentiment-ranked-top-\nfollowers of anti-fracking campaigns. We also sent those \nmessages to randomly selected 500 followers of those \ncampaigns, randomly selected 500 users from Twitter, and \nrandomly selected 500 topic-relevant users from Twitter. \nFigure 1 shows the retweet rates obtained in each case.  We \nobserve that sentiment-ranked approach clearly outper-\nformed the other approaches. Our study shows that a cam-\npaign can be more effective by sending targeted messages \nto followers with stronger sentiment towards the campaign.  \nConclusion \nIn this paper, we investigated the relationship between sen-\ntiment of social media users who are following a campaign \nand their likelihood of spreading campaign messages. We \nfound that sentiment has a statistically significant effect on \nsuch activity. Furthermore, spreading topically relevant \ncampaign messages has stronger correlation with sentiment \nof the campaign followers. Our engagement study provides \nfurther insight for designing better intervention to spread \ncampaign messages. There are various directions for future \nresearch. First, we like to apply similar analysis for other \ntypes of actions such as hashtag usage, mentions or tweet \ncreation. Second, we will explore how  other factors (e.g., \ngeneral activity, prior interaction as suggested by Chen et \nal. 2012) together with sentiment predict such actions. \nThird, we will conduct a survey among campaign follow-\ners to understand their demographics, personality, network \nsize, etc. and analyze whether these factors affect their en-\ngagement with the campaign. Finally, we would like to in-\nvestigate the generality of our findings by applying similar \nanalysis for social media campaigns on different topics.  \nReferences  \nBarbosa, L. and Feng, J. Robust sentiment detection on twitter \nfrom biased and noisy data. In Proc. COLING, 2010. \nBoyd, D. Golder, S., Lotan, G. 2010, Tweet, Tweet, Retweet: \nConversational Aspects of Retweeting on Twitter. In Proc. \nHICSS 2010.  \nChen, J. Pirolli, P. 2012. Why You Are More Engaged: Factors \nInfluencing Twitter Engagement.  In Proc. ICWSM 2012.   \nDavidov, D., Tsur, O. and Rappoport, A. Enhanced sentiment \nlearning using twitter hashtags and smileys. In Proc. COLING \n2010.  \nSchiffman, L.G. Kanuk, L.L. Wisenblit, J. Consumer Behavior. \n10th edition. Prentice Hall - 2010.  \n \nFigure 1.  Retweet rates for different target groups (500 mes-\nsages were sent in each case) \n",
      "id": 17187601,
      "identifiers": [
        {
          "identifier": "386121020",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1403.6067",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.485.8858",
          "type": "OAI_ID"
        },
        {
          "identifier": "1403.6067",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "25013051",
          "type": "CORE_ID"
        },
        {
          "identifier": "100481049",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ojs.aaai.org:article/14561",
          "type": "OAI_ID"
        }
      ],
      "title": "Why Do You Spread This Message? Understanding Users Sentiment in Social\n  Media Campaigns",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:ojs.aaai.org:article/14561",
        "oai:arxiv.org:1403.6067",
        "oai:citeseerx.psu:10.1.1.485.8858"
      ],
      "publishedDate": "2014-03-24T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1403.6067",
        "http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewFile/8030/8090/"
      ],
      "updatedDate": "2022-07-14T04:40:33",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1403.6067"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17187601"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Altrabsheh, Nabeela"
        },
        {
          "name": "Cocea, Mihaela"
        },
        {
          "name": "Gaber, M."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/52398881",
        "https://api.core.ac.uk/v3/outputs/29584880"
      ],
      "createdDate": "2015-09-29T10:13:11",
      "dataProviders": [
        {
          "id": 695,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/695",
          "logo": "https://api.core.ac.uk/data-providers/695/logo"
        },
        {
          "id": 1899,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1899",
          "logo": "https://api.core.ac.uk/data-providers/1899/logo"
        }
      ],
      "depositedDate": "",
      "abstract": null,
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/29584880.pdf",
      "fieldOfStudy": null,
      "fullText": "SA-E: Sentiment Analysis for Education \n \nNabeela Altrabsheh, Mohamed Medhat Gaber, Mihaela Cocea  \nSchool of Computing, Buckingham Building, \nLionTerrace,Portsmouth,Hampshire,PO13HE,UK \nE-mail: nabeela.altrabsheh@port.ac.uk, mohamed.gaber@port.ac.uk, \nmihaela.cocea@port.ac.uk  \nAbstract. Educational data mining (EDM) is an important research area that is used to \nimprove education by monitoring students performance and trying to understand the \nstudents’ learning. Taking feedback from students at the end of the semester, however, \nhas the disadvantage of not benefitting the students that have already taken the course. \nTo benefit the cur-rent students, feedback should be given in real time and addressed in \nreal time. This would enable students and lecturers to address teaching and learning \nissues in the most beneficial way for the students. Analysing students’ feedback using \nsentiment analysis techniques can identify the students’ positive or negative feelings, \nor even more refined emotions, that students have towards the current teaching. \nFeedback can be collected in a variety of ways, with previous research using student \nresponse systems such as clickers, SMS and mobile phones. This paper will discuss \nhow feedback can be collected via social media such as Twitter and how using \nsentiment analysis on educational data can help improve teaching. The paper also \nintroduces our proposed system Sentiment Analysis for Education (SA-E).  \nKeywords. Education Data Mining, Sentiment Analysis, Naive Bayes, SVM, Student \nResponse Systems  \n1. Introduction  \nEducational Data Mining (EDM) is application area of data mining that is developed to \naddress problems in education. Addressing such problems can lead to helping students who \nneed advice, removing and adding material to the unit according to students comprehension \nand finding students opinions about the course. Feedback in education can be categorised \nin to: 1. Feedback from the lecturer to the students, this is for the self improvement of the \nstudents; 2. Feedback from the students to the lecturer, this allows them to guide the \nlecturer into teaching the course in ways they understand best.  \nStudent Response Systems (SRS) is used for feedback in the classroom, given by \nstudents to the lecturer, via devices such as clickers and mobiles. However SRSs fall short \nto provide detailed feedback about what might have gone wrong. Social media can be a \ngood tool for students to provide such detailed feedback. Among those media comes \nTwitter. Twitter can provide students with a convenient way to express their feedback in \nfree text. This paper will show that using Sentiment Analysis on students feedback \nprovided by Twitter has many advantages. This is not the first time twitter has been used to \ntake students feedback. In [1] Twitter was used and the lecturer had to analyse the results, \nwhich caused work overload on the lecturer.  \nSentiment Analysis is a field that works on making sense out of textual feedback and \nopinions. Opinions can be negative or positive, different emotions can be associated with \nthose opinions. Emotions can be negative such as confused, bored, and irritated an example \nfor this is I do not know [2]. Positive emotions such as confident and enthusiastic can be \nexpressed such as when students writes in a louder style such as bold writing [2]. Neutral \nfeedback is when a student does not express negative or positive emotion [2]. Different \ntechniques have been used in sentiment analysis and a few have proved to give superior \nperformance such as Naive Bayes (NB), Max Entropy (MaxEnt) and Support Vector \nMachines (SVM).  \nThe paper is organised as follows. Section 2 will focus on student feedback through-\nout the lecture and discuss the disadvantages of clickers and Short Message Service (SMS). \nSection 3 will highlight the advantages and disadvantages of the sentiment analysis \ntechniques mentioned above and discuss more about sentiment analysis. Section 4 will \nshow how both areas can be combined and what can be implemented. Finally the paper is \nconcluded with a summary and future directions to work in Section 5.  \n2. Students Feedback  \nStudents feedback is important because it can help the lecturers understand the students \nlearning behaviour. Sometimes students do not understand what the lecturer is trying to \nexplain, thus by providing feedback students can indicate this to the lecturer. Students \nfeedback can also help in understanding different issues that students have including the \nstudents not understanding the lecture.  \nFeedback needs to be taken in order to make improvements in teaching [5]. If the \nstudents do not participate in giving feedback then there is no way in finding out if the \nteaching needs improvement. Students often act as observers in the classroom and expect \nthe lecturer to feed them with information, this is a problem especially for international \nstudents that have come from different backgrounds and experienced different teaching \nmethods. Student engagement is important in education and one way of measuring it is \nthrough participation [6].The traditional way of students asking questions is raising their \nhand to ask, although this way does not suit everyone such as shy people. A study about \nstudent engagement [6] showed that participation was lower by means of raising hands \ncompared with the use of clickers, meaning that clickers are more popular with students \nthan raising hands as means of engaging in learning. Another disadvantage to hand raising \nis that students can look to see other responses before making a response and copy from \nother students [7]  \nStudents lack of participation is a common concern for educators [6]. In large classes it \nmay be time consuming if every person needed one question then not much material will \nbe covered. In [6] the author attempted to find out how student response systems impact \nstudent learning in large lectures [6], where students have less chance to ask questions \nbecause of the class size [8]. Lack of participation could be from students not paying \nattention. Students can be daydreaming due to having difficulty in maintaining attention \nthroughout classes [7].Some interesting results showed that students that participate in \nclass achieve better results than students who do not [9]  \nFeedback can be collected through a variety of SRSs, including clickers and mo-bile \nphones. However, in addition to specifically asking for feedback lecturers can ask students \nquestions and students can ask lecturer questions. Therefore, SRS devices can help in \ncommunication between the lecturer and student. One common disadvantage of SRS is the \ncost of the clickers and mobile phones, but nowadays it is very rare to find a student \nwithout a mobile so there is not an extra cost for the student or the university. The \nfollowing subsections present in more detail three types of SRS: Clickers, Mobile Phones \nand Social Media.  \n2.1. Clickers  \nClickers are handheld devices which usually contain one or more buttons. In [10] study, the \nclickers that were used had one button which was labelled yes and this button was used to \nrespond to the lecturers enquiries. The lecturer explained how the clickers are used and \nthen throughout the lecture the lecturer asked the students questions for instance if they \nwere ready for the lecture to be continued and if they understood a certain point.  \nOne the many advantages of clickers is allowing students to focus longer on the ma-\nterial and learning it by participation instead of focusing on taking notes throughout the \nentire lecture [7]. On the other hand, in addition to cost, clickers have the disadvantage of \nstudents losing them, breaking them or forgetting to bring them to class. Moreover, clickers \ncan distract students in class [6].To solve the problems of clicker cost and the limited \ninformation derived from the data mobile phones came as an alternative solution.  \n2.2. Mobile phones  \nMobile phones are popular with students, with 98% of students owning a mobile phone \n[11]. Students usually leave their mobiles on throughout class, therefore being able to use \nthem for feedback. However, students can also use mobile in other activities such as \nsending messages to their friends [11] ,[12] . One example of students wrongly using their \nmobiles is flooding the system with inappropriate messages [12] .The use of mobile phones \nin the classroom also has some negative outcomes or distractions such as students phones \nringing during class. Feedback can be taken from mobile phones using applications and \ntext messages which are presented in more detail below.  \n \n2.2.1. Clicker Application  \nAn example of Clicker Application is found in [13]. A study that analysed the feedback of \nthe students via mobile phones this project name is Crowd Feedback. The students could \npost feedback about the lesson and the lecturers phone would buzz if something interesting \nwas found from feedback analysis. The system was not only giving feedback for the \nlecturer but for students as well in order for them to interact and know what each of the \nchildren comprehended. The system also showed that the students had given feedback all \nthroughout the lecture and that there were two high ”dislike” peaks. Further study can be \ndone to find out what had gone wrong at those two times. The paper acknowledges that \nfeedback would be more beneficial if it were richer than just providing yes and no answers. \nAs clicker application solved the solution to the cost of devices it still did not solve the fact \nthat the data received from the students was limited. There is need for students to write \nsentences to give a cleared feedback and SMS solved this problem.  \n2.2.2. Short Message Service (SMS)  \nThe idea of collecting feedback through SMS in the education system was proposed in \n[14]. In their research they created a model that collected learning feedback from the \nstudents via SMS messages. One aim to their system is to improve the delivery of the \nlesson by finding out students opinions. Although the system had many benefits they found \nthat taking feedback via SMS had many flaws such as the limitation of the message space \nto a certain amount of characters and incomplete SMS due to this limitation. Other \ndownsides to this project found were the spelling mistakes that the audience texted, this \nwas solved by a model called the corrected model. The corrected model corrected text to \nsimilar words such as slp to sleep. Another model that they created was the sentiment \nmodel. This model implemented sentiment mining on the correction model to find \ninterestingness and divide the concepts into true and false. Although words can have \ndifferent meanings and can be positive and negative according to the student involved.  \nOne aspect that the authors [14] did not take into consideration is the cost of the SMS \ntexting service, [15] suggested lowering the cost barrier may help in increasing feedback in \nlectures. Also the authors in [14] tested the students feedback at the end of each lesson, this \nis great for improving the lectures over time as the lecturer can modify the teaching after \nreceiving this feedback in the next lesson, however this would be more effective if the \nfeedback was throughout the lesson in timescales to insure that students get the greatest \npossible out of each lesson. It was suggested in [14] that including timestamp and the date \nwith the feedback can help monitor the improvement of teaching over time. Such a trend \nanalysis would be a useful tool for lecturers that would help them improve their teaching.  \n \n2.3. Social Media  \nIt would be easier to compare between tweet or Facebook posts especially with the social \nnetworks becoming the most popular communication nowadays. Social media is popular, \nstatistics show that 93.5% of 18 year olds and 95.4% of 19 year olds in the USA were \nfound to use social networking on a regular basis [1]. 52.1% of academics in 2010 say that \nthey have used Twitter [1]. In addition to that over 470 universities worldwide are using \nsocial networks such as Facebook and Twitter to communicate with students [1].  \nFrom Twitter advantages in education is that students are familiar with the tool and \ntraining will not be needed [1]. Another great benefit to using twitter as feedback is that it \nis free as twitter can be opened from the their own mobiles on the university wireless \nnetwork. Twitter has some disadvantages such as it being a distraction for the students or \nthe lecturer to have to multitask [8]. Also the tweets appear sequentially so the lecturer has \nto read from the beginning to understand what is going on therefore time loss happens \n[8].Twitter solved the issue of the SMS cost issue as it is a free tool that anyone can use \nand students can access it via the universities network. The characters are limited which \ncan be seen as an advantage to students using as less words as possible and this makes \nthem focus on the important words to create a sentence as meaningful as possible. Some of \nthe disadvantages where that the lecturer has to scroll through the tweets in order to \nunderstand the students opinions and analyse the text manually. This is where Sentiment \nAnalysis for Education tool will help as the lecturer will only have to stop a minute every \n20 minutes for the student to post their tweets and the tool will analyse the results and \nreturn them immediately.  \n3. Sentiment Analysis  \nSentiment Analysis also known as subjectivity analysis, opinion mining, and appraisal \nextraction [16] is an application of natural language processing, computational linguistics \nand text analytics to identify and retrieve certain information from the text, this is done by \nstudying the subjectivity or the opinion. When looking for the success of a product it is \nimportant to know what features the user liked or disliked, the term for this is feature \nextraction. Sentiment polarity is usually either positive or negative but polarity can also be \nexpressed as a range such as how much the user liked the product this range can be into an \nn-point scale, e.g., very good, good, satisfactory, bad, very bad [17]. Finding positive and \nnegative words without the n point scale is easier than trying to determine the weight of \nthat word. Sentiment analysis can also be used to extract different users emotions from the \ntext such as Love, Joy, Anger, Frustration, and Neutral [18]. Subjectivity and emotion are \ntwo close concepts subjectivity represents facts and also emotion, feelings, views and \nbeliefs. [19] found that sometimes positive words can be put into negative reviews and vice \nversa. They also found that different users have different opinions about positive rooms \nsome like it to be clean some like it to be big so there is a variety in what people put as \nnegative and positive but in the end a pattern can be made to find that most people like a \ncertain characteristic and then it can be positive. The word can have a positive meaning in \nsome ways and negative in other ways such as the word small it can be negative if \ndescribing a hotel and positive if describing a mobile. It was found through tests that \nsentences do matter in the sentiment and a word by itself can be positive or negative \naccording to the sentence it is put it.  \nReviews are affected by students emotions [20] and this is why it is important to \nunderstand if the students are struggling in the course and what they dislike about the \ncourse. [18] looked into emotions of the e-learners through their texts this allows us to \nbreak the distant of emotions between the e-learner and the lecturer. [18] found that most \nof the research that has been done toward the e-learners cognition and not their emotion or \nsentiment, the distant between the lecturer and the student is a main cause for this. The \nfacial expression plays an important role in understanding the persons emotions. E-learning \nhas rarely any face to face communication unless the lecturer and student have both \nequipped web cam and this is why there is need to understand the students text and to \nanalyse it to find what the students feelings are toward the lecture and this will hopefully \nimprove from the quality of teaching.  \nAnalysing text can help the lecturer understand the student more carefully , the \nemotion types in [18] were categorised into Love, Joy, Anger, Frustration, and Neutral. \nThe student can express his feelings in short expressions or words. [18] created patterns to \nfind what words are associated more with emotions, they also give solutions to relieve the \ne-learner such when an e-learner typed that he is depressed because his supervisor does not \nlike his report. Table 1 shows the adjustment strategy to help the user:  \nTable [1]: Example to E-learners Emotion \nEvent set  emotion  Sentiment adjustment strategy  \ncriticized by teachers  frustration  Suggest him to communicate with \nthe teacher  \nBlamed by teacher  frustration and anxiety  Say something like that you had \ndone your best, so dont blame \nyourself  \nYelled by teacher  frustration  Tell him that Experience is the \nmother of wisdom, do a better job \nnext time  \n \n3.1. Sentiment Analysis Techniques  \nThe most common technique in sentiment analysis are Naive Bayes , Maximum entropy \nand Support vector machine. These features have been proved to work well with sentiment \ndata and have been used and praised in the following: 1: Joachims, 1998 cited in Prabowo \nand Thelwall, 2009; 2: Pang, Lee and Vaithyanathan, 2002; 3: Vachaspati and Wu 2012; 4: \nGo Bhayani and Huang 2009 and 5: Lu, Peng, Li and Ahmed, 2006.  \nIn [21] Naive Bayes gave good accurate results when implemented on reviews, and \nblog posts. As for Lexical method it varied, and gave good and bad results. SVM method \nwas used for reviews, and alone it showed small accuracy but when accompanied with \nNaive Bayes or Lexical methods the results become higher. Therefore, to summarise it is \nbetter to combine the methods to get better accuracy results. The research showed that[21] \nhad different results with[22] which claimed that SVM is the best classifier after comparing \nthe Naive Bayes and the SVM classifier. And [23] concluded that the three classifier he \nused, Naive Bayes, maximum entropy and SVM had similar performance. in [31] the best \nresults was found with the SVM classifier, comparing the same three classifier and  \n[22] also said that although SVM is the best classifier there are some differences in the \noverall performance.  \n3.2. Features  \nAfter deciding which technique to use features that will be considered in the experiment \nshould be decided. Features allow a more accurate analysis of the sentiments, and for a \nmore detailed summarization of the results [21]. These are some features that can be used : \n1. Term Presence and Frequency : the frequency of the word and the presence of some \nwords. 2. N-grams: position of the word and taking one two or three words.  \n3. Part-of-Speech: Adjective are good indicators that there is some kind of opinion 4. \nSyntax 5. Negation: Negation is flipping the positive into negative or negative to positive, \nNegators can affect the word and make it the opposite Examples to some negators are not, \nnone, nobody, never, and nothing [24], problems with negation could be: a. Include the \nnegation to a term close to negation such as like and not , the location sometimes affects \nthis like including not at the end [21] ; b. Negating the sentiment in the sentence such as No \nwonder everyone loves it. [21] Some examples of negators from [24] are shown in Table 2:  \n \nTable [2]: Negator Examples \nExample  Negation  \nNobody gives a good performance \nin this movie.  \nNobody negates good  \nOut of every one of the fourteen \ntracks, none of them approach be-\ning weak and are all stellar.  \nNone negates weak  \nJust a V-5 engine, nothing \nspectacular  \nNothing negates spectacular  \n \n3.3. Data Source  \nThe internet has become a main source for the users to express their opinion . More users \nare willing to express their opinions online [25]. This has a good advantage as more \nopinions can be extracted from a wider source. Social networks has been used in data \nmining for many years.[26] gave some good advantages in using twitter such as twitter is \nup-to-date and reflects the current news and events happening around the world. For this \nproject the data has to be in real-time and twitter will be used  \n3.4. Pre-processing data  \nAfter obtaining the data from the source, the data has to be pre-processed before the \nsentiment analysis stage this is to increase accuracy and to reduce error in the data. Pre-\nprocessing can be discovering emoticons, words in upper case, removing stop words, \nremoving unnecessary punctuation, finding exclamation marks or question marks, and \nremoving inconsistent casing of letters [27].  \n4. Sentiment Analysis in Educational Data  \nSentiment analysis has not been implemented on the educational sector yet. When \ncombining these areas together education can be improved by saving time in analysing \nreal-time feedback. Students can use twitter to express their opinions about the lecturer for \nexample the lecture can change the pace of the lecture [28]. When finding out students \nopinions over time intervals the lecturer can alter teaching style according to the results, \nrepeat a part that the majority of the students did not understand and answer any questions \nwithout the need for student to interrupt the lecturer. The following areas will be discussed \nin the framework: 1. The method used to send feedback; 2. The time slots in the lecturer \nand how to post feedback 3. The Techniques used to analyse the feedback 4. The results  \n \n4.1. SA-E System Architecture  \n \nThe research steps will be implemented in the classroom as followed: Students will pro-\nvide feedback to the lecturer via social networks such as twitter. The reason that social \nnetworks will be the source of data is because it is free and popular among students \nnowadays. In 2010 Twitter had 106 million users, and 180 million visitors every month \n[29]. Twitter company revealed that 300,000 new users were signing up every day and that \nit received 600 million queries daily through its search engine. 37 % of Twitters users used \ntheir phone to send messages [29].This will overcome the problems of SMS being \nexpensive or clickers giving limited feedback.  \nThe students feedback can be taken at anytime in the lecture but there will also be time \nslots chosen by the lecturer according to stop point which he thinks that the previous topic \nwill follow on to the next topic. The time slots can also be randomly chosen at a 20 minute \ntime slot to insure that students understand every part of the lecturer and that they will not \nget lost in understanding any part of the lecture. The lecturer can also change the slots \nwhile giving the lecture according to the results, so for example if the students had not \nunderstood a part the lecturer may decide to put more slots at the next part of the session to \nmake the students understand more. Within a amount of time if the lecturer found that \nstudents are often finding difficulty in one part then he may decide to explain it in another \nway. Time series is an important part in this project, students may show a high response \nwithin a certain time, this can be traced back to find what went wrong at the time, and why \nstudents are not understanding a particular module and many more. After a certain time \npatterns can be derived from this data and future students will have an even better \neducation.  \nStudents feedback will be pre-processed and then analysed via Naive Bayes and \nSupport Vector Machine individually or combined. These techniques were chosen be-cause \nthey have been proved to work well with reviews and educational data. This will analyse \nthat the document phrase and this is the post or tweet as a whole is positive or negative. \nThere could be positive words in negative reviews and vice versa, this will be looked into \nas well. The raw data will be pre-processed to insure the quality of the data. Then different \nfeatures will be applied to insure that the focus is going to the correct part of data. The \nfeatures will be decided throughout experiments. The results will be passed through an \napplication where the lecturer can decide to act according to the results.  \nAfter obtaining the results verification methods can be applied to assure that the \nsentiment analysis has been a success. From these methods can be exploring students facial \nexpressions to find if students emotions match with the results obtained. The lecturer can \nask questions to insure that the students responses are correct. The results will be kept for \nfurther study and other use will be made out of the data by analysing it over long periods of \ntime.  \n \nFigure 1: SA-E System Architecture  \nSentiment Analysis in the education sector can be a wide area and hold many \npossibilities. Although this paper has only shown a few point on how this can be \nimplemented, there can be further points that can be researched. Figure 1 shows SA-E \nArchitecture.  \nThis paper provides the reader with a new perspective that can open areas of research \nin educational data mining. This can be achieved with the adoption of the fast growing area \nof Sentiment Analysis.  \n5. Conclusion  \nIn this paper, a brief background of educational data mining and sentiment analysis was \npresented including different methods for collecting feedback from students. We noted that \nthe adoption of the emerging area of data mining sentiment analysis in educational systems \nhas a great potential. Sentiment analysis techniques were explained briefly with a \ncomparison, and it was decided that Naive Bayes and SVM techniques were superior for \neducation data [21] . We have also discussed how these two are could be combined for the \nanalysis of students feedback in real-time. We have introduced our system architecture \ntermed Systems Analysis for Education (SA-E). The realisation for SA-E opens the door \nfor a potentially fruitful application of sentiment analysis.  \nReferences  \n[1] Novak, Jeremy, and Michael Cowling. ”The implementation of social networking as a tool for improving \nstudent participation in the classroom.” (2011).  \n[2] Litman, Diane J., and Kate Forbes-Riley. ”Predicting student emotions in computer-human tutoring \ndialogues.” Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. \nAssociation for Computational Linguistics, 2004.  \n[3] Cummins, Stephen, Liz Burd, and Andrew Hatch. ”Using Feedback Tags and Sentiment Analysis to \nGenerate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags in a \nProgramming Course.” Advanced Learning Technologies (ICALT), 2010 IEEE 10th International \nConference on. IEEE, 2010.  \n[4] Agrawal, Rakesh, et al. ”Data mining for improving textbooks.” ACM SIGKDD Explorations Newsletter  \n13.2 (2012): 7-19.  \n[5] Poulos, Ann, and Mary Jane Mahony. ”Effectiveness of feedback: the students perspective.” Assessment \nand Evaluation in Higher Education 33.2 (2008): 143-154.  \n[6] Denker, Katherine J. ”Student Response Systems and Facilitating the Large Lecture Basic Communica-tion \nCourse: Assessing Engagement and Learning.” Communication Teacher 27.1 (2013).  \n[7] Padhy, Neelamadhab, Pragnyaban Mishra, and Rasmita Panigrahi. ”The Survey of Data Mining Appli-\ncations And Feature Scope.” International Journal of Computer Science (2012).  \n[8] Gehringer, Edward F. ”Ac 2012-4769: Applications For Supporting Collaboration In The Classroom.” \n(2012).  \n[9] Gauci, Sally A., et al. ”Promoting student-centered active learning in lectures with a personal response \nsystem.” Advances in Physiology Education 33.1 (2009): 60-71.  \n[10] Poulis, J., et al. ”Physics lecturing with audience paced feedback.” American Journal of Physics 66 (1998): \n439.  \n[11] Scornavacca, Eusebio, Sid Huff, and Stephen Marshall. ”Mobile phones in the classroom: If you can´t beat \nthem, join them.” Communications of the ACM 52.4 (2009): 142-146.  \n[12] Br, Henning, Erik Tews, and Guido Rling. ”Improving feedback and classroom interaction using mobile \nphones.” Proceedings of Mobile Learning (2005): 55-62.  \n[13] Teevan, Jaime, et al. ”Displaying Mobile Feedback During a Presentation.” (2012).  \n[14] Leong, Chee Kian, Yew Haur Lee, and Wai Keong Mak. ”Mining sentiments in SMS texts for teaching \nevaluation.” Expert Systems with Applications 39.3 (2012): 2584-2589.  \n[15] Kinsella, Stephen. ”Many to one: Using the mobile phone to interact with large classes.” British Journal of \nEducational Technology 40.5 (2009): 956.  \n[16] Pang, Bo, and Lillian Lee. Opinion mining and sentiment analysis. Now Pub, 2008.  \n[17] Prabowo, Rudy, and Mike Thelwall. ”Sentiment analysis: A combined approach.” Journal of Informet-rics \n3.2 (2009): 143-157.  \n[18] Tian, Feng, et al. ”Can e-Learner’s emotion be recognized from interactive Chinese texts?.” Computer \nSupported Cooperative Work in Design, 2009. CSCWD 2009. 13th International Conference on. IEEE, \n2009.  \n[19] Rahayu, D. A., et al. ”RnR: Extracting Rationale from Online Reviews and Ratings.” Data Mining \nWorkshops (ICDMW), 2010 IEEE International Conference on. IEEE, 2010.  \n[20] Binali, Haji H., Chen Wu, and Vidyasagar Potdar. ”A new significant area: Emotion detection in E-learning \nusing opinion mining techniques.” Digital Ecosystems and Technologies, 2009. DEST’09. 3rd IEEE \nInternational Conference on. IEEE, 2009.  \n[21] Mejova, Yelena. ”Sentiment Analysis: An Overview. Comprehensive Exam Paper.” Computer Science \nDepartment (2009).  \n[22] de Groot, R. ”Data Mining for Tweet Sentiment Classification.” (2012).  \n[23] Go, Alec, Richa Bhayani, and Lei Huang. ”Twitter sentiment classification using distant supervision.” \nCS224N Project Report, Stanford (2009): 1-12.  \n[24] Taboada, Maite, et al. ”Lexicon-based methods for sentiment analysis.” Computational Linguistics 37.2 \n(2011): 267-307.  \n[25] Zuo, Mingzhang, et al. ”Data mining strategies and techniques of internet education public sentiment \nmonitoring and analysis system.” Future Computer and Communication (ICFCC), 2010 2nd International \nConference on. Vol. 2. IEEE, 2010.  \n[26] Sriram, Bharath, et al. ”Short text classification in twitter to improve information filtering.” Proceeding of \nthe 33rd international ACM SIGIR conference on research and development in information retrieval. ACM, \n2010.  \n[27] Prasad, Suhaas. Micro-blogging Sentiment Analysis Using Bayesian Classification Methods. Technical \nReport, 2010.  \n[28] Cummings, Richard G., and Maxwell Hsu. ”The effects of student response systems on performance and \nsatisfaction: An investigation in a tax accounting class.” Journal of College Teaching and Learning (TLC) \n4.12 (2011).  \n[29] Bifet, Albert, and Eibe Frank. ”Sentiment knowledge discovery in twitter streaming data.” Discovery \nScience. Springer Berlin/Heidelberg, 2010.  \n[30] Bhargavi, P., and S. Jyothi. ”Applying Naive Bayes data mining technique for classification of agricul-tural \nland soils.” International Journal of Computer Science and Network Security 9.8 (2009): 117-122.  \n[31] Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. ”Thumbs up?: sentiment classification using \nmachine learning techniques.” Proceedings of the ACL-02 conference on Empirical methods in natural \nlanguage processing-Volume 10. Association for Computational Linguistics, 2002.  \n",
      "id": 18098469,
      "identifiers": [
        {
          "identifier": "29584880",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:researchportal.port.ac.uk:publications/c99ed217-4c60-494f-8c2c-4f049cf43585",
          "type": "OAI_ID"
        },
        {
          "identifier": "52398881",
          "type": "CORE_ID"
        }
      ],
      "title": "SA-E: Sentiment Analysis for Education",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:researchportal.port.ac.uk:publications/c99ed217-4c60-494f-8c2c-4f049cf43585"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 38691011,
          "title": "A new significant area: Emotion detection in E-learning using opinion mining techniques.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1109/dest.2009.5276726",
          "raw": "Binali, Haji H., Chen Wu, and Vidyasagar Potdar. ”A new significant area: Emotion detection in E-learning using opinion mining techniques.” Digital Ecosystems and Technologies, 2009. DEST’09. 3rd IEEE International Conference on. IEEE, 2009.",
          "cites": null
        },
        {
          "id": 38690986,
          "title": "Ac 2012-4769: Applications For Supporting Collaboration In The Classroom.”",
          "authors": [],
          "date": "2012",
          "doi": null,
          "raw": "Gehringer, Edward F. ”Ac 2012-4769: Applications For Supporting Collaboration In The Classroom.” (2012).",
          "cites": null
        },
        {
          "id": 38691030,
          "title": "Applying Naive Bayes data mining technique for classification of agricul-tural land soils.”",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "Bhargavi, P., and S. Jyothi. ”Applying Naive Bayes data mining technique for classification of agricul-tural land soils.” International Journal of Computer Science and Network Security 9.8 (2009): 117-122.",
          "cites": null
        },
        {
          "id": 38691007,
          "title": "Can e-Learner’s emotion be recognized from interactive Chinese texts?.” Computer Supported Cooperative Work in Design,",
          "authors": [],
          "date": "2009",
          "doi": "10.1109/cscwd.2009.4968116",
          "raw": "Tian, Feng, et al. ”Can e-Learner’s emotion be recognized from interactive Chinese texts?.” Computer Supported Cooperative Work in Design, 2009. CSCWD 2009. 13th International Conference on. IEEE, 2009.",
          "cites": null
        },
        {
          "id": 38690977,
          "title": "Data mining for improving textbooks.”",
          "authors": [],
          "date": "2012",
          "doi": "10.1145/2207243.2207246",
          "raw": "Agrawal, Rakesh, et al. ”Data mining for improving textbooks.” ACM SIGKDD Explorations Newsletter 13.2 (2012): 7-19.",
          "cites": null
        },
        {
          "id": 38691014,
          "title": "Data Mining for Tweet Sentiment Classification.”",
          "authors": [],
          "date": "2012",
          "doi": null,
          "raw": "de Groot, R. ”Data Mining for Tweet Sentiment Classification.” (2012).",
          "cites": null
        },
        {
          "id": 38691019,
          "title": "Data mining strategies and techniques of internet education public sentiment monitoring and analysis system.”",
          "authors": [],
          "date": "2010",
          "doi": "10.1109/icfcc.2010.5497355",
          "raw": "Zuo, Mingzhang, et al. ”Data mining strategies and techniques of internet education public sentiment monitoring and analysis system.” Future Computer and Communication (ICFCC), 2010 2nd International Conference on. Vol. 2. IEEE, 2010.",
          "cites": null
        },
        {
          "id": 38690994,
          "title": "Displaying Mobile Feedback During a Presentation.”",
          "authors": [],
          "date": "2012",
          "doi": "10.1145/2371574.2371633",
          "raw": "Teevan, Jaime, et al. ”Displaying Mobile Feedback During a Presentation.” (2012).",
          "cites": null
        },
        {
          "id": 38690981,
          "title": "Effectiveness of feedback: the students perspective.” Assessment and Evaluation",
          "authors": [],
          "date": "2008",
          "doi": "10.1080/02602930601127869",
          "raw": "Poulos, Ann, and Mary Jane Mahony. ”Effectiveness of feedback: the students perspective.” Assessment and Evaluation in Higher Education 33.2 (2008): 143-154.",
          "cites": null
        },
        {
          "id": 38690993,
          "title": "Improving feedback and classroom interaction using mobile phones.”",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": "Br, Henning, Erik Tews, and Guido Rling. ”Improving feedback and classroom interaction using mobile phones.” Proceedings of Mobile Learning (2005): 55-62.",
          "cites": null
        },
        {
          "id": 38691016,
          "title": "Lexicon-based methods for sentiment analysis.”",
          "authors": [],
          "date": "2011",
          "doi": "10.1162/coli_a_00049",
          "raw": "Taboada, Maite, et al. ”Lexicon-based methods for sentiment analysis.” Computational Linguistics 37.2 (2011): 267-307.",
          "cites": null
        },
        {
          "id": 38691004,
          "title": "Many to one: Using the mobile phone to interact with large classes.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1111/j.1467-8535.2008.00888.x",
          "raw": "Kinsella, Stephen. ”Many to one: Using the mobile phone to interact with large classes.” British Journal of Educational Technology 40.5 (2009): 956.",
          "cites": null
        },
        {
          "id": 38691023,
          "title": "Micro-blogging Sentiment Analysis Using Bayesian Classification Methods.",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "Prasad, Suhaas. Micro-blogging Sentiment Analysis Using Bayesian Classification Methods. Technical Report, 2010.",
          "cites": null
        },
        {
          "id": 38690992,
          "title": "Mobile phones in the classroom: If you can´t beat them, join them.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1145/1498765.1498803",
          "raw": "Scornavacca, Eusebio, Sid Huff, and Stephen Marshall. ”Mobile phones in the classroom: If you can´t beat them, join them.” Communications of the ACM 52.4 (2009): 142-146.",
          "cites": null
        },
        {
          "id": 38691005,
          "title": "Opinion mining and sentiment analysis. Now Pub,",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "Pang, Bo, and Lillian Lee. Opinion mining and sentiment analysis. Now Pub, 2008.",
          "cites": null
        },
        {
          "id": 38690991,
          "title": "Physics lecturing with audience paced feedback.”",
          "authors": [],
          "date": "1998",
          "doi": "10.1119/1.18883",
          "raw": "Poulis, J., et al. ”Physics lecturing with audience paced feedback.” American Journal of Physics 66 (1998): 439.",
          "cites": null
        },
        {
          "id": 38690961,
          "title": "Predicting student emotions in computer-human tutoring dialogues.”",
          "authors": [],
          "date": "2004",
          "doi": "10.3115/1218955.1219000",
          "raw": "Litman, Diane J., and Kate Forbes-Riley. ”Predicting student emotions in computer-human tutoring dialogues.” Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2004.",
          "cites": null
        },
        {
          "id": 38690989,
          "title": "Promoting student-centered active learning in lectures with a personal response system.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1152/advan.00109.2007",
          "raw": "Gauci, Sally A., et al. ”Promoting student-centered active learning in lectures with a personal response system.” Advances in Physiology Education 33.1 (2009): 60-71.",
          "cites": null
        },
        {
          "id": 38691009,
          "title": "RnR: Extracting Rationale from Online Reviews and Ratings.” Data Mining Workshops (ICDMW),",
          "authors": [],
          "date": "2010",
          "doi": "10.1109/icdmw.2010.167",
          "raw": "Rahayu, D. A., et al. ”RnR: Extracting Rationale from Online Reviews and Ratings.” Data Mining Workshops (ICDMW), 2010 IEEE International Conference on. IEEE, 2010.",
          "cites": null
        },
        {
          "id": 38691006,
          "title": "Sentiment analysis: A combined approach.”",
          "authors": [],
          "date": "2009",
          "doi": "10.1016/j.joi.2009.01.003",
          "raw": "Prabowo, Rudy, and Mike Thelwall. ”Sentiment analysis: A combined approach.” Journal of Informet-rics 3.2 (2009): 143-157.",
          "cites": null
        },
        {
          "id": 38691013,
          "title": "Sentiment Analysis: An Overview.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "Mejova, Yelena. ”Sentiment Analysis: An Overview. Comprehensive Exam Paper.” Computer Science Department (2009).",
          "cites": null
        },
        {
          "id": 38691029,
          "title": "Sentiment knowledge discovery in twitter streaming data.” Discovery Science.",
          "authors": [],
          "date": "2010",
          "doi": "10.1007/978-3-642-16184-1_1",
          "raw": "Bifet, Albert, and Eibe Frank. ”Sentiment knowledge discovery in twitter streaming data.” Discovery Science. Springer Berlin/Heidelberg, 2010.",
          "cites": null
        },
        {
          "id": 38691022,
          "title": "Short text classification in twitter to improve information filtering.”",
          "authors": [],
          "date": "2010",
          "doi": "10.1145/1835449.1835643",
          "raw": "Sriram, Bharath, et al. ”Short text classification in twitter to improve information filtering.” Proceeding of the 33rd international ACM SIGIR conference on research and development in information retrieval. ACM, 2010.",
          "cites": null
        },
        {
          "id": 38690985,
          "title": "Student Response Systems and Facilitating the Large Lecture Basic Communica-tion Course: Assessing Engagement and",
          "authors": [],
          "date": "2013",
          "doi": "10.1080/17404622.2012.730622",
          "raw": "Denker, Katherine J. ”Student Response Systems and Facilitating the Large Lecture Basic Communica-tion Course: Assessing Engagement and Learning.” Communication Teacher 27.1 (2013).",
          "cites": null
        },
        {
          "id": 38691025,
          "title": "The effects of student response systems on performance and satisfaction: An investigation in a tax accounting class.”",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Cummings, Richard G., and Maxwell Hsu. ”The effects of student response systems on performance and satisfaction: An investigation in a tax accounting class.” Journal of College Teaching and Learning (TLC) 4.12 (2011).",
          "cites": null
        },
        {
          "id": 38690959,
          "title": "The implementation of social networking as a tool for improving student participation in the classroom.”",
          "authors": [],
          "date": "2011",
          "doi": null,
          "raw": "Novak, Jeremy, and Michael Cowling. ”The implementation of social networking as a tool for improving student participation in the classroom.” (2011).",
          "cites": null
        },
        {
          "id": 38691034,
          "title": "Thumbs up?: sentiment classification using machine learning techniques.”",
          "authors": [],
          "date": "2002",
          "doi": "10.3115/1118693.1118704",
          "raw": "Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. ”Thumbs up?: sentiment classification using machine learning techniques.” Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics, 2002.",
          "cites": null
        },
        {
          "id": 38691015,
          "title": "Twitter sentiment classification using distant supervision.” CS224N Project Report,",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "Go, Alec, Richa Bhayani, and Lei Huang. ”Twitter sentiment classification using distant supervision.” CS224N Project Report, Stanford (2009): 1-12.",
          "cites": null
        },
        {
          "id": 38690963,
          "title": "Using Feedback Tags and Sentiment Analysis to Generate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags",
          "authors": [],
          "date": "2010",
          "doi": "10.1109/icalt.2010.186",
          "raw": "Cummins, Stephen, Liz Burd, and Andrew Hatch. ”Using Feedback Tags and Sentiment Analysis to Generate Sharable Learning Resources Investigating Automated Sentiment Analysis of Feedback Tags in a Programming Course.” Advanced Learning Technologies (ICALT), 2010 IEEE 10th International Conference on. IEEE, 2010.",
          "cites": null
        },
        {
          "id": 38691000,
          "title": "Yew Haur Lee, and Wai Keong Mak. ”Mining sentiments in SMS texts for teaching evaluation.” Expert Systems with",
          "authors": [],
          "date": "2012",
          "doi": "10.1016/j.eswa.2011.08.113",
          "raw": "Leong, Chee Kian, Yew Haur Lee, and Wai Keong Mak. ”Mining sentiments in SMS texts for teaching evaluation.” Expert Systems with Applications 39.3 (2012): 2584-2589.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "https://researchportal.port.ac.uk/portal/services/downloadRegister/209654/idt13-014-published.pdf"
      ],
      "updatedDate": "2021-08-17T07:17:57",
      "yearPublished": 2013,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/29584880.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/29584880"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/29584880/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/29584880/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/18098469"
        }
      ]
    },
    {
      "acceptedDate": "2009-07-28T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Adam",
        "James"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/143907117",
        "https://api.core.ac.uk/v3/outputs/192641348",
        "https://api.core.ac.uk/v3/outputs/11309106",
        "https://api.core.ac.uk/v3/outputs/147598315"
      ],
      "createdDate": "2013-07-10T11:53:32",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        }
      ],
      "depositedDate": "2009-01-01T00:00:00",
      "abstract": "Evaluation of sentiment analysis, like large-scale IR evalu-\n\nation, relies on the accuracy of human assessors to create\n\njudgments. Subjectivity in judgments is a problem for rel-\n\nevance assessment and even more so in the case of senti-\n\nment annotations. In this study we examine the degree to\n\nwhich assessors agree upon sentence-level sentiment anno-\n\ntation. We show that inter-assessor agreement is not con-\n\ntingent on document length or frequency of sentiment but\n\ncorrelates positively with automated opinion retrieval per-\n\nformance. We also examine the individual annotation cate-\n\ngories to determine which categories pose most di±culty for\n\nannotators",
      "documentType": "research",
      "doi": "10.1145/1571941.1572127",
      "downloadUrl": "https://core.ac.uk/download/11309106.pdf",
      "fieldOfStudy": "computer science",
      "fullText": "A Study of Inter-Annotator Agreement for Opinion Retrieval\nAdam Bermingham and Alan F. Smeaton\nCLARITY: Centre for Sensor Web Technologies\nDublin City University\nDublin, Ireland.\n{abermingham,asmeaton}@computing.dcu.ie\nABSTRACT\nEvaluation of sentiment analysis, like large-scale IR evalu-\nation, relies on the accuracy of human assessors to create\njudgments. Subjectivity in judgments is a problem for rel-\nevance assessment and even more so in the case of senti-\nment annotations. In this study we examine the degree to\nwhich assessors agree upon sentence-level sentiment anno-\ntation. We show that inter-assessor agreement is not con-\ntingent on document length or frequency of sentiment but\ncorrelates positively with automated opinion retrieval per-\nformance. We also examine the individual annotation cate-\ngories to determine which categories pose most difficulty for\nannotators.\nCategories and Subject Descriptors\nH.3.4 [Information Retrieval]: Systems and Software Per-\nformance evaluation (efficiency and effectiveness)\nGeneral Terms\nExperimentation, Measurement, Human Factors\n1. INTRODUCTION\nWith the abundance of user-generated content on the Inter-\nnet in recent years, there has been much effort to model the\nsentiment in online texts. Annotated documents are nec-\nessary to evaluate systems designed to automatically clas-\nsify, rank or score documents with respect to opinion. In\nsome domains, such as film reviews, a sentiment polarity\nscore is often readily available as users annotate their doc-\numents with a quantified summary e.g. 4 out of 5 stars.\nIn other domains an author annotation is not available and\nwe rely on human assessors to create annotations or judge-\nments. There are a number of subjective variables associ-\nated specifically with opinion annotation which affect agree-\nment including domain expertise, personal opinion, ambigu-\nity of language, and context of interpretation. One other\nissue is granularity of sentiment and previous annotation ef-\nforts have varied from the document level [3] to sentence-\nand sub-sentence-levels [5]. There have also been efforts at\nmulti-lingual sentence-level opinion annotation which have\nyielded moderately high rates of agreement for Japanese and\nChinese but low agreement for English texts [4].\nCopyright is held by the author/owner(s).\nSIGIR’09, July 19–23, 2009, Boston, Massachusetts, USA.\nACM 978-1-60558-483-6/09/07.\nUsing documents from the Blogs06 corpus used at the\nTREC Blog Track [3], we asked participants to identify\nopinion at sentence-level. We then measure sentence-level\ninter-annotator reliability for all sentences and repeat this\nfor each document and topic. Extrapolating annotations\nto the document-level, we then draw comparisons between\nsentence-level and document-level agreement. Finally, we\nconvert the annotations to binary judgements for each an-\nnotation class to allow per-class analysis.\n2. EXPERIMENTAL SETUP\nOur 15 participants were postgraduate students and post-\ndoctoral researchers, 5 of whom have worked in sentiment\nanalysis and 13 of whom were native English speakers. 15\ntopics were selected out of the 150 topics used in the Blog\nTrack at TREC 2008 based on median TREC participant\nperformance per topic, evenly distributed from low-performing\ntopics to high-performing topics. A pool of documents was\nselected from our own baseline TREC run [1], up to a max-\nimum of 8 documents per topic. All of the documents se-\nlected were judged by the TREC relevance assessments to\ncontain opinion on the topic and consisted of plain text blog\nentries extracted from HTML and passed through the noise\nremoval portion of our TREC system.\nIn the annotation process, participants were presented\nwith a series of 30 topic/document pairings and asked to\nannotate the sentences in each document as one of five cate-\ngories: “non-relevant”, “relevant” (relevant and no opinion),\n“positive”, “negative”, “mixed”. When a document is initially\npresented to a participant, all of the sentences are annotated\nas non-relevant. After completing the sentence-level anno-\ntation, participants were then asked to rate the document\nfor negative opinion from 1 (“no negative topic-directed opin-\nion”) to 5 (“very obvious and intense negative topic-directed\nopinion”) and similarly for positive opinion.\nIn total, 115 documents were judged by an average of 3.6\nannotators yielding 26,375 sentence annotations.\n3. RESULTS AND EVALUATION\nWe use Krippendorff’s alpha[2] for measuring inter-annotator\nagreement. This is a robust statistic which takes into ac-\ncount the probability that observed variability is due to\nchance and does not require that each annotator annotate\neach document. α for sentence-level annotation with respect\nto the 5 classes in Section 2 is 0.4219. This indicates a signif-\nicant agreement between annotators but is less than the level\nrecommended by Krippendorff for reliable data (α = 0.8) or\nfor tentative reliability (α = 0.667).\nFigure 1: α for Binary Judgements\nFigure 2: α and mean TREC MAP (ρ = 0.53,τ = 0.41)\nIf we examine α for each of the 115 documents, we see\nlittle correlation between α and the number of sentences per\ndocument (Pearson’s ρ = −0.123, Kendall’s τ = −0.13) or\nbetween α and the proportion of sentences annotated as con-\ntaining opinion (ρ = −0.045, τ = −0.015). This indicates\nthat the consistency between annotators is not dependent\nupon the proportion of sentiment-bearing sentences or the\noverall length of the document.\nCalculating α for each of the 15 topics, we see a significant\npositive correlation between the retrieval performance of the\ntopics at TREC and α for each topic (ρ = 0.53, τ = 0.41).\nThis reflects the increased ambiguity and obscurity among\nthe low-performing topics which hampers both automated\nopinion retrieval and manual annotation efforts similarly. A\nranking of the 15 topics by α demonstrates no discernable\npattern in terms of topic nature.\nIn order to simulate document-level annotations, we ex-\ntrapolate document-level annotations from sentence-level an-\nnotations. For example, a document containing positive sen-\ntences but no mixed or negative sentences would be consid-\nered a positive document. Agreement for these document-\nlevel annotations is slightly higher than for sentence-level\n(α = 0.4461) suggesting that although annotators may dif-\nfer in their reasons for document annotation, they converge\na small amount at document-level. It should be noted that\nthe simulated document-level annotations are not necessar-\nily the same as would be obtained had the annotators been\nexplicitly asked to annotate at the document-level.\nTo look at the individual classes more closely we map\nthe 5-way annotations to binary judgements for each of the\nclasses (Figure 1). The most striking difference in agree-\nment between document and sentence-level annotation is for\nthe mixed class. Agreement for this class is highest at the\ndocument-level and lowest at the sentence-level. It is also\nworth noting that there is much less agreement for negativity\nthan positivity at document-level and that agreement is very\nlow for the relevant class, particularly at the sentence-level.\nIf we look at an additional aggregate class, All Relevant,\nthere is a surprisingly low α for extrapolated document-level\nrelevance. This possibly reflects the fact that 11% of docu-\nments were annotated as non-relevant, despite none of them\nbeing judged that way at TREC.\nFinally, we determine a binary opinion judgement from\nthe two document-level 5 point scales. A document is de-\nfined as opinionated if either of the scales record a value\ngreater than 1 for that document. If each of these opinion\njudgements is compared with its corresponding opinion an-\nnotation extrapolated from sentence annotations, we see a\nvery high level of agreement (α = 0.8263). This shows con-\nsistency between each annotator’s sentence and document-\nlevel annotations.\n4. CONCLUSIONS AND DISCUSSION\nWe have found that sentence-level sentiment annotation yields\na moderate level of inter-annotator agreement and that this\nis independent of the nature of the sentiment, specifically the\nfrequency of the sentiment and document length. We sug-\ngest that the 5 classifications used here (and in TREC) are\nnot ideal categories for sentiment annotation. In particular,\nthe mixed category shows very low agreement at sentence-\nlevel. At document-level there is high agreement but only\ndue to the broad definition of mixed as a document con-\ntaining both positive and negative opinions. This does not\nnecessarily reflect the overriding sentiment in a document as\nboth sides of a discussion are frequently cited in distinctly\npolarised documents, yielding an artificially high proportion\nof mixed documents.\nAnnotators reported frequently feeling uneasy about their\njudgements, particularly where domain or background knowl-\nedge was required. For this reason we suggest an indetermi-\nnate class which they are encouraged to use when they are\nnot confident about their annotation. We would also like to\nexamine the task description and annotator training more\nclosely to see what effect it may have on agreement.\nFinally, we show an increase in agreement can be achieved\nby simulating document-level judgements, suggesting that\nsentence-level annotation is too granular. For future work\nwe would like to compare annotation at the document, para-\ngraph, sentence and passage levels in an effort to identify the\nmost appropriate sentiment granularity, both for annotation\nand automated opinion retrieval.\nAcknowledgments\nThis work is supported by Science Foundation Ireland under\ngrant 07/CE/I1147.\n5. REFERENCES\n[1] A. Bermingham, A. Smeaton, J. Foster, and D. Hogan.\nDCU at the TREC 2008 Blog Track. In The\nSeventeenth Text REtrieval Conference (TREC 2008)\nProc., 2008.\n[2] A. F. Hayes and K. Krippendorff. Answering the call\nfor a standard reliability measure for coding data. In\nCommunication Methods and Measures, 2007.\n[3] I. Ounis, C. MacDonald, and I. Soboroff. Overview of\nthe TREC-2008 Blog Track. In The Text REtrieval\nConference (TREC 2008) Proc. NIST, 2008.\n[4] Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and\nN. Kando. Overview of multilingual opinion analysis\ntask at NTCIR-7. 2008.\n[5] J. Wiebe, T. Wilson, and C. Cardie. Annotating\nexpressions of opinions and emotions in language.\nLanguage Resources and Evaluation, 1(2):0, 2005.\n",
      "id": 4760700,
      "identifiers": [
        {
          "identifier": "oai:doras.dcu.ie:14779",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.147.6497",
          "type": "OAI_ID"
        },
        {
          "identifier": "147598315",
          "type": "CORE_ID"
        },
        {
          "identifier": "21085589",
          "type": "CORE_ID"
        },
        {
          "identifier": "192641348",
          "type": "CORE_ID"
        },
        {
          "identifier": "143907117",
          "type": "CORE_ID"
        },
        {
          "identifier": "2002121299",
          "type": "MAG_ID"
        },
        {
          "identifier": "10.1145/1571941.1572127",
          "type": "DOI"
        },
        {
          "identifier": "11309106",
          "type": "CORE_ID"
        }
      ],
      "title": "A study of inter-annotator agreement for opinion retrieval",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:doras.dcu.ie:14779",
        "oai:citeseerx.psu:10.1.1.147.6497"
      ],
      "publishedDate": "2009-01-01T00:00:00",
      "publisher": "'Association for Computing Machinery (ACM)'",
      "pubmedId": null,
      "references": [
        {
          "id": 16581608,
          "title": "Annotating expressions of opinions and emotions in language.",
          "authors": [],
          "date": "2005",
          "doi": "10.1007/s10579-005-7880-9",
          "raw": "J. Wiebe, T. Wilson, and C. Cardie. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2):0, 2005.",
          "cites": null
        },
        {
          "id": 16581596,
          "title": "Answering the call for a standard reliability measure for coding data.",
          "authors": [],
          "date": "2007",
          "doi": "10.1080/19312450709336664",
          "raw": "A. F. Hayes and K. Krippendor®. Answering the call for a standard reliability measure for coding data. In Communication Methods and Measures, 2007.",
          "cites": null
        },
        {
          "id": 16581605,
          "title": "Overview of multilingual opinion analysis task at NTCIR-7.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multilingual opinion analysis task at NTCIR-7. 2008.",
          "cites": null
        },
        {
          "id": 16581601,
          "title": "Overview of the TREC-2008 Blog Track.",
          "authors": [],
          "date": "2008",
          "doi": "10.1145/1842890.1842899",
          "raw": "I. Ounis, C. MacDonald, and I. Soboro®. Overview of the TREC-2008 Blog Track. In The Text REtrieval Conference (TREC 2008) Proc. NIST, 2008.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/14779/1/sigir146-bermingham.pdf",
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.6497"
      ],
      "updatedDate": "2021-12-13T06:11:14",
      "yearPublished": 2009,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11309106.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11309106"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11309106/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11309106/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4760700"
        }
      ]
    },
    {
      "acceptedDate": "2017-10-23T00:00:00",
      "arxivId": "1706.02141",
      "authors": [
        {
          "name": "Alonso-Alonso, Iago"
        },
        {
          "name": "Gómez-Rodríguez, Carlos"
        },
        {
          "name": "Vilares, David"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/243152629"
      ],
      "createdDate": "2017-06-19T02:47:25",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2017-10-23T00:00:00",
      "abstract": "Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence\napplications that need to extract meaning from natural language text or speech.\nSentiment analysis is one example of application for which parsing has recently\nproven useful.\n  In recent years, there have been significant advances in the accuracy of\nparsing algorithms. In this article, we perform an empirical, task-oriented\nevaluation to determine how parsing accuracy influences the performance of a\nstate-of-the-art rule-based sentiment analysis system that determines the\npolarity of sentences from their parse trees. In particular, we evaluate the\nsystem using four well-known dependency parsers, including both current models\nwith state-of-the-art accuracy and more innacurate models which, however,\nrequire less computational resources.\n  The experiments show that all of the parsers produce similarly good results\nin the sentiment analysis task, without their accuracy having any relevant\ninfluence on the results. Since parsing is currently a task with a relatively\nhigh computational cost that varies strongly between algorithms, this suggests\nthat sentiment analysis researchers and users should prioritize speed over\naccuracy when choosing a parser; and parsing researchers should investigate\nmodels that improve speed further, even at some cost to accuracy.Comment: 19 pages. Accepted for publication in Artificial Intelligence Review.\n  This update only adds the DOI link to comply with journal's term",
      "documentType": "research",
      "doi": "10.1007/s10462-017-9584-0",
      "downloadUrl": "http://arxiv.org/abs/1706.02141",
      "fieldOfStudy": null,
      "fullText": "Noname manuscript No.\n(will be inserted by the editor)\nHow Important is Syntactic Parsing Accuracy?\nAn Empirical Evaluation on Rule-Based Sentiment\nAnalysis\nCarlos Go´mez-Rodr´ıguez · Iago\nAlonso-Alonso · David Vilares\nReceived: date / Accepted: date\nThis is the accepted manuscript (final peer-reviewed manuscript) accepted for publication\nin Artificial Intelligence Review, and may not reflect subsequent changes resulting from the\npublishing process such as editing, formatting, pagination, and other quality control mecha-\nnisms. The final publication in Artificial Intelligence Review is available at link.springer.com\nvia http://dx.doi.org/10.1007/s10462-017-9584-0.\nAbstract Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence applica-\ntions that need to extract meaning from natural language text or speech. Sentiment\nanalysis is one example of application for which parsing has recently proven useful.\nIn recent years, there have been significant advances in the accuracy of parsing\nalgorithms. In this article, we perform an empirical, task-oriented evaluation to\ndetermine how parsing accuracy influences the performance of a state-of-the-art\nrule-based sentiment analysis system that determines the polarity of sentences\nfrom their parse trees. In particular, we evaluate the system using four well-known\ndependency parsers, including both current models with state-of-the-art accuracy\nand more innacurate models which, however, require less computational resources.\nThe experiments show that all of the parsers produce similarly good results in\nthe sentiment analysis task, without their accuracy having any relevant influence\non the results. Since parsing is currently a task with a relatively high computational\ncost that varies strongly between algorithms, this suggests that sentiment analysis\nresearchers and users should prioritize speed over accuracy when choosing a parser;\nCarlos Go´mez-Rodr´ıguez has received funding from the European Research Council (ERC),\nunder the European Union’s Horizon 2020 research and innovation programme (FASTPARSE,\ngrant agreement No 714150), Ministerio de Economı´a y Competitividad (FFI2014-51978-C2-\n2-R), and the Oportunius Program (Xunta de Galicia). Iago Alonso-Alonso was funded by an\nOportunius Program Grant (Xunta de Galicia). David Vilares has received funding from the\nMinisterio de Educacio´n, Cultura y Deporte (FPU13/01180) and Ministerio de Economı´a y\nCompetitividad (FFI2014-51978-C2-2-R).\nCarlos Go´mez-Rodr´ıguez · Iago Alonso-Alonso · David Vilares\nFASTPARSE Lab, Grupo LyS, Departamento de Computacio´n, Universidade da Corun˜a\nCampus de A Corun˜a s/n, 15071, A Corun˜a, Spain\nTel.: +34 881 01 1396\nFax: +34 981 167 160\nE-mail: carlos.gomez@udc.es, iago.alonso@udc.es, david.vilares@udc.es\nar\nX\niv\n:1\n70\n6.\n02\n14\n1v\n3 \n [c\ns.C\nL]\n  2\n4 O\nct \n20\n17\n2 Carlos Go´mez-Rodr´ıguez et al.\nand parsing researchers should investigate models that improve speed further, even\nat some cost to accuracy.\nKeywords Syntactic Parsing · Sentiment Analysis · Natural Language Process-\ning · Artificial Intelligence\n1 Introduction\nHaving computers successfully understand the meaning of sentences in human lan-\nguages is a long-standing key goal in artificial intelligence (AI). While full under-\nstanding is still far away, recent advances in the field of natural language processing\n(NLP) have made it possible to implement systems that can successfully extract\nrelevant information from natural language text or speech. Syntactic parsing, the\ntask of finding the internal structure of a sentence, is a key step in that process,\nas the predicate-argument structure of sentences encodes crucial information to\nunderstand their semantics. For example, a text mining system that needs to gen-\nerate a report on customers’ opinions about phones may find statements like “the\niPhone is much better than the HTC 10” and “the HTC 10 is much better than the\niPhone”, which are identical in terms of the individual words that they contain. It\nis the syntactic structure – in this case, the subject and the attribute of the verb\nto be – that tells us which of the phones is preferred by the customer.\nIn recent years, parsing has gone from a merely promising basic research\nfield to see widespread use in useful AI applications such as machine transla-\ntion (Miceli Barone and Attardi 2015; Xiao et al 2016), information extraction\n(Song et al 2015; Yu et al 2015), textual entailment recognition (Pado´ et al 2015),\nlearning for game AI agents (Branavan et al 2012) or sentiment analysis (Joshi and\nPenstein-Rose´ 2009; Vilares et al 2015b,a). Meanwhile, researchers have produced\nimprovements in parsing algorithms and models that have increased their accuracy,\nup to a point where some parsers have achieved levels comparable to agreement\nbetween experts on English newswire text (Berzak et al 2016), although this does\nnot generalize to languages that present extra challenges for parsing (Farghaly and\nShaalan 2009) or to noisy text such as tweets (Kong et al 2014). However, parsers\nconsume significant computational resources, which can be an important concern\nin large-scale applications (Clark et al 2009), and the most accurate models often\ncome at a higher computational cost (Andor et al 2016; Go´mez-Rodr´ıguez 2016).\nTherefore, an interesting question is how much influence parsing accuracy has on\nthe performance of downstream applications, as this can be essential to make an\ninformed choice of a parser to integrate in a given system.\nIn this article, we analyze this issue for sentiment analysis (SA), i.e., the use of\nnatural language processing to extract and identify subjective information (opin-\nions about relevant entities) from natural language texts. Sentiment analysis is\none of the most relevant practical applications of NLP, it has been recently shown\nto benefit from parsing (Socher et al 2013; Vilares et al 2015b) and it is especially\nuseful at a large scale (as millions of texts of potential interest for opinion extrac-\ntion are generated every day in social networks), making the potential accuracy\nvs. speed tradeoff especially relevant.\nFor this purpose, we take a state-of-the-art syntax-based sentiment analysis\nsystem (Vilares et al 2017), which calculates the polarity of a text (i.e., whether it\nHow Important is Syntactic Parsing Accuracy? 3\nexpresses a positive, negative or neutral stance) relying on its dependency parse\ntree; and we test it with a set of well-known syntactic parsers, including mod-\nels with state-of-the-art accuracy and others that are less accurate, but have a\nsmaller computational cost, evaluating how the choice of parser affects the accu-\nracy of the polarity classification. Our results show that state-of-the-art parsing\naccuracy does not provide additional benefit for this sentiment analysis task, as\nall of the parsers tested produce similarly good polarity classification accuracy\n(no statistically significant differences, all p-values ≥ 0.49). Therefore, our results\nsuggest that it makes sense to use the fastest parsers for this task, even if they are\nnot the most accurate.\nThe remainder of this article is organized as follows: we review the state of\nthe art in syntactic parsing and syntax-based sentiment analysis in Section 2, we\ndescribe our experimental setup in Section 3, we report the results in Section 4,\nand discuss their implications in Section 5. Finally, Section 6 draws our conclusion\nand discusses possible avenues for future work.\n2 Background\nWe now provide an overview of research in parsing and sentiment analysis that is\nrelevant to this study.\n2.1 Parsing\nDifferent linguistic theories define different ways in which the syntactic structure\nof a sentence can be described. In particular, the overwhelming majority of natural\nlanguage parsers in the literature adhere to one of two dominant representations.\nIn constituency grammar (or phrase structure grammar), sentences are analyzed by\nbreaking them up into segments called constituents, which are in turn decomposed\ninto smaller constituents, as in the example of Figure 1. In dependency grammar,\nthe syntax of a sentence is represented by directed binary relations between its\nwords, called dependencies, which are most useful when labeled with their syntactic\nroles, such as subject and object, as in Figure 2. Each of these representation\ntypes provides different information about the sentence, and it is not possible to\nfully map constituency to dependency representations or vice versa (Kahane and\nMazziotta 2015).\nIn this paper we will focus on dependency parsing, as it is the predominant\nrepresentation used by most of the downstream AI applications mentioned above\n– with machine translation as an arguable exception, where constituent parsing\nis often used due to the adequacy of phrase structure grammar for modeling re-\nordering of words between languages (DeNeefe and Knight 2009; Xiao et al 2016)\n–, and it is also the alternative used by the syntax-based SA system we will use in\nour experiments.\nMost dependency parsing systems in the literature can be grouped into two\nbroad categories (McDonald and Nivre 2007): graph-based and transition-based\n(shift-reduce) parsers.\nGraph-based parsers use models that score dependency relations or groups of\nthem, and perform a global search for a parse that will maximize the combined\n4 Carlos Go´mez-Rodr´ıguez et al.\nThe    kid    broke    the    red    toy    with    a    hammer\nDET NOUN VERB DET NOUNADJ PREP DET NOUN\nNP\nNP PP\nVP\nS\nS   →  NP VP\nNP→  DET NOUN\nNP→  DET ADJ NOUN\nPP →  PREP DET NOUN\nVP→  VERB NP PP\nFig. 1 A valid constituency parse for the sentence ‘The kid broke the red toy with a hammer’ .\nThe sentence is divided into constituents according to the constituency grammar defined at\nthe left part of the picture\nThe    kid    broke    the    red    toy    with    a    hammer\nsubj\ndobj\nadpmod\ndet det\ndet\namod\nadpobj\nFig. 2 A valid dependency parse for the sentence: ‘The kid broke the red toy with a hammer’ .\nThe sentence is represented as a graph of binary relations between words that represent the\nexisting syntactic relation between them (e.g. ‘kid’ is the subject of the verb ‘broke’)\nscore of all dependencies. Under the assumption of projectivity (i.e., that there are\nno crossing dependencies), there are several dynamic programming algorithms that\nperform exact search in cubic time (Eisner 1996; Go´mez-Rodr´ıguez et al 2008), but\nthis restriction is not realistic in practice (Go´mez-Rodr´ıguez 2016). Unfortunately,\nexact inference has been shown to be intractable for models that support arbitrary\nnon-projectivity, except under strong independence assumptions (McDonald and\nSatta 2007) which enable parsing in quadratic time with maximum spanning tree\nalgorithms (McDonald et al 2005), but severely limit the expressivity of the feature\nmodels that can be used. This restriction can be avoided by using so-called mildly\nnon-projective parsing algorithms, which support the overwhelming majority of\nnon-projective analyses that can be found in real linguistic structures (Go´mez-\nRodr´ıguez et al 2011; Cohen et al 2011; Pitler et al 2013); but they have super-\ncubic complexities that make them too slow for practical use. Another option is\nto forgo exact inference, using approximate inference algorithms with rich feature\nmodels instead. This is the approach taken by TurboParser (Martins et al 2010,\n2013), which currently is the most popular graph-based parser as it can provide\nstate-of-the-art accuracy with a reasonable computational cost.\nTransition-based parsers are based on a state machine that builds syntactic\nanalyses step by step, typically from left to right. A statistical or machine learn-\ning model scores each of the possible transitions to take at each state, and a\nsearch strategy is used to find a high-scoring sequence of transitions. The earlier\napproaches to transition-based parsing, like the MaltParser system (Nivre et al\n2007) used greedy deterministic search for this purpose, which is especially fast,\nbut is prone to obtain suboptimal solutions due to bad decisions at early stages\nthat result in error propagation. This problem is alleviated by instead performing\nbeam search (Zhang and Nivre 2011) or dynamic programming (Huang and Sagae\nHow Important is Syntactic Parsing Accuracy? 5\n2010; Kuhlmann et al 2011) to explore transition sequences, but this increases\nthe computational cost. Other alternatives that provide a good speed-accuracy\ntradeoff are selectional branching, which uses confidence estimates to decide when\nto employ a beam (Choi and McCallum 2013), or dynamic oracles, which reduce\nthe error propagation in greedy search by exploring non-optimal transition se-\nquences during training (Goldberg and Nivre 2012). In the last two years, several\ntransition-based parsers have appeared that use neural networks as their scoring\nmodel (Chen and Manning 2014; Dyer et al 2015; Andor et al 2016), providing\nvery good accuracy.\n2.2 Parsing Evaluation\nThe standard metrics to evaluate the accuracy of a dependency parser are the\nunlabeled attachment score (UAS: the proportion of words that are attached to\nthe correct head word by means of a dependency, regardless of its label), labeled\nattachment score (LAS: the proportion of words that are attached to the correct\nhead by means of a dependency that has the correct label) and label accuracy (LA:\nthe proportion of words that are assigned the correct dependency type). However,\nthe performance of a parser in terms of such scores is not necessarily proportional\nto its usefulness for a given task, as not all dependencies in a syntactic analysis are\nequally useful in practice, or equally difficult to analyze (Nivre et al 2010; Bender\net al 2011). Therefore, LAS, UAS and LA are of limited use for researchers and\npractitioners that work with downstream applications in NLP. For this purpose,\nit is more useful to perform task-oriented evaluation, i.e., to experiment with\nthe parsers in the actual tasks for which they are going to be used (Volokh and\nNeumann 2012).\nSuch evaluations have been performed for some specific NLP tasks, namely in-\nformation extraction (Miyao et al 2008; Buyko and Hahn 2010), textual entailment\nrecognition (Yuret et al 2010; Volokh and Neumann 2012) and machine translation\n(Quirk and Corston-Oliver 2006; Goto et al 2011; Popel et al 2011). However, these\ncomparisons are currently somewhat dated, as they were performed before the ad-\nvent of the major advances in parsing accuracy of the current decade reviewed in\nSection 2.1, such as beam-search transition-based parsing, dynamic oracles, ap-\nproximate variational inference (TurboParser) or neural network parsing. Even\nmore importantly, these analyses provide very different results depending on each\nspecific task and, to our knowledge, no evaluation of parsers has been performed\nfor sentiment analysis, a task where a good speed-accuracy tradeoff is especially\nimportant due to its extensive applications to the Web and social networks.\n2.3 Syntax-based Sentiment Analysis\nA number of state-of-the-art models for SA using different morphological (Khan\net al 2016a,b) and syntactic approaches have proven useful in recent years. Liu\net al (2016) pointed out the benefits of syntactical approaches with respect to\nstatistical models on opinion target extraction, such as domain independence, and\npropose two approaches to select a set of rules, that even being suboptimal, achieve\nbetter results than a state-of-the-art conditional random field supervised method.\n6 Carlos Go´mez-Rodr´ıguez et al.\nWu et al (2009) defined an approach to extract product features through phrase\ndependency parsing: they first combine the output of a shallow and a word-level\ndependency parser to then extract features and feed a support vector machine\n(SVM) with a novel tree kernel function. Their experimental results outperformed\na number of bag-of-words baselines. Jia et al (2009) and Asmi and Ishaya (2012)\ndefined a set of syntax-based rules for identifying and handling negation on nat-\nural language texts represented as dependency trees. They also pointed out the\nadvantage of using this kind of methods with respect to traditional lexicon-based\nperspectives in tasks such as opinion mining or information retrieval. Poria et al\n(2014) posed a set of syntax-based patterns for a concept-level approach to de-\ntermine how the sentiment flows from concept to concept, assuming that such\nconcepts present in texts are represented as nodes of a dependency tree.\nJoshi and Penstein-Rose´ (2009) introduced the concept of generalized triplets,\nusing them as features for a supervised classifier and showing its usefulness for\nsubjectivity detection. Given a dependency triplet, the authors proposed to gen-\neralize the head or the dependent term (or even both at the same time) to its\ncorresponding part-of-speech tag. Thus, the triplet (car, modified, good) could be\ngeneralized as (NOUN, modifier, good), which can be useful to correctly classify\nsimilar triplets that did not appear in the training set (e.g. (bicycle, modifier,\ngood) or (job, modifier, good)). In a similar line, Vilares et al (2015c) enriched the\nconcept of generalized dependency triplets and showed that they can be exploited\nas features to feed a supervised SA system for polarity classification, as long as\nenough labeled data is available. The same authors (Vilares et al 2015b) proposed\nan unsupervised syntax-based approach for polarity classification on Spanish re-\nviews represented as Ancora trees (Taule´ et al 2008). They showed that their\nsystem outperforms the equivalent lexical-based approach (Taboada et al 2011).\nIn this line, however, Taboada et al (2011) pointed out that one of the challenges\nwhen using parsing techniques for sentiment analysis is the need of fast parsers\nthat are able to process in real-time the huge amount of information shared by\nusers in social media.\nWith the recent success of deep learning, Socher et al (2013) syntactically\nannotated a sentiment treebank to then train a recursive neural network that\nlearns how to apply semantic composition for relevant phenomena in SA, such as\nnegation or ‘but’ adversative clauses, over dependency trees. Kalchbrenner et al\n(2014) introduced a convolutional neural network for modeling sentences and used\nit for polarity classification among other tasks. Their approach does not explicitly\nrely on any parser, but the authors argue that one of the strengths of their model\ncomes from the capability of the network to implicitly learn internal syntactic\nrepresentations.\n3 Materials and Methods\nWe now describe the systems, corpora and methods used for our task-oriented\nevaluation.\nHow Important is Syntactic Parsing Accuracy? 7\n3.1 Parsing systems\n– MaltParser: Introduced by Nivre et al (2007), this system can be used to train\ntransition-based parsers with greedy deterministic search. Although its accu-\nracy has fallen behind the state of the art, it is still widely used, probably owing\nto its maturity and solid documentation. Additionally, due to its greedy nature,\nMaltParser is very fast. Following common practice, we use it together with\nthe feature optimization tool MaltOptimizer1 (Ballesteros and Nivre 2012) to\noptimize the parameters and train a suitable model. The trained MaltParser\nmodel uses a standard arc-eager (transition-based) parsing algorithm, where\nat each step the movement to apply is selected among the set of possible tran-\nsitions, previously scored by a linear model, which is faster than using models\nbased on SVMs.\n– TurboParser (Martins et al 2013): A graph-based parser that uses approximate\nvariational inference with non-local features. It has become the most widely\nused graph-based parser, as it provides better speed and accuracy than previous\nalternatives. We use its default configuration, training a second-order non-\nprojective parser with features for arcs, consecutive siblings and grandparents,\nusing the AD3 algorithm as a decoder.\n– YaraParser (Rasooli and Tetreault 2015): A recent transition-based parser,\nwhich uses beam search (Zhang and Nivre 2011) and dynamic oracles (Gold-\nberg and Nivre 2012) to provide state-of-the-art accuracy. Its default configu-\nration is used.\n– Stanford RNN Parser (Chen and Manning 2014): The most popular among\nthe recent wave of transition-based parsers that employ neural networks, it can\nachieve robust accuracy in spite of using greedy deterministic search. We use\npretrained GloVe (Pennington et al 2014) embeddings as input to the parser:\nin particular, 50-dimensional word embeddings2 trained on Wikipedia and the\nEnglish Gigaword (Napoles et al 2012).\n3.2 Parsing corpus\nTo train and evaluate the parsing accuracy of such parsers, we are using the En-\nglish Universal Treebank v2.0 created by McDonald et al (2013). It is a mapping\nfrom the (constituency) Penn treebank (Marcus et al 1993) to a universal de-\npendency grammar annotation. The choice of the treebank is due to the already\nexisting predefined compositional operations in the SA system used for evaluation\n(see §3.3), that are intended for this type of universal guidelines. The corpus con-\ntains 39 833, 1 701 and 2 416 dependency trees for the training, development and\ntest sets, respectively, and it represents one of the largest available treebanks for\nEnglish.\n1 MaltParser often requires feature optimization to obtain acceptable results for the target\nlanguage.\n2 http://nlp.stanford.edu/data/glove.6B.zip\n8 Carlos Go´mez-Rodr´ıguez et al.\n3.3 Sentiment analysis system\nFor the task-oriented evaluation, we will rely on UUUSA, the universal, unsuper-\nvised, uncovered approach for sentiment analysis described by (Vilares et al 2017),\nwhich is based on syntax and the concept of compositional operations. Briefly, given\na text represented as a dependency tree, a compositional operation defines how a\nnode of the tree modifies the semantic orientation (a real value representing a\npolarity and its strength) of a different branch or node, based on features such\nas its word form, part-of-speech tag or dependency type, without any limitation\nin terms of its location inside such tree. The associated system queues operations\nand propagates them through the tree, until the moment they must be dequeued\nand applied to their target. The model has outperformed other state-of-the-art\nlexicon-based methods on a number of corpora and languages, showing the ad-\nvantages of using syntactic information for sentiment analysis. Due to the way\nthe system works, in such a way that the application of the operation relies on\npreviously assigning dependency types and heads correctly, it also constitutes a\nproper environment to test how parsing accuracy affects polarity classification.\nThe system already includes a predefined set of universal syntactic operations,\nthat we are using in this study to determine the importance of parsing accuracy.\nFor the sake of brevity, we are not detailing how the system computes the semantic\norientation of the trees, but we specify which universal dependencies UUUSA is\nrelying on to identify relevant linguistic phenomena that should trigger a compo-\nsitional operation. To apply an operation, usually a dependency type must match\nat the node a branch is rooted at. The existing set of predefined operations that\nwe are considering involve phenomena such as:\n– Intensification: A branch amplifies or decreases the semantic orientation of its\nhead node or other branch (that must be labeled with the acomp (adjectival\ncomplement) dependency type). The intensifier branch must be labeled as one\nof these three dependency types: advmod (adverb modifier), amod (adjective\nmodifier), nmod (noun modifier). Dependencies are relevant in this case be-\ncause they help avoid false positive cases when applying intensification (e.g. in\n‘It is huge’ , ‘huge’ should be (probably) a positive adjective, meanwhile in ‘I\nhave huge problems’ it acts as an intensifier as it is an adjective modifier of the\nnegative word ‘problems’ , and in ‘I have huge exciting news’ it acts again as\nan intensifier, but of a positive term).\n– ‘But’ clauses: To trigger this compositional operation, which decreases the rel-\nevance of the semantic orientation of the main sentence, the dependent branch\nrooted at ‘but’ must be labeled as cc.\n– Negation: The negating terms, that might shift the sentiment of other branches,\nare labeled in a dependency tree with the dependency type neg.\n– ‘If’ : We also include experiments using the proposed rule in Vilares et al (2017)\nfor the ‘if’ clause, which is labeled with the mark dependency type, assuming\nthat the part of the sentence under the scope of influence of the conditional\nclause should be ignored.\nTherefore, the accuracy obtained by UUUSA on the sentiment corpora is re-\nlated to the parsing accuracy: a LAS of zero makes it impossible to trigger any\ncompositional operation, since no dependency type would match; obtaining as\nHow Important is Syntactic Parsing Accuracy? 9\noutput a global polarity which is the result of simply summing the semantic ori-\nentation of individual words.\nFigure 3.a) and Figure 3.b) illustrate two simple examples where part-of-speech\ntags, dependencies and types play a relevant role to accurately capture the seman-\ntic orientation of the sentence. Additionally, Figure 3.c) illustrates with an ad-\nditional example how semantic composition is managed when a negation and an\nintensification appear in the same sentence and affect the same subjective word.\na) 'I do not plan to make you su er'\ndo\nplan\nto\nmake\nyou\nsu erI not\nGreta\nis\nvery\nintelligenta\nperson\nb) 'Greta is a very intelligent person'\nb) 'My dog is not a very clean animal'\ndog\nis\nvery\ncleana\nanimal\nMy\nnot\nattr\n[phrase: you su er]\nSO: -3\n[phrase: I do not plan to make you su er], \nSO= -3 + 4 = 1\n[phrase: a very intelligent person], \nSO: 3.75\n[phrase: very intelligent], \nSO= (1+intensi cation)*3= 3.75\n[phrase: very], \nSO = 0,\nintens cation factor: +0.25\n[phrase: very], \nSO = 0,\nintens cation factor: +0.25\n[phrase: very clean] \nSO= (1+intensi cation)* 2= 2.5\n[phrase: a very clean animal]\nSO: 2.5\n[phrase: My dog is not a very clean animal]\n SO= 2.5 - 4 = -1.5\nneg\nneg\nadjmod\nadjmod\nFig. 3 Analysis of three sentences using the UUUSA approach. The analysis corresponds\nto a post-order recursive traversal. Semantic orientation, intensification and negation values\nare orientative. At each level, we show for the relevant nodes, those playing any role in the\ncomputation of the semantic orientation, the corresponding phrase rooted at that node, and\nits corresponding semantic orientation (SO), once compositional operations have been applied\nat that level. Sentence a) shows an example where the semantic scope of the negation is non-\nlocal, but thanks to dependency parsing and syntactic rules, the system can accurately identify\nsuch scope and shift the semantic orientation coming from that branch. Note that, if either\nthe dependency type neg or attr were assigned incorrectly, the calculation of the SO would\nbe wrong. Sentence b) illustrates how the term ‘very’ increases the semantic orientation of its\nhead. It is important to remark that if the dependency type adjmod were assigned incorrectly,\nthe analysis would be again unaccurate. Sentence c) illustrates a more complex compositional\nexample, where first, the intensifier ‘very’ amplifies the semantic orientation of the word ‘clean’,\nand the negating word ‘not’ shifts the sentiment rooted at the phrase ‘a very clean animal’\n10 Carlos Go´mez-Rodr´ıguez et al.\nWe chose this system among others for three main reasons:\n1. It supports separate compositional operations to address very specific linguistic\nphenomena, which can be enabled or disabled individually. This gives us great\nflexibility to carry out experiments including and excluding a number of lin-\nguistic constructions, allowing us to determine how relevant parsing accuracy\nis to tackle each of them.\n2. It is a modular system where the parser is an independent component that\ncan be swapped with another parser, allowing us to use it for task-oriented\nevaluation of various parsers. This contrasts with Socher et al (2013), a system\nthat also uses syntax, but where the parsing process is tightly woven with the\nsentiment analysis process (a neural network architecture is trained to perform\nboth tasks at the same time) so that it is not possible to use it with the output\nof external parsers.\n3. Symbolic or knowledge-based systems like this perform robustly across different\ndatasets and domains, which we cannot guarantee for the case of many machine\nlearning models, that do not generalize so well (Aue and Gamon 2005; Taboada\net al 2011; Vilares et al 2017).\n3.4 Sentiment analysis corpora\nThree standard corpora for document- and sentence-level sentiment analysis are\nused for the extrinsic evaluation:\n– Taboada and Grieve (2004) corpus: A general-domain dataset composed of 400\nlong reviews (50% positive, 50% negative) about different topics (e.g. washing-\nmachines, books or computers).\n– Pang and Lee (2004) corpus: A collection of 2 000 long movie reviews (50%\npositive, 50% negative).\n– Pang and Lee (2005) corpus: A collection of short (i.e. single-sentence) movie\nreviews. We relied on the test split used by Socher et al (2013), removing the\nneutral ones, as they did, for the binary classification task (1 821 subjective\nsentences: ∼ 49% positive, ∼ 51% negative).\n3.5 Experimental methodology\nThe aim of our experiments is to show how parsing accuracy influences polarity\nclassification, following a task-oriented evaluation. To do so, we first compare the\nperformance of different parsers on a standard treebank test set and metrics. We\nthen extrinsically evaluate the performance of such parsers by parsing sentiment\ncorpora, and using the obtained parse trees to determine the polarity of the texts\nin the corpora by means of a state-of-the-art syntax-based model. The performance\nof this model relies on previous correct assignment of dependency types and heads,\nto be able to handle relevant linguistic phenomena for the purpose at hand (e.g.\nintensification, ‘but’ clauses or negation). This makes it possible to relate parsing\nand syntax-based sentiment performance.\nHow Important is Syntactic Parsing Accuracy? 11\n3.6 Hardware and software used in the experiments\nExperiments were carried in a Dell XPS 8500 Intel Core i7 @ 3.4GHz and 16GB\nof RAM. Operating system was Ubuntu 14.04 64 bits.\n4 Results\nTable 1 shows the performance obtained by the different parsers according to the\nstandard metrics: LAS, UAS and LA. Table 2 illustrates how much time each\nparser consumes to analyze the Pang and Lee (2005) corpus, and the total time\nonce the SA system is run on it.\nTables 3, 4 and 5 show the accuracy obtained by UUUSA on different sentiment\ncorpora, when the output of each of the parsers is used as input to the syntax-based\nsentiment analysis system.3 We take accuracy as the reference metric for the SA\nsystems, because it is the most suitable metric in this case, since the three corpora\nare balanced. In particular, we compare the performance when no syntactic rules\nare used (which would be equivalent to a lexicon-based system that only sums the\nsemantic orientation of individual words), with respect to the one obtained when\ndifferent rules are added. The aim is to determine if different parsers manage\nrelevant linguistic phenomena in a different way.\nFinally, Figure 4 relates the LAS performance on the test set of the universal\ntreebank with respect to the accuracy obtained by UUUSA, when we artificially\nreduce the training set size to simulate a low-accuracy parsing setting, as could\nhappen in low-resource languages.\n3 The results obtained in these corpora are slightly different from the ones reported by\nVilares et al (2017), due to the different tokenization techniques used in this work.\n12 Carlos Go´mez-Rodr´ıguez et al.\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\n 80\n 90\n0 0.1 1 5 10 100\n 0.64\n 0.66\n 0.68\n 0.7\n 0.72\n 0.74\n 0.76\nLA\nS \n(%\n)\nAc\ncu\nra\ncy\n (%\n)\nPercentage of the training treebank used (log scale)\nLAS\nTaboada and Grieve (2004) corpus\nPang and Lee (2004) corpus\nPang and Lee (2005) corpus\nFig. 4 Relationship between LAS (area graphic, left y-axis) and accuracy in different senti-\nment corpora (line graphics, right y-axis), using the Stanford RNN parser (Chen and Manning\n2014) trained with different portions (%) of the training treebank (x-axis). The plot shows that\nusing a larger training treebank improves LAS, but does not necessarily increase the UUUSA\nsentiment accuracy, especially when more than a 5 or 10% of such treebank is used to train\nsuch parser.\n5 Discussion\nThe results illustrated in Tables 1 and 2 indicate the relationship between the\nparsing time and accuracy. The slower parsers (Martins et al 2013; Rasooli and\nTetreault 2015) tend to obtain a better performance, meanwhile the faster ones\n(Nivre et al 2007; Chen and Manning 2014) attain worse LAS, UAS and LA.\nThis fact is expected, as there is a well-known tradeoff between speed and accu-\nracy in the spectrum of parsing algorithms, with one extreme at greedy search\napproaches that scan and parse the sentence in a single pass but are prone to er-\nror propagation, and the other at exact search algorithms that guarantee finding\nthe highest-scoring parse under a rich statistical model, but are prohibitively slow\n(Choi and McCallum 2013; Volokh 2013; Go´mez-Rodr´ıguez 2016). The tendency\nremains when looking at the performance on individual dependency types (where\nalso the head is assigned correctly).\nHowever, a better LAS or UAS does not necessarily translate into a higher\nsentiment accuracy, which is shown in Tables 3, 4 and 5. In most cases, the perfor-\nmance obtained by the different parsers under the same sets of rules is practically\nequivalent. To confirm this statistically, we applied chi-squared significance tests\nto compare the outputs obtained using the different parsers for each given dataset\nand set of sentiment rules. No significant differences in sentiment accuracy were\nfound in any of these experiments, which reinforces our conclusion. The minimum\nHow Important is Syntactic Parsing Accuracy? 13\nTable 1 Performance (LAS, UAS and LA) of the parsers on the English Universal treebank\ntest set. We also detail the performance in terms of Precision (P) and Recall (R) for the\ndependency types that are playing a role in the predefined compositional operations of UUUSA.\nThe subscripts indicate the rank of the parser with respect to the others, given a particular\nmetric\nMetric MaltParser Stanford RNN parser TurboParser YaraParser\nLAS 88.354 88.773 91.362 91.841\nUAS 90.274 90.473 93.292 93.341\nLA 93.014 93.593 95.022 95.721\nP(acomp) 88.663 88.314 88.752 91.031\nR(acomp) 90.293 89.244 91.082 93.181\nP(advmod) 83.183 82.384 84.962 85.851\nR(advmod) 84.043 81.974 85.461 85.222\nP(amod) 95.453 95.014 96.251 96.232\nR(amod) 95.453 95.404 96.302 96.601\nP(attr) 87.173 86.004 89.002 94.881\nR(attr) 88.633 86.294 91.972 92.981\nP(cc) 77.064 77.683 83.561 83.462\nR(cc) 76.954 77.023 83.822 83.971\nP(mark) 83.444 84.213 85.152 88.471\nR(mark) 83.444 88.313 90.262 92.211\nP(neg) 92.992 92.624 94.771 92.683\nR(neg) 94.722 93.484 95.651 94.413\nP(nmod) 80.664 82.173 84.452 85.381\nR(nmod) 79.672 78.664 79.473 80.691\nTable 2 Average, maximum and minimum execution time (seconds) out of 5 runs on the\nPang and Lee (2005) test set. We also include the total execution time, after the SA system\nhas been run on the Pang and Lee (2005) corpus\nParser Average Minimum Maximum Average + UUUSA\ntime (∼ 1.2 sec)\nMaltParser 4.02 3.83 4.23 5.22\nStanford RNN Parser 5.99 5.82 6.23 7.19\nTurbo Parser 97.34 94.79 99.23 98.54\nYara Parser 39.85 38.94 41.23 41.05\nTable 3 Accuracy on the Pang and Lee (2004) corpus considering different subsets of rules\nParser All None Intensification ‘but’ ‘if’ Negation\nMaltParser 72.75 68.05 70.35 67.75 68.20 69.80\nStanford RNN Parser 72.95 68.05 70.60 67.85 68.35 70.30\nTurbo Parser 72.20 68.05 70.35 67.95 68.25 69.60\nYara Parser 72.00 68.05 70.30 67.85 68.55 70.15\nTable 4 Accuracy on the Pang and Lee (2005) corpus considering different subsets of rules\nParser All None Intensification ‘but’ ‘if’ Negation\nMaltParser 74.79 73.75 74.41 73.86 73.75 74.14\nStanford RNN Parser 74.68 73.75 74.35 73.86 73.97 74.19\nTurbo Parser 74.57 73.75 74.41 73.92 73.86 74.19\nYara Parser 74.68 73.75 74.41 73.86 73.97 73.92\n14 Carlos Go´mez-Rodr´ıguez et al.\nTable 5 Accuracy on the Taboada and Grieve (2004) corpus considering different subsets of\nrules\nParser All None Intensification ‘but’ ‘if’ Negation\nMaltParser 74.00 64.75 66.25 64.75 64.00 72.25\nStanford RNN Parser 74.25 64.75 66.25 64.75 64.25 72.75\nTurbo Parser 75.00 64.75 66.50 64.50 63.75 72.75\nYara Parser 73.50 64.75 66.50 64.50 63.75 72.00\np-value obtained was 0.49. It is important to remark that this is very different from\nstating that parsing is not relevant for SA. In the case of UUUSA, Vilares et al\n(2017) already showed that their syntax-based SA approach is able to beat purely\nlexicon-based methods on a number of languages. In this line, Tables 3, 4 and 5\nalso show that the sets of syntactic rules outperform the baseline that does not\nuse any syntactic-based rules (‘None’ column) in almost all cases, proving again\nthat syntax-based rules are useful to handle relevant linguistic phenomena in the\nfield of SA.\nThe specific reasons that explain why the choice of syntactic parsing algorithm\ndoes not significantly affect accuracy lie out of the scope of our empirical work, as\nthey require an exhaustive linguistic analysis. In view of the data, possible factors\nthat may contribute are the following:\n– Low difficulty of some of the most decisive dependencies involved: as can be\nseen in Table 1, even the least accurate parsers analyzed are obtaining well over\n92% precision and recall in adjectival modifiers (amod) and negations (neg),\nwhich are crucial for handling intensification and negation. This is likely be-\ncause these tend to be short-distance dependencies, which are easier to parse\n(McDonald and Nivre 2007), and are common so they do not suffer from train-\ning sparsity problems. Thus, a highly accurate parser is not needed to detect\nthese particular dependencies correctly.\n– Redundancy in sentences: a sentence may include several expressions of sen-\ntiment, so that even if the parse tree contains inaccuracies in a part of the\nsentence, we may still be able to extract the correct sentiment from the rest.\nThis can be especially frequent in long sentences, which are the most difficult\nto parse (McDonald and Nivre 2007).\n– Irrelevance of fine-grained distinctions: in some cases, the parser provides more\ninformation than is strictly needed to evaluate the sentiment of a sentence. For\nexample, the UUUSA rule for intensifiers works in the same way for adverbial\nmodifiers (advmod), adjectival modifiers (amod) or nominal modifiers (nmod).\nThus, if a parser mistakes e.g. an advmod for an amod, this counts as a parsing\nerror, but has no influence in the sentiment output.\nHowever, verifying and quantifying the influence of each of these factors remains\nas an open question, which we would like to explore in the near future.\nAn interesting conclusion that could be extracted from these results is that\nparsing should prioritize speed over accuracy for syntax-based polarity classifica-\ntion. We draw Figure 4 to reinforce this hypothesis. The figure illustrates how LAS\nand sentiment accuracy vary when training the Stanford RNN parser (Chen and\nManning 2014) with different training data size. To do so, we trained a number of\nparsers using the first x% of the training treebank. As expected, it was observed\nthat adding more training data increased the LAS obtained by the parser. How-\nHow Important is Syntactic Parsing Accuracy? 15\never, this same tendency did not remain with respect to sentiment accuracy, which\nremains stable once LAS reaches an acceptable level. Based on empirical evalua-\ntion, sentiment accuracy stops increasing when using the first 5% (82.57% LAS)\nor 10% (84.99% LAS) of the English Universal training treebank, with which it is\npossible to already obtain a performance close to the state of the art (88.77% when\nusing the whole training treebank). On the other hand, there is a clear increasing\ntendency when x < 5, because in those cases the LAS is still not good enough\n(using the first 0.1% and 1% of the training treebank we only are able to achieve\na LAS of 51.39% and 75.58%, respectively).\n6 Conclusions\nIn this article, we have carried out a task-oriented empirical evaluation to deter-\nmine the relevance of parsing accuracy on the primary challenge of sentiment anal-\nysis: polarity classification. We chose English as the target language and trained a\nnumber of standard and freely available parsers on the Universal Dependency Tree-\nbank v2.0 (McDonald et al 2013). The output of such parsers on different standard\nsentiment corpora is then used as input for a state-of-the-art and syntax-based\nsystem that aims to classify the polarity of those texts. Experimental results let\nus draw two interesting and promising conclusions: (1) a better labeled/unlabeled\nattachment score on parsing does not necessarily imply a significantly better accu-\nracy on polarity classification when using syntax-based algorithms and (2) parsing\nfor sentiment analysis should focus on speed instead of accuracy, as a LAS of\naround 80% (which we obtained in the experiments by using only the first 10% of\nthe training treebank) is already good enough to fully take advantage of depen-\ndency trees and exploit syntax-based rules. Using larger training portions produces\nincreases in the labeled attachment score up to the maximum value of close to 92%\nthat we obtained with the most accurate parser, but the performance for senti-\nment accuracy remains stable. Hence, there is no reason to use a slower parser to\nmaximize LAS as long as one is above said “good enough” threshold for sentiment\nanalysis, which is clearly surpassed by all the parsers tested.\nBased on the results, we believe there is room for improvements. We plan to\ndesign algorithms for faster parsing (Volokh 2013), prioritizing speed over accuracy.\nWe also would like to explore the influence of parsing accuracy on other high-level\ntasks analysis, such as aspect extraction (Wu et al 2009) or question answering\n(Rajpurkar et al 2016), where dependencies have played an important role.\nReferences\nAndor D, Alberti C, Weiss D, Severyn A, Presta A, Ganchev K, Petrov S, Collins M (2016)\nGlobally normalized transition-based neural networks. arXiv 1603.06042 [cs.CL], URL\nhttp://arxiv.org/abs/1603.06042\nAsmi A, Ishaya T (2012) Negation identification and calculation in sentiment analysis. In: The\nSecond International Conference on Advances in Information Mining and Management,\npp 1–7\nBallesteros M, Nivre J (2012) Maltoptimizer: A system for maltparser optimization. In: Chair)\nNCC, Choukri K, Declerck T, Dogan MU, Maegaard B, Mariani J, Moreno A, Odijk J,\nPiperidis S (eds) Proceedings of the Eight International Conference on Language Resources\nand Evaluation (LREC’12), European Language Resources Association (ELRA), Istanbul,\nTurkey\n16 Carlos Go´mez-Rodr´ıguez et al.\nBender EM, Flickinger D, Oepen S, Zhang Y (2011) Parser evaluation over local and non-local\ndeep dependencies in a large corpus. In: Proceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, Association for Computational Linguistics, Ed-\ninburgh, Scotland, UK., pp 397–408, URL http://www.aclweb.org/anthology/D11-1037\nBerzak Y, Huang Y, Barbu A, Korhonen A, Katz B (2016) Bias and agreement in syntactic\nannotations. arXiv 1605.04481 [cs.CL], URL https://arxiv.org/abs/1605.04481\nBranavan SRK, Silver D, Barzilay R (2012) Learning to win by reading manuals in a monte-\ncarlo framework. J Artif Int Res 43(1):661–704, URL http://dl.acm.org/citation.cfm?\nid=2387915.2387932\nBuyko E, Hahn U (2010) Evaluating the impact of alternative dependency graph encodings\non solving event extraction tasks. In: Proceedings of the 2010 Conference on Empiri-\ncal Methods in Natural Language Processing, Association for Computational Linguistics,\nCambridge, MA, pp 982–992, URL http://www.aclweb.org/anthology/D10-1096\nChen D, Manning C (2014) A fast and accurate dependency parser using neural networks. In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), Doha, Qatar, pp 740–750, URL http://www.aclweb.org/anthology/D14-1082\nChoi JD, McCallum A (2013) Transition-based dependency parsing with selectional branching.\nIn: Proceedings of the 51st Annual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), Sofia, Bulgaria, pp 1052–1062, URL http://www.aclweb.\norg/anthology/P13-1104\nClark S, Copestake A, Curran JR, Zhang Y, Herbelot A, Haggerty J, Ahn BG, Wyk CV,\nRoesner J, Kummerfeld J, Dawborn T (2009) Large-scale syntactic processing: Parsing\nthe web. Tech. rep., Johns Hopkins University\nCohen SB, Go´mez-Rodr´ıguez C, Satta G (2011) Exact inference for generative probabilistic\nnon-projective dependency parsing. In: Proceedings of the 2011 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP), Association for Computational\nLinguistics, pp 1234–1245, URL http://www.aclweb.org/anthology/D11-1114\nDeNeefe S, Knight K (2009) Synchronous tree adjoining machine translation. In: Proceedings\nof the 2009 Conference on Empirical Methods in Natural Language Processing, Associ-\nation for Computational Linguistics, Singapore, pp 727–736, URL http://www.aclweb.\norg/anthology/D/D09/D09-1076\nDyer C, Ballesteros M, Ling W, Matthews A, Smith NA (2015) Transition-based dependency\nparsing with stack long short-term memory. In: Proceedings of the 53rd Annual Meeting\nof the Association for Computational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1: Long Papers), Association for Computa-\ntional Linguistics, Beijing, China, pp 334–343, URL http://www.aclweb.org/anthology/\nP15-1033\nEisner J (1996) Three new probabilistic models for dependency parsing: An exploration. In:\nProceedings of the 16th International Conference on Computational Linguistics (COLING-\n96), San Francisco, CA, USA, pp 340–345\nFarghaly A, Shaalan K (2009) Arabic natural language processing: Challenges and solutions.\nACM Transactions on Asian Language Information Processing (TALIP) 8(4):14:1–14:22,\nDOI 10.1145/1644879.1644881, URL http://doi.acm.org/10.1145/1644879.1644881\nGoldberg Y, Nivre J (2012) A dynamic oracle for arc-eager dependency parsing. In: Proceedings\nof the 24th International Conference on Computational Linguistics (COLING), Association\nfor Computational Linguistics, pp 959–976, URL http://aclweb.org/anthology/C/C12/\nC12-1059.pdf\nGo´mez-Rodr´ıguez C (2016) Restricted non-projectivity: Coverage vs. efficiency. Comput Lin-\nguist 42(4):809–817, DOI 10.1162/COLI\\ a\\ 00267, URL http://dx.doi.org/10.1162/\nCOLI_a_00267\nGo´mez-Rodr´ıguez C, Carroll J, Weir D (2008) A deductive approach to dependency parsing. In:\nProceedings of the 46th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies (ACL’08:HLT), Association for Computational Linguistics,\npp 968–976, URL http://www.aclweb.org/anthology/P/P08/P08-1110\nGo´mez-Rodr´ıguez C, Carroll JA, Weir DJ (2011) Dependency parsing schemata and mildly\nnon-projective dependency parsing. Computational Linguistics 37(3):541–586\nGoto I, Utiyama M, Onishi T, Sumita E (2011) A comparison study of parsers for patent\nmachine translation. In: Proceedings of the 13th Machine Translation Summit (MT\nSummit XIII), International Association for Machine Translation, pp 448–455, URL\nhttp://www.mt-archive.info/MTS-2011-Goto.pdf\nHow Important is Syntactic Parsing Accuracy? 17\nHuang L, Sagae K (2010) Dynamic programming for linear-time incremental parsing. In:\nProceedings of the 48th Annual Meeting of the Association for Computational Linguis-\ntics, ACL ’10, pp 1077–1086, URL http://portal.acm.org/citation.cfm?id=1858681.\n1858791\nJia L, Yu C, Meng W (2009) The effect of negation on Sentiment Analysis and Retrieval\nEffectiveness. In: CIKM’09 Proceeding of the 18th ACM conference on Information and\nknowledge management, ACM, ACM Press, Hong Kong, pp 1827–1830\nJoshi M, Penstein-Rose´ C (2009) Generalizing dependency features for opinion mining. In:\nProceedings of the ACL-IJCNLP 2009 Conference Short Papers, Association for Compu-\ntational Linguistics, Stroudsburg, PA, USA, ACLShort ’09, pp 313–316\nKahane S, Mazziotta N (2015) Syntactic polygraphs. a formalism extending both constituency\nand dependency. In: Proceedings of the 14th Meeting on the Mathematics of Language\n(MoL 2015), Association for Computational Linguistics, Chicago, USA, pp 152–164, URL\nhttp://www.aclweb.org/anthology/W15-2313\nKalchbrenner N, Grefenstette E, Blunsom P (2014) A Convolutional Neural Network for Mod-\nelling Sentences. In: The 52nd Annual Meeting of the Association for Computational Lin-\nguistics. Proceedings of the Conference. Volume 1: Long Papers, ACL, Baltimore, Mary-\nland, USA, pp 655–665\nKhan FH, Qamar U, Bashir S (2016a) esap: A decision support framework for enhanced\nsentiment analysis and polarity classification. Information Sciences 367:862–873\nKhan FH, Qamar U, Bashir S (2016b) Swims: Semi-supervised subjective feature weighting and\nintelligent model selection for sentiment analysis. Knowledge-Based Systems 100:97–111\nKong L, Schneider N, Swayamdipta S, Bhatia A, Dyer C, Smith NA (2014) A dependency\nparser for tweets. In: Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar,\npp 1001–1012, URL http://www.aclweb.org/anthology/D14-1108\nKuhlmann M, Go´mez-Rodr´ıguez C, Satta G (2011) Dynamic programming algorithms for\ntransition-based dependency parsers. In: Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human Language Technologies (ACL 2011),\nAssociation for Computational Linguistics, Portland, Oregon, USA, pp 673–682, URL\nhttp://www.aclweb.org/anthology/P11-1068\nLiu Q, Gao Z, Liu B, Zhang Y (2016) Automated rule selection for opinion target extraction.\nKonwledge-Based Systems 104:74–88\nMarcus MP, Marcinkiewicz MA, Santorini B (1993) Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics 19(2):313–330\nMartins A, Smith N, Xing E, Aguiar P, Figueiredo M (2010) Turbo parsers: Dependency\nparsing by approximate variational inference. In: Proceedings of the 2010 Conference on\nEmpirical Methods in Natural Language Processing, Association for Computational Lin-\nguistics, Cambridge, MA, pp 34–44, URL http://www.aclweb.org/anthology/D10-1004\nMartins A, Almeida M, Smith NA (2013) Turning on the turbo: Fast third-order non-\nprojective turbo parsers. In: Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, pp 617–622, URL\nhttp://www.aclweb.org/anthology/P13-2109\nMcDonald R, Nivre J (2007) Characterizing the errors of data-driven dependency parsing\nmodels. In: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural Language Learning (EMNLP-CoNLL),\npp 122–131\nMcDonald R, Satta G (2007) On the complexity of non-projective data-driven dependency\nparsing. In: IWPT 2007: Proceedings of the 10th International Conference on Parsing\nTechnologies, pp 121–132\nMcDonald R, Pereira F, Ribarov K, Hajicˇ J (2005) Non-projective dependency parsing using\nspanning tree algorithms. In: HLT/EMNLP 2005: Proceedings of the conference on Human\nLanguage Technology and Empirical Methods in Natural Language Processing, pp 523–530\nMcDonald R, Nivre J, Quirmbach-brundage Y, Goldberg Y, Das D, Ganchev K, Hall K, Petrov\nS, Zhang H, Ta¨ckstro¨m O, Bedini C, Castello´ N, Lee J (2013) Universal Dependency\nAnnotation for Multilingual Parsing. In: Proceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics, Association for Computational Linguistics, pp\n92–97\nMiceli Barone AV, Attardi G (2015) Non-projective dependency-based pre-reordering with\nrecurrent neural network for machine translation. In: Proceedings of the 53rd Annual\n18 Carlos Go´mez-Rodr´ıguez et al.\nMeeting of the Association for Computational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), Association for\nComputational Linguistics, Beijing, China, pp 846–856, URL http://www.aclweb.org/\nanthology/P15-1082\nMiyao Y, Sætre R, Sagae K, Matsuzaki T, Tsujii J (2008) Task-oriented evaluation of syn-\ntactic parsers and their representations. In: Proceedings of ACL-08: HLT, Association\nfor Computational Linguistics, Columbus, Ohio, pp 46–54, URL http://www.aclweb.org/\nanthology/P/P08/P08-1006\nNapoles C, Gormley M, Van Durme B (2012) Annotated gigaword. In: Proceedings of the\nJoint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge\nExtraction, Association for Computational Linguistics, pp 95–100\nNivre J, Hall J, Nilsson J, Chanev A, Eryigˇit G, Ku¨bler S, Marinov S, Marsi E (2007) Malt-\nparser: A language-independent system for data-driven dependency parsing. Natural Lan-\nguage Engineering 13:95–135\nNivre J, Rimell L, McDonald R, Go´mez Rodr´ıguez C (2010) Evaluation of dependency parsers\non unbounded dependencies. In: Proceedings of the 23rd International Conference on Com-\nputational Linguistics (COLING 2010), Association for Computational Linguistics, pp\n833–841, URL http://www.aclweb.org/anthology/C10-1094\nPado´ S, Noh TG, Stern A, Wang R, Zanoli R (2015) Design and realization of a modular\narchitecture for textual entailment. Natural Language Engineering 21(2):167–200\nPang B, Lee L (2004) A sentimental education: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In: Proceedings of the 42nd annual meeting on As-\nsociation for Computational Linguistics, Association for Computational Linguistics, pp\n271–278\nPang B, Lee L (2005) Seeing stars: Exploiting class relationships for sentiment categorization\nwith respect to rating scales. In: Proceedings of the 43rd Annual Meeting on Association\nfor Computational Linguistics, Association for Computational Linguistics, pp 115–124\nPennington J, Socher R, Manning CD (2014) Glove: Global Vectors for Word Representation.\nIn: EMNLP, vol 14, pp 1532–1543\nPitler E, Kannan S, Marcus M (2013) Finding optimal 1-endpoint-crossing trees. Transac-\ntions of the Association of Computational Linguistics 1:13–24, URL http://aclweb.org/\nanthology/Q13-1002\nPopel M, Marecˇek D, Green N, Zabokrtsky Z (2011) Influence of parser choice on dependency-\nbased mt. In: Proceedings of the Sixth Workshop on Statistical Machine Translation,\nAssociation for Computational Linguistics, Edinburgh, Scotland, pp 433–439, URL http:\n//www.aclweb.org/anthology/W11-2153\nPoria S, Cambria E, Winterstein G, Huang GB (2014) Sentic patterns: Dependency-based\nrules for concept-level sentiment analysis. Knowledge-Based Systems 69:45–63\nQuirk C, Corston-Oliver S (2006) The impact of parse quality on syntactically-informed sta-\ntistical machine translation. In: Proceedings of the 2006 Conference on Empirical Methods\nin Natural Language Processing, Association for Computational Linguistics, Sydney, Aus-\ntralia, pp 62–69, URL http://www.aclweb.org/anthology/W06-1608\nRajpurkar P, Zhang J, Konstantin L, Liang P (2016) SQuAD: 100,000+ Questions for Machine\nComprehension of Text. arXiv preprint arXiv:160605250\nRasooli MS, Tetreault JR (2015) Yara parser: A fast and accurate dependency parser. CoRR\nabs/1503.06733, URL http://arxiv.org/abs/1503.06733\nSocher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng A, Potts C (2013) Recursive Deep\nModels for Semantic Compositionality Over a Sentiment Treebank. In: EMNLP 2013.\n2013 Conference on Empirical Methods in Natural Language Processing. Proceedings of\nthe Conference, ACL, Seattle, Washington, USA, pp 1631–1642\nSong M, Kim WC, Lee D, Heo GE, Kang KY (2015) PKDE4J: entity and relation extraction\nfor public knowledge discovery. Journal of Biomedical Informatics 57:320–332, DOI 10.\n1016/j.jbi.2015.08.008, URL http://dx.doi.org/10.1016/j.jbi.2015.08.008\nTaboada M, Grieve J (2004) Analyzing appraisal automatically. In: Proceedings of AAAI\nSpring Symposium on Exploring Attitude and Affect in Text (AAAI Technical Report\nSS0407), Stanford University, CA, AAAI Press, pp 158–161\nTaboada M, Brooke J, Tofiloski M, Voll K, Stede M (2011) Lexicon-based methods for senti-\nment analysis. Computational Linguistics 37(2):267–307\nTaule´ M, Mart´ı MA, Recasens M (2008) AnCora: Multilevel Annotated Corpora for Catalan\nand Spanish. In: Calzolari N, Choukri K, Maegaard B, Mariani J, Odjik J, Piperidis S,\nHow Important is Syntactic Parsing Accuracy? 19\nTapias D (eds) Proceedings of the Sixth International Conference on Language Resources\nand Evaluation (LREC’08), Marrakech, Morocco, pp 96–101\nVilares D, Alonso MA, Go´mez-Rodr´ıguez C (2015a) A linguistic approach for determining the\ntopics of Spanish Twitter messages. Journal of Information Science 41(02):127–145\nVilares D, Alonso MA, Go´mez-Rodr´ıguez C (2015b) A syntactic approach for opinion mining\non Spanish reviews. Natural Language Engineering 21(01):139–163\nVilares D, Alonso MA, Go´mez-Rodr´ıguez C (2015c) On the usefulness of lexical and syntactic\nprocessing in polarity classification of Twitter messages. Journal of the Association for\nInformation Science Science and Technology 66(9):1799–1816\nVilares D, Go´mez-Rodr´ıguez C, Alonso MA (2017) Universal, unsupervised (rule-based), un-\ncovered sentiment analysis. Knowledge-Based Systems 118:45–55, DOI https://doi.org/10.\n1016/j.knosys.2016.11.014\nVolokh A (2013) Performance-oriented dependency parsing. Doctoral dissertation, Saarland\nUniversity, Saarbru¨cken, Germany\nVolokh A, Neumann G (2012) Task-oriented dependency parsing evaluation methodology. In:\nIEEE 13th International Conference on Information Reuse & Integration, IRI 2012, Las\nVegas, NV, USA, August 8-10, 2012, pp 132–137, DOI 10.1109/IRI.2012.6303001, URL\nhttp://dx.doi.org/10.1109/IRI.2012.6303001\nWu Y, Zhang Q, Huang X, Wu L (2009) Phrase Dependency Parsing for Opinion Mining. In:\nProceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,\nACL, Singapore, pp 1533–1541\nXiao T, Zhu J, Zhang C, Liu T (2016) Syntactic skeleton-based translation. In: Proceedings of\nthe Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA., pp 2856–2862, URL http://www.aaai.org/ocs/index.php/AAAI/AAAI16/\npaper/view/11933\nYu M, Gormley MR, Dredze M (2015) Combining word embeddings and feature embeddings\nfor fine-grained relation extraction. In: Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Association for Computational Linguistics, Denver, Colorado, pp 1374–1379,\nURL http://www.aclweb.org/anthology/N15-1155\nYuret D, Han A, Turgut Z (2010) Semeval-2010 task 12: Parser evaluation using tex-\ntual entailments. In: Proceedings of the 5th International Workshop on Semantic Eval-\nuation, Association for Computational Linguistics, Uppsala, Sweden, pp 51–56, URL\nhttp://www.aclweb.org/anthology/S10-1009\nZhang Y, Nivre J (2011) Transition-based dependency parsing with rich non-local features. In:\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies: short papers - Volume 2, pp 188–193, URL http://dl.\nacm.org/citation.cfm?id=2002736.2002777\n",
      "id": 42871895,
      "identifiers": [
        {
          "identifier": "2765558935",
          "type": "MAG_ID"
        },
        {
          "identifier": "1706.02141",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "10.1007/s10462-017-9584-0",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1706.02141",
          "type": "OAI_ID"
        },
        {
          "identifier": "83867333",
          "type": "CORE_ID"
        },
        {
          "identifier": "info:doi/10.1007%2fs10462-017-9584-0",
          "type": "OAI_ID"
        },
        {
          "identifier": "243152629",
          "type": "CORE_ID"
        }
      ],
      "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on\n  Rule-Based Sentiment Analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1706.02141",
        "info:doi/10.1007%2fs10462-017-9584-0"
      ],
      "publishedDate": "2017-10-23T00:00:00",
      "publisher": "'Springer Science and Business Media LLC'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1706.02141"
      ],
      "updatedDate": "2024-02-27T12:10:48",
      "yearPublished": 2017,
      "journals": [
        {
          "title": "Artificial Intelligence Review",
          "identifiers": [
            "0269-2821",
            "1573-7462"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1706.02141"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/42871895"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Ferdinand Graf"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/6234553"
      ],
      "createdDate": "2012-07-06T03:40:29",
      "dataProviders": [
        {
          "id": 153,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/153",
          "logo": "https://api.core.ac.uk/data-providers/153/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "I analyze company news from Reuters with the 'General Inquirer' and relate measures of positive sentiment, negative sentiment and disagreement to abnormal stock returns, stock and option trading volume, the volatility spread and the CDS spread. I test hypotheses derived from market microstructure models. Consistent with these models, sentiment and disagreement are strongly related to trading volume. Moreover, sentiment and disagreement might be used to predict stock returns, trading volume and volatility. Trading strategies based on positive and negative sentiment are profitable if the transaction costs are moderate, indicating that stock markets are not fully efficient.Content Analysis, Company News, Market Microstructure",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/pdf/6234553.pdf",
      "fieldOfStudy": null,
      "fullText": " \n \n \n \n \n \nUniversity of Konstanz \nDepartment of Economics \n \nMechanically Extracted Company \nSignals and their Impact on Stock \nand Credit Markets \n \n \nFerdinand Graf \n \n \nhttp://www.wiwi.uni-konstanz.de/workingpaperseries \n                           Working Paper Series  \n2011-18 Mechanically Extracted Company Signals\nand their Impact on Stock and Credit\nMarkets\nFerdinand Graf1\nUniversity of Konstanz\nMay 31, 2011\nAbstract: I analyze company news from Reuters with the `General In-\nquirer' and relate measures of positive sentiment, negative sentiment and\ndisagreement to abnormal stock returns, stock and option trading vol-\nume, the volatility spread and the CDS spread. I test hypotheses derived\nfrom market microstructure models. Consistent with these models, sen-\ntiment and disagreement are strongly related to trading volume. More-\nover, sentiment and disagreement might be used to predict stock returns,\ntrading volume and volatility. Trading strategies based on positive and\nnegative sentiment are pro\ftable if the transaction costs are moderate,\nindicating that stock markets are not fully e\u000ecient.\nKeywords: Content Analysis, Company News, Market Microstructure\nJEL-classi\fcation: G12, G14\n1Department of Economics, Box D-147, 78457 Konstanz, Germany, Phone: +49 7531 88 3620,\nE-Mail: ferdinand.graf@uni-konstanz.de1 Introduction\nInvestors read daily newspapers, internet articles, watch TV news and listen to\nthe radio. The information obtained might a\u000bect their trading decision and, hence,\nmarket prices, trading volume and volatility. Barber and Odean (2008) show that the\nnumber of news releases by Dow Jones News Service is related to the trading behavior\nof individual investors, but not institutional investors. Engelberg and Parsons (2011)\n\fnd a causal relationship between \fnancial news articles in local newspapers and\nthe trading volume of local retail investors. However, news have many dimensions.\nThe number of relevant news articles for a company is a very restrictive measure and\nignores much information that might be important for \fnancial markets, e.g. the\nsentiment. Tetlock (2007) and Gro\u0019-Klu\u0019mann and Hautsch (2011) \fnd that the\nsentiment of news articles predicts daily index returns, and intraday liquidity and\nvolatility, respectively. The sentiment of chat-room postings, which could contain\nnews as well, may have predictive power for \fnancial markets too, see Antweiler\nand Frank (2004), Das, Matinez-Jeres and Tufano (2005) and Das and Chen (2007).\nI build on these studies and construct a \nexible content analysis algorithm and\nanalyze company news from Reuters.\nReuters company news usually describe and interpret a wide range of facts and\nevents which might be relevant for companies. The author's interpretation and her\nword choice may provide valuable information for \fnancial markets. The author's\nview might account for the economic environment, the \frm's industry position,\nthe management quality and much more aspects which are rather hard to measure\nquantitatively. If the author concludes that some fact is positive news for a company,\nshe will use friendly and positive words to write the news story. If the facts are\nconsidered as negative, alarmed and sad words will probably characterize the news\nstory. Of course, the quality of the author's comments depends on her background.\nThis makes the analysis of chat-room postings and their impact on the market\ndi\u000ecult, since everybody can post her opinion, rumors or lies without reputation\ndamage. Another advantage of Reuters company news is that it allows to study the\nimpact of heterogeneous events on \fnancial markets simultaneously.\nI use the `General Inquirer' to measure the sentiment of a news story with respect to a\ncompany and disagreement among news stories mechanically. The `General Inquirer'\nassigns words to word categories. The word categories `positive' and `negative' are\nused to measure positive and negative sentiment of news stories. Also, I use the\nword categories `strong' and `weak' to measure the uncertainty of a news story. I\ntest if positive sentiment, negative sentiment and disagreement of Reuters company\nnews articles impact \fnancial markets. The data cover 62 large U.S. companies\nlisted at the NYSE or the Nasdaq with liquid stock option and CDS markets for the\ntime period June 01, 2007 to December 31, 2010.\nFirst, I investigate the impact of sentiment and disagreement on the abnormal stock\nreturn derived from the three factor Fama-French model, stock and option trading\nvolume, the volatility spread and the CDS spread using daily data. This analysis\n1allows to test implications given by market microstructure models where investors\ninterpret public signals individually. My results are consistent with these models.\nSecond, I show that sentiment and disagreement have predictive power for (abnor-\nmal) stock returns, stock trading volume and the volatility spread. Positive senti-\nment is frequently followed by positive (abnormal) returns and disagreement tends\nto lower the (abnormal) return on the following day. The volatility spread increases\nafter negative sentiment and disagreement. Stock trading volume is signi\fcantly\nhigher after news with positive sentiment, but disagreement reduces stock trading\nvolume at the following day. The latter \fnding is surprising and might be due to\nimmediate execution of scheduled orders, giving contradicting news articles.\nFinally, I test the economic relevance of positive and negative sentiment by analyzing\ntrading strategies based on sentiment. Even with realistic transaction costs of 10\nbps per round-trip, the trading strategies are comparable to approximate arbitrage\nopportunities, indicating that the stock market is not fully e\u000ecient. For transaction\ncosts of 20 bps, the trading strategies are on average still pro\ftable, but bear a\nsubstantial loss potential. The strategies cannot compensate transaction costs of 30\nbps and more.\nThe contribution of this paper is manyfold. (1) I consider a large number of compa-\nnies with liquid stock and derivative markets and analyze the relationship between\nnews articles and abnormal stock returns, stock and option trading volume, the\nvolatility spread and the CDS spread company individually. Hence, I do not aggre-\ngate returns, etc., at the same day across companies. This distinguishes this study\nfrom Tetlock (2007), who considers index returns, and from Das et. al. (2005),\nwho analyze four representative companies individually. (2) I analyze a comprehen-\nsive and hand-collected dataset of news stories, downloaded from the homepage of\nReuters with a \nexible procedure, and extend Gro\u0019-Klu\u0019man and Hautsch (2011),\nwho relate pre-calculated dummy variables for positive and negative sentiment to\nthe stock market, using a continuous sentiment score. (3) The Reuters company\nnews are highly credible. This distinguishes this analysis from Antweiler and Frank\n(2004) and Das and Chen (2007), who study chat-room postings. Das et. al. (2005)\nanalyze chat-room postings, too, claiming that these postings disseminate public\ninformation. This paper analysis might contribute to those study since I analyze\nnews articles which might be closer to public information and, hence, less noisy. (4)\nTo my best knowledge, this study is the \frst that analyzes the relationship between\nsentiment respectively disagreement of general news articles and the CDS spread.\nThe rest of the paper is organized as follows. Section 2 gives a literature review.\nSection 3 derives testable hypotheses from market microstructure models. There-\nafter, I explain how market activity is measured. I describe my hand-collected news\ndatabase in section 4. Section 5 describes the content analysis and de\fnes measures\nfor sentiment and disagreement. Thereafter, I relate these measures to the market\nvariables and develop trading strategies based on sentiment. Section 8 concludes\nand gives an outlook for further research.\n22 Related Literature\nSeveral papers investigate the relationship between a company's publicity and the\nstock market. Publicity often refers to the number of news articles on the company.\nIn an early study, Mitchell and Mulherin (1994) relate the number of news releases by\nDow Jones & Company to the absolute value of the market return, the absolute value\nof \frm-speci\fc return and the trading volume. By controlling for macroeconomic\nannouncements and weekday e\u000bects, the study documents a signi\fcant relationship\nbetween news activity and market activity. Barber and Odean (2008) de\fne atten-\ntion grabbing stocks as stocks with high abnormal trading volume, extreme returns\nor news coverage. They show that individual investors are more likely to purchase\nattention-grabbing stocks than other stocks. Engelberg and Parsons (2011) address\nthe causality between news articles and investors' behavior. They identify articles\non earnings announcement in local newspapers. Local news coverage predicts trad-\ning volume of local investors and gives strong support to a causal relationship from\nnews coverage to trading. Fang and Peress (2009) study the cross-section of stock\nreturns. They \fnd that stocks with media coverage, measured by the number of\narticles on the company in the four major US newspapers (New York Times, USA\nToday, Wall Street Journal, Washington Post), underperform stocks without media\ncoverage.\nOf course, the number of news per day ignores the content of the news article.\nTetlock (2007) identi\fes weak or negative words in the daily article 'Abreast of the\nMarket' in the Wall Street Journal with a content analysis algorithm, the `General\nInquirer'. He \fnds that the number of negative or weak words predicts the return of\nthe Dow Jones Industrial Average on the following day. This e\u000bect is o\u000bset within the\nsubsequent \fve days and disappears after one week. Gro\u0019-Klu\u0019mann and Hautsch\n(2011) show that the sentiment of news articles and their relevance for stocks listed\nat the LSE predict high frequency returns, volatility and liquidity. The sentiment\nof a news article is calculated by Reuters and can take on only the values +1, 0 and\n-1. The relevance of the news story determines the sensitivity of the market with\nrespect to the news article. Tetlock et. al. (2008) show that print news can predict\nfundamental value as well as market value. However, trading strategies based on\nthese forecasts generate pro\fts only if transaction costs are excluded. Carretta et.\nal. (2010) study the Italian stock market and its reaction to corporate governance\nnews. News stories are analyzed with respect to content and tone, revealing that\nthe content of news on pro\ftable corporations is important to explain stock returns.\nSeveral studies use a more general de\fnition of news and analyze chat-room post-\nings. However, this kind of information is presumably more noisy and, hence, less\ncredible than regular news articles. Antweiler and Frank (2004) relate measures for\nbullishness and disagreement in chat-room postings and chat-room activity to mar-\nket activity. Their main \fnding is that chat-room postings predict realized volatility\nand trading volume, given high frequency data. Das, Martinez-Jerez and Tufano\n(2005) analyze chat-room postings of four representative companies from di\u000berent\nindustries and \fnd a contemporaneous relationships between the sentiment in in-\n3vestors' conversations and market returns, but no predictive power. This motivates\ntheir conclusion that investors \frst trade and then talk. Das and Chen (2007) apply\na wide spectrum of text analysis algorithms to chat-rooms postings and develop\nmeasures for sentiment and disagreement. Relating these measures to the stock\nmarket return of a company shows that market activity is related to small investors'\nsentiment. Tumarkin and Whitelaw (2001) analyze chat-room postings, too. How-\never, their \fndings on the interdependence between market observations and posted\nnews are inconclusive.\nBy using a narrow de\fnition of news / events, the number of articles might be re-\nduced signi\fcantly and a mechanical content analysis might be redundant. Brooks,\nPatel and Su (2003) analyze stock responses to rare, negative surprises like the\nExxon Valdes catastrophe, plane crashes or the sudden death of a CEO. They \fnd\nthat stocks respond with a delay to fully unanticipated news, but overreact, see also\nBrourn and Derwall (2010), who study terrorist attacks and earthquakes, respec-\ntively. Yu (2011) uses the dispersion in analyst forecasts to measure disagreement.\nA portfolio of stocks with high disagreement underperforms compared to a portfolio\nwith low disagreement. Boyd, Hu and Jagannathan (2005) do not focus on \frm spe-\nci\fc news, they analyze unemployment reports and \fnd that stock markets respond\nto unemployment news conditional on the state of the economy.\nNot only stock prices seem to respond to textual information, there is evidence that\nthe price of credit derivatives and \fxed income securities do so as well. Norden (2008)\nstudies the relationship between the credit spread of credit default swaps and rating\nannouncements. He \fnds that the rating downgrade of a company is anticipated\nby the company's major lenders, concluding that information spills over from the\nmajor lenders to the market, see also Hull, Predrescu and White (2004). Hess et. al.\n(2008) study the impact of macroeconomic news on commodity future price indices.\nThe index return responds to news about the in\nation rate or real activity only in\na recession. Hautsch and Hess (2002) analyze the U.S. employment report impact\non the mean and the volatility of T-bond futures returns. The mean's reaction\nis related to surprises and the volatility's reaction is related to uncertainty in the\nannouncements. Besides of liquidity patterns, the study documents asymmetries in\nthe T-bond future price reaction to positive and negative news. Coval and Shumway\n(2001) propose a very remarkable measure of information arrival, the ambient noise\nin the CBOT trading pit. This measure predicts returns, liquidity and the customer\norder \now of the 30 year U.S. treasury bond for several minutes.\n3 Market Reactions\n3.1 Hypotheses\nThe e\u000ecient market hypothesis says that market prices adjust immediately to public\ninformation. I test this hypothesis. Hence\n4Hypothesis 1: Market prices adjust immediately to public information,\nleaving no predictive power for public company news.\nAssuming homogeneous beliefs, the absence of private information and homogeneous\npreferences, investors do not trade if new information becomes public, starting at an\nequilibrium, see Milgrom and Stokey (1982). However, this is inconsistent with the\nempirical studies cited before. Harris and Raviv (1993) and Kandel and Pearson\n(1995) drop the assumption of homogeneous beliefs. They assume that investors\nobserve noisy, public signals and update their beliefs consistent with their individual\ninterpretation. Di\u000berent levels of con\fdence with respect to the noisy, public signal\nacross investors (di\u000berence of opinion) might cause heterogenous changes in the\ndemand for risky assets and, hence, trading. Furthermore, Cao and Ou-Yang (2009)\nextend this framework and show that public signals and heterogeneous priors may\nalso cause trading in stock options. Banerjee and Kremer (2010) relate a time-\nvarying magnitude of di\u000berence of opinion to trading volume and price volatility\nand \fnd that `periods of major disagreement are periods of higher volume and also\nof higher absolute price changes'. The latter might be used as measure for volatility.\nCompany news might be closely related to public signals. Therefore, I approximate\nthe intensity of public company signals by the sentiment of relevant news stories.\nThe degree of di\u000berences of opinion is approximated by the variation in the sentiment\nof relevant news articles within on trading day, hereafter called disagreement. Hence\nHypothesis 2: Trading volume of stocks and options increases with\npositive sentiment and negative sentiment.\nHypothesis 3: An increase in disagreement raises trading volume of\nstocks and options.\nHypothesis 4: The stock return volatility increases with disagreement.\nAccording to Hong and Stone (2007), heterogenous priors of investors are one ex-\nplanation why disagreement a\u000bects the stock market. Others are limited attention\nor gradual information \now. However, these explanations have similar implications\non the relationship between the stock market and disagreement.\nAnother strand of literature explains trading patterns by information asymmetries\nacross investors. Blume, Easley and O'Hara (1994) show theoretically that trading\nvolume might contain valuable information to determine the precision of noisy, pri-\nvate information and might be useful for stock pricing, see also Suominen (2001).\nTetlock (2010) analyzes market data around company announcements and \fnds pat-\ntern which are consistent with information asymmetries. Sarwar (2005) and Kyr-\niacou and Sarno (1999) study option trading volume and market volatility. Both\nstudies \fnd a strong predictive power of option trading volume for volatility and\nvice versa. Adjusting hedged portfolios to changes in volatility might explain why\nvolatility predicts option trading volume. Also, investors with private information\nmight exploit their informational advantage aggressively with options and use the\n5leverage e\u000bect or bet on volatility via derivatives. Hence, trading volume might\npredict volatility. By analyzing the ratio of traded put and call options, Pan and\nPoteshman (2006) \fnd that stocks with low ratios signi\fcantly outperform stocks\nwith high ratios. Again, this indicates that informed traders use derivates to bene\ft\nfrom their informational advantage.\nEmpirically, it is likely that evidence for - at least parts of - both strands of literature,\ni.e. di\u000berence of opinion and asymmetric information, appears jointly. I test the\nimplications given by the di\u000berence of opinion theory, but allow for inter-temporal\ndependencies between trading volume, stock volatility and returns to account for\ninformation asymmetries. Furthermore, I include the CDS spread of a company for\ntwo reasons: (1) Structure models for credit derivatives like Merton (1974) imply\nthat the equity market and the credit market are closely linked. Cremers et. al.\n(2006) and Zhang, Zhou and Zhu (2009) document a close relationship between\ncredit markets and equity markets. Hence, I control for information spillovers from\ndebt to equity markets and vice versa. (2) I test if the CDS spread is related to the\ndegree of di\u000berence of opinion and to public signals. Since equity volatility and the\nunobservable asset volatility in structural models are positively related, the CDS\nspread might also respond to a change in di\u000berence of opinion, given that the equity\nvolatility reacts. Therefore\nHypothesis 5: The CDS spread increases with disagreement.\nFurthermore, the CDS spread represents the market price of a traded derivative.\nPredictability of the CDS spread might be related to market ine\u000eciency and to\nHypothesis 1.\n3.2 Measures of Market Reactions\nThe daily close-to-close excess stock return of company i at day t, denoted ri;t,\nmight be used as a measure for the stock market's response to news releases. More\nappropriate and in line with many other studies is the abnormal stock return, mea-\nsured by the residuum in the three factor Fama-French model (hereafter FF model\n/ factors / residuum), see Fama and French (1993). The residual measures the\nstock price movements that are not due to common market risk factors but might\nbe due to \frm-speci\fc risk respectively news. The FF factors and the risk-free\ninterest rate are downloaded from the homepage of Kenneth French (see http:\n//mba.tuck.dartmouth.edu/pages/faculty/ken.french/index.html), the divi-\ndend adjusted stock prices are downloaded from Thomson Reuters Datastream. I\nestimate\nri;t = \u000bi + \fi;MarketXMarket;t + \fSMB;iXSMB;t + \fHML;iXHML;t + \"i;t; (1)\nwhere \fi;\u0001 denotes the factor loadings of the corresponding factor X\u0001;t (Market, Small\nMinus Big market capitalization, High Minus Low book to market ratio). The\n6estimated residuum is de\fned by ^ \"i;t := ri;t \u0000 ^ ri;t, where ^ ri;t is the explained stock\nreturn. If ^ \"i;t is large in absolute value, it is likely that important \frm-speci\fc\ninformation arrives. However, a residuum close to zero does not necessarily indicate\na calm trading day.\nI measure the stock trading volume by the daily turnover volume, divided by the\naverage turnover volume in the preceding 3 months. This measure is denoted Ti;t.\nTo address the study of Sarwar (2005) and Kyriacou and Sarno (1999), and to test\nthe model of Cao and Ou-Yang (2009), I also include the cumulated option trading\nvolume of all outstanding options on stock i at day t, divided by its 3-month moving\naverage. This measure is denoted Oi;t. All time series are provided by Thomson\nReuters Datastream. I use the 3-month volatility spread, de\fned by\nVi;t = IVi;t \u0000 RVi;t;\nto measure the investor expectations on volatility relative to realized volatility. IVi;t\nis the at-the-money implied volatility of 3-month constant maturity options, calcu-\nlated by Thomson Reuters Datastream. According to Martens and van Dijk (2007),\nthe 3-month realized volatility is well approximated by\nRVi;t =\nv u u\nt1\n2\nt X\ns=t\u000060\n[(lnHi;s \u0000 lnLi;s)2 \u0000 (2ln2 \u0000 1)(lnRi;s)2];\nwhere Hi;s is the highest intraday stock price within day s and Li;s is the lowest\nintraday stock price. Ri;s is the close-to-close gross stock return of day s2. Finally, I\nuse the 5-year CDS spread on senior debt as an indicator for the company's default\nrisk. The CDS spreads is denoted Ci;t, the data are provided by CMA.\n4 Company News\n4.1 Data Description\nMy hand-collected database consists of mainly fundamental and unscheduled news\nstories on companies in the S&P500, FTSE100 or EuroStoxx50 for the time period\nJune 01, 2007 to December 31, 2010. Given a date (mmddyyyy) and a company,\nidenti\fed with its RIC (= Reuters instrument code), the domain Reuters.com\nreturns a list of up to ten news articles on the uniform resource locator http://www.\nreuters.com/finance/stocks/companyNews?symbol=RIC&date=mmddyyyy. All\ncompany news stories are downloaded mechanically. Long news stories might span\n2The volatility spread measures the expected excess volatility relative to the realized volatility.\nIt might measure the risk aversion of the market, too. However, the time series V is non-stationary\nfor many companies and is, hence, di\u000berentiated. Since the realized volatility moves very slowly,\nit has only little impact on the \frst di\u000berence of V such that the results do not depend on the\nrealized volatility.\n7over more than one internet page. However, the download routine recognizes this\nand controls for it.\nA news story consists of a headline, the full text or body, a time stamp (date and\ntime), keywords and a list of companies indicating for whom the news story might\nbe important. In the following, this list is called 'related RICs'. The assignment of\nkeywords and related RICs to a news story is done by Reuters. Keywords provide\na rough, standardized categorization of the news story (e.g. Major Breaking News,\nDebt ratings news, Corporate Results, Mergers and Acquisitions). In a nutshell,\ncompany news inform about rating adjustments, analyst reports and changes for\nthe stock price target, give summarizing \fgures on quarterly and annual reports\nand general news (e.g. macro-economic indicators, political events, articles in the\nWashington Post, New York Times, Wall Street Journal, etc.). Corrected or updated\nnews are not excluded to capture the information \now correctly. Even though the\nlist of company news on the homepage of Reuters is limited to ten, the number\nof daily news articles per company, e.g. identi\fed by searching for the company's\nRIC in `related RICs', is not bounded in my database because there are many news\narticles that mention a company or have it in the \feld `related RICs' and do not\nappear in the list for the company on the homepage of Reuters. For the observation\nperiod June 01, 2007 to December 31, 2010, there are more than 350,000 unique\nnews stories with respect to the url. The average news article consists of 301.29\nwords (including numerical expressions and symbols) with a standard deviation of\n239.64 words. The median of words per news article is 272 and indicates that the\ndistribution is skewed to the right. The 99% quantile is 961 words. On average,\na news article consists of 14.02 sentences with a standard deviation of 33.20. The\nmedian is 11, again, indicating that the distribution of sentences per news story is\nskewed to the right, and the 99% quantile is 44.\nTable 1 provides descriptive statistics for the number of news articles per day for all\nS&P500 companies jointly, for the index components of the Dow Jones Industrial\nAverage by January 01, 2011 and for some frequently used keywords. I have 210,495\nnews articles for all S&P500 companies on 1311 days. Hence, the daily, average\nnumber of news articles for all S&P500 companies is 160.56 with a standard deviation\nof 94.01. Ignoring Saturdays and Sundays, the average number of news releases per\nday increases to 212.63 with standard deviation 52.94. On October 22, 2009, 354\nnews stories were published, this is the maximum number of news stories per day\nin the observation period. There are 17,525 news stories labeled with the keywords\n`Corporate Result', `Result Forecast' or `Warnings', this gives a daily average of\n13.36 with a standard deviation of 20.13. The total number of news stories with\n`Broker research and recommendation' is 1,867, the daily average is, hence, 1.42\nand the standard deviation is 2.26. News stories on e.g. Bank of America (BAC.N),\nidenti\fed by searching for `BAC.N' in `related RICs', sum up to 11,974, with a mean\nof 9.13 news stories per day and standard deviation of 8.23.\nFigure 1, upper plot, shows the time series of the daily number of news stories with\nthe keywords `Bankruptcy' or `Insolvency' (blue curve) and its 3 day moving average\n(red curve). Since there are no such news stories prior to November 23, 2007, the\n8plot excludes the period June 1, 2007 to November 23, 2007. The large number of\nnews stories in the middle of September 2008 indicates the bankruptcy of Lehman\nBrothers and the peaks in 2009 and 2010 are mainly due to the sovereign debt crisis\nin Europe. The lower plot shows the time series of the daily number of news stories\nfor the Bank of America and the corresponding 3-day moving average. This time\nseries starts on June 1, 2007. The time series displays a weekly cyclicality caused by\nthe low number of news articles during the weekend. Again, the default of Lehman\nBrothers at September 15, 2008 can be clearly identi\fed. The peak in January 2009\nis caused by the arranged acquisition of Merrill Lynch by Bank of America.\n[Table 1 about here.]\n[Figure 1 about here.]\n4.2 News Coverage\nTo improve the understanding of the news database, in the following I investigate\nthe company characteristics that expose a company to news coverage. I consider\n62 large companies in the S&P500 with liquid option and CDS market. Table 9\nlists these companies. The news exposure of a company is measured by its average\nnumber of news articles per day, identi\fed with the company's RIC in `related\nRICs'. This measure is denoted Qi. Alternatively, news coverage is measured by\nthe number of days with at least one news story. This measure is denoted Yi.\nCompanies are characterized by the average market capitalization in the observation\nperiod, CAPi, the average price-to-book ratio, P2Bi, the stock return during the\nobservation period, Reti, and the corresponding realized volatility, \u001b(Reti).\nThe average company has an average market capitalization during June 01, 2007\nto December 31, 2010 of 8,3223 billions USD and an average price-to-book ratio of\n2.67. The average stock market performance in this period and across companies is\n-9.41% and the average stock return volatility is 42.63. I estimate an ordinary linear\nregression model, i.e.\nQi = \u000b + \f1P2Bi + \f2 ln(CAPi) + \f3Reti + \f4\u001b(Reti) + \u0011i: (2)\nTable 2 shows the regression estimates for (2) and some straightforward adjustments.\na indicates signi\fcance at the 1% con\fdence level, b at the 5% level and c at the\n10% level. Even though this analysis excludes small and mid-sized companies, the\ncompany size is still a signi\fcant, positive determinant of the news coverage. The\nprice-to-book ratio is signi\fcant and negative in all regressions. This indicates that\ncompanies with high ratios are less often in the news than companies with low\nratios. One reason for this pattern might be that the latter companies have a\nhigher potential for stock price increases. The stock return is weakly signi\fcant and\nnegative. The stock return volatility is signi\fcant, too, and positive. Both indicate\n9that troubled companies are frequently in the media. However, this result might\nalso be due to the \fnancial crisis.\nAll results are qualitatively the same if Yi is considered instead of Qi. Hence, large\ncompanies with low price-to-book ratios and volatile stock returns have a high media\ncoverage, or conversely, companies with a high media coverage are large, have a low\nprice-to-book ratio and their stock price is rather volatile.\n[Table 2 about here.]\n5 Content Analysis and Variable Construction\nA company, a news article is relevant for a company if\n(a) contains the company's RIC in the \feld 'related RICs', or\n(b) mentions the company name or its nickname in the headline and has the com-\npany's RIC in the \feld `related RICs'.\nDe\fnition (a) is, of course, a broader de\fnition than (b), and sensitive to news\nregarding the whole industry or direct competitors. The term company name in\nde\fnition (b) and in the following refers to the shortest fraction of the full company\nname that clearly identi\fes the company, e.g. `Disney' instead of `Walt Disney Co' or\n`Conoco' instead of `ConocoPhillips'. For most companies I am able to identify the\ncompany's nickname very accurately. For example, Bank of America is frequently\ncalled BofA, Johnson & Johnson is called J&J and American Express is AmEx.\nTexas Instruments, often called TI, and General Electrics, shortened GE, can only\nbe identi\fed with a small error rate. Filtering for related RICs in de\fnition (b)\nensures that a news story with a headline such as 'BofA cuts Google price target' is\nassigned to Google, but not to Bank of America.\nEven though a news article is relevant for a company, it is unlikely that all words in\nthe full text are important for the company as well. Hence, I de\fne which passages\nin the full text of a relevant news story have to be analyzed. Given a relevant news\nstory according to de\fnition (a) or (b), I de\fne the relevant text by:\n(c) All words in a sentence are relevant if the company name or nickname is men-\ntioned within the same sentence, or\n(d) All words with a distance of at most 5 words to the company name or nickname\nare relevant.\nWords that contain numerical expressions (e.g. `B787', `A330-200', `$35') are not\ncounted for the word distance since they are not related to the sentiment of a news\nstory.\n10Given a company, I analyze the content of the relevant text of a relevant news story\nand assign a numerical value to it. The approach relies on the `General Inquirer'\n(http://www.webuse.umd.edu:9090/). The `General Inquirer' is a dictionary based\ncontent analysis algorithm. It assigns words to word categories and reports the\nnumber of hits in each cluster, relative to all analyzed words, see Stone et. al. (1966).\nThere are more than 80 word categories. However, I restrict myself to the categories\n`positive', `negative', `strong' and `weak'. Even though being very popular, the\n`General Inquirer' is not perfect. Many words have more than one meaning and\nmight be incorrectly assigned to a word category, see for example Loughran and\nMcDonald (2011), who test the performance of the `General Inquirer' by analyzing\nannual and 10-K reports, and \fnd that a substantial fraction of negative words\n(about 60%) is misinterpreted. However, the content of the Reuters news articles is\nvery general and hardly comparable to annual reports, hence I expect a low error\nrate.\nConsider, for example, the following news stories:\nFeb. 29, 2008, Northrop-EADS beats Boeing to built U.S. tanker\nWASHINGTON, Feb. 29, 2008 - The U.S. Air Force said on Friday it\nhad picked a transatlantic team led by Northrop Grumman, instead of\nBoeing, to start building a new aerial refueling \neet in a surprise choice\nworth about $35 billion. Northrop Grumman Corp (NOC.N) and its\nEuropean partner, Airbus parent EADS (EAD.PA), \"clearly provided\nthe best value to the government,\" Sue Payton, the Air Force's chief\nweapons buyer, told reporters at a brie\fng. The contract is to supply up\nto 179 tanker aircraft in a deal valued at about $35 billion over the next\n15 years, the Air Force said in a statement. The aircraft will replace [...].\nSept. 29, 2009, Kenya Airways eyes Airbus A330-200s sources\nNAIROBI, Sept. 29, 2009 - Kenya Airways (KQNA.NR) is in talks with\nAirbus (EAD.PA) about buying several A330-200 planes after delays to\nBoeing's (BA.N) much-anticipated B787 Dreamliner jet, senior o\u000ecials\nat the airline said on Tuesday. The carrier's Chief Executive O\u000ecer\nTitus Naikuni said on Friday the company was in talks with Airbus [...].\nClearly, the news stories are rather positive for Northrop and EADS, respectively,\nand negative for Boeing. According to the 'General Inquirer' dictionary, there are\nseveral positive words in the second sentence of the \frst news story (`clearly', `pro-\nvide', `best'). There, Northrop and EADS are mentioned, but not Boeing. Re-\ngarding the second news story, approach (c) might fail since Airbus and Boeing are\nmentioned in the same sentence. However, the \fve word distance around Boing\ncovers the word `delay', which clearly signals negative sentiment for Boeing, but\nthere are no negative words within the \fve word distance around EADS. Of course,\nthe performance of both approaches depends on the structure of the news story. If\n11a news article describes complex events where many companies interact, both ap-\nproaches might fail to measure the correct sentiment. However, both approaches\nperform very well for simple or well structured news. Furthermore, the company\nname is sometimes replaced by a synonym, e.g. `planemaker' instead of `Boeing'.\nSuch cases are not recognized.\nTo homogenize market data and news stories, I assign news stories which were\nreleased after 4 p.m. New York time to the following trading day. News stories\npublished between Friday, 4 p.m. and Monday, 4 p.m. are assigned to Monday.\nAssume there are Qi;t 2 N news stories for company i on day t according to de\fnition\n(a) or (b). Given the relevant text following de\fnition (c) or (d), let Posi;t;j [Negi;t;j]\ndenote the number of positive [negative] words relative to the total number of words\nin the relevant text of news story j = 1;:::;Qi;t. Then, the average, relative number\nof positive words and the average, relative number of negative words are used to\nmeasure positive signals, Pi;t, and negative signals, Ni;t, at t and for company i, i.e.\nPi;t = max\n8\n<\n:\n1\nQi;t\nQi;t X\nj=1\n(Posi;t;j \u0000 Negi;t;j);0\n9\n=\n;\n;\nNi;t = max\n8\n<\n:\n1\nQi;t\nQi;t X\nj=1\n(Negi;t;j \u0000 Posi;t;j);0\n9\n=\n;\n: (3)\nPi;t and Ni;t might be interpreted as positive and negative public signals in the style\nof Harris and Raviv (1993). High values of Pi;t or Ni;t indicate strong signals.\nIt is likely that there is a monotone relationship between the average of net sentiment\nof a day, i.e. 1\nQi;t\nPQi;t\nj=1(Posi;t;j\u0000Negi;t;j), and the abnormal stock returns or the CDS\nspread, but trading volume and volatility are presumably not monotonically related\nto the net sentiment. Therefore, positive and negative signals are disentangled. I do\nnot exclude days without news releases since these days are important as well and\nmight indicate `neutral' or `calm' days. The sentiment for these days is set to zero.\nFurthermore, I de\fne two disagreement scores. Usually, the investors' view on a\ncompany is in\nuenced, perhaps driven, by public information. If news stories dis-\nagree heavily, it is likely that investors disagree as well. Hence, the degree of di\u000ber-\nence of opinion among investors might well be approximated by the variation in the\nsentiment of news stories. I de\fne\nDstd\ni;t = \u001b\n\u0000\n(Posi;t;j \u0000 Negi;t;j)j2Qi;t\n\u0001\n; (4)\nwhere \u001b(\u0001) is the standard deviation. I set Dstd\ni;t = 0 if Qi;t \u0014 1.\nInspired by Das and Chen (2007), I construct a second measure for disagreement.\nDe\fne the auxiliary variable Ai;t;j := 1(Negi;t;j < Posi;t;j) \u0000 1(Negi;t;j > Posi;t;j).\n1(\u0001) is the indicator function. It is one if and only if the argument is true. Hence,\nAi;t;j = 1 if the net sentiment of news j is positive, it is zero if Posi;t;j = Negi;t;j\nand \u00001 otherwise. A might be interpreted as a buy- or sell-signal for investors.\n12Disagreement is alternatively measured by\nD\npol\ni;t =\nmax\nnPQi;t\nj=1 jAi;t;jj;1\no\nmax\nn\f \f \f\nPQi;t\nj=1 Ai;t;j\n\f \f \f;1\no: (5)\nIf all news stories on day t and for company i have a positive sentiment or all news\nstories have a negative sentiment, D\npol\ni;t = 1. This might indicate no disagreement.\nFor days without news stories (Qi;t = 0) I set D\npol\ni;t = 1, too. For all other days\nD\npol\ni;t > 1. D\npol\ni;t is high if there are many news stories with positive or negative\nsentiment (numerator is large) and the number of positive and negative news sto-\nries is balanced (denominator is small). These days might be associated with high\ndisagreement across investors. Whereas Dpol measures the polarity of (Ai;t;j)j2Qi;t\nand ignores the magnitude of the net sentiment, Dstd is sensitive to variations in\nthe net sentiment even though the sign of the net sentiment might be the same for\nall news stories.\n6 Regression Results\n6.1 Contemporaneous Analysis\nI analyze the contemporaneous relationship between sentiment respectively disagree-\nment and the \fnancial market. This analysis is motivated by Das, Martinez-Jeres\nand Tufano (2005) and the literature on di\u000berence of opinion. This analysis allows\nto test Hypotheses 2 to 5 on the co-movement of market variables and public signals\nrespectively the degree of di\u000berence of opinion. The analysis does not allow to con-\nclude on market e\u000eciency and the predictability of market returns. Even though\nthe news stories are unscheduled, a signi\fcant relationship between sentiment re-\nspectively disagreement and market prices or returns on a daily frequency might be\nconsistent with e\u000ecient markets if the market anticipates the news.\nAccording to Gro\u0019-Klu\u0019mann and Hautsch (2011), the relevance of a news article\nfor a company determines the strength of the relationship between sentiment of the\nnews and the market. Hence, I apply the more restrictive de\fnition of relevance,\ni.e. de\fnition (b), and use de\fnition (c) to identify the relevant words. The other\nde\fnitions are used for robustness tests.\n6.1.1 Company Individual Analysis\nAs shown in Blume, Easley and O'Hara (1994), volatility and historical stock prices\nmight be valuable information for future stock returns. Pan and Poteshman (2006)\ndocument that option trading contains relevant information for stock returns, too,\n13and according to Sarwar (2006) and Kyriacou and Sarno (1999), option trading vol-\nume and volatility interact. Cremers et. al. (2008) report a signi\fcant relationship\nbetween equity markets and credit markets. Chordia, Sarkar and Subrahmanyam\n(2005) study the intertemporal association between liquidity, volatility and returns\nby applying a vector autoregressive model. Also, the di\u000berence of opinion liter-\nature implies positive autocorrelation in trading volume, negative autocorrelation\nin returns and positive correlation between trading volume and volatility. To con-\ntrol for these associations and to determine the relationship between the \fnancial\nmarket and sentiment and disagreement, respectively, accurately, I choose the most\nparsimonious regression model that allow for the aforementioned pattern, a vector\nautoregressive process with one lag. I estimate\nh\n^ \"i;t Ti;t \u0001Vi;t Oi;t \u0001Ci;t\ni0\n(6)\n= \u0003i\nh\n^ \"i;t\u00001 Ti;t\u00001 \u0001Vi;t\u00001 Oi;t\u00001 \u0001Ci;t\u00001\ni0\n+ \fi[Pi;t Ni;t Di;t]\n0 + KiUt + \u0011i;t:\nD stands for the disagreement score and refers to Dstd or Dpol. Frequently, the\naugmented Dicky-Fuller test cannot reject the unit-root hypothesis for the CDS\nspread and for the volatility spread. Hence, I replace these time series by the\n\frst di\u000berence for all companies. \u0001V and \u0001C denote the \frst di\u000berence of the\nvolatility spread and the CDS spread, respectively. ^ \"i;t, Ti;t and Oi;t are always\nstationary. \u0003i is a 5 \u0002 5 matrix and captures possible inter-temporal associations\nbetween the abnormal returns, trading volume in stock and options and the change\nin the volatility spread and the CDS spread, respectively. \fi is a 5 \u0002 3 matrix\nand measures the association between sentiment respectively disagreement and the\nmarket. Ut is 5 \u0002 1 vector with weekday dummies and Ki's dimension is 5 \u0002 5. \u0011i;t\nis white noise.\n[Table 3 about here.]\n[Table 4 about here.]\n[Table 5 about here.]\nTables 3, 4 and 5 show the estimates for \fi. The companies listed in these tables\nare chosen because I have a su\u000ecient number of daily observations for all time\nseries jointly to calculate reliable regression coe\u000ecients and p-values. A list of\ncompany names and RIC is provided in the appendix. The option data is available\nfor most companies since June 2008 and determine the beginning of the estimation\nperiod, whereas the CDS spread is available until October 2010 and determine the\nend. With the exception of Intel Technology (INTC.O) and Travelers Companies\n(TRV.N), I have 597 days without missing observations for each company. There are\n479 observations for Intel and 660 observations for Travelers. The FF residuum is\nestimated in-sample using the time span June 01, 2008 to September 30, 2010. The\n14average FF-R2 across all companies is about 54%, indicating that the general market\nmovements explain a substantial fraction of the variation in the stock returns. The\naverage correlation between the abnormal return and the change in the volatility\nspread across all companies is -0.2859. Stock and option trading volume are on\naverage correlated by 0.3201 and the change in the volatility spread and the CDS\nspread are on average correlated by 0.2219. All other correlations between the\nmarket variables are close to zero. On average, positive and negative sentiment\nare correlated by -0.0725, positive sentiment and Dstd are correlated by 0.2613 and\nnegative sentiment and Dstd by 0.1727. The average correlation between Dpol and\nP respectively N is insigni\fcantly higher.\nTable 3 gives the estimated, contemporaneous relationship between positive sen-\ntiment and abnormal returns, stock trading volume, the change in the volatility\nspread, option trading volume and the change in the CDS spread (this is the \frst\ncolumn of ^ \fi), as well as the number of days with positive sentiment, #(P > 0), the\nmean of positive sentiment, given all days with positive sentiment, m(PjP > 0), and\nthe standard deviation, \u001b(PjP > 0). Table 4 shows the second column of ^ \fi, this is\nthe estimated relationship between negative sentiment and the market, and the cor-\nresponding descriptive statistics. Table 5 shows the estimated regression coe\u000ecients\nof disagreement. In all tables, a indicates signi\fcance at the 1% con\fdence level, b\nat the 5% level and c at the 10% level. I do not show the regression estimates for\n\u0003i and Ki.\nAs can be seen in the \frst column of Tables 3 and 4, positive sentiment and nega-\ntive sentiment are frequently signi\fcant for the FF residuum. Often, the coe\u000ecient\nof positive sentiment is positive and the coe\u000ecient of negative sentiment is nega-\ntive, indicating that positive news are associated with positive abnormal returns\nand negative news with negative abnormal returns. This suggests that the General\nInquirer and the relevant text identi\fcation procedure approximate the `true' sen-\ntiment or signal accurately. Disagreement is frequently signi\fcant, but the sign of\nthe signi\fcant coe\u000ecients varies among companies. There are 9 signi\fcant, posi-\ntive coe\u000ecients and 10 signi\fcant, negative coe\u000ecients. Hence, it is unclear which\ne\u000bect dominates on average. The average R2 across all companies with respect to\nthe abnormal return is 4.57%. Compared to an average R2 of 2.99% in regression\nmodel (6) and omitting \fi[Pi;tNi;tDi;t], shows that positive and negative sentiment\nand disagreement account on average for 1.58 percentage points in the R2. This\nsigni\fcant increase by more than 50% is exclusively due to the content analysis and\nhighlights its accuracy. Even though the news articles are usually unscheduled and\nfundamental, the signi\fcant link between returns and news does not allow conclusion\non market e\u000eciency.\nThe average R2 of regression model (6) with respect to stock trading volume is\n38.48% and the average R2 of (6) and without the regressors [Pi;tNi;tDi;t] is 34.92%,\nindicating that the content analysis explains about 4 percentage points. Regarding\noption trading volume, the average R2 increases from 15.43% without the content\nanalysis to 16.46%. To test Hypothesis 2, I use positive sentiment and negative\nsentiment to approximate the public signal's intensity and study its association\n15with trading volume on the same day. As shown in the second column of Tables\n3 and 4, the estimated coe\u000ecient of positive sentiment on stock trading volume\nis positive and signi\fcant for 19 out of 62 companies. The coe\u000ecient of negative\nsentiment is positive and signi\fcant for 12 companies. Option trading volume shows\nsimilar patterns, but the dependencies are less pronounced. However, the signal's\nintensity seems to be positively related to trading volume, as stated in Hypothesis\n2. Hypothesis 3 relates stock and option trading volume to disagreement. Table 5,\ncolumns 2 and 4, show the estimated relationship between disagreement across news\nand trading volume. High disagreement is associated with signi\fcantly higher stock\ntrading volume for 49 companies out of 62. There is no company with a signi\fcant,\nnegative regression coe\u000ecient. Regarding the relationship between option trading\nvolume and disagreement, I \fnd 23 out of 62 positive and signi\fcant relationships.\nHence, I have strong support for Hypotheses 3. Investors seem to trade on public\nsignals and the degree of disagreement accelerates the trading volume.\nThe relationship between the change in the volatility spread and disagreement is in-\nconclusive, see Table 5, column 3. The number of signi\fcant regression coe\u000ecients\nis low, and the number of signi\fcantly negative regression coe\u000ecients and signif-\nicantly positive regression coe\u000ecients are almost balanced. Hence, it is infeasible\nto draw robust conclusions on the relationship between volatility and disagreement.\nHowever, Table 4 indicates that the volatility spread widens subsequent to days\nwith negative sentiment (12 positive and signi\fcant coe\u000ecients in Table 4, column\n3). This \fnding is consistent with evidence on negative correlation between index\nreturns and volatility, since days with negative sentiment are also associated with\nnegative abnormal returns. The average R2 of the full regression model with respect\nto the change in the volatility spread is 6.80% and the contribution of the content\nanalysis in terms of average R2 is 1.13 percentage points. Nevertheless, Hypothesis\n4 is not supported.\nThe change in the CDS spread is often negatively correlated with positive sentiment\nand positively correlated with negative sentiment. This is consistent with the rela-\ntionship between the abnormal stock returns and sentiment, and with the response\nof the volatility spread. Given a negative signal, the value of equity decreases and the\nequity volatility goes up. Consistent with structural models, the distance to default\nis reduced and the expected default loss, measured by the CDS spread, increases. As\ncan be seen in Table 5, column 5, disagreement has frequently a signi\fcant, positive\nregression coe\u000ecient and supports Hypothesis 5. The content analysis increases the\naverage R2 of the change in the CDS spread from 5.91% to 7.11%\nMost results remain qualitatively unchanged if I consider Dpol as a measure of dis-\nagreement instead of Dstd. Therefore, the results are not shown but only discussed\nbrie\ny. The relationship between abnormal returns and positive sentiment becomes\nslightly stronger and the coe\u000ecient of disagreement is now frequently signi\fcant,\nnegative for abnormal returns. This is consistent with Yu (2011), who shows that\nstocks with high analyst forecast dispersion underperform relative to stocks with low\nforecast dispersion. Another noteworthy change is that the CDS spread increases\nwith the alternative disagreement measure for many companies. This gives further\n16support to Hypothesis 5.\n6.1.2 Pooled Analysis\nNext, I analyze all companies jointly. The purpose of this analysis is to investigate\nthe dominating e\u000bects between the \fnancial market and sentiment and disagreement,\nrespectively, for all companies. It simpli\fes the interpretation of the regression\ncoe\u000ecients. I estimate\nh\ns^ \"i;t sTi;t s\u0001Vi;t sOi;t s\u0001Ci;t\ni0\n(7)\n= \u0003\nh\ns^ \"i;t\u00001 sTi;t\u00001 s\u0001Vi;t\u00001 sOi;t\u00001 s\u0001Ci;t\u00001\ni0\n+ \f[sPi;t sNi;t sDi;t]\n0 + KUt + \u0011i;t:\nThe regression coe\u000ecients \u0003, \f and K are now independent of the company index\ni. Hence, I make the strong assumption that the relationship between the market\nvariables, measured by \u0003, and between the market variables and the information\nextracted from company news, measured by \f, is described by the same coe\u000ecients\nfor all \frms. I standardize and pool all time series (with the exception of the weekday\ndummies, which are pooled without further manipulation) by subtracting the time\nseries' individual mean and dividing by the time series' standard deviation. The\nstandardized, pooled time series are marked with the pre\fx s. As an example, the\npooled, standardized stock trading volume is given by the vectors\nsT\u00001 =\n\"\u0014\nT1;t \u0000 m(T1;\u0001)\n\u001b(T1;\u0001)\n\u0015\nt=1;:::;G1\u00001\n;:::;\n\u0014\nTL;t \u0000 m(TL;\u0001)\n\u001b(TL;\u0001)\n\u0015\nt=1;:::;GL\u00001\n#0\nand\nsT =\n\"\u0014\nT1;t \u0000 m(T1;\u0001)\n\u001b(T1;\u0001)\n\u0015\nt=2;:::;G1\n;:::;\n\u0014\nTL;t \u0000 m(TL;\u0001)\n\u001b(TL;\u0001)\n\u0015\nt=2;:::;GL\n#0\n;\nwhere m(\u0001) denotes the mean, \u001b(\u0001) is the standard deviation, L is the number of\ncompanies and Gi is the number of observations for company i. Then, the estimates\nin the pooled regression model are given by\nf^ \u0003; ^ \f; ^ Kg = argmin\u0003;\f;K\nn\n11\u0002G\n\u0010\n[s^ \" sT s\u0001V sO s\u0001C] (8)\n\u0000 [s^ \"\u00001 sT\u00001 s\u0001V\u00001 sO\u00001 s\u0001C\u00001]\u0003 \u0000 [sP sN sD]\f \u0000 UK\n\u00112\n15\u00021\no\n;\nwhere G =\nPL\ni=1(Gi \u0000 1), 1a\u0002b is a matrix of dimension a \u0002 b with 1s everywhere\nand U is the pooled matrix of weekday dummies. The square symbol in (8) refers\nto each component in the vector of residuals and is not a matrix operator.\nPooling all company-speci\fc observations gives in total 36,229 company-day obser-\nvations. Table 6 shows ^ \u0003, ^ \f and ^ K. Disagreement is measured by Dstd in the upper\npanel and by Dpol in the lower panel. The regression estimates of \u0003 and K are very\nsimilar for Dstd and Dpol. Stock trading volume displays positive autocorrelation,\nwhich is consistent with the models of Harris and Raviv (1993) and Banerjee and\n17Kremer (2010). Furthermore, there are several pattern which are consistent with in-\nformation asymmetry. Trading volume predicts abnormal stock returns, as discussed\nin Blume, Easley and O'Hara (1994). Also option trading volume predicts abnormal\nreturns, which might be related to the results of Pan and Poteshman (2006), even\nthough I do not study the ratio of traded put and call options, but the sum.\n[Table 6 about here.]\nAbnormal stock returns, the change in the volatility spread (which is closely related\nto the absolute return) and the change in the CDS spread positively predict stock\ntrading volume. This \fnding is consistent with Barber and Odean (2008), who\nidentify attention-grabbing stocks also with large stock price movements, and \fnd\nthat these stocks have a higher turnover volume than stocks that do not attract\nattention. However, attention might also be gained by large movements in the CDS\nspread and an increase in volatility. Consistent with structural models on credit\nderivatives, the CDS spread increases given an increase in volatility. Surprisingly,\nit also increases given a large abnormal return. This might be due to analyzing\nabnormal returns instead of gross returns. The weekday dummies are frequently\nsigni\fcant, indicating the presence of weekday e\u000bects.\nPositive and negative sentiment are still highly signi\fcant for abnormal returns.\nConsistent with the results in the previous section, positive sentiment is positively\nrelated to abnormal returns and negative sentiment negatively. The coe\u000ecient of\nDstd is insigni\fcant, see upper panel. This does not necessarily mean that disagree-\nment is not relevant for the abnormal return. The insigni\fcance might be due to\nthe heterogeneous relationship between stock prices and disagreement among news\narticles, e.g. Table 5 shows 9 signi\fcant, positive and 10 signi\fcant, negative re-\nlationships. Hence, both e\u000bects are likely to cancel out in the pooled regression.\nFurthermore, the alternative disagreement score Dpol detects a signi\fcant, negative\nrelationship between abnormal returns and disagreement in the pooled analysis, see\nTable 6, lower panel. Again, this is consistent with Yu (2011) and Das et. al. (2005).\nThe R2s in Table 6 with respect to the abnormal return are lower than the average R2\nof the \frm individual regressions. So, the R2 of s^ \" is 0.4% and 0.51%, respectively,\nwhereas the average R2 of ^ \"i is 4.57%. This decrease might be due to the restrictive\nassumption of identical regression coe\u000ecients for all companies. The contribution\nof the content analysis to the R2 of s^ \" is about 0.2 percentage point and doubles\nthe explained variation in abnormal returns.\nThe average R2 of stock trading volume and allowing for company individual regres-\nsion coe\u000ecients is 38.48% and reduces to 34.99% respectively 35.52% in the pooled\nanalysis. The R2 of standardized option trading volume is 10.67% and 10.97%, re-\nspectively, whereas the average R2 of the company individual analysis is 16.46%.\nThis moderate decrease in terms of R2 might indicate that the assumption of iden-\ntical regression coe\u000ecients is not too restrictive for trading volume. Investors' trad-\ning behavior seems to be similarly related to information such as sentiment, lagged\n18volatility, etc. for all companies. The contribution of the text analysis to the R2 is\nabout 3 percentage points for stock trading volume and about 0.65 percentage points\nfor option trading volume. In the upper panel, standardized stock and option trad-\ning volume increase with positive and negative sentiment and disagreement. The\nrelationship is highly signi\fcant and consistent with the company individual anal-\nysis and Hypotheses 2 and 3. Surprisingly, negative sentiment is negatively related\nto trading volume in the lower panel of Table 6. As discussed in Barber and Odean\n(2008), investors might be subject to investment restrictions such as short-selling re-\nstrictions. Then, negative signals are only relevant for investors who already own the\nstock. On the other hand, positive signals are relevant for all investors. Therefore,\nnegative signals might reduce trading whereas positive signals increase trading.\nThe volatility spread narrows with positive sentiment and it increases with negative\nsentiment. However, the estimated relationship between the volatility spread and\ndisagreement is inconclusive. Whereas the coe\u000ecient of Dstd is insigni\fcant in the\nupper panel of Table 6, the coe\u000ecient of Dpol is negative and weakly signi\fcant.\nBoth results are inconsistent with Hypothesis 4. Nevertheless, and consistent with\nHypothesis 5, the CDS spread increases with high disagreement. This increase is\npresumably due to the decrease in the equity value, given high disagreement, and not\ndue to an increase in the equity volatility and asset volatility, respectively. Moreover,\nthe CDS spread increases with negative sentiment and, at least in the lower panel\nof Table 6, decreases with positive sentiment as expected.\nThe regression results in this section and the previous sections show that simple mea-\nsures of sentiment and disagreement based on company news articles of Reuters add\nuseful information to standard factors which are frequently used to explain market\nactivity. However, the predictive power of sentiment and disagreement is ambiguous,\neven though the news articles may be fundamental news and unscheduled.\n6.2 Predicting Market Activity\nNow, I use the pooled regression model to study the predictive power of senti-\nment and disagreement. Hence, I do not analyze the contemporaneous relationship\nbetween market activity and sentiment respectively disagreement, but the relation-\nship between the market and sentiment respectively disagreement from the previous\ntrading day. Hence, the regression model changes to\nh\ns^ \"i;t sTi;t s\u0001Vi;t sOi;t s\u0001Ci;t\ni0\n= \u0003\nh\ns^ \"i;t\u00001 sTi;t\u00001 s\u0001Vi;t\u00001 sOi;t\u00001 s\u0001Ci;t\u00001\ni0\n(9)\n+ \f[sPi;t\u00001 sNi;t\u00001 sDi;t\u00001]\n0 + KUt + \u0011i;t:\nNow, the residual in the objective function (8) is\nh\ns^ \" sT s\u0001V sO s\u0001C\ni\n\u0000\nh\ns^ \"\u00001 sT\u00001 s\u0001V\u00001 sO\u00001 s\u0001C\u00001\ni\n^ \u0003\n\u0000\nh\nsP\u00001 sN\u00001 sD\u00001\ni\n^ \f \u0000 U ^ K: (10)\n19Table 7 shows the regression estimates for \u0003, \f and K. The results in the upper\npanel are based on the disagreement measure Dstd and the results in the lower panel\non Dpol. The estimates for \u0003 and for K are very similar to the contemporaneous\nanalysis. The R2s decrease compared to the contemporaneous analysis of sentiment,\ndisagreement and market activity. Positive sentiment is still highly signi\fcant and\npredicts positive abnormal returns on the following day. Both disagreement mea-\nsures predict negative abnormal returns on the following trading day. Negative\nsentiment is insigni\fcant. This might be due to incorrect assignments of words to\nthe word category `negative' by the `General Inquirer', see Loughran and McDonald\n(2011). However, the relationship between abnormal stock returns and sentiment\nand disagreement, respectively, are still unexpected and might be inconsistent with\nHypothesis 1. Assuming e\u000ecient markets, prices should respond to new information\nimmediately. However, the signi\fcance of positive sentiment and disagreement hints\ntowards market ine\u000eciencies even on a daily frequency. These results become even\nstronger if I consider the excess stock return instead of the abnormal stock return.\nThen, the R2 of the excess return is 1.16%, positive sentiment is positive, signi\fcant\nand negative sentiment and disagreement are signi\fcant, negative. The results are\nnot shown.\nThere is no signi\fcant relationship between the change in the CDS spread and the\none day lagged sentiment and disagreement, respectively. Hence, the credit market\nseems to be e\u000ecient with respect to the information extracted from the Reuters com-\npany news and in this framework. However, the company individual analysis (the\nresults are not shown) indicates that negative sentiment and disagreement predict\nthe change in the CDS spread for several companies. The sign of the company-\nindividual regression coe\u000ecients varies across \frms and might destroy a signi\fcant\nrelationship in the pooled regression model.\n[Table 7 about here.]\nFurthermore, positive sentiment predicts stock trading volume on the following day,\nindicating that positive signals have a long-lasting impact on trading volume. How-\never, negative sentiment is insigni\fcant. This heterogeneity might be due to in-\nvestment restrictions, see Barber and Odean (2008). Surprisingly, the regression\ncoe\u000ecients of both disagreement measures are signi\fcantly negative. One possible\nexplanation might be that investors tend to over-react to disagreement. Then, the\nstock trading volume might be lower during the following days.\nThe volatility spread increases signi\fcantly after negative sentiment and after dis-\nagreement, measured by Dstd. Dpol is insigni\fcant. Compared to the contem-\nporaneous relationship between disagreement and the volatility spread, which is\ninconclusive, the result for the one day lagged Dstd is more consistent with Hypoth-\nesis 4. The delayed response of the volatility spread could be due to a rather slow\ninformation processing and might also hint at market ine\u000eciencies. Positive and\nnegative sentiment and disagreement have only little predictive power for option\ntrading volume. These results are con\frmed by the company individual analysis.\n206.3 Robustness\nA further extension of these simple analyses is to weight the sentiment with its degree\nof uncertainty. I measure uncertainty with two approaches. (1) Uncertainty in news\narticles might be measured with the `General Inquirer' word categories `strong' and\n`weak'. However, there is a substantial overlap between the categories `positive' and\n`strong' respectively `negative' and `weak'. This might bias the results. Nevertheless,\nI measure the uncertainty attached to a news article by\nH\n(1)\ni;t;j =\nZi;t;j + #\nWi;t;j + Zi;t;j + 2#\n;\nwhere Zi;t;j denotes the number of strong words and Wi;t;j is the number of weak\nwords. # is a small, positive constant that ensures the existence of Hi;t;j even though\nthere are neither strong nor weak words in the news story j. Then, Hi;t;j = 0:5. If\nthere are only strong words, Hi;t;j \u0019 1 and if there are only weak words Hi;t;j is close\nto zero. (2) Alternatively, if the author of a news article uses many positive words\nand negative words in the relevant text for a company, she might be unsure about\nthe \fnal consequences. Therefore, uncertainty is measured by\nH\n(2)\ni;t;j =\njPosi;t;j \u0000 Negi;t;jj\nmaxfPosi;t;j + Negi;t;j;#g\n:\nIf positive and negative words are almost balanced, H\n(2)\ni;t;j is close to zero. If either\npositive words clearly dominate negative words or negative words clearly dominate\npositive words, H\n(2)\ni;t;j is close to 1.\nNow, by multiplying the net sentiment Posi;t;j \u0000 Negi;t;j in (3) with H\n(1)\ni;t;j or H\n(2)\ni;t;j,\nthe uncertainty that is related to a news article can be taken into account. The\nresults with respect to both measures of uncertainty stay qualitatively the same\ncompared to the results discussed above and, hence, are not shown.\nMoreover, zooming into the news story and analyzing words within the close neigh-\nborhood of the company name or nickname, as described in de\fnition (d), gives very\nsimilar results. The results are not shown, too, but indicate that a small fraction of\nthe full news text already contains valuable information for the \fnancial market.\n7 Trading Strategies\nAccording to the previous section, positive sentiment and disagreement are statis-\ntically signi\fcant to predict abnormal stock returns and excess returns. However,\nthis does not allow to conclude on the economic signi\fcance and on market e\u000e-\nciency. Therefore, I trading strategies based on positive and negative sentiment to\ngain insights on the economic signi\fcance of news articles for the stock market.\n21Assume that an investor trades in the 62 stocks simultaneously. The investor has\nno initial endowment. She observes the signal\nXi;t = 1(Pi;t > Ni;t) \u0000 1(Pi;t < Ni;t):\nXi;t can take on the value +1;0 and \u00001. Xi;t = 1 might be interpreted as a buy-signal\nand Xi;t = \u00001 as a sell-signal3. Xi;t = 0 might indicate a neutral position. Since\nXi;t incorporates news from 4 p.m. at t\u00001 to 4 p.m. at day t, trading on Xi;t might\nimply a substantially delayed response to new information. Whenever Xi;t = 1, the\ninvestor borrows one USD at the risk-free rate and purchases (a fraction of) stock i\nat the closing price of day t. At the following day, the stock is sold at the closing\nprice of day t + 1 and the loan is repaid, if the signal changes to neutral or to sell.\nOtherwise, the position is not closed until the buy-signal disappears. If Xi;t = \u00001,\nthe investor short-sells one USD in stock i, invests this one USD at the risk-free\nrate Rf and holds the position until the signal changes. Pro\fts and losses, due to\ntrading are collected in her money account, which is grossed up with the risk-free\nrate. Furthermore, the investor has to pay transaction costs for one round-trip. For\nsimplicity, I assume that the risk-free rate for lending and borrowing is the same\nand that the transaction costs are payed when the position is closed.\nMore precisely, let Mt denote the value of the money account at day t, Si;t the closing\nprice of stock i at day t and R\nf\nt is the daily gross risk-free interest rate, taken from\nthe data library of Kenneth French. By assumption, M0 = 0. The money account\nat t is given by\nMt = Mt\u00001R\nf\nt +\n62 X\ni=1\n\u0012\nLongi;t + Shorti;t\n\u0013\n; (11)\nLongi;t =\n0\n@\nt Y\ns=\u001c(t)+1\nSi;s\nSi;s\u00001\n\u0000\nt Y\ns=\u001c(t)+1\nR\nf\ns \u0000 TC\n1\nA1(Xi;t\u00001 = 1 _ Xi;t 6= 1); (12)\nShorti;t =\n0\n@\nt Y\ns=\u001a(t)+1\nR\nf\ns \u0000\nt Y\ns=\u001a(t)+1\nSi;s\nSi;s\u00001\n\u0000 TC\n1\nA1(Xi;t\u00001 = \u00001 _ Xi;t 6= \u00001); (13)\nwhere \u001c(t) = maxfs < tjXi;s\u00001 6= 1 _ Xi;s = 1g denote the most recent `buy'-signal\nand \u001a(t) = maxfs < tjXi;s\u00001 6= \u00001 _ Xi;s = \u00001g the most recent `sell'-signal. TC\ndenotes the transaction costs. The indicator function in (12) and (13) is one if and\nonly if a position is closed. Then, the pro\ft is assigned to the money account.\nAlternatively, I test this trading strategy against the market. This means that the\ninvestor does not \fnance trades at the risk-free rate and invest at the risk free-rate\nif a stock is short-sold, respectively, but at the market return. Then, Rf in (12) and\n(13) is replaced by RMarket, both benchmarks are downloaded from the homepage\nof Kenneth French.\n3The variables X and A di\u000ber since A is de\fned for each news story individually whereas X\nrefers to the average net sentiment of a trading day.\n22In addition to Xi;t, I consider trading strategies that are based on the signals X\n+\ni;t =\nmaxfXi;t;0g and X\n\u0000\ni;t = minfXi;t;0g. Whereas X\n+\ni;t consists only of buy-signals, X\n\u0000\ni;t\nincorporates only sell-signals. I do not consider trading strategies that are based\non disagreement to avoid con\nicting signals between sentiment and disagreement.\nFurthermore, I do not incorporate the signal intensity, i.e. Pi;t\u0000Ni;t, nor the trading\nvolume in the corresponding stock, the stock volatility or the company's CDS spread.\nThose trading strategies might depend on parameter values and, hence, require an\nin-sample optimization and an out-of-sample performance evaluation. However, the\nshort time span of my data sample is insu\u000ecient for this approach.\nThe full observation period June 01, 2007 to December 31, 2010 covers 56110\ncompany-day observations (62 companies \u0002 905 days). Using de\fnition (b) and\n(c) to calculate Pi;t and Ni;t results in 8757 buy-signals and 3816 sell-signals. This\nyields 6062 long-positions and 3042 short positions with an average duration of\n1.44 days and 1.25 days, respectively. Excluding transaction costs and re\fnancing\ncosts, the average gain of a long-position is 29 bps with a standard deviation of\n272 bps and the average gain of a short-position is 51 bps with standard deviation\n365 bps. Hence, trades on sell-signals are more pro\ftable and less frequent. The\nlower number of sell-signals and their shorter duration compared to buy-signals is\nsomewhat surprising since the observation period covers the \fnancial crisis. Further-\nmore, transaction and \fnancing costs of 30 bps and more would render trading on\nbuy-signals, on average, non-pro\ftable. Sell-signals seem to be more robust against\ntransaction costs. Moreover, the pro\fts of the daily, aggregated long and short\ntrades are correlated by -0.45. Therefore, the trading strategy on X\n+\ni;t might be an\ne\u000ecient hedge for the strategy on X\n\u0000\ni;t.\nTable 8 shows summary statistics for the money accounts of the trading strategies\nbased on the signal Xi;t, X\n+\ni;t and X\n\u0000\ni;t and for the benchmarks risk-free rate and mar-\nket return, assuming di\u000berent levels of transaction costs. Without transaction costs\nand by benchmarking against the risk-free rate, the money account of the trading\nstrategy that incorporates buy- and sell-signals increases from 0 USD by June 01,\n2007, to 33.32485 USD by December 31, 2010. The money account's minimum is\n-0.0406 USD and it turns negative only for one day. Hence, there is almost no risk of\nlosing money, indicating that the strategy might be interpreted as an approximate\narbitrage opportunity. The trading strategies based on buy- respectively sell-signals\nexclusively have similar gain-loss pro\fles and might be seen as approximate arbitrage\nopportunities as well. The gain-loss pro\fles of the trading strategies are almost un-\nchanged if the market return is used as a benchmark. However, the short-positions\nsu\u000ber slightly presumably due to long-investments in the poorly performing stock\nmarket during the \fnancial crisis.\nBy assuming 10 bps transaction costs4 per round-trip, the terminal values of the\nmoney account of the joint trading strategy on buy- and sell-signals are 24.0594 USD\nrespectively 19.8684 USD, depending on the benchmark, and the gain-loss ratios\n4The transaction costs might also cover the bid-ask spread and di\u000berent rates for borrowing\nand lending.\n23are still very attractive and comparable to an approximate arbitrage opportunity.\nThe 5% quantile, q0:05(Mt), is positive for both strategies, and the money accounts\nturn negative for only 3 respectively 4 days with a minimum value of -0.1286 USD\nrespectively -0.1429 USD. However, trading on buy-signals only, \fnanced at the risk-\nfree rate becomes quite risky compared to the scenario without transaction costs.\nThe 5% quantile of the money account is -0.5732 USD and the money account is\nnegative for 155 days. The reason might be that long-signals generate only little\npro\fts in the \fnancial crisis. These pro\fts hardly cover the transaction costs and\nincrease the probability that the money account turns negative. Also, trading on\nsell-signals only and investing into the stock market bear some shortfall risk now.\nFigure 2 depicts the value of the money accounts of the three strategies when the\nrisk-free rate is used as benchmark. The blue, solid curve shows the money account\nof trading on Xi;t, the green, dashed curve is the money account of trading on\nX\n+\ni;t and the red, dotted curve of X\n\u0000\ni;t. The money account of Xi;t increases almost\nmonotonically. During the heydays of the \fnancial crisis (June 2007 to April 2010),\nthe trading strategy on buy-signals generates signi\fcant losses, but the performance\nof trades on sell-signals is excellent and compensates the losses of the buy-signals\nfully. However, in spring 2009, governments and central banks successfully calmed\ndown the \fnancial markets and the stock market recovered. In the aftermath, the\ntrading strategy on sell-signals fails to generate pro\fts and becomes unpro\ftable.\nAt the same time, buy-signals work very well. This underlines the hedging quality\nof trading on both, buy- and sell-signals, jointly. Furthermore, Figure 2 shows the\nstrongest decrease in the value of the money account of Xi;t (black line, 1.81 USD\nin May and June 2009) and the longest waiting period to establish a new high\nwatermark (light blue line, 112 days during Spring and Summer 2010). Both \fgures\nare moderate5.\nIncreasing the transaction costs to more than 10 bps reduces the performance of all\ntrading strategies and increases the likelihood of a negative money account value\nsigni\fcantly. The assumption of 20 bps transaction costs per round-trip reduces\nthe terminal value of the money account of the joint trading strategy on buy- and\nsell-signals to 14.8703 USD, including 197 days with a negative value and a mini-\nmum of -1.5261 USD. This trading strategy might be still an attractive investment\nopportunity, but it now bears a substantial shortfall risk. Transaction costs of 30\nbps and more imply that the investor looses money on almost every buy-signal and\non many sell-signals. Hence, the terminal values of the money accounts of Xi;t and\nX\n+\ni;t are negative. However, X\n\u0000\ni;t might still be pro\ftable.\n[Table 8 about here.]\n[Figure 2 about here.]\n5The worst case, i.e. the strongest downturn and the longest waiting period to exceed the high\nwatermark appear jointly at day zero, might be an indication for the minimum equity bu\u000ber in\nthe approximate arbitrage portfolio.\n248 Conclusion and Outlook\nDas et. al. (2005) analyzed chat-room postings and conclude that `investors \frst\ntrade and then talk'. I analyze company news of Reuters. These news are more\nreliable than chat room postings which, at best, disseminate company news. Simple\ndictionary based content analysis algorithms with rather high error rates might be\napplied to measure sentiment and disagreement of those news articles. Both contain\nvaluable information for \fnancial markets.\nMy results are mostly consistent with models on di\u000berence of opinion, i.e. investors\nare more likely to trade stocks and options after observing public signals. Dis-\nagreement across news articles is also positively correlated with stock and option\ntrading volume and expected stock volatility. Moreover, sentiment and disagreement\nare statistically signi\fcant to predict returns, volatility and trading volume. With\nmoderate transaction costs, it might be possible to exploit market ine\u000eciencies by\ntrading on buy- and sell-signals based on the mechanical evaluation of company\nnews. However, transaction costs of more than 10 bps destroy this approximate\narbitrage opportunity. Therefore, only institutional investors might be able to take\nadvantage of this ine\u000eciency. For transaction costs in the range of 10 bps up to 30\nbps, the expected pro\fts of the trading strategies are still positive, but the strategies\nbecome risky, i.e. there might be a substantial probability that the terminal value\nof money account is negative. Even higher transaction costs render the strategies\nuseless.\nI consider the following extensions. (1) Classi\fer: The 'General Inquirer' dictionary\nis very general. The dictionary is not designed for analyzing \fnancial news. Hence,\nit is likely that the results will improve signi\fcantly if I adjust the dictionary to\naccount for important characteristics in \fnancial and economic news, which might\nbe misinterpreted right now. Furthermore, it might be interesting to determine\nthe sentiment / disagreement by applying di\u000berent methods that are not based on\na dictionary approach. Bayesian classi\fers, adjective-adverb classi\fers and vector\ndistance classi\fers could be used as well. The grammar, text lengths or readability\nmight be incorporated to evaluate the sentiment. (2) Industry Portfolios: Compa-\nnies within the same industry might have a similar exposures to news. Hence, by\nstudying industry portfolios instead of all companies separately or the overall pooled\nsample, the results might become even stronger. Furthermore, it might be possible\nto compare information processing among industries.\nAppendix\n[Table 9 about here.]\n25References\n[1] Antweiler, Werner and Murry Z. Frank, 2004, \"Is all that just Noise? The\nInformation Content of internet Stock Message Boards\", Journal of Finance 59,\n1259-1294.\n[2] Banerjee, Snehal and Ilan Kremer, 2010, \"Disagreement and Learning: Dynamic\nPattern of Trading Volume\", Journal of Finance 65, 1269-1302.\n[3] Barber, Brad M. and Terrance Odean, 2008, \"All That Glitters: The E\u000bect\nof Attention and News on the Buying Behavior of Individual and Institutional\nInvestors\", Review of Financial Studies 21, 785-818.\n[4] Blume, Lawrence, David Easley, Maureen O'Hara, 1994, \"Market Statistics and\nTechnical Analysis: The Role of Volume\", Journal of Finance 49, 153-181.\n[5] Boyd, John H., Jian Hu and Ravi Jagannathan, 2005, \"The Stock Market's\nReaction to Unemployment News: Why Bad News Is Usually Good for Stocks\"\nJournal of Finance 60, 649-672.\n[6] Brooks, Raymond M., Ajay Patel and Tie Su, 2003, \"How the Equity Market\nResponds to Unanticipated Events\", Journal of Business 76, 109-133.\n[7] Brounrn, Dirk and Jeroen Derwall, 2010, \"The Impact of Terrorist Attacks on\nInternational Stock Markets\", European Financial Management 16, 585-598.\n[8] Carretta, Alessandro, Vincenzo Farina, Duccio Martelli, Franco Fiordelisi and\nPaola Schwizer, 2010, \"The Impact of Corporate Governance Press News on\nStock Market Return\", European Financial Management 17, 100-119.\n[9] Cao, H. Henry and Hui Ou-Yang, 2009, \"Di\u000berences of Opinion of Public In-\nformation and Speculative Trading in Stocks and Options\", Review of Financial\nStudies 22, 299-335.\n[10] Charida, Tarum, Asani Sarkar and Avanidhar Subrahmanyam, 2005, \"An Em-\npirical Analysis of Stock and Bond Market Liquidity\", Review of Financial Stud-\nies 18, 85-129.\n[11] Coval, Joshua D. and Tyler Shumway, 2001, \"Is Sound Just Noise?\", Journal\nof Finance 56, 1887-1910.\n[12] Cremers, Martijn, Joost Driessen, Pascal Maenhout and David Weinbaum,\n2008, \"Individual stock-option prices and credit spreads\", Journal of Banking\nand Finance 32, 2706-2715.\n[13] Das, Sanjiv R. and Mike Y. Chen, 2007, \"Yahoo! for Amazon; Sentiment\nExtraction form Small Talk on the Web\", Management Science 53, 1375-1388.\n26[14] Das, Sanjiv R., Asis Martinez-Jerez and Peter Tufano, 2005, \"e-Information:\nA Clinical Study of Investor Discussion and Sentiment\", Financial Management\n34, 103-137.\n[15] Engelberg, Joseph E. and Christopher A. Parsons, 2011, \"The Causal Impact\nof Media in Financial Markets\", Journal of Finance 66, 67-98.\n[16] Fama, Eugene F., 1998, \"Market E\u000eciency, Long-term Returns, and Behavioral\nFinance\", Journal of Financial Economics 49, 283-306.\n[17] Fama, Eugene F. and Kenneth French, 1993, \"Common Risk Factors in the\nReturns on Stocks and Bonds\", Journal of Financial Economics 33, 3-56.\n[18] Fang, Lily and Joel Peress, 2009, \"Media Coverage and the Cross-section of\nStock Returns\", Journal of Finance 64, 2023-2052.\n[19] Gro\u0019-Klu\u0019mann, Axel and Nikolas Hautsch, 2011, \"When machines read the\nnews: Using automated text analysis to quantify high frequency news-implied\nmarket reactions\", Journal of Empirical Finance 18, 321-340.\n[20] Harris, Milton and Artur Raviv, 1993, \"Di\u000berences of Opinion Make a Horse\nRace\", Review of Financial Studies 6, 473-506.\n[21] Hautsch, Nikolaus and Dieter Hess, 2002, \"The Processing of Non-Anticipated\nInformation in Financial Markets: Analyzing the Impact of Surprises in the\nEmployment Report\", European Financial Review 6, 133-161.\n[22] Hess, Dieter, 2008, \"How Do Commodity Futures Respond to Macroeconomic\nNews?\", Financial Markets and Portfolio Management 22, 127-146.\n[23] Hong, Harrison and Jeremy C. Stein, 2007, \"Disagreement and the Stock Mar-\nket\", Journal of Economic Perspectives 21, 109-128.\n[24] Hull, John, Mirela Predescu and Alan White, 2004, \"The relationship between\ncredit default swap spreads, bond yields, and credit rating announcements\",\nJournal of Banking and Finance 28, 2789-2811.\n[25] Kandel, Eugene and Neil D. Pearson, 1995, \"Interpretation of Public Signals\nand Trade in Speculative Makets, Journal of Political Economy 103, 831-872.\n[26] Kyriacou, Kyriaos and Lucio Sarno, 1999, \"The Temporal Relationship between\nDerivatives Trading and Spot Market Volatility in the U.K.: Empirical Analysis\nand Monte Carlo Evidence\", Journal of Futures Markets 19, 245-270.\n[27] Loughran, Tim and Bill McDonald, 2011, \"When Is a Liability Not a Liability?\nTextual Analysis, Dictionaries, and 10-Ks\", Journal of Finance 66, 35-66.\n[28] Milgrom, Paul and Nancy Stokey, 1982, \"Information, Trade and Common\nKnowledge\", Journal of Economic Theory 26, 17-27.\n27[29] Mitchell, Mark L. and J. Harold Mulherin, 1994, \"The Impact of Public Infor-\nmation on the Stock Market\", Journal of Finance 49, 923-950.\n[30] Merton, Robert C., 1974, \"On the Pricing of Corporate Debt: The Risk Struc-\nture of Interest Rates\", Journal of Finance 29, 449-70\n[31] Norden, Lars, 2009, \"Credit Derivatives, Corporate News, and Credit Ratings\",\nworking paper.\n[32] Pan, Jun and Allen M. Poteshman, 2006, \"The Information in Option Volume\nfor Future Stock Prices\", Review of Financial Studies 19, 871-908.\n[33] Sarwar, Ghulam, 2005, \"The Informational Role of Option Trading Volume in\nEquity Index Options Markets\", Review of Quantitative Finance and Accounting\n24, 159-176.\n[34] Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith, Daniel M. Ogilvie, and\nassociates. \"The General Inquirer: A Computer Approach to Content Analysis,\"\nThe MIT Press, 1966.\n[35] Suominen, Matti, (2001), \"Trading Volume and Information Revelation in\nStock Markets\", Journal of Financial and Quantitative Analysis 36, 545-565.\n[36] Tetlock, Paul C., 2007, \"Giving Content to Investor Sentiment: The Role of\nMedia in the Stock Market\", Journal of Finance 62, 1139-1167.\n[37] Tetlock, Paul C., 2010, \"Does Public Financial News Resolve Asymmetric In-\nformation?\", Review of Financial Studies 23, 3520-3557.\n[38] Tetlock, Paul C., Maytal Saar-Tsechansky and Sofus Macskassy, 2008, \"More\nthan Words: Quantifying Language to Measure Firms' Fundamentals\", Journal\nof Finance 63, 1437-1467.\n[39] Tumarkin, Robert and Robert F. Whitelaw, 2001, \"News or Noise? Internet\nPosting and Stock Prices\", Financial Analysts Journal 57, 41-51.\n[40] Yu, Jialin, 2011, \"Disagreement and return predictability of stock portfolios\",\nJournal of Financial Economics 99, 162-183.\n[41] Zhang, Benjamin Yibin, Hou Zhou and Haibin Zhu, 2009, \"Explaining Credit\nDefault Swap Spreads with the Equity Volatility and Jump Risks of Individual\nFirms\", Review of Financial Studies 22, 5099-5131.\n28Figure 1: The upper \fgure shows the daily number of news stories with keywords\n`Bankruptcy' or `Insolvency' (blue curve) and the 3 day moving average (red curve),\nstarting in June 01, 2007 to December 31, 2010. The lower \fgure shows the daily\nnumber of news stories for the Bank of America.\n29Figure 2: The \fgure shows the value of the money accounts of trading on buy- and\nsell-signals (blue, solid curve), buy-signals (green, dashed curve) and sell-signals\n(red, dotted curve), assuming 10 bps transaction costs per round-trip and the risk-\nfree interest rate as benchmark. For the trading strategy on buy- and sell-signals,\nthe black line marks the strongest downturn, realized in May and June 2009, and the\nlight blue line marks the longest waiting period to establish a new high watermark,\nobserved in Summer 2010.\n30Summary statistics for Reuters news Sum Mean Std. Max.\nAll 210495 160.56 94.01 354\nAll w/o Saturdays / Sundays 199238 220.64 52.91 354\nEconomic news / Macroeconomics 56531 43.12 38.61 196\nGeneral News 28085 21.42 21.00 118\nDebt ratings / Credit Market News 2461 1.88 2.65 22\nSociety / Science / Nature 2426 1.85 4.17 30\nMajor Breaking News 5042 3.85 9.82 65\nBankruptcy / Insolvency 671 0.51 1.21 11\nBroker Research and Recommendation 1867 1.42 2.26 17\nCorporate Results / Results Forecasts / Warnings 17525 13.37 20.13 150\nMergers / Acquisitions / Takeovers 13598 10.37 11.47 60\nAA.N 2770 2.11 4.16 49\nAXP.N 2341 1.79 3.55 40\nBA.N 4163 3.18 3.65 21\nBAC.N 11974 9.13 8.23 77\nCAT.N 2442 1.86 3.65 50\nCSCO.O 3085 2.35 3.83 35\nCVX.N 5687 4.34 3.67 29\nDD.N 980 0.75 1.86 22\nDIS.N 5326 4.06 3.86 26\nGE.N 10236 7.81 5.98 42\nHD.N 1546 1.18 2.99 34\nHPQ.N 4593 3.50 4.31 31\nIBM.N 4790 3.65 4.64 40\nINTC.O 5219 3.98 5.26 40\nJNJ.N 2860 2.18 3.01 29\nJPM.N 11723 8.94 7.62 45\nKFT.N 2171 1.66 3.43 35\nKO.N 2080 1.59 2.54 18\nMCD.N 2120 1.62 3.02 38\nMMM.N 1043 0.80 2.32 28\nMRK.N 3223 2.46 3.40 41\nMSFT.O 10495 8.01 7.21 68\nPG.N 2096 1.60 2.86 42\nPFE.N 3803 2.90 3.61 52\nT.N 4559 3.47 3.95 33\nTRV.N 407 0.31 1.22 19\nUTX.N 2026 1.55 2.59 20\nVZ.N 3435 2.62 3.32 28\nWMT.N 6676 5.09 5.19 45\nXOM.N 8096 6.18 4.98 33\nTable 1: This table gives summary statistics (sum, mean, standard deviation and\nmaximum) for the number of news articles per day. The upper panel classi\fes news\non the S&P500 companies by keywords and the lower panel show the statistics for all\nmembers of the Dow Jones Industrial Average separately. A news story is considered\nas relevant for a company if the company's RIC is mentioned in the \feld `related\nRICs'.\n31Qi Yi\nconstant \u000020.8872a \u000023.0588a \u00002.0450a \u00002.0569a\nP2B \u00000.3051a \u00000.2101a \u00000.0251a \u00000.0225a\nln(CAP) 2.2701a 2.3374a 0.2481a 0.2501a\nRet - \u00001.9734b - \u00000.1547b\n\u001b(Ret) - 6.9538a - 0.2916b\nR2 38.73% 68.26% 54.58% 65.89%\n# Obs. 61 61 61 61\nTable 2:\nThe table shows the regression estimates for model (2). The subscript a, b and c indicate\nsigni\fcance at the 1%, 5% and 10% con\fdence level. Qi denotes the average number of\nnews per day of company i, and Yi is the average number of days with at least on news\nstory. A news story is relevant for a company if the company's RIC is mentioned in the\n\feld `related RICs'.\n32Optimism ^ \" T V O C #(P > 0) m(PjP > 0) \u001b(PjP > 0)\nAA.N 0.0017b 0.0182 \u00000.0002 0.0343b \u00000.0609 79.0 3.2504 2.4254\nABT.N 0.0007 0.0249b 0.0007 0.0046 \u00000.0322 64.0 2.9495 1.9430\nAIG.N \u00000.0002 0.0011 0.0001 \u00000.0107 \u00000.3763 173.0 3.6032 2.3316\nAMGN.O \u00000.0001 0.0125 0.0003 0.0773a \u00000.0121 60.0 2.4562 1.4674\nAPC.N 0.0002 0.0164 \u00000.0004 \u00000.0057 \u00000.2079 56.0 4.2889 2.6812\nAXP.N \u00000.0002 0.0011 0.0001 0.0285a \u00000.4751 64.0 3.8703 4.7347\nBA.N 0.0002 \u00000.0057 0.0004 0.0087 \u00000.1926 172.0 2.3197 1.7603\nBAC.N \u00000.0007 0.0104 0.0003 \u00000.0031 0.0503 229.0 2.6885 2.0884\nBAX.N 0.0004 0.0296 \u00000.0010 \u00000.0017 \u00000.0543 27.0 3.1350 2.3361\nBMY .N 0.0010b 0.0273c \u00000.0009 0.0177 \u00000.0914c 68.0 3.5177 2.1955\nBSX.N 0.0017 \u00000.0332 \u00000.0001 \u00000.0070 \u00000.1993 28.0 2.4508 1.9893\nC.N \u00000.0003 0.0132 \u00000.0038c 0.0059 \u00000.5522 302.0 3.1279 2.0583\nCAT.N 0.0031a 0.0587a \u00000.0008 0.0223 \u00000.6997b 52.0 3.0266 1.8953\nCOP.N 0.0004 \u00000.0024 \u00000.0002 0.0037 \u00000.1246 130.0 2.9440 1.5714\nCSC.N \u00000.0009 0.0238 \u00000.0010 0.0358 \u00000.0119 7.0 5.2307 3.1978\nCSCO.O 0.0004 \u00000.0006 0.0002 \u00000.0071 \u00000.1287c 120.0 3.6380 2.6329\nCV X.N \u00000.0006c 0.0031 \u00000.0002 \u00000.0244 0.0150 136.0 2.7571 2.0611\nDD.N 0.0001 \u00000.0094 \u00000.0007 \u00000.0253 \u00000.0287 42.0 3.4745 2.2462\nDELL.O 0.0006 0.0122 0.0006 0.0468b 0.0704 99.0 2.7777 2.1504\nDIS.N \u00000.0002 \u00000.0023 0.0009 0.0082 \u00000.0449 123.0 3.1999 2.1781\nDOW.N 0.0010 0.0029 0.0011 0.0137 \u00000.5548 28.0 4.2514 2.3947\nDV N.N 0.0005 0.0191 0.0008 \u00000.0012 \u00000.1463 32.0 3.4291 2.1876\nF.N 0.0008 0.0268b \u00000.0022 \u00000.0008 1.1325 198.0 3.0296 2.2643\nFDX.N 0.0040a 0.0758a \u00000.0001 0.1048a \u00000.5887b 40.0 2.7887 2.2183\nGE.N \u00000.0008 0.0266c 0.0033a 0.0092 0.7432 59.0 3.8617 2.7574\nGLW.N 0.0018b 0.0226c \u00000.0008 0.0625b \u00000.5179 19.0 5.2515 3.9589\nGR.N \u00000.0015a 0.1334a \u00000.0012 0.0872c \u00000.2023b 20.0 4.7729 5.7485\nGS.N 0.0003 \u00000.0049 \u00000.0003 \u00000.0003 \u00000.3873 235.0 2.9275 2.1619\nHON.N 0.0000 0.0282b 0.0014 0.0273 \u00000.0772 45.0 3.3022 2.1268\nHPQ.N 0.0010b 0.0023 0.0012c 0.0029 0.2170a 119.0 3.0873 2.2698\nIBM.N 0.0002 0.0023 \u00000.0004 0.0024 0.0542 138.0 3.6778 2.9647\nINTC.O 0.0001 \u00000.0014 0.0003 \u00000.0013 \u00000.0736 112.0 2.9896 2.2001\nJNJ.N 0.0000 0.0196b 0.0009c \u00000.0046 \u00000.0076 102.0 3.1333 2.6229\nJPM.N 0.0010c 0.0041 \u00000.0001 0.0002 0.1774 183.0 2.7744 2.2258\nKFT.N 0.0004 0.0223 \u00000.0002 \u00000.0271 0.0710 94.0 2.6039 1.6123\nKO.N 0.0009c 0.0328b \u00000.0001 0.0166 \u00000.0389 50.0 3.3672 2.3124\nMCD.N 0.0010b 0.0310b \u00000.0006 0.0001 \u00000.0065 70.0 3.4556 2.3593\nMDT.N 0.0003 \u00000.0236 0.0000 \u00000.0129 0.0866 38.0 3.5020 2.7540\nMO.N 0.0002 0.0080 \u00000.0022a 0.0058 \u00000.1456 27.0 2.9978 2.6200\nMON.N 0.0010c \u00000.0034 0.0006 0.0109 0.0525 65.0 3.9448 3.6470\nMMM.N 0.0022a 0.0532a \u00000.0010 0.0687 \u00000.0954 27.0 3.5051 2.1892\nMRK.N 0.0001 0.0282a \u00000.0003 \u00000.0122 0.0518 99.0 3.4270 2.6114\nMS.N 0.0008 0.0167 \u00000.0036c 0.0259c 0.7892 201.0 2.8148 2.1083\nMSFT.O 0.0005 0.0056 0.0003 0.0038 \u00000.0363 226.0 2.6009 1.6957\nLLY .N \u00000.0013b 0.0223 0.0005 \u00000.0185 0.0591 56.0 2.9219 1.7841\nLMT.N 0.0002 0.0122 \u00000.0008c 0.0413c 0.0365 105.0 3.5112 2.4568\nORCL.O 0.0006 0.0599a \u00000.0010 0.0971a \u00000.0725 70.0 2.8478 2.2948\nOXY .N \u00000.0002 0.0172 \u00000.0006 0.0324b 0.2416b 23.0 4.5061 3.3075\nPFE.N 0.0002 \u00000.0021 0.0036 \u00000.0088 0.0384 122.0 3.0920 2.3710\nPG.N 0.0001 0.0289c \u00000.0016 \u00000.0161 0.0033 52.0 2.5772 1.7435\nSLB.N \u00000.0018b 0.0143 \u00000.0018 0.0229 0.1169 34.0 3.2163 2.5419\nT.N 0.0003 0.0104 0.0003 0.0066 0.0119 102.0 3.0401 1.8108\nTRV .N 0.0105a 0.1612b \u00000.0140c 0.1683 0.1582 9.0 1.8596 0.9999\nTWX.N 0.0000 0.0091 0.0010 0.0239 \u00000.0975 75.0 3.0055 2.1811\nTXN.N 0.0006 0.0124 0.0003 0.0119 0.0478 21.0 5.6115 4.4692\nUTX.N 0.0021a 0.0053 \u00000.0018 \u00000.0035 \u00000.2170 8.0 5.0200 3.0939\nV Z.N 0.0004 \u00000.0003 0.0003 0.0057 0.1712 121.0 2.8905 2.0383\nWFC.N 0.0011 0.0387b \u00000.0066c 0.0086 \u00000.2366 84.0 3.4648 2.4066\nWMT.N 0.0003 0.0094 \u00000.0003 \u00000.0408 \u00000.0443 180.0 2.7180 2.0239\nWLP.N \u00000.0011 0.0387b \u00000.0003 0.0257 0.4564b 36.0 3.7594 2.3134\nXOM.N \u00000.0002 0.0006 \u00000.0001 \u00000.0216 \u00000.0209 160.0 3.2283 2.1963\nPos. & sig. 13 19 3 11 3\nNeg. & sig. 4 0 6 0 5\nTable 3: The table shows the estimated, company-individual co-movement of posi-\ntive sentiment Pi;t\u00001 and abnormal returns (^ \"), stock trading volume (T), the \frst\ndi\u000berence of the volatility spread (V ), cumulated option trading volume (O) and\nthe \frst di\u000berence of the CDS spread (C), according to regression model (6). The\nsubscript a, b and c indicate signi\fcance at the 1%, 5% and 10% con\fdence level.\nColumn 7, 8 and 9 show the number of days with positive sentiment, the conditional\nmean of positive sentiment and its standard deviation.\n33Pessimism ^ \" T V O C #(N > 0) m(NjN > 0) \u001b(NjN > 0)\nAA.N \u00000.0035a \u00000.0356c 0.0088a \u00000.0501c 2.9180a 48.0 2.7442 2.0826\nABT.N 0.0001 \u00000.0258 \u00000.0010 0.0271 \u00000.0018 20.0 2.3062 1.1289\nAIG.N \u00000.0008 \u00000.0327c \u00000.0048 0.0035 \u00000.0403 136.0 3.9970 3.0133\nAMGN.O \u00000.0007 0.1162a 0.0048a 0.1479a 0.3611c 27.0 1.9520 1.8325\nAPC.N \u00000.0030b 0.0184 0.0050a 0.0395b 3.0280a 25.0 3.7985 2.7814\nAXP.N \u00000.0001 0.0115 0.0022 0.0160 2.1222a 65.0 3.1979 2.4588\nBA.N \u00000.0001 0.0162 \u00000.0004 \u00000.0045 \u00000.1733 138.0 2.4733 2.0844\nBAC.N \u00000.0010 0.0154 \u00000.0026 0.0108 0.0427 107.0 2.4113 2.1768\nBAX.N \u00000.0240a 1.2724a 0.0108b 0.2341 0.0491 6.0 1.3261 1.0626\nBMY .N 0.0012 0.0125 \u00000.0025c \u00000.0037 0.0375 20.0 2.0438 1.6211\nBSX.N \u00000.0039 0.7366a \u00000.0073 0.0303 0.8928 17.0 1.3504 0.9238\nC.N \u00000.0015 \u00000.0022 0.0010 \u00000.0014 \u00001.4546 99.0 1.9581 1.6824\nCAT.N \u00000.0011 \u00000.0276 \u00000.0009 \u00000.0301 \u00000.6743 16.0 1.7822 1.3524\nCOP.N \u00000.0002 0.0016 0.0004 0.0067 \u00000.1752c 84.0 2.8550 2.0177\nCSC.N 0.0091a 0.1801b \u00000.0080c \u00000.0537 \u00000.7654 2.0 2.8250 2.5809\nCSCO.O \u00000.0010 0.0783a \u00000.0034 0.0588c \u00000.1064 19.0 2.0095 1.6628\nCV X.N \u00000.0000 0.0117 \u00000.0002 \u00000.0156 0.0176 94.0 3.0215 1.9161\nDD.N \u00000.0008 0.0323c \u00000.0015 \u00000.0388 1.8261a 23.0 2.8427 1.9362\nDELL.O \u00000.0045a 0.0706a 0.0008 0.0472 0.5376c 42.0 1.8228 1.7795\nDIS.N \u00000.0024b \u00000.0076 0.0040 0.0174 0.1655 31.0 1.9420 1.3118\nDOW.N 0.0000 \u00000.0144 \u00000.0001 \u00000.0133 \u00000.0967 31.0 3.9294 2.7804\nDV N.N \u00000.0088a 0.0188 0.0220a 0.0137 \u00000.6530 8.0 1.8642 1.5859\nF.N \u00000.0020 \u00000.0082 0.0017 \u00000.0090 11.9571 88.0 2.4773 2.0080\nFDX.N 0.0005 \u00000.0053 \u00000.0001 \u00000.0261 0.1863 30.0 3.2106 3.6588\nGE.N 0.0007 0.0529b 0.0000 0.0107 0.9876 37.0 3.0648 1.9041\nGLW.N \u00000.0073a 0.2269a 0.0026 0.0348 0.6894 15.0 2.6617 3.9609\nGR.N 0.0159c \u00000.0328 \u00000.0025 1.4221c 0.0063 2.0 1.4533 0.5704\nGS.N \u00000.0019b 0.0480b 0.0034b 0.0338c 0.1399 116.0 2.3064 1.9284\nHON.N 0.0005 0.0029 \u00000.0033 \u00000.0353 \u00000.4228 9.0 2.5706 1.5175\nHPQ.N \u00000.0022a 0.0652a 0.0012 0.0419c 0.3178b 52.0 2.1668 1.8391\nIBM.N \u00000.0003 0.0054 0.0014c 0.0092 0.1860 37.0 2.7328 2.5395\nINTC.O \u00000.0012 \u00000.0199 0.0021 \u00000.0652 \u00000.7197a 48.0 2.2750 1.5690\nJNJ.N \u00000.0001 0.0025 0.0000 \u00000.0145 0.0012 46.0 2.8465 2.7153\nJPM.N 0.0003 0.0017 0.0032b 0.0040 0.1679 106.0 2.9178 2.6319\nKFT.N \u00000.0019c 0.0219 \u00000.0005 0.1888 \u00000.1957 14.0 2.4609 1.9463\nKO.N 0.0001 0.0169 0.0005 \u00000.0139 0.0210 20.0 5.2119 4.8020\nMCD.N 0.0006 0.0104 0.0005 0.0140 0.0247 31.0 2.0148 1.5340\nMDT.N \u00000.0019 0.1235a 0.0051b 0.0897 0.4595 21.0 1.6765 1.1224\nMO.N 0.0026 \u00000.0224 \u00000.0024 \u00000.0074 \u00000.0677 6.0 1.0153 1.2205\nMON.N \u00000.0003 \u00000.0204 0.0006 \u00000.0103 \u00000.0741 30.0 2.3710 2.2904\nMMM.N 0.0019 \u00000.0005 0.0050 \u00000.0135 \u00000.3744 5.0 2.2973 0.9381\nMRK.N \u00000.0023a 0.0267 0.0003 0.0119 0.1090 48.0 2.3156 1.9083\nMS.N \u00000.0028c 0.0269 0.0043 \u00000.0106 9.7139 75.0 2.4378 1.9433\nMSFT.O \u00000.0004 0.0088 \u00000.0009 \u00000.0615c 0.0550 72.0 1.8498 1.4563\nLLY .N \u00000.0003 0.0022 0.0002 \u00000.0225 \u00000.0258 25.0 3.2295 2.9857\nLMT.N 0.0013 \u00000.0065 0.0005 \u00000.0117 \u00000.0058 44.0 2.1761 1.8434\nORCL.O \u00000.0001 0.0128 0.0001 0.0119 0.0585 51.0 2.9248 2.5158\nOXY .N \u00000.0019 \u00000.0035 \u00000.0135 0.2157 \u00000.7786 2.0 1.3525 0.6824\nPFE.N \u00000.0007 0.0185 0.0015 \u00000.0478 0.0943 62.0 2.3397 1.5395\nPG.N 0.0002 \u00000.0186 \u00000.0023c \u00000.1008c 0.0270 48.0 2.2890 1.4934\nSLB.N \u00000.0026 \u00000.0102 0.0084a 0.0284 0.5508b 12.0 2.5346 1.9957\nT.N \u00000.0002 0.0007 \u00000.0010 \u00000.0013 \u00000.1212 36.0 2.0412 1.8221\nTRV .N 0.0075a 0.0632 \u00000.0072 0.0580 0.3904 8.0 3.3083 2.2270\nTWX.N \u00000.0010 0.0136 \u00000.0006 \u00000.0571 0.0371 21.0 1.8660 2.7775\nTXN.N 0.0028 0.0238 0.0011 \u00000.0238 4.5247a 5.0 2.4740 1.1307\nUTX.N \u00000.0019 0.0468 \u00000.0006 \u00000.0560 0.1638 7.0 3.9548 0.7271\nV Z.N \u00000.0021 0.0348 0.0045 0.0133 \u00000.3960 26.0 1.5650 0.8817\nWFC.N \u00000.0033b 0.0730b 0.0019 0.0207 0.1401 54.0 2.3091 1.6347\nWMT.N \u00000.0012b 0.0121 0.0016b \u00000.0899 0.0103 62.0 2.6808 2.1770\nWLP.N \u00000.0001 0.0251 \u00000.0008 \u00000.0011 \u00001.2920 9.0 1.3725 1.2788\nXOM.N \u00000.0002 \u00000.0002 \u00000.0003 0.0032 0.0288 89.0 2.6779 1.9955\nPos. & sig. 3 12 11 6 9\nNeg. & sig. 14 2 3 2 2\nTable 4: The table shows the estimated, company-individual co-movement of nega-\ntive sentiment Pi;t\u00001 and abnormal returns (^ \"), stock trading volume (T), the \frst\ndi\u000berence of the volatility spread (V ), cumulated option trading volume (O) and\nthe \frst di\u000berence of the CDS spread (C), according to regression model (6). The\nsubscript a, b and c indicate signi\fcance at the 1%, 5% and 10% con\fdence level.\nColumn 7, 8 and 9 show the number of days with negative sentiment, the conditional\nmean of negative sentiment and its standard deviation.\n34Disagreement ^ \" T V O C #(D > 0) m(DjD > 0) \u001b(DjD > 0)\nAA.N \u00000.0001 0.1077a \u00000.0028 0.0469c \u00000.5025 54.0 2.4188 2.2592\nABT.N \u00000.0022b 0.0682b \u00000.0005 0.0195 0.1882 31.0 1.6257 1.6121\nAIG.N 0.0017 0.0653a 0.0029 0.0437b \u00002.3837 209.0 3.2710 2.1547\nAMGN.O 0.0085a 0.2571a \u00000.0051a 0.1350a \u00000.6805a 33.0 1.7816 1.6824\nAPC.N 0.0027 0.1083a \u00000.0009 0.0215 \u00000.3629 32.0 1.8993 1.7249\nAXP.N \u00000.0004 0.0749a \u00000.0019 0.0277c \u00001.0286 54.0 2.7427 2.8239\nBA.N \u00000.0018a 0.0514a 0.0013b 0.0091 0.2428 202.0 2.0917 1.7702\nBAC.N 0.0000 0.0628a 0.0028 0.0367a \u00000.2434 216.0 2.2327 1.9952\nBAX.N \u00000.0024c 0.3327a \u00000.0022 0.2572a \u00000.0154 13.0 2.8396 2.5411\nBMY .N 0.0002 0.0427 \u00000.0006 0.0091 0.0492 43.0 1.7008 1.7800\nBSX.N \u00000.0124a 0.3925a 0.0169a 0.0894 3.3971a 19.0 1.9882 2.4059\nC.N 0.0003 0.0515a 0.0021 0.0234b 0.5848 276.0 2.2967 1.6895\nCAT.N 0.0002 0.1059a \u00000.0024 0.0315 \u00000.4019 27.0 2.1223 2.3512\nCOP.N \u00000.0004 0.0207c 0.0016 0.0008 0.2143c 103.0 2.3616 1.5052\nCSC.N 0.0128b 0.3420b 0.0217a \u00000.4354 7.2457a 2.0 1.8420 0.9650\nCSCO.O \u00000.0013b 0.0349b 0.0001 0.0169 \u00000.0436 73.0 1.7396 2.1665\nCV X.N 0.0008c 0.0021 0.0002 0.0041 0.1201 133.0 2.1836 1.8328\nDD.N 0.0007 0.1190a 0.0026 \u00000.0062 \u00000.4010 21.0 1.1783 1.1597\nDELL.O \u00000.0021 0.0898a 0.0013 0.1456a 0.2407 78.0 1.3372 1.3688\nDIS.N 0.0017b 0.0797a \u00000.0068 0.0144 \u00000.3150c 86.0 1.3702 1.4605\nDOW.N 0.0002 0.2729a 0.0011 0.1078a 0.7003 29.0 3.1736 2.5388\nDV N.N 0.0061a 0.0848a \u00000.0015 0.1263a \u00000.0593 20.0 2.4095 2.5574\nF.N \u00000.0001 0.0440b 0.0028 0.0411a \u000010.0795 177.0 1.6024 1.5340\nFDX.N \u00000.0029b 0.2532a \u00000.0009 0.0986a 0.9380a 31.0 2.4819 2.4811\nGE.N 0.0021c 0.0072 \u00000.0058b 0.0584 \u00001.1426 42.0 2.3322 1.9647\nGLW.N \u00000.0017 0.0610a \u00000.0006 \u00000.0232 0.3987 17.0 3.2953 3.3540\nGR.N 0.0096a 0.3587a \u00000.0019 0.4344c 0.3461 8.0 1.8870 1.7545\nGS.N 0.0006 0.0229 0.0008 0.0098 1.0095c 207.0 2.1838 1.7548\nHON.N 0.0001 0.0832b \u00000.0046c \u00000.0144 0.2443 27.0 1.2727 1.2025\nHPQ.N \u00000.0002 0.0813a \u00000.0025b 0.0591a 0.1156 88.0 2.1268 1.8925\nIBM.N 0.0002 0.0592a \u00000.0009 0.0982a \u00000.0697 111.0 1.6985 2.2262\nINTC.O 0.0007 0.0745a \u00000.0015 0.0864b 0.2383c 94.0 1.8811 1.9041\nJNJ.N \u00000.0006 0.0272c \u00000.0003 0.0177 0.0037 72.0 2.1668 1.9909\nJPM.N \u00000.0010c 0.0080 \u00000.0025 \u00000.0056 \u00000.2332 162.0 2.4532 2.5032\nKFT.N \u00000.0013 0.0557 0.0003 \u00000.5110 \u00000.0836 82.0 1.3556 1.1954\nKO.N 0.0007 0.0599a \u00000.0022b 0.0502 0.1193 23.0 3.1491 2.7196\nMCD.N 0.0006 0.0846a 0.0008 0.0246 0.0502 53.0 1.8974 1.3850\nMDT.N \u00000.0045a 0.2090a 0.0001 0.0732 \u00000.2749 32.0 1.7002 1.7557\nMO.N \u00000.0033c 0.1571a 0.0023 0.0858 0.8789 13.0 1.7484 0.9833\nMON.N \u00000.0003 0.0819a \u00000.0006 0.0666a 0.1395 35.0 2.3683 3.0473\nMMM.N \u00000.0020 0.1856a \u00000.0028 0.3179c 0.6947c 12.0 1.4302 1.3572\nMRK.N 0.0009 0.0479b 0.0019c 0.0110 0.0390 69.0 2.2193 1.8914\nMS.N \u00000.0009 0.0284 0.0037 \u00000.0044 \u00001.4121 151.0 2.1024 1.8497\nMSFT.O \u00000.0004 0.0423a \u00000.0013 0.0135 \u00000.0051 212.0 1.8450 1.5528\nLLY .N \u00000.0008 0.0413b 0.0017 \u00000.0208 0.2304b 33.0 2.3060 1.8998\nLMT.N \u00000.0016 0.0861a \u00000.0010 0.0193 0.0244 55.0 1.6729 1.3771\nORCL.O 0.0009 0.0151 0.0012 0.0388 \u00000.0426 65.0 2.2632 2.4857\nOXY .N 0.0022 0.0646 0.0012 0.1249b \u00000.2937 7.0 2.1770 1.7105\nPFE.N \u00000.0009 0.0358a 0.0038 0.0817 0.3807a 87.0 1.9404 1.7370\nPG.N \u00000.0011 0.0588b 0.0007 0.1997a \u00000.1966 49.0 1.8380 1.7458\nSLB.N 0.0064a 0.0920b \u00000.0071b 0.0708c \u00000.8166a 22.0 1.7485 1.7624\nT.N \u00000.0000 0.0526a \u00000.0019 0.0084 0.5071b 77.0 1.2798 1.3578\nTRV .N \u00000.0081a 0.0590 0.0027 \u00000.0531 \u00000.7709 8.0 2.2566 2.0838\nTWX.N \u00000.0016 0.0070 \u00000.0008 \u00000.0466 \u00000.1464 59.0 1.1914 1.5567\nTXN.N \u00000.0026 0.2319a \u00000.0001 0.2729a \u00000.1736 15.0 2.6765 1.2774\nUTX.N 0.0052 \u00000.0015 \u00000.0026 0.1113 \u00000.5210 3.0 1.4775 1.5095\nV Z.N 0.0019b 0.0550a 0.0006 0.0204 \u00000.0501 76.0 1.4279 1.4429\nWFC.N 0.0019 0.1155a 0.0016 0.0590c 0.0182 62.0 1.6387 1.5371\nWMT.N 0.0007 0.0763a \u00000.0014c 0.3018a 0.0805 137.0 1.7030 1.5085\nWLP.N 0.0023 0.1351a \u00000.0017 \u00000.0064 1.1201c 18.0 1.7603 1.6106\nXOM.N \u00000.0007 0.0050 0.0000 0.0645 0.0096 149.0 2.0730 1.6049\nPos. & sig. 9 49 4 23 11\nNeg. & sig. 10 0 7 0 3\nTable 5: The table shows the estimated, company-individual co-movement of dis-\nagreement, measured by Dstd, and abnormal returns (^ \"), stock trading volume (T),\nthe \frst di\u000berence of the volatility spread (V ), cumulated option trading volume\n(O) and the \frst di\u000berence of the CDS spread (C), according to regression model\n(6). The subscript a, b and c indicate signi\fcance at the 1%, 5% and 10% con\f-\ndence level. Column 7, 8 and 9 show the number of days with disagreement, the\nconditional mean of disagreement and its standard deviation.\n35Pooled Analysis s^ \" sT sV sO sC\ns^ \"\u00001 \u00000.0010 0.0088b \u00000.0034 0.0063 0.0151a\nsT\u00001 0.0221a 0.5380a \u00000.0528a 0.0854a \u00000.0049\nsV\u00001 0.0349a 0.0734a \u00000.1591a 0.0184a 0.1232a\nsO\u00001 \u00000.0190a 0.0262a 0.0037 0.2701a \u00000.0163a\nsC\u00001 \u00000.0082 0.0274a 0.0035 \u00000.0018 0.0876a\nsP 0.0273a 0.0455a \u00000.0111a 0.0237a \u00000.0069\nsN \u00000.0321a 0.0374a 0.0128a 0.0088c 0.0194a\nsDstd 0.0008 0.1187a \u00000.0058 0.0599a 0.0142a\nMonday 0.0493a \u00000.2202a 0.1149a \u00000.0089 \u00000.1259a\nTuesday 0.0349b 0.0616a \u00000.0621a 0.0489a \u00000.1059a\nWednesday \u00000.0085 \u00000.0541a 0.0538a \u00000.0091 \u00000.0880a\nThursday 0.0203 0.0095 0.1196a 0.0184 \u00000.0408b\nConstant \u00000.0199 0.0387a \u00000.0445a \u00000.0097 0.0713a\nR2 0.0041 0.3499 0.0340 0.1067 0.0297\ns^ \"\u00001 \u00000.0017 0.0103b \u00000.0033 0.0063 0.0159a\nsT\u00001 0.0237a 0.5345a \u00000.0522a 0.0849a \u00000.0043\nsV\u00001 0.0353a 0.0731a \u00000.1605a 0.0175a 0.1220a\nsO\u00001 \u00000.0169a 0.0228a 0.0044 0.2683a \u00000.0159a\nsC\u00001 \u00000.0077 0.0250a 0.0046 \u00000.0034 0.0870a\nsP 0.0429a 0.0124a \u00000.007 0.0016 \u00000.0125b\nsN \u00000.0146b \u00000.0137a 0.0176a \u00000.0230a 0.0115b\nsDpol \u00000.0388a 0.1650a \u00000.0131b 0.0969a 0.0237a\nMonday 0.0472a \u00000.2201a 0.1138a \u00000.0120 \u00000.1259a\nTuesday 0.0359b 0.0568a \u00000.0616a 0.0454a \u00000.1043a\nWednesday \u00000.0058 \u00000.0561a 0.0528a \u00000.0119 \u00000.0893a\nThursday 0.0206 0.0091 0.1199a 0.0170 \u00000.0416b\nConstant \u00000.0194 0.0402a \u00000.0443a \u00000.0076 0.0716a\nR2 0.0051 0.3552 0.0341 0.1097 0.0299\n#Obs. 36229 36229 36229 36229 36229\nTable 6: The table shows the regression estimates for \u0003, \f and K in the pooled\nregression model with contemporaneous relationships between the market variables\nand sentiment and disagreement, respectively, i.e. (7). The upper panel measures\ndisagreement with Dstd and the lower panel with sDpol. a denotes signi\fcance at\nthe 1% con\fdence level, b at the 5% con\fdence level and c at the 10% level.\n36Pooled Analysis s^ \" sT sV sO sC\ns^ \"\u00001 \u00000.0005 0.0079c \u00000.0028 0.0054 0.0155a\nsT\u00001 0.0239a 0.5475a \u00000.0548a 0.0910a \u00000.0044\nsV\u00001 0.0347a 0.0751a \u00000.1605a 0.0187a 0.1224a\nsO\u00001 \u00000.0182a 0.0350a 0.0033 0.2749a \u00000.0147a\nsC\u00001 \u00000.0081 0.0272a 0.0041 \u00000.0022 0.0871a\nsP\u00001 0.0105a 0.0093b \u00000.0058 0.0043 \u00000.0011\nsN\u00001 \u00000.0042 \u00000.0027 0.0149a 0.0070 0.0074\nsDstd\n\u00001 \u00000.0163a \u00000.0248a 0.0097c \u00000.0104a 0.0071\nMonday 0.0501a \u00000.2338a 0.1152a \u00000.0190 \u00000.1275a\nTuesday 0.0342b 0.0687a \u00000.0615a 0.0524a \u00000.1021a\nWednesday \u00000.0064 \u00000.0366a 0.0513a \u00000.0013 \u00000.0874a\nThursday 0.0200 0.0248c 0.1189a 0.0252 \u00000.0401b\nConstant \u00000.0196 0.0331a \u00000.0440a \u00000.0116 0.0707a\nR2 0.0025 0.3291 0.0340 0.1018 0.0291\ns^ \"\u00001 \u00000.0010 0.0069 \u00000.0026 0.0052 0.0154a\nsT\u00001 0.0242a 0.5487a \u00000.0544a 0.0906a \u00000.0033\nsV\u00001 0.0345a 0.0746a \u00000.1605a 0.0187a 0.1222a\nsO\u00001 \u00000.0179a 0.0356a 0.0033 0.2749a \u00000.0144a\nsC\u00001 \u00000.0080 0.0275a 0.0041 \u00000.0022 0.0872a\nsP\u00001 0.0131b 0.0151a \u00000.0055 0.0042 0.0015\nsN\u00001 0.0007 0.0067 0.0142b 0.0082 0.0096\nsD\npol\n\u00001 \u00000.0179a \u00000.0319a 0.0058 \u00000.0069 \u00000.0020\nMonday 0.0500 \u00000.2342a 0.1150a \u00000.0188 \u00000.1279a\nTuesday 0.0337b 0.0676a \u00000.0616a 0.0524 \u00000.1026a\nWednesday \u00000.0064 \u00000.0367a 0.0513a \u00000.0012 \u00000.0874a\nThursday 0.0201 0.0251b 0.1189a 0.0252 \u00000.0399a\nConstant \u00000.0193 0.0334a \u00000.0440a \u00000.0116 0.0708a\nR2 0.0025 0.3292 0.0340 0.1017 0.0291\n#Obs. 36229 36229 36229 36229 36229\nTable 7: The table shows the regression estimates for \u0003, \f and K in the pooled\nregression model without contemporaneous relationships, i.e. (9). The upper panel\nmeasures disagreement with Dstd and the lower panel with sDpol. a denotes signif-\nicance at the 1% con\fdence level, b at the 5% con\fdence level and c at the 10%\nlevel.\n37Risk-free rate Market rate\nTrading Strategy Xi;t X\n+\ni;t X\n\u0000\ni;t Xi;t X\n+\ni;t X\n\u0000\ni;t\nNo transaction costs\nMT 33.2485 17.7226 15.5405 29.0575 15.4238 13.6477\nmaxt2[0;T]fMtg 33.3755 17.7738 16.3134 29.1761 15.6800 13.7278\nmint2[0;T]fMtg -0.0406 -0.1168 -0.0917 -0.0549 0.0000 -0.2398\nq0:05(Mt) 0.8345 0.4743 0.2734 1.1774 1.0912 0.0000 P\nt 1(Mt < 0) 1 1 12 1 0 43\n10 bps\nMT 24.0594 11.6034 12.4646 19.8684 9.3046 10.5718\nmaxt2[0;T]fMtg 24.2044 11.6657 14.7080 20.2108 10.5371 10.6630\nmint2[0;T]fMtg -0.1286 -2.2423 -0.1940 -0.1429 -0.0399 -0.3421\nq0:05(Mt) 0.2535 -0.5732 0.0557 0.3913 0.4594 -0.1355 P\nt 1(Mt < 0) 3 155 22 4 1 134\n20 bps\nMT 14.8703 5.4843 9.3888 10.6793 3.1855 7.4959\nmaxt2[0;T]fMtg 15.3437 5.0716 13.1026 14.0891 6.6094 8.6155\nmint2[0;T]fMtg -1.5261 -4.8499 -0.3238 -1.6210 -0.9771 -0.6888\nq0:05(Mt) -0.9640 -3.0753 -0.1363 -0.8408 -0.3894 -0.4805 P\nt 1(Mt < 0) 197 413 82 156 116 212\n30 bps\nMT 5.6812 -0.6349 6.3129 1.4902 -2.9337 4.4201\nmaxt2[0;T]fMtg 8.954 1.1072 11.4972 9.1693 3.2732 7.0032\nmint2[0;T]fMtg -3.4651 -7.6135 -0.6654 -3.5721 -3.1743 -1.2930\nq0:05(Mt) -2.6450 -5.7681 -0.3723 -2.4114 -2.7774 -0.9527 P\nt 1(Mt < 0) 321 687 176 311 510 285\n\u001b(\u0001Mt) 0.1667 0.1819 0.1224 0.1774 0.2018 0.1340\nNumber of trades 9104 6062 3042 9104 6062 3042\nAverage duration 1.39 days 1.46 days 1.26 days 1.39 days 1.46 days 1.26 days\nTable 8: The table shows the terminal value, the maximum and the minimum value,\nthe 5% quantile and the number of days with a negative value of the money account\nfor trading strategies on company signals and di\u000berent levels of transaction costs.\nXi;t incorporates of buy- and sell-signals, X\n+\ni;t only buy-signals and X\n\u0000\ni;t only sell-\nsignals. The benchmark in the left half is the risk-free rate. The benchmark in the\nright half is the stock market. The lower panel shows the volatility of the change in\nthe value of the money account, the number of trades and the average duration per\ntrade.\n38RIC Company Name\nAA.N Alcoa Incorporated\nABT.N Abbott Laboratories\nAIG.N American International Group Inc\nAMGN.O Amgen Inc\nAPC.N Anadarko Petroleum Corp\nAXP.N American Express Co\nBA.N The Boeing Company\nBAC.N Bank of America Corp\nBAX.N Baxter International Inc\nBMY .N Bristol Myers Squibb Co\nBSX.N Boston Scienti\fc Corp\nC.N Citigroup Inc\nCAT.N Caterpillar Inc\nCOP.N ConocoPhillips\nCSC.N Computer Sciences Corp\nCSCO.O Cisco Systems Inc\nCV X.N Chevron Corp\nDD.N E I Du Pont De Nemours And Company\nDELL.O Dell Inc\nDIS.N Walt Disney Co\nDOW.N The Dow Chemical Co\nDV N.N Devon Energy Corp\nF.N Ford Motor Co\nFDX.N Fedex Corp\nGE.N General Electric Co\nGLW.N Corning Inc\nGR.N Goodrich Corp\nGS.N The Goldman Sachs Group Inc\nHD.N The Home Depot Inc\nHON.N Honeywell International Inc\nHPQ.N Hewlett Packard Co\nIBM.N International Business Machines Corp\nINTC.O Intel Corp\nJNJ.N Johnson & Johnson\nJPM.N Jpmorgan Chase & Co\nKFT.N Kraft Foods Inc\nKO.N The Coca Cola Co\nMCD.N McDonald's Corp\nMDT.N Medtronic Inc\nMO.N Altria Group Inc\nMON.N Monsanto Co\nMMM.N 3m Co\nMRK.N Merck and Co Inc\nMS.N Morgan Stanley\nMSFT.O Microsoft Corp\nLLY .N Eli Lilly And Co\nLMT.N Lockheed Martin Corp\nORCL.O Oracle Corp\nOXY .N Occidental Petroleum Corp\nPFE.N P\fzer Inc\nPG.N Procter & Gamble Co\nSLB.N Schlumberger NV\nT.N AT&T Inc\nTRV .N Travelers Companies Inc\nTWX.N Time Warner Inc\nTXN.N Texas Instruments Inc\nUTX.N United Technologies Corp\nV Z.N Verizon Communications Inc\nWFC.N Wells Fargo and Co\nWMT.N Wal Mart Stores Inc\nWLP.N WellPoint Inc\nXOM.N Exxon Mobil Corp\nTable 9: The table gives the list of companies that are included in the analyses and\nmatches the company name with the company's RIC (= Reuters instrument code)\n39",
      "id": 2459847,
      "identifiers": [
        {
          "identifier": "6234553",
          "type": "CORE_ID"
        }
      ],
      "title": "Mechanically Extracted Company Signals and their Impact on Stock and Credit Markets",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [],
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://www.wiwi.uni-konstanz.de/workingpaperseries/WP_18-11-Graf.pdf"
      ],
      "updatedDate": "2014-10-24T12:19:59",
      "yearPublished": null,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/pdf/6234553.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/6234553"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/6234553/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/6234553/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/2459847"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Bermingham, Adam"
        },
        {
          "name": "Smeaton, Alan F."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/11309727",
        "https://api.core.ac.uk/v3/outputs/143909190",
        "https://api.core.ac.uk/v3/outputs/147599699"
      ],
      "createdDate": "2013-07-10T11:53:34",
      "dataProviders": [
        {
          "id": 3365,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3365",
          "logo": "https://api.core.ac.uk/data-providers/3365/logo"
        },
        {
          "id": 2921,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2921",
          "logo": "https://api.core.ac.uk/data-providers/2921/logo"
        },
        {
          "id": 346,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/346",
          "logo": "https://api.core.ac.uk/data-providers/346/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The advent of the real-time web is proving both challeng-\n\ning and at the same time disruptive for a number of areas of research,\n\nnotably information retrieval and web data mining. As an area of research reaching maturity, sentiment analysis oers a promising direction for modelling the text content available in real-time streams. This paper reviews the real-time web as a new area of focus for sentiment analysis\n\nand discusses the motivations and challenges behind such a direction",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/11309727.pdf",
      "fieldOfStudy": null,
      "fullText": "Crowdsourced Real-world Sensing:\nSentiment Analysis and the Real-Time Web\nAdam Bermingham and Alan F. Smeaton\nCLARITY: Centre for Sensor Web Technologies,\nSchool of Computing,\nDublin City University\n{abermingham, asmeaton}@computing.dcu.ie\nAbstract. The advent of the real-time web is proving both challeng-\ning and at the same time disruptive for a number of areas of research,\nnotably information retrieval and web data mining. As an area of re-\nsearch reaching maturity, sentiment analysis offers a promising direction\nfor modelling the text content available in real-time streams. This paper\nreviews the real-time web as a new area of focus for sentiment analysis\nand discusses the motivations and challenges behind such a direction.\nKeywords: sentiment analysis, real-time web, microblog\n1 Introduction\nIn the last 10 years, user-generated content has come to dominate a large por-\ntion of the web. Reviews, blogs, social networks, discussion forums and wikis are\nall familiar concepts to the average Internet user. User-generated content has\nnow earned respect as a credible source for exploring both factual and subjec-\ntive information. This has inspired research in the area of automatic sentiment\nanalysis: methods for automatic detection of negative and positive emotions,\nopinions and other evaluations in text.\nThe real-time web refers to the portion of the web where information is\navailable shortly after it is created and where it is connected in some way with\nevents that are happening in the real world either at, or close to that time. In\nterms of user-generated content, the information takes the form of blog posts,\nmicroblog posts, news feeds and social network content amongst others. This\ncontent is often reactionary in nature, disseminating news of real-world events\nin real-time and expressing associated opinion and commentary. Just as events\nin the real-world can happen at specific times and are scheduled, or can be\nunpredicted and occur spontaneously, so too does user-generated content have\na prominent time component. Examples of scheduled real-world events would\nbe sporting contests and TV programs and spontanous real-word events would\ninclude riots and civil disturbances.\nThe microblogging service, Twitter, is a good example of information making\nup the real-time web. Twitter allows users to publish short text messages and\nthese messages then appear in their followers’ feeds and may appear in searches.\nTwitter users write about a wide variety of topics including both scheduled\nand spontaneous real-world and real-time events. The diversity of content, the\nlarge volume and the availability of data mean that Twitter provides us with a\nunique opportunity to mine sentiment in real-time in a way not possible before.\nThroughout this paper we use Twitter as a case study for sentiment in the\nreal-time web.\nThe task of applying sentiment analysis to the real-time web however has a\nnumber of challenges. Due to the dynamic nature of the real-time web, topics\nof interest are constantly evolving. There are also infrastructural challenges as\nstatic indexes and latency in computation are no longer acceptable. This also\nposes difficulties for experimental methodology as methods for evaluating tech-\nniques for real-time knowledge discovery are not well established.\nThis paper offers a review of sentiment analysis in the real-time web as a\nfuture direction for research. The rest of this paper is laid out as follows: in\nSection 2, background to sentiment analysis is presented followed by a motivation\nfor pursuing this line of research in Section 3. Challenges are discussed in Sections\n4 and we conclude in Section 5.\nPUBLIC\nPRIVATE\nARCHIVAL REAL-TIME\nBooks\nInstant \nMessaging\nBreaking News \nBlogs\nEditorial \nNews Microblogs\nEmail\nSocial \nNetworks\nResearch \nPapers\nREAL-TIME WEB\nDiscussion \nForums\nSMS\nReviews\nCompany \nDocuments\nPersonal \nArchives\nFig. 1. A conceptualization of the the real-time web in terms of digital content.\n2 Background\nMuch sentiment analysis research concerns the study of static data. A number of\npublicly available collections have been widely used in research such as Pang and\nLee’s movie review corpus [6], Wilson at al.’s MPQA corpus [10], Blogs06 from\nthe TREC Blog Track [4] and the data provided as part of the NTCIR opinion\nfinding task [8] to name a few of the more common examples. These corpora\nmay all be thought of as static corpora: all of the topics are predefined and the\nlabels are independent of any temporal aspect. This has many advantages, the\nreplicability of experiments being one. However, in the real-time web, documents,\ntopics and sentiment all have an inherent temporal component.\nRecently there have been research works based on Twitter. Bollen et al. [1]\nmodelled trends in mood on Twitter in the 6 dimensions defined by the psy-\nchometric test, the Profile of Mood States (POMS): tension, depression, fatigue,\nvigour, anger and confusion. They developed a term-based emotional rating sys-\ntem by extending the 65 adjectives defined in POMS to a lexicon of 793 terms.\nThey then use the variance of these terms in Twitter posts over a series of 153\ndays to model the trends in mood along the 6 dimensions. They found that\nsentiment on Twitter correlated with real-world values such as stock prices and\ncoincides with events. They conclude:\n“events in the social, political, cultural and economical sphere do\nhave a significant, immediate and highly specific effect on the various\ndimensions of public mood.”\nSimilarly, Diakapolous and Shamma analysed sentiment in Twitter posts\nto characterise the reaction to various issues in a US Presidential election de-\nbate in 2008 [2]. Rather than use an automated sentiment analysis approach,\nthey crowdsourced the annotations using Amazon Mechanical Turk (AMT). The\nAMT annotators were asked to annotate documents which discuss the debate as\npositive, negative, mixed or other towards each of the presidentatial candidates.\nDiakopolous notes significant shift in sentiment as the debate moves between\nspeakers and topics. The also suggest that by correlating the positive and neg-\native sentiment they can identify controversial issues, though they note that\nthis requires further investigation. They conclude by offering two caveats sur-\nrounding modelling sentiment in real-time streams: (i) the relationship between\nsentiment and a real-world occurence is inferred based on document timestamp\nand relevant terms and is not necessarily accurate in all cases and (ii) the au-\nthors of the documents in the stream are not necessarily representative of the\nwider population.\nThese works support the assumption that the sentiment in the document\nstream is indicative of people’s reaction to real-world events. This is encouraging\nas it demonstrates that real-time social content, such as Twitter, is a valid data\nsource for gauging public sentiment.\nAnother promising avenue of research is in market research. One such work\nis Jansen et al. who studied the Word-Of-Mouth effect on Twitter, focusing on\nhow and why positive and negative sentiment towards brands spreads on Twitter\n[3]. They use supervised learning to classify twitter posts which mention brands\nfor sentiment towards that brand using n-gram features. Interestingly, they find\nautomated sentiment analysis accuracy comparable to manual classification. Of\nthe documents they analyse, 19% contain a reference to a product, company or\nservice and of these, 20% contained sentiment towards that product, company\nor service. They also observe large temporal swings in sentiment and suggest\nthat marketing companies are required to continually monitor their streams of\ndocuments over time. Again the results demonstrate that Twitter data provides\na means to sense the collective sentiment towards topics of interest.\nThese exploratory works hint at the potential of researching automatic tech-\nniques to model the sentiment in the real-time web. There remains much work\nto be done to explore fully the possibilities presented by the real-time web.\n3 Motivation and Applications\nIn applying sentiment analysis to the real-time web, and in specific user-generated\ncontent, we are in essence crowd-sourcing our sensing of the real world in real-\ntime. The online conversation becomes a sea of data from which we can infer\nsentiment and extrapolate information about the real-world around us. This is\nnot something that has been possible until now in any meaningful way and so\nwe are presented with a unique avenue for research.\nFor some time there have been methods of near-instantaneous computer-\nmediated communication. Instant messaging (IM) and text messaging on mobile\nphones (SMS) are two such examples. Each of these types of communication\nhowever are intrinsically private and obtaining and publishing datasets based on\nthe private correspondence of users is problematic at best. The public nature of\nthe Internet means that no such privacy restriction exists in terms of mining the\ninformation in online content, real-time or otherwise. The standardised way in\nwhich this content is made available not only encourages developers and users to\nbetter use the content, but also us as researchers to efficiently construct datasets\nand data streams to be used for study.\nThe recent growth in the volume in the real-time web, specifically on Twitter,\nis staggering. At least one website has recently measured the rate of Twitter posts\nbeing published is 2 billion per month, or 64 million per day, and increasing1.\nThis is undoubtedly a large volume of information to analyse, even given the\nshort length of twitter posts. But what portion of this deluge are relevant to\na given topic interest? In the recent Soccer World Cup in South Africa, even\nthe early matches saw activity in the region of hundreds of thousands of tweets\nper match. Similar activity was seen during the NBA play-offs. High levels of\nactivity are also seen in relation to unfolding news stories and live television.\nThus the need for automated analytical and aggregation techniques is clear.\nSearch on Twitter2 is dominated by inverse chronologically ordered results, fil-\n1 http://royal.pingdom.com/2010/06/08/twitter-now-2-billion-tweets-per-month/\n2 http://search.twitter.com\ntered by keyword. In this model, the assumption is that recency is the single\nmost important measure of relevance. With many relevant documents being\nproduced, there will be many more before a user has time to finish reading the\nsearch results. This simple model does not scale well. The problem of search\nin the real-time web is still an unsolved problem. Perhaps real-time streams of\nuser generated content are destined to be passively observed rather than ac-\ntively searched. The problem definition and methodologies are still in flux. By\nenriching the documents with sentiment information, the opportunity is there\nto employ more sophisticated methods to help users find useful information. For\nexample, ensuring a level of diversity and representativeness of sentiment in the\nresults list.\nAs well as helping users find documents of interest, there is also value in being\nable to determine the aggregate sentiment in a real-time stream of documents.\nBeing able to quantify sentiment for a given topic over time permits us to use\nsentiment as we would a stream of any other source of data: stock prices, sensor\ndata. A real-time sentiment trend then allows us to find events that trigger\ndeviations in sentiment and to integrate this with other data feeds both from the\nonline and oﬄine world. This type of aggregation and high performance analytics\nis of obvious benefit to many areas of industry, government and research.\nSentiment analysis is an area of research reaching maturity (see [7] for a de-\ntailed history of the field). There are now established methodologies, in particular\nfor machine learning techniques, for obtaining accuracies comparable with the\ntraditionally easier task of topical classification. It is the intersection between (i)\nthe abundance and availability of data, (ii) the maturity and of sentiment anal-\nysis as an area of research and (iii) the dearth of research into sentiment-based\nstrategies for real-time information analysis that motivate this area of research.\n4 Challenges\nThe primary challenge in the real-time web is understanding the information\nneeds and interaction patterns. We have already seen how real-time services\nsuch as Twitter are both a disruptive and a challenging and opportunistic tech-\nnology. As of now it is unclear what are the common interaction patterns and\nperhaps more importantly, which ones will prove to be beneficial as the technol-\nogy matures to a significant degree of productivity. Again taking Twitter as an\nexample, perhaps search will prove to be the most valuable way of interacting\nwith Twitter, as it has been for the traditional web. On the other hand, perhaps\nthe real value is being able to navigate the people you personally follow, a more\nsocial network oriented perspective. Perhaps focus will shift from journal style\ncontent to more topical content, as has arguably happened in the blogosphere.\nPerhaps the real-time web in the context of an event behaves quite differently to\nthat at another arbitrary time. Or most likely, perhaps the overall picture is one\nwhich is more complex and which warrants careful thought and consideration.\nFrom a sentiment analysis point of view, the real-time web means pushing\nsentiment analysis beyond review classification. Review classification serves as a\ngood constrained experiment to evaluate sentiment analysis techniques but is less\nrelevant in a real-world ad-hoc domain. In review classification often the topics\nare homogenous and there is little or no topic drift in the documents. Twitter by\nit’s nature is dynamic and unpredictable, even the sentiment topics of interest\nmay themselves may only become apparent as a real-world event unfolds and not\nbe conceived beforehand. These types of ad-hoc scenarios can be troublesome.\nAdd to this the variability in topic nature and the temporal dependency of\ntraining data used and there are a lot of challenges in approaching the accuracy\nenjoyed in classifying reviews.\nComputational efficiency is also a consideration. Some of the higher perform-\ning sentiment analysis systems (for example [5]) have relied on computationally\nexpensive feature extraction techniques such as parsing and dependency extrac-\ntion to achieve their results. Without extensive computing resources, this would\nlikely be unfeasible for the forseeable future with a required throughput of many\ndocuments per second. These problems can be mitigated by sampling strategies\nor, for aggregate sentiment, by allowing for a latency or less granular sentiment\nreading.\nAs an informal communication platform, Twitter exhibits characteristics of\nnoisy text. Twitter posts often contain spelling errors, grammatical contrac-\ntions, non-standard punctuation, emoticons etc. Tagliamonte analysed English\nlanguage use in Instant Messenger (IM) by teenagers and adolescents and found\nthat although the text exhibited features of noisy text, these patterns were not\nprevalent in his older participants and that this type of text was not as com-\nmon as construed in the media [9]. In any case, with sufficient training data\nn-gram models (or extensions thereof) should robustly handle arbitrary tokens.\nThis could degrade performance of approaches who rely on parsing to extract\nfeatures from the text. Such approaches may benefit from a step of language\nnormalisation where the text is amended, either heuristically or using a machine\ntranslation approach, to a more standard form. However, non-standard language\nusage may even prove to be beneficial to learning approaches. Often an author\nmay do this to express themselves in a more concise way where they may be\nconstrained by length of by the modality they are using for input. For example,\nthe following punctuation sequences all add tone to the content of a document:\n“...”, “:-)”, “?!”, “!!!!”.\nEvaluating methods for sentiment analysis in real-time also poses a signifi-\ncant challenge for research methodologies. It is likely that a true measure of the\neffectiveness of improving information discovery using sentiment analysis is not\npossible to determine outside of real-time. In real-time, perhaps our only option\nis the expensive task of evaluating users’ interactions with systems by inferring\neffectiveness from their use of the system. Another option is to prompt users for\nreal-time system feedback. This type of evaluation can borrow from the field of\nuser interface evaluation where contrasting methods of interaction are evaluated.\nIn either case, this is not as scalable or as reproducible as some common eval-\nuation methodologies such as multiple fold cross validation in machine learning\nor the Cranfield model for evaluating information retrieval systems.\nThese challenges collectively are not insurmountable and form a number of\ninteresting research questions for the field of sentiment analysis to pursue.\n5 Conclusion\nThe real-time web has disrupted and challenged our methods for managing in-\nformation retrieval and knowledge discovery on the web. Our research method-\nologies for analysing static corpora, independent of time, need to be rethought\nand we need to adapt our infrastructures for dealing with such time dependent\ninformation. At its core this is an exciting time for user-generated content. In a\nshort space of time, the Internet has a become a place where information and\nthoughts about a vast array of topics can be gathered nearly instantaneously.\nWithout a well-designed method for organising this information and allowing\nusers to access and leverage it, the data becomes redundant, and we are not able\nto take advantage of the opportunities presented by the advent of the real-time\nweb.\nAcknowledgments\nThis work is supported by Science Foundation Ireland under grant 07/CE/I1147\nReferences\n1. J. Bollen, A. Pepe, and H. Mao. Modeling public mood and emotion: Twitter\nsentiment and socio-economic phenomena. CoRR, abs/0911.1583, 2009.\n2. N. A. Diakopoulos and D. A. Shamma. Characterizing debate performance via\naggregated Twitter sentiment. In Conference on Human Factors in Computing\nSystems (CHI 2010), 2010.\n3. B. Jansen, M. Zhang, K. Sobel, and A. Chowdury. Twitter power: Tweets as\nelectronic word of mouth. Journal of the American Society for Information Science\nand Technology, 2009.\n4. C. Macdonald and I. Ounis. The TREC Blogs06 collection : Creating and analysing\na blog test collection. Technical report, University of Glasgow, Department of\nComputing Science, 2006.\n5. S. Matsumoto, H. Takamura, and M. Okumura. Sentiment classification using word\nsub-sequences and dependency sub-trees. In Proceedings of PAKDD’05, the 9th\nPacific-Asia Conference on Advances in Knowledge Discovery and Data Mining,\n2005.\n6. B. Pang and L. Lee. A sentimental education: sentiment analysis using subjectivity\nsummarization based on minimum cuts. In ACL ’04: Proceedings of the 42nd An-\nnual Meeting on Association for Computational Linguistics, page 271, Morristown,\nNJ, USA, 2004. Association for Computational Linguistics.\n7. B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundation and\nTrends in Information Retrieval, 2(1-2):1–135, 2008.\n8. Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multi-\nlingual opinion analysis task at NTCIR-7. 2008.\n9. S. A. Tagliamonte and D. Denis. LINGUISTIC RUIN? LOL! INSTANT MESSAG-\nING AND TEEN LANGUAGE. American Speech, 83(1):3–34, 2008.\n10. T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase-\nlevel sentiment analysis. Proceedings of the 2005 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 347–354, 2005.\n",
      "id": 4763956,
      "identifiers": [
        {
          "identifier": "oai:doras.dcu.ie:15585",
          "type": "OAI_ID"
        },
        {
          "identifier": "147599699",
          "type": "CORE_ID"
        },
        {
          "identifier": "143909190",
          "type": "CORE_ID"
        },
        {
          "identifier": "11309727",
          "type": "CORE_ID"
        }
      ],
      "title": "Crowdsourced real-world sensing: sentiment analysis and the real-time web",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:doras.dcu.ie:15585"
      ],
      "publishedDate": "2010-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [
        {
          "id": 16549230,
          "title": "A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.",
          "authors": [],
          "date": "2004",
          "doi": "10.3115/1218955.1218990",
          "raw": "B. Pang and L. Lee. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 271, Morristown, NJ, USA, 2004. Association for Computational Linguistics.",
          "cites": null
        },
        {
          "id": 16549214,
          "title": "Characterizing debate performance via aggregated Twitter sentiment.",
          "authors": [],
          "date": "2010",
          "doi": "10.1145/1753326.1753504",
          "raw": "N. A. Diakopoulos and D. A. Shamma. Characterizing debate performance via aggregated Twitter sentiment. In Conference on Human Factors in Computing Systems (CHI 2010), 2010.",
          "cites": null
        },
        {
          "id": 16549209,
          "title": "Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena.",
          "authors": [],
          "date": "2009",
          "doi": null,
          "raw": "J. Bollen, A. Pepe, and H. Mao. Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena. CoRR, abs/0911.1583, 2009.",
          "cites": null
        },
        {
          "id": 16549235,
          "title": "Opinion mining and sentiment analysis. Foundation and Trends in Information Retrieval,",
          "authors": [],
          "date": "2008",
          "doi": "10.1561/1500000011",
          "raw": "B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundation and Trends in Information Retrieval, 2(1-2):1{135, 2008.",
          "cites": null
        },
        {
          "id": 16549236,
          "title": "Overview of multilingual opinion analysis task at NTCIR-7.",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "Y. Seki, D. K. Evans, L. Ku, L. Sun, H. Chen, and N. Kando. Overview of multilingual opinion analysis task at NTCIR-7. 2008.9. S. A. Tagliamonte and D. Denis. LINGUISTIC RUIN? LOL! INSTANT MESSAGING AND TEEN LANGUAGE. American Speech, 83(1):3{34, 2008.",
          "cites": null
        },
        {
          "id": 16549241,
          "title": "Recognizing contextual polarity in phraselevel sentiment analysis.",
          "authors": [],
          "date": "2005",
          "doi": "10.3115/1220575.1220619",
          "raw": "T. Wilson, J. Wiebe, and P. Homann. Recognizing contextual polarity in phraselevel sentiment analysis. Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 347{354, 2005.",
          "cites": null
        },
        {
          "id": 16549226,
          "title": "Sentiment classi using word sub-sequences and dependency sub-trees.",
          "authors": [],
          "date": null,
          "doi": "10.1007/11430919_37",
          "raw": "S. Matsumoto, H. Takamura, and M. Okumura. Sentiment classication using word sub-sequences and dependency sub-trees. In Proceedings of PAKDD'05, the 9th Pacic-Asia Conference on Advances in Knowledge Discovery and Data Mining,",
          "cites": null
        },
        {
          "id": 16549222,
          "title": "The TREC Blogs06 collection : Creating and analysing a blog test collection.",
          "authors": [],
          "date": "2006",
          "doi": null,
          "raw": "C. Macdonald and I. Ounis. The TREC Blogs06 collection : Creating and analysing a blog test collection. Technical report, University of Glasgow, Department of Computing Science, 2006.",
          "cites": null
        },
        {
          "id": 16549219,
          "title": "Twitter power: Tweets as electronic word of mouth.",
          "authors": [],
          "date": "2009",
          "doi": "10.1002/asi.21149",
          "raw": "B. Jansen, M. Zhang, K. Sobel, and A. Chowdury. Twitter power: Tweets as electronic word of mouth. Journal of the American Society for Information Science and Technology, 2009.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://doras.dcu.ie/15585/1/aics2010.pdf"
      ],
      "updatedDate": "2021-12-13T06:11:25",
      "yearPublished": 2010,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/11309727.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/11309727"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/11309727/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/11309727/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/4763956"
        }
      ]
    },
    {
      "acceptedDate": "2016-07-07T00:00:00",
      "arxivId": "1512.01818",
      "authors": [
        {
          "name": "A Abbasi"
        },
        {
          "name": "A Esuli"
        },
        {
          "name": "A Hannak"
        },
        {
          "name": "A Tamersoy"
        },
        {
          "name": "A Tumasjan"
        },
        {
          "name": "AB Warriner"
        },
        {
          "name": "ADI Kramer"
        },
        {
          "name": "B Liu"
        },
        {
          "name": "B Pang"
        },
        {
          "name": "B Pang"
        },
        {
          "name": "C Biever"
        },
        {
          "name": "C Hutto"
        },
        {
          "name": "C Levallois"
        },
        {
          "name": "C Strapparava"
        },
        {
          "name": "D Garcia"
        },
        {
          "name": "D Tang"
        },
        {
          "name": "D Watson"
        },
        {
          "name": "DH Wolpert"
        },
        {
          "name": "E Cambria"
        },
        {
          "name": "E Cambria"
        },
        {
          "name": "E Kouloumpis"
        },
        {
          "name": "GA Miller"
        },
        {
          "name": "H Wang"
        },
        {
          "name": "J Reis"
        },
        {
          "name": "J Reis"
        },
        {
          "name": "J Wiebe"
        },
        {
          "name": "JR Landis"
        },
        {
          "name": "M Araujo"
        },
        {
          "name": "M Brysbaert"
        },
        {
          "name": "M Cha"
        },
        {
          "name": "M Hu"
        },
        {
          "name": "M Taboada"
        },
        {
          "name": "M Taboada"
        },
        {
          "name": "M Taboada"
        },
        {
          "name": "M Tsytsarau"
        },
        {
          "name": "ML Berenson"
        },
        {
          "name": "N Godbole"
        },
        {
          "name": "N Kalchbrenner"
        },
        {
          "name": "N Oliveira"
        },
        {
          "name": "N Pappas"
        },
        {
          "name": "N Pappas"
        },
        {
          "name": "NA Diakopoulos"
        },
        {
          "name": "P Gonçalves"
        },
        {
          "name": "P Nakov"
        },
        {
          "name": "PJ Stone"
        },
        {
          "name": "PS Dodds"
        },
        {
          "name": "PS Dodds"
        },
        {
          "name": "R Feldman"
        },
        {
          "name": "R Jain"
        },
        {
          "name": "R Johnson"
        },
        {
          "name": "R Plutchik"
        },
        {
          "name": "R Snow"
        },
        {
          "name": "R Socher"
        },
        {
          "name": "R Valitutti"
        },
        {
          "name": "S Baccianella"
        },
        {
          "name": "S Mohammad"
        },
        {
          "name": "S Mohammad"
        },
        {
          "name": "S Mohammad"
        },
        {
          "name": "S Narr"
        },
        {
          "name": "SM Mohammad"
        },
        {
          "name": "T Smedt De"
        },
        {
          "name": "T Wilson"
        },
        {
          "name": "T Wilson"
        },
        {
          "name": "YR Tausczik"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Filipe N"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/202415923",
        "https://api.core.ac.uk/v3/outputs/194925623",
        "https://api.core.ac.uk/v3/outputs/646587179"
      ],
      "createdDate": "2016-08-03T02:33:42",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 2612,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2612",
          "logo": "https://api.core.ac.uk/data-providers/2612/logo"
        },
        {
          "id": 7959,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/7959",
          "logo": "https://api.core.ac.uk/data-providers/7959/logo"
        }
      ],
      "depositedDate": "2016-07-07T00:00:00",
      "abstract": "In the last few years thousands of scientific papers have investigated\nsentiment analysis, several startups that measure opinions on real data have\nemerged and a number of innovative products related to this theme have been\ndeveloped. There are multiple methods for measuring sentiments, including\nlexical-based and supervised machine learning methods. Despite the vast\ninterest on the theme and wide popularity of some methods, it is unclear which\none is better for identifying the polarity (i.e., positive or negative) of a\nmessage. Accordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, \\textit{as they are\nused in practice}, across multiple datasets originated from different data\nsources. Such a comparison is key for understanding the potential limitations,\nadvantages, and disadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-four popular sentiment\nanalysis methods (which we call the state-of-the-practice methods). Our\nevaluation is based on a benchmark of eighteen labeled datasets, covering\nmessages posted on social networks, movie and product reviews, as well as\nopinions and comments in news articles. Our results highlight the extent to\nwhich the prediction performance of these methods varies considerably across\ndatasets. Aiming at boosting the development of this research area, we open the\nmethods' codes and datasets used in this article, deploying them in a benchmark\nsystem, which provides an open API for accessing and comparing sentence-level\nsentiment analysis methods",
      "documentType": "research",
      "doi": "10.1140/epjds/s13688-016-0085-1",
      "downloadUrl": "https://core.ac.uk/download/478016225.pdf",
      "fieldOfStudy": null,
      "fullText": "SentiBench - a benchmark comparison of\nstate-of-the-practice sentiment analysis methods\nFilipe N Ribeiro\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfiliperibeiro@dcc.ufmg.br\nMatheus Arau´jo\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nPollyanna Gonc¸alves\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nMarcos Andre´ Gonc¸alves\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nFabrı´cio Benevenuto\nFederal University of Minas\nGerais\nBelo Horizonte, Brazil\nfabricio@dcc.ufmg.br\nABSTRACT\nIn the last few years thousands of scientific papers have in-\nvestigated sentiment analysis, several startups that measure\nopinions on real data have emerged and a number of inno-\nvative products related to this theme have been developed.\nThere are multiple methods for measuring sentiments, inclu-\nding lexical-based and supervised machine learning methods.\nDespite the vast interest on the theme and wide popularity\nof some methods, it is unclear which one is better for iden-\ntifying the polarity (i.e., positive or negative) of a message.\nAccordingly, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, as\nthey are used in practice, across multiple datasets originated\nfrom different data sources. Such a comparison is key for\nunderstanding the potential limitations, advantages, and di-\nsadvantages of popular methods. This article aims at filling\nthis gap by presenting a benchmark comparison of twenty-\nfour popular sentiment analysis methods (which we call the\nstate-of-the-practice methods). Our evaluation is based on a\nbenchmark of eighteen labeled datasets, covering messages\nposted on social networks, movie and product reviews, as\nwell as opinions and comments in news articles. Our results\nhighlight the extent to which the prediction performance of\nthese methods varies considerably across datasets. Aiming at\nboosting the development of this research area, we open the\nmethods’ codes and datasets used in this article, deploying\nthem in a benchmark system, which provides an open API for\naccessing and comparing sentence-level sentiment analysis\nmethods.\nIntroduction\nSentiment analysis has become an extremely popular tool, ap-\nplied in several analytical domains, especially on the Web and\nsocial media. To illustrate the growth of interest in the field,\nFigure 1 shows the steady growth on the number of searches\non the topic, according to Google Trends1, mainly after the\npopularization of online social networks (OSNs). More than\n7,000 articles have been written about sentiment analysis and\nvarious startups are developing tools and strategies to extract\nsentiments from text [20].\nThe number of possible applications of such a technique is\nalso considerable. Many of them are focused on monitoring\nthe reputation or opinion of a company or a brand with the\nanalysis of reviews of consumer products or services [27].\nSentiment analysis can also provide analytical perspectives\nfor financial investors who want to discover and respond to\nmarket opinions [46, 8]. Another important set of applicati-\nons is in politics, where marketing campaigns are interested\nin tracking sentiments expressed by voters associated with\ncandidates [67].\nFigure 1: Searches on Google for the Query: “Sentiment\nAnalysis”.\nDue to the enormous interest and applicability, there has been\na corresponding increase in the number of proposed sentiment\nanalysis methods in the last years. The proposed methods rely\non many different techniques from different computer science\nfields. Some of them employ machine learning methods that\n1https://www.google.com/trends/explore#q=sentiment%\n20analysis\nar\nX\niv\n:1\n51\n2.\n01\n81\n8v\n5 \n [c\ns.C\nL]\n  1\n4 J\nul \n20\n16\noften rely on supervised classification approaches, requiring\nlabeled data to train classifiers [48]. Others are lexical-based\nmethods that make use of predefined lists of words, in which\neach word is associated with a specific sentiment. The lexical\nmethods vary according to the context in which they were\ncreated. For instance, LIWC [64] was originally proposed to\nanalyze sentiment patterns in formally written English texts,\nwhereas PANAS-t [25] and POMS-ex [9] were proposed as\npsychometric scales adapted to the Web context.\nOverall, the above techniques are acceptable by the research\ncommunity and it is common to see concurrent important\npapers, sometimes published in the same computer science\nconference, using completely different methods. For example,\nthe famous Facebook experiment [33] which manipulated\nusers feeds to study emotional contagion, used LIWC [64].\nConcurrently, Reis et al. used SentiStrength [65] to measure\nthe negativeness or positiveness of online news headlines [53,\n52], whereas Tamersoy [62] explored VADER’s lexicon [28]\nto study patterns of smoking and drinking abstinence in social\nmedia.\nAs the state-of-the-art has not been clearly established, resear-\nchers tend to accept any popular method as a valid methodo-\nlogy to measure sentiments. However, little is known about the\nrelative performance of the several existing sentiment analysis\nmethods. In fact, most of the newly proposed methods are\nrarely compared with all other pre-existing ones using a large\nnumber of existing datasets. This is a very unusual situation\nfrom a scientific perspective, in which benchmark compari-\nsons are the rule. In fact, most applications and experiments\nreported in the literature make use of previously developed\nmethods exactly how they were released with no changes and\nadaptations and with none or almost none parameter setting.\nIn other words, the methods have been used as a black-box,\nwithout a deeper investigation on their suitability to a particular\ncontext or application.\nTo sum up, existing methods have been widely deployed for\ndeveloping applications without a deeper understanding regar-\nding their applicability in different contexts or their advanta-\nges, disadvantages, and limitations in comparison with each\nanother. Thus, there is a strong need to conduct a thorough\napple-to-apple comparison of sentiment analysis methods, as\nthey are used in practice, across multiple datasets originated\nfrom different data sources.\nThis state-of-the-practice situation is what we propose to inves-\ntigate in this article. We do this by providing a thorough bench-\nmark comparison of twenty-four state-of-the-practice methods\nusing eighteen labeled datasets. In particular, given the recent\npopularity of online social networks and of short texts on the\nWeb, many methods are focused in detecting sentiments at\nthe sentence-level, usually used to measure the sentiment of\nsmall sets of sentences in which the topic is known a priori.\nWe focus on such context – thus, our datasets cover messa-\nges posted on social networks, movie and product reviews,\nand opinions and comments in news articles, Ted talks, and\nblogs. We survey an extensive literature on sentiment analysis\nto identify existing sentence-level methods covering several\ndifferent techniques. We contacted authors asking for their\ncodes when available or we implemented existing methods\nwhen they were unavailable but could be reproduced based on\ntheir descriptions in the original published paper. We should\nemphasize that our work focus on off-the-shelf methods as\nthey are used in practice. This excludes most of the supervised\nmethods which require labeled sets for training, as these are\nusually not available for practitioners. Moreover, most of the\nsupervised solutions do not share the source code or a trained\nmodel to be used with no supervision.\nOur experimental results unveil a number of important fin-\ndings. First, we show that there is no single method that\nalways achieves the best prediction performance for all dif-\nferent datasets, a result consistent with the “there is no free\nlunch theorem” [74]. We also show that existing methods vary\nwidely regarding their agreement, even across similar datasets.\nThis suggests that the same content could be interpreted very\ndifferently depending on the choice of a sentiment method.\nWe noted that most methods are more accurate in correctly\nclassifying positive than negative text, suggesting that current\napproaches tend to be biased in their analysis towards positi-\nvity. Finally, we quantify the relative prediction performance\nof existing efforts in the field across different types of datasets,\nidentifying those with higher prediction performance across\ndifferent datasets.\nBased on these observations, our final contribution consists\non releasing our gold standard dataset and the codes of the\ncompared methods2. We also created a Web system through\nwhich we allow other researchers to easily use our data and\ncodes to compare results with the existing methods 3. More\nimportantly, by using our system one could easily test which\nmethod would be the most suitable to a particular dataset\nand/or application. We hope that our tool will not only help\nresearchers and practitioners for accessing and comparing a\nwide range of sentiment analysis techniques, but can also help\ntowards the development of this research field as a whole.\nThe remainder of this paper is organized as follows. In Section\n2, we briefly describe related efforts. Then, in Section 3 we\ndescribe the sentiment analysis methods we compare. Section\n4 presents the gold standard data used for comparison. Section\n5 summarizes our results and findings. Finally, Section 6\nconcludes the article and discusses directions for future work.\nBackground and Related Work\nNext we discuss important definitions and justify the focus of\nour benchmark comparison. We also briefly survey existing\nrelated efforts that compare sentiment analysis methods.\nFocus on Sentence-Level Sentiment Analysis\nSince sentiment analysis can be applied to different tasks, we\nrestrict our focus on comparing those efforts related to de-\ntect the polarity (i.e. positivity or negativity) of a given short\ntext (i.e. sentence-level). Polarity detection is a common\nfunction across all sentiment methods considered in our work,\n2Except for paid methods\n3http://www.ifeel.dcc.ufmg.br\nproviding valuable information to a number of different appli-\ncations, specially those that explore short messages that are\ncommonly available in social media [20].\nSentence-level sentiment analysis can be performed with su-\npervision (i.e. requiring labeled training data) or not. An\nadvantage of supervised methods is their ability to adapt and\ncreate trained models for specific purposes and contexts. A\ndrawback is the need of labeled data, which might be highly\ncostly, or even prohibitive, for some tasks. On the other hand,\nthe lexical-based methods make use of a pre-defined list of\nwords, where each word is associated with a specific senti-\nment. The lexical methods vary according to the context in\nwhich they were created. For instance, LIWC [64] was origi-\nnally proposed to analyze sentiment patterns in English texts,\nwhereas PANAS-t [25] and POMS-ex [9] are psychometric\nscales adapted to the Web context. Although lexical-based\nmethods do not rely on labeled data, it is hard to create a\nunique lexical-based dictionary to be used for all different\ncontexts.\nWe focus our effort on evaluating unsupervised efforts as\nthey can be easily deployed in Web services and applicati-\nons without the need of human labeling or any other type of\nmanual intervention. As described in Section 3, some of the\nmethods we consider have used machine learning to build\nlexicon dictionaries or even to build models and tune specific\nparameters. We incorporate those methods in our study, since\nthey have been released as black-box tools that can be used in\nan unsupervised manner.\nExisting Efforts on Comparison of Methods\nDespite the large number of existing methods, only a limited\nnumber of them have performed a comparison among senti-\nment analysis methods, usually with restricted datasets. Ove-\nrall, lexical methods and machine learning approaches have\nbeen evolving in parallel in the last years, and it comes as no\nsurprise that studies have started to compare their performance\non specific datasets and use one or another strategy as baseline\nfor comparison. A recent survey summarizes several of these\nefforts [66] and conclude that a systematic comparative study\nthat implements and evaluates all relevant algorithms under\nthe same framework is still missing in the literature. As new\nmethods emerge and compare themselves only against one, at\nmost two other methods, using different evaluation datasets\nand experimental methodologies, it is hard to conclude if a\nsingle method triumphs over the remaining ones, or even in\nspecific scenarios. To the best of our knowledge, our effort\nis the first of kind to create a benchmark that provides such\nthorough comparison.\nAn important effort worth mentioning consists of an annual\nworkshop – The International Workshop on Semantic Evalu-\nation (SemEval). It consists of a series of exercises grouped\nin tracks, including sentiment analysis, text similarity, among\nothers, that put several together competitors against each other.\nSome new methods such as Umigon [35] have been proposed\nafter obtaining good results on some of these tracks. Although,\nSemEval has been playing an important role for identifying\nrelevant methods, it requires authors to register for the chal-\nlenge and many popular methods have not been evaluated in\nthese exercises. Additionally, SemEval labeled datasets are\nusually focused on one specific type of data, such as tweets,\nand do not represent a wide range of social media data. In our\nevaluation effort, we consider one dataset from SemEval 2013\nand two methods that participated in the competition in that\nsame year.\nAhmadi et al. [1] performed a comparision of twitter-based\nsentiment analysis tools. They selected twenty tools and tested\nthem across five twitter datasets. This benchmark is the work\nthat most approximate from ours, but it is different in some\nmeaningful aspects. Firstly, we embraced distinct contexts\nsuch as reviews, comments and social networks aiming at pro-\nviding a broader evaluation of the tools. Secondly, the methods\nthey selected included supervised and unsupervised approa-\nches which, in our view, could be unfair for the unsupervised\nones. Although the results have been presented separately,\nthe supervised methods, as mentioned by authors, required\nextensive parameter tuning and validation in a training environ-\nment. Therefore, supervised approaches tend to adapt to the\ncontext they were applied to. As previously highlighted, our\nfocus is on off-the-shelf tools as they have been extensively\nand recently used. Many researchers and practitioners have\nalso used supervised approaches but this is out of scope of\nour work. Finally, most of the unsupervised methods selected\nin the Twitter Benchmark are paid tools, except from two of\nthem, both of which were developed as a result of published\nacademic research. Oppositely we made an extensive biblio-\ngraphy review to include relevant academic outcomes without\nexcluding the most used commercial options.\nFinally, in a previous effort [24], we compared eight sentence-\nlevel sentiment analysis methods, based on one public dataset\nused to evaluate SentiStrength [65]. This article largely ex-\ntends our previous work by comparing a much larger set of\nmethods across many different datasets, providing a much\ndeeper benchmark evaluation of current popular sentiment\nanalysis methods. The methods used in this paper were also\nincorporated as part of an existing system, namely iFeel [4].\nSentiment Analysis Methods\nThis section provides a brief description of the twenty-four\nsentence-level sentiment analysis methods investigated in this\narticle. Our effort to identify important sentence-level senti-\nment analysis methods consisted of systematically search for\nthem in the main conferences in the field and then checking for\npapers that cited them as well as their own references. Some\nof the methods are available for download on the Web; others\nwere kindly shared by their authors under request; and a small\npart of them were implemented by us based on their descripti-\nons in the original paper. This usually happened when authors\nshared only the lexical dictionaries they created, letting the\nimplementation of the method that use the lexical resource to\nourselves.\nTable 1 and Table 2 present an overview of these methods, pro-\nviding a description of each method as well as the techniques\nthey employ (L for Lexicon Dictionary and ML for Machine\nLearning), their outputs (e.g. -1,0,1, meaning negative, neutral,\nand positive, respectively), the datasets they used to validate,\nthe baseline methods used for comparison and finally lexicon\ndetails, as well as the Lexicon size column describing the num-\nber of terms contained in the method’s lexicon. The methods\nare organized in chronological order to allow a better over-\nview of the existing efforts over the years. We can note that\nthe methods generate different outputs formats. We colored\nin blue the positive outputs, in black the neutral ones, and\nin red those that are negative. Note that we included LIWC\nand LIWC15 entries in Table 2, which represents the former\nversion, launched in 2007, and the latest version, from 2015,\nrespectively. We considered both versions because the first\none was extensively used in the literature. This also allows to\ncompare the improvements between both versions.\nAdapting Lexicons for the Sentence Level Task\nSince we are comparing sentiment analysis methods on a\nsentence-level basis, we need to work with mechanisms that\nare able to receive sentences as input and produce polarities\nas output. Some of the approaches considered in this paper,\nshown in Table 2, are complex dictionaries built with great\neffort. However, a lexicon alone has no natural ability to infer\npolarity in sentence level tasks. The purpose of a lexicon goes\nbeyond the detection of polarity of a sentence [20, 37], but it\ncan also be used with that purpose [23, 32].\nSeveral existing sentence-level sentiment analysis methods,\nlike VADER [28] and SO-CAL [61], combine a lexicon and\nthe processing of the sentence characteristics to determine a\nsentence polarity. These approaches make use of a series of\nintensifiers, punctuation transformation, emoticons, and many\nother heuristics.\nThus, to evaluate each lexicon dictionaries as the basis for a\nsentence-level sentiment analysis method, we considered the\nVADER’s implementation. In other words, we used VADER’s\ncode for determining if a sentence is positive or not conside-\nring different lexicons. The reasons for choosing VADER are\ntwofold: (i) the fact it is an open source tool, allowing easy re-\nplication of the procedures we performed in our study; and (ii)\nVADER’s expressive results observed in previous experiments.\nVADER’s heuristics were proposed based on qualitative analy-\nses of textual properties and characteristics which affect the\nperceived sentiment intensity of the text. VADER’s author\nidentified five heuristics based on grammatical and syntactical\ncues to convey changes to sentiment intensity that go beyond\nthe bag-of-words model. The heuristics include treatments\nfor: 1) punctuation (e.g number of ‘!’s); 2) capitalization (e.g\n”I HATE YOU”is more intense than ”i hate you”); 3) degree\nmodifiers (e.g ”The service here is extremely good”is more\nintense than ”The service here is good”); 4) constructive con-\njunction ”but”to shift the polarity; 5) Tri-gram examination\nto identify negation (e.g ”The food here isn’t really all that\ngreat.”). We choose VADER as a basis for such heuristics\nas it is one of the most recent methods among those we con-\nsidered. Moreover, it is becoming widely used, being even\nimplemented as part of the well-known NLTK python library4.\nWe applied such heuristics to the following lexicons:\nANEW SUB, AFINN, Emolex, EmoticonsDS, NRC Hash-\ntag, Opinion Lexicon, PANAS-t, Sentiment 140 Lexicon and\n4http://www.nltk.org/ modules/nltk/sentiment/vader.html\nSentiWordNet. We notice that those strategies drastically im-\nproved most of the results of the lexicons for sentence-level\nsentiment analysis in comparison with a simple baseline ap-\nproach that averages the occurrence of positive and negative\nwords to classify the polarity of a sentence. The results for\nthe simplest usage of the above lexicons as plain methods are\navailable in the last four Tables in the Additional File 1 of the\nelectronic version of the manuscript. LIWC dictionary was\nnot included in these adaptations due to its very restrictive\nlicense, which does not allow any derivative work based on\nthe original application and lexicon. Table 2 has also a column\n(Lexicon size) that describes the number of terms contained in\nthe the proposed dictionary.\nOutput Adaptations\nIt is worth noticing that the output of each method varies\ndrastically depending on the goal it was developed for and the\napproach it employs. PANAS-t, for instance, associates each\nword with eleven moods as described in Table 1 and it was\ndesigned to track any increase or decrease in sentiments over\ntime. Emolex lexicon provides the association of each word\nwith eight sentiments. The word ’unhappy’ for example is\nrelated to anger, disgust, and sadness and it is not related to joy,\nsurprise, etc. SentiWordNet links each word with a synset (i.e.\na set of synonyms) characterized by a positive and a negative\nscore, both of them represented with a value between 0 and 1.\nThe aforementioned lexicons were used as dictionary input\nto VADER’s code. We had to adapt the way the words are\nprocessed as follows. For PANAS-t we assumed that joviality,\nassurance, serenity, and surprise are positive affect. Fear, sad-\nness, guilt, hostility, shyness, and fatigue are negative affect.\nAttentiveness was considered neutral. In the case of Emolex,\nwe considered two other entries released by the authors. The\nfirst one defines the positiviy of a word (0 or 1) and the second\ncharacterizes the negativity (0 or 1). For SentiWordNet we cal-\nculate an overall score to the word by subtracting the positive\nvalue from negative value defined to that word. For example,\nthe positive value for the word faithful is 0.625 while its nega-\ntive score is 0.0. Then the overall score score is 0.625. Finally,\nfor ANEW SUB we employed only the valence emotion of\neach word. This metric ranges from 1 to 9 and indicates the\nlevel of pleasantness of a specific word – we considered the\nvalues one to four as negative, five as neutral, and six to nine\nas positive.\nOther lexicons included in our evaluation already provide po-\nsitive and negative scores such as SentiWordNet or an overall\nscore ranging from a negative to a positive value. After ap-\nplying VADER’s heuristics for each one of these lexicons we\nget scores in the same way VADER’s output (see Table 2).\nOther methods also required some output handling. The availa-\nble implementation of OpinionFinder 5, for instance, generates\npolarity outputs (-1,0, or 1) for each sentiment clue found in\na sentence so that a single sentence can have more than one\nclue. We considered the polarity of a single sentence as the\nsum of the polarities of all the clues.\n5http://mpqa.cs.pitt.edu/opinionfinder/\nTable 1: Overview of the sentence-level methods available in the literature.\nName Description L ML\nEmoticons [24] Messages containing positive/negative emoticons are positive/negative. Messages without emoticons are notclassified. X\nOpinion Lexicon [27] Focus on Product Reviews. Builds a Lexicon to predict polarity of product features phrases that are summarized toprovide an overall score to that product feature. X\nOpinion Finder (MPQA)\n[72] [73]\nPerforms subjectivity analysis trough a framework with lexical analysis former and a machine learning approach\nlatter. X X\nSentiWordNet [19] [5] Construction of a lexical resource for Opinion Mining based on WordNet [38]. The authors grouped adjectives, nouns,etc in synonym sets (synsets) and associated three polarity scores (positive, negative and neutral) for each one. X X\nLIWC [64]\nAn acronym for Linguistic Inquiry and Word Count, LIWC is a text analysis paid tool to evaluate emotional,\ncognitive, and structural components of a given text. It uses a dictionary with words classified into categories (anxiety,\nhealth, leisure, etc). An updated version was launched in 2015.\nX\nSentiment140 [22]\nSentiment140 (previously known as ”Twitter Sentiment”) was proposed as an ensemble of three classifiers (Naive\nBayes, Maximum Entropy, and SVM) built with a huge amount of tweets containing emoticons collected by the\nauthors. It has been improved and transformed into a paid tool.\nX\nSenticNet [12] Uses dimensionality reduction to infer the polarity of common sense concepts and hence provide a resource formining opinions from text at a semantic, rather than just syntactic level. X\nAFINN [45] - A new\nANEW\nBuilds a Twitter based sentiment Lexicon including Internet slangs and obscene words. AFINN can be considered as\nan expansion of ANEW [10], a dictionary created to provides emotional ratings for English words. ANEW dictionary\nrates words in terms of pleasure, arousal and dominance.\nX\nSO-CAL [61]\nCreates a new Lexicon with unigrams (verbs, adverbs, nouns and adjectives) and multi-grams (phrasal verbs and\nintensifiers) hand ranked with scale +5 (strongly positive) to -5 (strongly negative). Authors also included part of\nspeech processing, negation and intensifiers.\nX\nEmoticons DS (Distant\nSupervision)[26]\nCreates a scored lexicon based on a large dataset of tweets. Its based on the frequency each lexicon occurs with\npositive or negative emotions. X\nNRC Hashtag [39]\nBuilds a lexicon dictionary using a Distant Supervised Approach. In a nutshell it uses known hashtags (i.e #joy,\n#happy etc) to “classify” the tweet. Afterwards, it verifies frequency each specific n-gram occurs in a emotion and\ncalculates its Strong of Associaton with that emotion.\nX\nPattern.en [15] Python Programming Package (toolkit) to deal with NLP, Web Mining and Sentiment Analysis. Sentiment analysis isprovided through averaging scores from adjectives in the sentence according to a bundle lexicon of adjective. X\nSASA [69] Detects public sentiments on Twitter during the 2012 U.S. presidential election. It is based on the statistical modelobtained from the classifier Naı¨ve Bayes on unigram features. It also explores emoticons and exclamations. X\nPANAS-t [25]\nDetects mood fluctuations of users on Twitter. The method consists of an adapted version (PANAS) Positive Affect\nNegative Affect Scale [70], well-known method in psychology with a large set of words, each of them associated with\none from eleven moods such as surprise, fear, guilt, etc .\nX\nEmoLex [41]\nBuilds a general sentiment Lexicon crowdsourcing supported. Each entry lists the association of a token with 8 basic\nsentiments: joy, sadness, anger, etc defined by [51]. Proposed Lexicon includes unigrams and bigrams from\nMacquarie Thesaurus and also words from GI and Wordnet.\nX\nUSent [49] Infer additional reviews user ratings by performing sentiment analysis (SA) of user comments and integrating itsoutput in a nearest neighbor (NN) model that provides multimedia recommendations over TED Talks. X X\nSentiment140 Lexicon [42]\nA lexicon dictionary based on the same dataset used to train the Sentiment140 Method. The lexicon was built in a\nsimilar way to [39] but authors used the occurrency of emoticons to classify the tweet as positive or negative. Then,\nthe n-gram score was calculated based on the frequency of occurrence in each class of tweets.\nX\nSentiStrength [65] Builds a lexicon dictionary annotated by humans and improved with the use of Machine Learning. X X\nStanford Recursive Deep\nModel [56]\nProposes a model called Recursive Neural Tensor Network (RNTN) that processes all sentences dealing with their\nstructures and compute the interactions between them. This approach is interesting since RNTN take into account the\norder of words in a sentence, which is ignored in most of methods.\nX X\nUmigon [35] Disambiguates tweets using lexicon with heuristics to detect negations plus elongated words and hashtags evaluation. X\nANEW SUB [3]\nAnother extension of the ANEW dictionary [10] including the most common words from the SubtlexUS corpus [11].\nSubtlexUS was an effort to propose a different manner to calculate word frequencies considering film and TV\nsubtitles.\nX\nVADER [28] It is a human-validated sentiment analysis method developed for twitter and social media contexts. VADER wascreated from a generalizable, valence-based, human-curated gold standard sentiment lexicon. X\nSemantria [36]\nIt is a paid tool that employs multi-level analysis of sentences. Basically it has four levels: part of speech, assignment\nof previous scores from dictionaries, application of intensifiers and finally machine learning techniques to delivery a\nfinal weight to the sentence.\nX X\nTable 2: Overview of the sentence-level methods available in the literature.\nName Output Validation Compared To Lexiconsize\nEmoticons -1, 1 - - 79\nOpinion\nLexicon Provides polarities for lexicons\nProduct Reviews from\nAmazon and CNet - 6,787\nOpinion\nFinder\n(MPQA)\nNegative, Objective, Positive MPQA [71] Compared to itself in different versions 20,611\nSentiWordNet Provides positive, negative and objectivescores for each word (0.0 to 1.0 ) - General Inquirer (GI)[57] 117,658\nSentiment140 0, 2, 4\nTheir own datasets -\n359 tweets\n(Tweets STF, presented\nat Table 3)\nNaive Bayes, Maximum Entropy, and SVM\nclassifiers as described in [48] -\nLIWC07 negEmo, posEmo - Their previous dictionary (2001) 4,500\nSenticNet Negative, Positive Patient Opinions(Unavailable) SentiStrength [65] 15,000\nAFINN Provides polarity score for lexicons (-5 to 5). Twiter [7] OpinonFinder [72], ANEW [10], GI [57] andSentistrength [65] 2,477\nSO-CAL [<0), 0, (>0]\nEpinion [59],\nMPQA[71],\nMyspace[65],\nMPQA[71], GI[57], SentiWordNet\n[19],”Maryland”Dict [40], Google Generated Dict\n[60]\n9,928\nEmoticons DS\n(Distant\nSupervision)\nProvides polarity score for lexicons\nValidation with\nunlabeled twitter data\n[14]\n- 1,162,894\nNRC Hashtag Provides polarities for lexicons\nTwitter (SemEval-2007\nAffective Text\nCorpus) [58]\nWordNet Affect [58] 679,468\nPattern.en Objective, [<0.1, ≥0.1] Product reviews, but thesource was not specified - 2,973\nSASA [69] Negative, Neutral, Unsure, Positive\n“Political” Tweets\nlabeled by “turkers”\n(AMT) (unavailable)\n- -\nPANAS-t\nProvides association for each word with\neleven moods (joviality, attentiveness, fear,\netc )\nValidation with\nunlabeled twitter data\n[14]\n- 50\nEmoLex Provides polarities for lexicons - Compared with existing gold standard data but itwas not specified 141,820\nUSent neg, neu, pos Their own dataset - TedTalks\nComparison with other multimedia\nrecommendation approaches\nMPQA\n(8,226) /\nTheir own\n(9,176)\nSentiment140\nLexicon Provides polarity scores for lexicon\nTwitter and SMS from\nSemeval 2013, task 2\n[43]\nOther Semeval 2013, task 2 approaches 1,220,176\nSentiStrength -1,0,1\nTheir own datasets -\nTwitter, Youtube, Digg,\nMyspace, BBC Forums\nand Runners World\nThe best of nine Machine Learning techniques for\neach test 2,698\nStanford\nRecursive\nDeep Model\nvery negative, negative, neutral, positive,\nvery positive Movie Reviews [47]\nNaı¨ve Bayes and SVM with bag of words features\nand bag of bigram features 227,009\nUmigon Negative, Neutral, Positive\nTwitter and SMS from\nSemeval 2013, task 2\n[43]\n[42] 1,053\nANEW SUB\nProvides ratings for words in terms of\nValence, Arousal and Dominance. Results\ncan also be grouped by gender, age and\neducation\n-\nCompared to similar works, including\ncross-language studies, by means of correlations\nbetween emotional dimensions\n13,915\nVADER [<-0,05), (-0,05..0,05), (>0,05]\nTheir own datasets -\nTwitter, Movie Reviews,\nTechnical Product\nReviews, NYT User’s\nOpinions\n(GI)[57], LIWC, [64], SentiWordNet [19], ANEW\n[10], SenticNet [13] and some Machine Learning\nApproaches\n7,517\nLIWC15 negEmo, posEmo - Their previous dictionary (2007) 6,400\nSemantria negative, neutral, positive not available not available notavailable\nThe outputs from the remaining methods were easily adap-\nted and converted to positive, negative or neutral. SO-CAL\nand Pattern.en delivery float numbers greater than a threshold,\nindicating positive, and lesser than the threshold, indicating\nnegative. LIWC, SenticNet, SASA, USent, SentiStrength,\nUmigon, VADER and Semantria already provide fixed out-\nputs indicating one of three desired classes while Stanford\nRecursive Deep Model yields very negative and very positive\nwhich in our experiments are handled as negative and positive,\nrespectively.\nPaid Softwares\nSeven out of the twenty-four methods evaluated in this work\nare closed paid softwares: LIWC (2007 and 2015), Semantria,\nSenticNet 3.0, Sentiment140 and SentiStrength. Although\nSentiStrength is paid, it has a free of charge academic license.\nSenticNet’s authors kindly processed all datasets with the com-\nmercial version and return the polarities for us. For SentiS-\ntrenght we used the Java version from May 2013 in a package\nwith all features of the commercial version. For LIWC we ac-\nquired the licenses from 2007 (LIWC07) and 2015 (LIWC15)\nversions. Finally, for Semantria and Sentiment140 we used a\ntrial account free of charge for a limited number of sentences,\nwhich was sufficient to run our experiments.\nMethods not included\nDespite our effort to include in our comparison most of the\nhighly cited and important methods we could not include a\nfew of them for different reasons. Profile of Mood States\n(POMS-ex) [9] is not available on the Web or under request\nand could not be re-implemented based on their descriptions\nin the original papers. The same situation occurs with the\nLearning Sentiment-Specific Word Embedding for Twitter\nSentiment Classification [63]. NRC SVM [42] is not available\nas well, although the lexical resources used by the authors are\navailable and were considered in our evaluation resulting in the\nmethods: NRC Hashtag and Sentiment140. The authors of the\nConvolutional Neural Network for Modeling Sentences [31]\nand of the Effective Use of Word Order for Text Categorization\nwith Convolutional Neural Networks [30] have made their\nsource code available but the first one lacks the train files\nand the second one requires a GPU to execute. There are a\nfew other methods for sentiment detection proposed in the\nliterature and not considered here. Most of them consists of\nvariations of the techniques used by the above methods, such\nas WordNet-Affect[68] and Happiness Index [18].\nDatasets and Comparison Among Methods\nFrom Table 2 we can note that the validation strategy, the\ndatasets used, and the comparison with baselines performed\nby these methods vary greatly, from toy examples to large\nlabeled datasets. PANAS-t and Emoticons DS used manually\nunlabeled twitter data to validate their methods, by presenting\nevaluations of events in which some bias towards positivity\nand negativity would be expected. PANAS-t is tested with\nunlabeled Twitter data related to Michael Jackson’s death and\nthe release of a Harry Potter movie whereas Emoticons DS\nverified the influence of weather and time on the aggregate\nsentiment from Twitter. Lexical dictionaries were validated\nin very different ways. AFINN[45] compared its Lexicon\nwith other dictionaries. Emoticon Distance Supervised [26]\nused Pearson Correlation between human labeling and the\npredicted value. SentiWordNet [19] validates the proposed\ndictionary with comparisons with other dictionaries, but it also\nused human validation of the proposed lexicon. These efforts\nattempt to validate the created lexicon, without comparing the\nlexicon as a sentiment analysis method by itself. VADER [28]\ncompared results with lexical approaches considering labeled\ndatasets from different social media data. SenticNet [13] was\ncompared with SentiStrength [65] with a specific dataset re-\nlated to patient opinions, which could not be made available.\nStanford Recursive Deep Model [56] and SentiStrength [65]\nwere both compared with standard machine learning approa-\nches, with their own datasets.\nThis scenario, where every new developed solution compares\nitself with different solutions using different datasets, happens\nbecause there is no standard benchmark for evaluating new\nmethods. This problem is exacerbated because many methods\nhave been proposed in different research communities (e.g.\nNLP, Information Science, Information Retrieval, Machine Le-\narning), exploiting different techniques, with low knowledge\nabout related efforts in other communities. Next, we describe\nhow we created a large gold standard to properly compare all\nthe considered sentiment analysis methods.\nGold Standard Data\nA key aspect in evaluating sentiment analysis methods consists\nof using accurate gold standard labeled datasets. Several exis-\nting efforts have generated labeled data produced by experts\nor non-experts evaluators. Previous studies suggest that both\nefforts are valid as non-expert labeling may be as effective\nas annotations produced by experts for affect recognition, a\nvery related task [55]. Thus, our effort to build a large and\nrepresentative gold standard dataset consists of obtaining labe-\nled data from trustful previous efforts that cover a wide range\nof sources and kinds of data. We also attempt to assess the\n“quality” of our gold standard in terms of the accuracy of the\nlabeling process.\nTable 3 summarizes the main characteristics of the eighteen ex-\nploited datasets, such as number of messages and the average\nnumber of words per message in each dataset. It also defines\na simpler nomenclature that is used in the remainder of this\npaper. The table also presents the methodology employed in\nthe classification. Human labeling was implemented in almost\nall datasets, usually done with the use of non-expert reviewers.\nReviews I dataset relies on five stars rates, in which users\nrate and provide a comment about an entity of interest (e.g. a\nmovie or an establishment).\nLabeling based on Amazon Mechanical Turk (AMT) was used\nin seven out of the eighteen datasets, while volunteers and\nother strategies that involve non-expert evaluators were used\nin ten datasets. Usually, an agreement strategy (i.e. majority\nvoting) is applied to ensure that, in the end, each sentence\nhas an agreed-upon polarity assigned to it. The number of\nannotators used to build the datasets is also shown in Table 3.\nTable 3: Labeled datasets.\nDataset Nomeclature # # # # Average # Average # Annotators # of CK\nMsgs Pos Neg Neu of phrases of words Expertise Annotators\nComments (BBC) [65] Comments BBC 1,000 99 653 248 3.98 64.39 Non Expert 3 0.427\nComments (Digg) [65] Comments Digg 1,077 210 572 295 2.50 33.97 Non Expert 3 0.607\nComments (NYT) [28] Comments NYT 5,190 2,204 2,742 244 1.01 17.76 AMT 20 0.628\nComments (TED) [50] Comments TED 839 318 409 112 1 16.95 Non Expert 6 0.617\nComments (Youtube) [65] Comments YTB 3,407 1,665 767 975 1.78 17.68 Non Expert 3 0.724\nMovie-reviews [47] Reviews I 10,662 5,331 5,331 - 1.15 18.99 User Rating - 0.719\nMovie-reviews [28] Reviews II 10,605 5,242 5,326 37 1.12 19.33 AMT 20 0.555\nMyspace posts [65] Myspace 1,041 702 132 207 2.22 21.12 Non Expert 3 0.647\nProduct reviews [28] Amazon 3,708 2,128 1,482 98 1.03 16.59 AMT 20 0.822\nTweets (Debate) [16] Tweets DBT 3,238 730 1249 1259 1.86 14.86 AMT + Expert Undef. 0.419\nTweets (Random) [65] Tweets RND I 4,242 1,340 949 1953 1.77 15.81 Non Expert 3 0.683\nTweets (Random) [28] Tweets RND II 4,200 2,897 1,299 4 1.87 14.10 AMT 20 0.800\nTweets (Random) [44] Tweets RND III 3,771 739 488 2,536 1.54 14.32 AMT 3 0.824\nTweets (Random) [2] Tweets RND IV 500 139 119 222 1.90 15.44 Expert Undef. 0.643\nTweets (Specific domains w/ emot.) [22] Tweets STF 359 182 177 - 1.0 15.1 Non Expert Undef. 1.000\nTweets (Specific topics) [54] Tweets SAN 3737 580 654 2503 1.60 15.03 Expert 1 0.404\nTweets (Semeval2013 Task2) [43] Tweets Semeval 6,087 2,223 837 3027 1.86 20.05 AMT 5 0.617\nRunners World forum [65] RW 1,046 484 221 341 4.79 66.12 Non Expert 3 0.615\nTweets DBT was the unique dataset built with a combination\nof AMT Labeling with Expert validation [16]. They selected\n200 random tweets to be classified by experts and compared\nwith AMT results to ensure accurate ratings. We note that\nthe Tweets Semeval dataset was provided as a list of Twitter\nIDs, due to the Twitter policies related to data sharing. While\ncrawling the respective tweets, a small part of them could not\nbe accessed, as they were deleted. We plan to release all gold\nstandard datasets in a request basis, which is in agreement\nwith Twitter policies.\nIn order to assess the extent to which these datasets are trustful,\nwe used a strategy similar to the one used by Tweets DBT.\nOur goal was not to redo all the performed human evaluation,\nbut simply inspecting a small sample of them to infer the level\nof agreement with our own evaluation. We randomly select\n1% of all sentences to be evaluated by experts (two of the\nauthors) as an attempt to assess if these gold standard data are\nreally trustful. It is important to mention that we did not have\naccess to the instructions provided by the authors. We also\ncould not get access to small amount of the raw data in a few\ndatasets, which was discarded. Finally, our manual inspection\nunveiled a few sentences in idioms other than English in a few\ndatasets, such as Tweets STA and TED, which were obviously\ndiscarded.\nColumn CK from Table 3 exhibits the level of agreement of\neach dataset in our evaluation by means of Cohen’s Kappa,\nan extensively used metric to calculate inter-anotator agree-\nment. After a close look in the cases of disagreement with\nthe evaluations in the Gold standard, we realized that other\ninterpretations could be possible for the given text, finding\ncases of sentences with mixed polarity. Some of them are\nstrongly linked to original context and are very hard to evalu-\nate. Some NYT comments, for instance, are directly related to\nthe news they were inserted to. We can also note that some of\nthe datasets do not contain neutral messages. This might be\na characteristic of the data or even a result of how annotators\nwere instructed to label their pieces of text. Most of the cases\nof disagreement involve neutral messages. Thus, we conside-\nred these cases, as well as the amount of disagreement we had\nwith the gold standard data, reasonable and expected, specially\nwhen taking into account that Landis and Koch [34] suggest\nthat Kappa values between 0.4 and 0.6 indicate moderate agre-\nement and values amid 0.60 and 0.8 correspond to substantial\nagreements.\nComparison Results\nNext, we present comparison results for the twenty-four\nmethods considered in this paper based on the eighteen consi-\ndered gold standard datasets.\nExperimental details\nAt least three distinct approaches have been proposed to deal\nwith sentiment analysis of sentences. The first of them, ap-\nplied by OpinionFinder and Pattern.en, for instance, splits this\ntask into two steps: (i) identifying sentences with no senti-\nment, also named as objective vs. neutral sentences and then\n(ii) detecting the polarity (positive or negative), only for the\nsubjective sentences. Another common way to detect sentence\npolarity considers three distinct classes (positive, negative and\nneutral) in a single task, an approach used by VADER, SO-\nCAL, USent and others. Finally, some methods like SenticNet\nand LIWC, classify a sentence as positive or negative only,\nassuming that only polarized sentences are presented, given\nthe context of a given application. As an example, reviews of\nproducts are expected to contain only polarized opinion.\nAiming at providing a more thorough comparison among these\ndistinct approaches, we perform two rounds of tests. In the first\nwe consider the performance of methods to identify 3-class\n(positive, negative and neutral). The second considers only po-\nsitive and negative as output and assumes that a previous step\nof removing the neutral messages needs to be executed firstly.\nIn the 3-class experiments we used only datasets containing\na considerable number of neutral messages (which excludes\nTweets RND II, Amazon, and Reviews II). Despite being 2-\nclass methods, as highlighted in Table 2, we decided to include\nLIWC, Emoticons and SenticNet in the 3-class experiments to\npresent a full set of comparative experiments. LIWC, Emoti-\ncons, and SenticNet cannot define, for some sentences, their\npositive or negative polarity, considering it as undefined. It\noccurs due to the absence in the sentence of emoticons (in\nthe case of Emoticons method) or of words belonging to the\nmethods’ sentiment lexicon. As neutral (objective) sentences\ndo not contain sentiments, we assumed, in the case of these\n2-class methods, that sentences with undefined polarities are\nequivalent to neutral sentences.\nThe 2-class experiments, on the other hand, were performed\nwith all datasets described in Table 3 excluding the neutral\nsentences. We also included all methods in these experiments,\neven those that produce neutral outputs. As discussed before,\nwhen 2-class methods cannot detect the polarity (positive or\nnegative) of a sentences they usually assign it to an undefined\npolarity. As we know all sentences in the 2-class experiments\nare positive or negative, we create the coverage metric to\ndetermine the percentage of sentences a method can in fact\nclassify as positive or negative. For instance, suppose that\nEmoticons’ method can classify only 10% of the sentences in\na dataset, corresponding to the actual percentage of sentences\nwith emoticons. It means that the coverage of this method\nin this specific dataset is 10%. Note that, the coverage is\nquite an important metric for a more complete evaluation in\nthe 2-class experiments. Even though Emoticons presents\nhigh accuracy for the classified phrases, it was not able to\nmake a prediction for 90% of the sentences. More formally,\ncoverage is calculated as the number of total sentences minus\nthe number of undefined sentences, all of this divided by the\ntotal of sentences, where the number of undefined sentences\nincludes neutral outputs for 3-class methods.\nCoverage=\n#Sentences−#Unde f ined\n#Sentences\nComparison Metrics\nConsidering the 3-class comparison experiments, we used\nthe traditional Precision, Recall, and F1 measures for the\nautomated classification.\nPredicted\nPositive Neutral Negative\nPositive a b c\nActual Neutral d e f\nNegative g h i\nEach letter in the above table represents the number of ins-\ntances which are actually in class X and predicted as class\nY, where X;Y ∈ positive; neutral; negative. The recall (R)\nof a class X is the ratio of the number of elements correctly\nclassified as X to the number of known elements in class X .\nPrecision (P) of a class X is the ratio of the number of elements\nclassified correctly as X to the total predicted as the class X .\nFor example, the precision of the negative class is computed as:\nP(neg) = i/(c+ f + i); its recall, as: R(neg) = i/(g+h+ i);\nand the F1 measure is the harmonic mean between both preci-\nsion and recall. In this case, F1(neg) = 2P(neg)·R(neg)P(neg)+R(neg) .\nWe also compute the overall accuracy as: A =\na+e+i\na+b+c+d+e+ f+g+h+i . It considers equally important the\ncorrect classification of each sentence, independently of the\nclass, and basically measures the capability of the method\nto predict the correct output. A variation of F1, namely,\nMacro-F1, is normally reported to evaluate classification\neffectiveness on skewed datasets. Macro-F1 values are\ncomputed by first calculating F1 values for each class\nin isolation, as exemplified above for negative, and then\naveraging over all classes. Macro-F1 considers equally\nimportant the effectiveness in each class, independently of\nthe relative size of the class. Thus, accuracy and Macro-F1\nprovide complementary assessments of the classification\neffectiveness. Macro-F1 is especially important when the\nclass distribution is very skewed, to verify the capability of\nthe method to perform well in the smaller classes.\nThe described metrics can be easily computed for the 2-class\nexperiments by just removing neutral columns and rows as in\nthe table below.\nPredicted\nPositive Negative\nPositive a b\nActual Negative c d\nIn this case, the precision of positive class is computed as:\nP(pos) = a/(a+ c); its recall as: R(pos) = a/(a+b); while\nits F1 is F1(pos) = 2P(pos)·R(pos)P(pos)+R(pos)\nAs we have a large number of combinations among the base\nmethods, metrics and datasets, a global analysis of the per-\nformance of all these combinations is not an easy task. We\npropose a simple but informative measure to assess the overall\nperformance ranking. The Mean Ranking is basically the sum\nof ranks obtained by a method in each dataset divided by the\ntotal number of datasets, as below:\nMR=\nnd\n∑\nj=1\nri\nnd\nwhere nd is the number of datasets and ri is the rank of the\nmethod for dataset i. It is important to notice that the rank was\ncalculated based on Macro F1.\nThe last evaluation metric we exploit is the Friedman’s Test [6].\nIt allows one to verify whether, in a specific experiment, the\nobserved values are globally similar. We used this test to tell\nif the methods present similar performance across different\ndatasets. More specifically, suppose that k expert raters evalu-\nated n item – the question that arises is: are rates provided by\njudges consistent with each other or do they follow comple-\ntely different patterns? The application in our context is very\nsimilar: the datasets are the judges and the Macro-F1 achieved\nby a method is the rating from the judges.\nThe Friedman’s Test is applied to rankings. Then, to proceed\nwith this statistical test, we sort the methods in decreasing\norder of Macro-F1 for each dataset. More formally, the Fried-\nman’s rank test in our experiment is defined as:\nFR = (\n12\nrc(c+1)\nc\n∑\nj=1\nR2j)−3r(c+1)\nwhere\nR2j = square of the sum of rank positions of method j (j =\n1,2,..,c)\nr = number of datasets\nc = number of methods\nAs the number of datasets increases, the statistical test can be\napproximated by using the chi-square distribution with c−1\ndegrees of freedom [29]. Then, if the FR computed value is\nlarger than the critical value for the chi-square distribution the\nnull hypothesis is rejected. This null hypothesis states that\nranks obtained per dataset are globally similar. Accordingly,\nrejecting the null hypothesis means that there are significant\ndifferences in the ranks across datasets. It is important to note\nthat, in general, the critical value is obtained with significance\nlevel α = 0.05. Synthesizing, the null hypothesis should be\nrejected if FR > X2α , where X\n2\nα is the critical value verified in\nthe chi-square distribution table with c−1 degrees of freedom\nand α equals 0.05.\nComparing Prediction Performance\nWe start the analysis of our experiments by comparing the\nresults of all previously discussed metrics for all datasets. Ta-\nble 4 and Table 5 present accuracy, precision, and Macro-F1\nfor all methods considering four datasets for the 2-class and\n3-class experiments, respectively. For simplicity, we choose to\ndiscuss results only for these datasets as they come from diffe-\nrent sources and help us to illustrate the main findings from\nour analysis. Results for all the other datasets are presented in\nthe Additional File 1. There are many interesting observations\nwe can make from these results, summarized next.\nMethods prediction performance varies considerably\nfrom one dataset to another: First, we note the same so-\ncial media text can be interpreted very differently depending\non the choice of a sentiment method. Overall, we note that all\nthe methods yielded with large variations across the different\ndatasets. By analyzing Table 4 we can note that VADER works\nwell for Tweets RND II, appearing in the first place, but it\npresents poor performance in Tweets STF, Comments BBC,\nand Comments DIGG, achieving the eleventh, thirteenth and\ntenth place respectively. Although the first two datasets con-\ntain tweets, they belong to different contexts, which affects\nthe performance of some methods like VADER. Another im-\nportant aspect to be analyzed in this Table is the coverage.\nAlthough SentiStrength has presented good Macro-F1 values,\nits coverage is usually low as this method tends to classify a\nhigh number of instances as neutral. Note that some datasets\nprovided by the SentiStrength’s authors, as shown in Table 3,\nspecially the Twitter datasets, have more neutral sentences\nthan positive and negative ones. Another expected result is the\ngood Macro-F1 values obtained by Emoticons, specially in\nthe Twitter datasets. It is important to highlight that, in spite\nof achieving high accuracy and Macro-F1, the coverage of\nmany methods, such as PANAS, VADER, and SentiStrength,\nis low (e.g. below 30%) as they only infer the polarity of part\nof the input sentences.Thus, the choice of a sentiment analysis\nis highly dependent on the data and application, suggesting\nthat researchers and practitioners need to take into account\nthis tradeoff between prediction performance and coverage.\nThe same high variability regarding the methods’s prediction\nperformance can be noted for the 3-class experiments, as pre-\nsented in Table 5. Umigon, the best method in five Twitter\ndatasets, felt to the eighteenth place in the Comments NYT\ndataset. We can also note the lower Macro-F1 values for some\nmethods like Emoticons are due to the high number of senten-\nces without emoticons in the datasets. Methods like Emoticons\nDS and PANAS tend do classify only a small part of instances\nas neutral and also presented a poor performance in the 3-class\nexperiments. Methods like SenticNet and LIWC were not\noriginally developed for detecting neutral sentences and also\nachieved low values of Macro-F1. However, they also do not\nappear among the best methods in the 2-class experiments,\nwhich is the task they were originally designed for. This ob-\nservation about LIWC is not valid for the newest version, as\nLIWC15 appears among the top five methods for 2-class and\n3-class experiments (see Table 6).\nFinally, Table 7 presents the Friedman’s test results showing\nthat there are significant differences in the mean rankings\nobserved for the methods across all datasets. It statistically\nindicates that in terms of accuracy and Macro-F1 there is no\nsingle method that always achieves a consistent rank position\nfor different datasets, which is something similar to the well-\nknown “no-free lunch theorem” [74]. So, overall, before using\na sentiment analysis method in a novel dataset, it is crucial\nto test different methods in a sample of data before simply\nchoose one that is acceptable by the research community.\nTable 7: Friedman’s Test Results\n2-class experiments 3-class experiments\nFR 275.59 FR 197.52\nCritical Value 35.17 Critical Value 35.17\nReject null hypothesis Reject null hypothesis\nThis last results suggests that, even with the good insights\nprovided by this work about which methods perform better\nin each context, a preliminary investigation needs to be per-\nformed when sentiment analysis is used in a new dataset in\norder to guarantee a reasonable prediction performance. In the\ncase in which prior tests are not feasible, this benchmark pre-\nsents valuable information for researchers and companies that\nare planning to develop research and solutions on sentiment\nanalysis.\nExisting methods let space for improvements: We can note\nthat the performance of the evaluated methods are ok, but\nthere is a lot of space for improvements. For example, if\nwe look at the Macro-F1 values only for the best method on\neach dataset (See Table 4 and Table 5), we can note that the\noverall prediction performance of the methods is still low –\ni.e. Macro-F1 values are around 0.9 only for methods with\nlow coverage in the 2-class experiments and only 0.6 for the\n3-class experiment. Considering that we are looking at the\nperformance of the best methods out of 24 unsupervised tools,\nthese numbers suggest that current sentence-level sentiment\nanalysis methods still let a lot of space for improvements.\nTable 4: 2-classes experiments results with 4 datasets\nDataset Method Accur. Posit. Sentiment Negat. Sentiment MacroF1 CoverageP R F1 P R F1\nAFINN 96.37 97.66 96.94 97.30 93.75 95.19 94.47 95.88 80.77\nANEW SUB 81.36 80.52 96.38 87.74 85.44 47.64 61.17 74.45 93.35\nEmolex 86.06 89.82 89.11 89.47 78.77 80.00 79.38 84.42 63.58\nEmoticons 97.75 97.90 99.42 98.65 96.97 89.72 93.20 95.93 14.82\nEmoticons DS 71.04 70.61 99.90 82.74 95.83 5.43 10.28 46.51 99.09\nNRC Hashtag 67.37 83.76 65.43 73.47 48.17 71.69 57.62 65.55 91.94\nLIWC07 66.47 74.46 78.81 76.58 44.20 38.31 41.04 58.81 73.93\nLIWC15 96.44 97.09 98.04 97.56 94.68 92.23 93.44 95.50 77.05\nOpinion Finder 78.32 93.86 71.11 80.92 63.42 91.50 74.92 77.92 41.23\nOpinion Lexicon 93.45 97.03 93.14 95.04 86.93 94.11 90.38 92.71 70.64\nTweets PANAS-t 90.71 96.95 88.19 92.36 82.11 95.12 88.14 90.25 5.39\nRND II Pattern.en 91.76 92.94 96.19 94.54 87.86 79.06 83.23 88.88 70.85\nSASA 70.06 82.81 72.81 77.49 49.05 63.39 55.30 66.40 63.04\nSemantria 91.61 96.94 90.55 93.64 82.25 93.88 87.68 90.66 63.61\nSenticNet 73.64 90.74 68.45 78.03 55.41 84.88 67.05 72.54 82.82\nSentiment140 94.75 97.10 95.71 96.40 88.64 92.13 90.35 93.37 49.95\nSentiment140 L 78.05 88.68 78.31 83.17 61.32 77.47 68.45 75.81 93.28\nSentiStrength 96.97 98.92 96.43 97.66 93.54 98.01 95.72 96.69 34.65\nSentiWordNet 78.57 87.88 80.91 84.25 61.09 72.87 66.46 75.36 61.49\nSO-CAL 87.76 94.25 86.99 90.47 77.34 89.32 82.90 86.68 67.18\nStanford DM 60.46 94.48 44.87 60.84 44.06 94.30 60.06 60.45 88.89\nUmigon 88.63 97.73 85.92 91.45 73.64 95.17 83.03 87.24 70.83\nUSent 84.46 89.28 87.67 88.47 74.77 77.63 76.17 82.32 38.94\nVADER 99.04 99.16 99.45 99.31 98.77 98.12 98.45 98.88 94.40\nAFINN 84.42 80.62 91.49 85.71 89.66 77.04 82.87 84.29 76.88\nANEW SUB 68.05 63.08 93.18 75.23 84.62 40.74 55.00 65.11 94.15\nEmolex 79.65 76.09 88.98 82.03 85.23 69.44 76.53 79.28 62.95\nEmoticons 85.42 80.65 96.15 87.72 94.12 72.73 82.05 84.89 13.37\nEmoticons DS 51.96 51.41 100.00 67.91 100.00 2.27 4.44 36.18 99.72\nNRC Hashtag 71.30 73.05 70.93 71.98 69.51 71.70 70.59 71.28 92.20\nLIWC07 64.29 63.75 76.12 69.39 65.22 50.85 57.14 63.27 70.39\nLIWC15 89.22 84.18 97.08 90.17 96.40 81.06 88.07 89.12 74.93\nOpinion Finder 80.77 81.16 76.71 78.87 80.46 84.34 82.35 80.61 43.45\nOpinion Lexicon 86.10 83.67 91.11 87.23 89.29 80.65 84.75 85.99 72.14\nTweets PANAS-t 94.12 88.89 100.00 94.12 100.00 88.89 94.12 94.12 4.74\nSTF Pattern.en 79.55 74.86 94.48 83.54 90.12 61.34 73.00 78.27 73.54\nSASA 68.52 65.65 78.90 71.67 72.94 57.94 64.58 68.12 60.17\nSemantria 88.45 89.15 88.46 88.80 87.70 88.43 88.07 88.43 69.92\nSenticNet 70.49 71.31 63.50 67.18 69.88 76.82 73.19 70.18 80.22\nSentiment140 93.29 91.36 94.87 93.08 95.18 91.86 93.49 93.29 45.68\nSentiment140 L 79.12 81.48 76.30 78.81 76.97 82.04 79.42 79.11 94.71\nSentiStrength 95.33 95.18 96.34 95.76 95.52 94.12 94.81 95.29 41.78\nSentiWordNet 72.99 73.17 78.95 75.95 72.73 65.98 69.19 72.57 58.77\nSO-CAL 87.36 82.89 93.33 87.80 92.80 81.69 86.89 87.35 77.16\nStanford DM 66.56 87.69 36.31 51.35 61.24 95.18 74.53 62.94 89.97\nUmigon 86.99 91.73 81.88 86.52 83.02 92.31 87.42 86.97 81.34\nUSent 73.21 69.35 82.69 75.44 78.82 63.81 70.53 72.98 58.22\nVADER 84.44 80.23 92.21 85.80 90.40 76.35 82.78 84.29 84.12\nAFINN 70.94 47.01 81.82 59.72 91.17 67.05 77.27 68.49 74.81\nANEW SUB 43.25 30.98 92.31 46.39 90.13 25.46 39.71 43.05 93.73\nEmolex 61.71 34.60 75.83 47.52 88.93 57.53 69.87 58.69 67.14\nEmoticons 73.08 72.22 86.67 78.79 75.00 54.55 63.16 70.97 3.32\nEmoticons DS 28.24 27.30 100.00 42.89 100.00 1.77 3.48 23.19 98.72\nNRC Hashtag 74.69 51.01 40.64 45.24 80.80 86.48 83.54 64.39 92.97\nLIWC07 46.15 27.44 58.40 37.34 72.49 41.52 52.79 45.07 58.18\nLIWC15 70.67 49.81 90.91 64.36 94.35 62.36 75.09 69.72 62.79\nOpinion Finder 71.14 43.04 64.76 51.71 86.88 73.13 79.42 65.56 56.27\nOpinion Lexicon 71.82 47.45 86.43 61.27 93.40 66.75 77.86 69.56 69.44\nComments PANAS-t 68.00 12.50 50.00 20.00 94.12 69.57 80.00 50.00 3.20\nDigg Pattern.en 60.05 43.73 92.14 59.31 92.57 45.21 60.75 60.03 56.65\nSASA 65.54 40.26 66.91 50.27 84.82 65.06 73.64 61.95 68.29\nSemantria 82.46 62.72 88.33 73.36 94.81 80.25 86.93 80.14 56.14\nSenticNet 69.40 46.30 72.46 56.50 86.77 68.25 76.40 66.45 96.55\nSentiment140 85.06 62.50 78.95 69.77 93.65 86.76 90.08 79.92 33.38\nSentiment140 L 67.76 42.07 73.45 53.50 88.01 65.84 75.33 64.41 89.64\nSentiStrength 92.09 78.69 92.31 84.96 97.40 92.02 94.64 89.80 27.49\nSentiWordNet 62.17 36.86 77.68 50.00 88.84 57.18 69.58 59.79 58.82\nSO-CAL 76.55 52.86 77.08 62.71 90.65 76.37 82.90 72.81 71.99\nStanford DM 69.16 35.29 20.27 25.75 75.21 86.68 80.54 53.15 78.90\nUmigon 83.37 66.22 75.38 70.50 90.72 86.23 88.42 79.46 63.04\nUSent 55.98 36.06 80.65 49.83 86.67 46.80 60.78 55.31 43.86\nVADER 69.05 45.48 85.88 59.47 92.55 63.00 74.97 67.22 82.23\nAFINN 66.56 23.08 81.08 35.93 96.32 64.66 77.38 56.65 85.11\nANEW SUB 31.37 15.48 95.79 26.65 97.18 21.73 35.52 31.08 97.07\nEmolex 59.64 21.52 89.04 34.67 97.38 55.62 70.80 52.73 80.72\nEmoticons 33.33 0.00 0.00 0.00 100.00 33.33 50.00 25.00 0.40\nEmoticons DS 13.33 13.10 100.00 23.17 100.00 0.31 0.61 11.89 99.73\nNRC Hashtag 84.45 33.33 25.27 28.75 89.76 92.83 91.27 60.01 97.47\nLIWC07 50.10 15.38 58.33 24.35 88.00 48.78 62.77 43.56 69.55\nLIWC15 63.21 25.86 90.67 40.24 97.55 58.86 73.42 56.83 73.01\nOpinion Finder 74.43 21.74 62.50 32.26 94.93 75.72 84.24 58.25 76.46\nOpinion Lexicon 74.14 29.81 84.93 44.13 97.24 72.66 83.17 63.65 80.72\nComments PANAS-t 58.73 20.00 75.00 31.58 93.94 56.36 70.45 51.02 8.38\nBBC Pattern.en 41.75 19.73 93.55 32.58 96.61 32.57 48.72 40.65 54.79\nSASA 61.61 23.50 66.20 34.69 90.80 60.77 72.81 53.75 61.30\nSemantria 83.43 40.00 84.75 54.35 97.64 83.26 89.88 72.11 67.42\nSenticNet 66.07 24.44 74.16 36.77 94.24 64.83 76.81 56.79 88.96\nSentiment140 68.51 24.00 69.77 35.71 94.04 68.33 79.15 57.43 45.61\nSentiment140 L 56.85 18.52 69.15 29.21 92.35 55.03 68.97 49.09 97.07\nSentiStrength 93.93 64.29 78.26 70.59 97.72 95.54 96.61 83.60 32.85\nSentiWordNet 57.49 20.00 88.06 32.60 97.13 53.45 68.96 50.78 76.33\nSO-CAL 75.28 28.93 80.28 42.54 96.71 74.64 84.25 63.40 82.85\nStanford DM 89.45 63.16 40.91 49.66 91.81 96.52 94.11 71.88 92.02\nUmigon 79.37 39.13 61.02 47.68 92.10 82.72 87.15 67.42 50.93\nUSent 52.60 18.33 80.49 29.86 94.56 48.60 64.20 47.03 43.48\nVADER 62.76 22.68 85.54 35.86 96.75 59.60 73.76 54.81 90.69\nTable 5: 3-classes experiments results with 4 datasets\nDataset Method Accur. Posit. Sentiment Negat. Sentiment Neut. Sentiment MacroF1P R F1 P R F1 P R F1\nAFINN 62.36 61.10 70.09 65.28 44.08 55.56 49.15 71.43 58.57 64.37 59.60\nANEW SUB 39.51 38.79 96.31 55.31 43.50 23.18 30.24 57.38 2.31 4.45 30.00\nEmolex 48.74 48.15 62.71 54.47 31.27 38.59 34.55 57.90 41.30 48.21 45.74\nEmoticons 52.88 72.83 11.34 19.62 55.56 5.38 9.80 34.05 96.53 50.34 26.59\nEmoticons DS 36.59 36.55 100.00 53.53 75.00 0.36 0.71 100.00 0.03 0.07 18.10\nNRC Hashtag 36.95 42.04 75.03 53.88 24.57 56.03 34.16 53.33 3.70 6.92 31.65\nLIWC07 39.54 36.52 42.33 39.21 15.14 13.02 14.00 48.64 44.83 46.66 33.29\nLIWC15 62.56 59.77 71.03 64.91 49.04 42.65 45.62 68.90 61.84 65.18 58.57\nOpinion Finder 57.63 67.57 27.94 39.53 40.75 33.69 36.89 58.20 86.06 69.44 48.62\nOpinion Lexicon 60.37 62.09 62.71 62.40 41.19 52.81 46.28 66.41 60.75 63.46 57.38\nTweets PANAS-t 53.08 90.95 9.04 16.45 51.56 3.94 7.33 51.65 99.01 67.89 30.55\nSemeval Pattern.en 57.99 57.97 68.74 62.89 34.83 35.24 35.04 65.55 56.39 60.63 52.85\nSASA 50.63 46.34 47.77 47.04 33.07 20.31 25.17 56.39 61.12 58.66 43.62\nSemantria 61.54 67.28 57.35 61.92 39.57 52.81 45.24 65.98 67.03 66.50 57.89\nSenticNet 49.68 51.85 1.26 2.46 29.79 1.67 3.17 49.82 98.51 66.17 23.93\nSentiment140 60.42 63.87 51.37 56.94 50.96 37.87 43.45 60.35 73.31 66.20 55.53\nSentiment140 L 39.44 43.52 74.72 55.00 27.67 65.35 38.88 65.87 6.38 11.63 35.17\nSentiStrength 57.83 78.01 27.13 40.25 47.80 23.42 31.44 55.49 89.89 68.62 46.77\nSentiWordNet 48.33 55.54 53.44 54.47 19.67 37.51 25.81 61.22 47.57 53.54 44.61\nSO-CAL 58.83 58.89 59.02 58.95 40.39 54.24 46.30 39.89 59.96 47.91 51.05\nStanford DM 22.54 72.14 18.17 29.03 14.92 90.56 25.61 47.19 6.94 12.10 22.25\nUmigon 65.88 75.18 56.14 64.28 39.66 55.91 46.41 70.65 75.78 73.13 61.27\nUSent 52.13 49.86 32.88 39.63 39.96 22.82 29.05 54.33 74.36 62.79 43.82\ns VADER 60.21 56.46 79.04 65.87 44.30 59.02 50.61 76.02 46.71 57.87 58.12\nAFINN 64.41 40.81 72.12 52.13 49.67 62.50 55.35 85.95 62.54 72.40 59.96\nANEW SUB 28.03 21.89 92.29 35.38 44.30 34.22 38.61 74.82 8.18 14.74 29.58\nEmolex 54.76 31.67 59.95 41.44 40.14 47.54 43.53 77.48 54.64 64.08 49.68\nEmoticons 70.22 70.06 16.78 27.07 65.62 8.61 15.22 41.29 97.56 58.02 33.44\nEmoticons DS 20.34 19.78 99.46 33.00 62.07 3.69 6.96 53.85 0.55 1.09 13.68\nNRC Hashtag 30.47 28.25 77.40 41.39 24.18 72.54 36.27 79.08 8.77 15.78 31.15\nLIWC 46.88 21.85 38.43 27.86 19.18 18.24 18.70 69.51 54.83 61.31 35.95\nLIWC15 67.75 44.78 78.35 56.99 57.49 57.38 57.44 85.18 66.67 74.80 63.07\nOpinion Finder 71.55 57.48 32.75 41.72 49.85 34.63 40.87 75.95 89.90 82.34 54.98\nOpinion Lexicon 63.86 40.65 66.17 50.36 48.84 56.15 52.24 81.96 64.66 72.29 58.30\nTweets PANAS-t 68.79 79.49 8.39 15.18 48.57 3.48 6.50 68.75 98.86 81.10 34.26\nRND III Pattern.en 59.56 36.20 77.00 49.24 52.87 45.29 48.79 81.75 57.23 67.33 55.12\nSASA 55.37 29.42 54.53 38.22 42.46 47.34 44.77 78.30 57.15 66.08 49.69\nSemantria 68.89 48.86 63.73 55.31 49.82 55.53 52.52 82.02 72.96 77.22 61.68\nSenticNet 29.97 31.08 74.83 43.92 20.98 73.98 32.68 79.70 8.49 15.35 30.65\nSentiment140 76.40 64.42 51.69 57.36 74.75 45.49 56.56 79.04 89.50 83.94 65.95\nSentiment140 L 31.32 25.83 77.13 38.70 30.05 78.69 43.49 79.37 8.92 16.04 32.74\nSentiStrength 73.80 70.94 41.95 52.72 57.53 25.82 35.64 75.35 92.26 82.95 57.10\nSentiWordNet 55.85 37.42 58.19 45.55 24.04 35.86 28.78 79.25 59.00 67.64 47.33\nSO-CAL 66.51 43.06 68.88 52.99 51.84 60.66 55.90 45.77 66.94 54.37 54.42\nStanford DM 31.90 64.48 38.57 48.26 15.58 85.04 26.33 75.64 19.77 31.35 35.32\nUmigon 74.12 57.67 70.23 63.33 48.83 68.44 57.00 88.80 76.34 82.10 67.47\nUSent 66.06 40.60 36.81 38.61 44.87 28.69 35.00 74.54 81.72 77.97 50.53\nVADER 60.14 37.69 81.60 51.56 48.56 65.57 55.80 88.96 52.87 66.32 57.89\nAFINN 50.10 16.22 60.61 25.59 82.62 56.05 66.79 40.11 30.24 34.48 42.29\nANEW SUB 24.30 11.38 91.92 20.24 84.15 21.13 33.78 38.89 5.65 9.86 21.30\nEmolex 44.10 15.51 65.66 25.10 83.19 45.48 58.81 35.27 31.85 33.47 39.13\nEmoticons 24.60 0.00 0.00 0.00 33.33 0.15 0.30 19.77 98.79 32.95 11.09\nEmoticons DS 10.00 9.85 98.99 17.92 66.67 0.31 0.61 0.00 0.00 0.00 6.18\nNRC Hashtag 64.00 20.72 23.23 21.90 70.20 91.27 79.36 52.50 8.47 14.58 38.62\nLIWC07 33.00 11.11 42.42 17.61 67.69 33.69 44.99 22.90 27.42 24.95 29.18\nLIWC15 43.70 17.94 68.69 28.45 85.06 42.73 56.88 30.72 36.29 33.27 39.53\nOpinion Finder 51.80 14.96 35.35 21.02 78.76 60.18 68.23 33.71 36.29 34.95 41.40\nOpinion Lexicon 55.00 20.67 62.63 31.08 85.27 59.42 70.04 40.82 40.32 40.57 47.23\nComments PANAS-t 27.10 16.67 6.06 8.89 75.61 4.75 8.93 25.35 94.35 39.97 19.26\nBBC Pattern.en 28.70 14.25 58.59 22.92 82.61 17.46 28.82 25.27 46.37 32.72 28.16\nSASA 38.20 17.03 47.47 25.07 70.75 36.29 47.98 25.19 39.52 30.77 34.60\nSemantria 56.00 28.90 50.51 36.76 83.82 57.12 67.94 35.86 55.24 43.49 49.40\nSenticNet 47.10 17.74 66.67 28.03 72.87 57.58 64.33 25.89 11.69 16.11 36.16\nSentiment140 40.00 17.75 30.30 22.39 79.77 31.39 45.05 28.75 66.53 40.15 35.86\nSentiment140 L 43.10 13.32 65.66 22.15 73.84 53.60 62.11 42.11 6.45 11.19 31.82\nSentiStrength 44.20 47.37 18.18 26.28 86.64 32.77 47.56 29.37 84.68 43.61 39.15\nSentiWordNet 42.40 14.90 59.60 23.84 81.63 41.50 55.03 34.56 37.90 36.15 38.34\nSO-CAL 55.50 20.88 57.58 30.65 80.47 63.09 70.73 28.57 34.68 31.33 44.23\nStanford DM 65.50 43.37 36.36 39.56 71.01 89.28 79.10 37.50 14.52 20.93 46.53\nUmigon 45.70 28.35 36.36 31.86 76.35 41.04 53.39 29.31 61.69 39.74 41.66\nUSent 33.80 13.75 33.33 19.47 82.25 21.29 33.82 28.09 66.94 39.57 30.95\nVADER 49.40 16.36 71.72 26.64 83.02 54.67 65.93 48.53 26.61 34.38 42.31\nAFINN 42.45 64.81 41.79 50.81 80.29 39.82 53.24 7.89 77.87 14.32 39.46\nANEW SUB 51.12 48.35 88.57 62.55 79.65 24.69 37.69 7.92 9.84 8.78 36.34\nEmolex 42.97 55.12 53.72 54.41 75.35 33.33 46.22 7.22 54.10 12.74 37.79\nEmoticons 4.68 0.00 0.00 0.00 0.00 0.00 0.00 4.47 99.59 8.56 2.85\nEmoticons DS 42.58 42.55 99.77 59.66 78.57 0.40 0.80 0.00 0.00 0.00 20.15\nNRC Hashtag 54.84 55.38 45.74 50.10 61.55 65.68 63.55 8.33 15.16 10.76 41.47\nLIWC07 24.35 42.88 27.72 33.67 53.42 19.07 28.11 4.67 53.28 8.58 23.45\nLIWC15 36.49 65.29 40.29 49.83 81.50 29.25 43.05 7.17 83.61 13.20 35.36\nOpinion Finder 29.38 68.77 18.78 29.51 76.52 32.68 45.80 6.29 88.11 11.75 29.02\nOpinion Lexicon 44.57 65.95 43.15 52.17 79.81 43.11 55.98 7.94 73.77 14.34 40.83\nComments PANAS-t 5.88 69.23 1.23 2.41 62.07 1.31 2.57 4.75 99.18 9.07 4.68\nNYT Pattern.en 31.60 55.23 45.05 49.63 72.80 17.76 28.55 5.88 65.57 10.79 29.66\nSASA 30.04 49.92 30.13 37.58 59.11 27.21 37.26 5.74 61.07 10.49 28.44\nSemantria 44.59 70.60 41.83 52.54 80.54 44.24 57.11 7.53 73.36 13.65 41.10\nSenticNet 61.85 58.19 59.48 58.83 65.01 69.26 67.07 0.00 0.00 0.00 41.97\nSentiment140 13.58 77.32 6.81 12.51 75.40 11.96 20.65 4.98 93.03 9.45 14.20\nSentiment140 L 54.61 54.72 59.12 56.84 67.00 54.41 60.05 6.70 15.98 9.44 42.11\nSentiStrength 18.17 78.51 8.62 15.54 81.12 18.96 30.74 5.41 95.49 10.24 18.84\nSentiWordNet 32.20 57.35 34.53 43.10 70.31 26.95 38.97 6.08 70.08 11.19 31.09\nSO-CAL 50.79 64.36 51.13 56.99 77.25 49.16 60.08 8.68 65.98 15.34 44.14\nStanford DM 51.93 73.39 21.14 32.83 59.48 77.90 67.46 9.65 38.11 15.40 38.56\nUmigon 24.08 68.76 16.38 26.46 68.78 24.51 36.14 5.88 88.93 11.04 24.54\nUSent 27.44 56.61 28.95 38.31 77.69 21.59 33.79 5.88 79.51 10.94 27.68\nVADER 48.03 62.67 51.63 56.62 79.91 43.07 55.97 9.18 71.31 16.26 42.95\nTable 6: Mean Rank Table for All Datasets\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 VADER 4.00(4.17) 1 SentiStrength 2.33(3.00) 29.30(28.91)\n2 LIWC15 4.62 2 Sentiment140 3.44 39.29\n3 AFINN 4.69 3 Semantria 4.61 62.34\n4 Opinion Lexicon 5.00 4 Opinion Lexicon 6.72 69.50\n5 Semantria 5.31 5 LIWC15 7.33 68.28\n6 Umigon 5.77 6 SO-CAL 7.61 72.64\n7 SO-CAL 7.23 7 AFINN 8.11 73.05\n8 Pattern.en 9.92 8 VADER 9.17(9.79) 82.20(83.18)\n9 Sentiment140 10.92 9 Umigon 9.39 64.11\n10 Emolex 11.38 10 PANAS-t 10.17 5.10\n11 Opinion Finder 13.08 11 Emoticons 10.39 10.69\n12 SentiWordNet 13.38 12 Pattern.en 12.61 65.02\n13 Sentiment140 L 13.54 13 SenticNet 13.61 84.00\n14 SenticNet 13.62 14 Emolex 14.50 66.12\n15 SentiStrength 13.69(13.71) 15 Opinion Finder 14.72 46.63\n16 SASA 14.77 16 USent 14.89 44.00\n17 Stanford DM 15.85 17 Sentiment140 L 14.94 93.36\n18 USent 15.92 18 NRC Hashtag 17.17 93.52\n19 NRC Hashtag 16.31 19 Stanford DM 17.39 87.32\n20 LIWC 16.46 20 SentiWordNet 17.50 61.77\n21 ANEW SUB 18.54 21 SASA 18.94 60.12\n22 Emoticons 21.00 22 LIWC 19.67 61.82\n23 PANAS-t 21.77 23 ANEW SUB 21.17 94.20\n24 Emoticons DS 23.23 24 Emoticons DS 23.61 99.36\nAdditionally, we also noted that the best method for each\ndataset varies considerably from one dataset to another. This\nmight indicate that each method complements the others in\ndifferent ways.\nMost methods are better to classify positive than negative\nor neutral sentences: Figure 2 presents the average F1 Score\nfor the 3-class experiments. It is easier to notice that twelve out\nof twenty-four methods are more accurate while classifying\npositive than negative or neutral messages, suggesting that\nsome methods may be more biased towards positivity. Neutral\nmessages showed to be even harder to detect by most methods.\nInterestingly, recent efforts show that human language have\na universal positivity bias ([21] and [17]). Naturally, part\nof the bias is observed in sentiment prediction, an intrinsic\nproperty of some methods due to the way they are designed.\nFor instance, [26] developed a lexicon in which positive and\nnegative values are associated to words, hashtags, and any sort\nof tokens according to the frequency with which these tokens\nappear in tweets containing positive and negative emoticons.\nThis method showed to be biased towards positivity due to the\nlarger amount of positivity in the data they used to build the\nlexicon. The overall poor performance of this specific method\nis credited to its lack of treatment of neutral messages and the\nfocus on Twitter messages.\nSome methods are consistently among the best ones:\nTable 6 presents the mean rank value, detailed before, for 2-\nclass and 3-class experiments. The elements are sorted by the\noverall mean rank each method achieved based on Macro-F1\nfor all datasets. The top nine methods based on Macro-F1\nfor the 2-class experiments are: SentiStrength, Sentiment140,\nSemantria, OpinionLexicon, LIWC15, SO-CAL, AFINN and\nVADER and Umigon. With the exception of Sentistrength,\nreplaced by Pattern.en, the other eight methods produce the\nbest results across several datasets for both, 2 and 3-class tasks.\nThese methods would be preferable in situations in which any\nsort of preliminary evaluation is not possible to be done. The\nmean rank for 2-class experiments is accompanied by the cove-\nrage metric, which is very important to avoid misinterpretation\nof the results. Observe that SentiStrength and Sentiment140\nexhibited the best mean ranks for these experiments, however\nboth present very low coverage, around 30% and 40%, a very\npoor result compared with Semantria and OpinionLexicon\nthat achieved a worse mean rank (4,61 and 6,62 respectively)\nbut an expressive better coverage, above 60%. Note also that\nSentiStrength and Sentiment140 present poor results in the\n3-class experiments which can be explained by their bias to\nthe neutral class as mentioned before.\nAnother interesting finding is the fact that VADER, the best\nmethod in the 3-class experiments, did not achieve the first\nposition for none of the datasets. It reachs the second place\nfive times, the third place twice, the seventh three times, and\nthe fourth, sixth and fifth just once. It was a special case of\nconsistency across all datasets. Tables 8 and 8 present the best\nmethod for each dataset in the 2-class and 3-class experiments,\nrespectively.\nMethods are often better in the datasets they were origi-\nnally evaluated: We also note those methods perform better\nin datasets in which they were originally validated, which is\nsomewhat expected due to fine tuning procedures. We could\ndo this comparison only for SentiStrength and VADER, which\nkindly allowed the entire reproducibility of their work, sharing\nboth methods and datasets. To understand this difference, we\ncalculated the mean rank for these methods without their ‘ori-\nTable 8: Best Method for each Dataset - 2-class experiments\nDataset Method F1-Pos F1-Neg Macro-F1 Coverage\nComments BBC SentiStrength 70.59 96.61 83.60 32.85\nComments Digg SentiStrength 84.96 94.64 89.80 27.49\nComments NYT SentiStrength 70.11 86.52 78.32 17.63\nComments TED Emoticons 85.71 94.12 89.92 1.65\nComments YTB SentiStrength 96.94 89.62 93.28 38.24\nReviews I SenticNet 97.39 93.66 95.52 69.41\nReviews II SenticNet 94.15 93.87 94.01 94.25\nMyspace SentiStrength 98.73 88.46 93.6 31.53\nAmazon SentiStrength 93.85 79.38 86.62 19.58\nTweets DBT Sentiment140 72.86 83.55 78.2 18.75\nTweets RND I SentiStrength 95.28 90.6 92.94 27.13\nTweets RND II VADER 99.31 98.45 98.88 94.4\nTweets RND III Sentiment140 97.57 95.9 96.73 50.77\nTweets RND IV Emoticons 94.74 86.76 88.6 58.27\nTweets STF SentiStrength 95.76 94.81 95.29 41.78\nTweets SAN SentiStrength 90.23 88.59 89.41 29.61\nTweets Semeval SentiStrength 93.93 83.4 88.66 28.66\nRW SentiStrength 90.04 75.79 82.92 23.12\nTable 9: Best Method for each Dataset - 3-class experiments\nDataset Method F1-Pos F1-Neg F1-Neu Macro-F1\nComments BBC Semantria 36.76 67.94 43.49 49.40\nComments Digg Umigon 49.62 62.04 44.27 51.98\nComments NYT SO-CAL 56.99 60.08 15.34 44.14\nComments TED Opinion Lexicon 64.95 56.59 30.77 50.77\nComments YTB LIWC15 73.68 49.72 48.79 57.4\nMyspace LIWC15 78.83 41.74 43.76 54.78\nTweets DBT Opinion Lexicon 43.44 47.71 48.84 46.66\nTweets RND I Umigon 60.53 51.39 65.22 59.05\nTweets RND III Umigon 63.33 57.00 82.10 67.47\nTweets RND IV Umigon 75.86 76.33 71.54 74.58\nTweets SAN Umigon 44.16 45.95 70.45 53.52\nTweets Semeval Umigon 64.28 46.41 73.13 61.27\nRW Sentiment140 62.24 51.17 42.66 52.02\nFigure 2: Average F1 Score for each class.\nginal’ datasets and put the results in parenthesis. Note that, in\nsome cases the rank order changes towards a lower value but it\ndoes not imply in major changes. We also note those methods\noften perform better in datasets in which they were originally\nvalidated, which is somewhat expected due to fine tuning pro-\ncedures. We could do this comparison only for SentiStrength\nand VADER, which kindly allowed the entire reproducibility\nof their work, sharing both methods and datasets. To unders-\ntand this difference, we calculated the mean rank for these\nmethods without their ‘original’ datasets and put the results in\nparenthesis. Note that, in some cases the rank order slightly\nchanges but it does not imply in major changes. Overall, these\nobservations suggest that initiatives like SemEval are key for\nthe development of the area, as they allow methods to compete\nin a contest for a specific dataset. More important, it highlight\nthat a standard sentiment analysis benchmark is needed and\nit needs to be constantly updated. We also emphasize that is\npossible that other methods, such as paid softwares, make use\nof some of the datasets used in this benchmark to improve\ntheir performance as most of gold standard used in this work\nis available in the Web or under request to authors.\nSome methods showed to be better for specific contexts:\nIn order to better understand the prediction performance of\nmethods in types of data, we divided all datasets in three\nspecific contexts – Social Networks, Comments, and Reviews\n– and calculated mean rank of the methods for each of them.\nTable 10 presents the contexts and the respective datasets.\nTables 11, 12 and 13 present the mean rank for each context\nseparately. In the context of Social Networks the best method\nfor 3-class experiments was Umigon, followed by LIWC15\nand VADER. In the case of 2-class the winner was SentiS-\ntrength with a coverage around 30% and the third and sixth\nplace were Emoticons and Panas-t with about 18% and 6%\nof coverage, respectively. This highlights the importance to\nTable 10: Contexts’ Groups\nContext Groups\nSocial Networks\nMyspace, Tweets DBT,\nTweets RND I, Tweets RND II,\nTweets RND III, Tweets RND IV,\nTweets STF, Tweets SAN,\nTweets Semeval\nComments\nComments BBC, Comments DIGG,\nComments NYT, Comments TED,\nComments YTB, RW\nReviews Reviews I, Reviews I, Amazon\nanalyze the 2-class results together with the coverage. Ove-\nrall, when there is an emoticon on the text or a word from the\npsychometric scale PANAS, these methods are able to tell the\npolarity of the sentences, but they are not able to identify the\npolarity of the input text for the large majority of the input\ntext. Recent efforts suggest these properties are useful for\ncombination of methods [24]. Sentiment140, LIWC15, Se-\nmantria, OpinionLexicon and Umigon showed to be the best\nalternatives for detecting only positive and negative polarities\nin social network data due to the high coverage and prediction\nperformance. It is important to highlight that LIWC 2007 ap-\npears on the 16th and 21th position for the 3-class and 2-class\nmean rank results for the social network datasets and it is a\nvery popular method in this community. On the other side,\nthe newest version of LIWC (2015) presented a considerable\nevolution obtaining the second and the fourth place in the same\ndatasets.\nSimilar analyses can be performed for the contexts Comments\nand Reviews. Sentistregth, VADER, Semantria, AFINN, and\nOpinion Lexicon showed to be the best alternatives for 2-class\nTable 11: Mean Rank Table for Datasets of Social Networks\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 Umigon 2.57 1 SentiStrength 2.22(2.57) 31.54(32.18)\n2 LIWC15 3.29 2 Sentiment140 3.00 46.98\n3 VADER 4.57(4.57) 3 Emoticons 5.11 18.04\n4 AFINN 5.00 4 LIWC15 5.67 71.73\n5 Opinion Lexicon 5.57 5 Semantria 5.89 61.98\n6 Semantria 6.00 6 PANAS-t 6.33 5.87\n7 Sentiment140 7.00 7 Opinion Lexicon 7.56 66.56\n8 Pattern.en 7.57 8 Umigon 8.00 71.67\n9 SO-CAL 9.00 9 AFINN 8.67 73.37\n10 Emolex 12.29 10 SO-CAL 8.78 67.81\n11 SentiStrength 12.43(11.60) 11 VADER 8.78(9.75) 83.29(81.90)\n12 Opinion Finder 13.00 12 Pattern.en 11.22 69.47\n13 SentiWordNet 13.57 13 Sentiment140 L 14.00 94.61\n14 SenticNet 14.14 14 Opinion Finder 14.33 39.58\n15 SASA 14.86 15 Emolex 14.56 62.63\n16 LIWC 15.43 16 USent 15.22 38.60\n17 Sentiment140 L 15.43 17 SenticNet 17.22 75.46\n18 USent 16.00 18 SentiWordNet 18.44 61.41\n19 ANEW SUB 19.14 19 NRC Hashtag 19.11 94.20\n20 Emoticons 19.14 20 SASA 19.44 58.57\n21 Stanford DM 19.43 21 LIWC 19.56 61.24\n22 NRC Hashtag 20.00 22 ANEW SUB 20.56 93.51\n23 PANAS-t 20.86 23 Stanford DM 22.56 89.06\n24 Emoticons DS 23.71 24 Emoticons DS 23.78 99.28\nTable 12: Mean Rank Table for Datasets of Comments\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 VADER 3.33(3.60) 1 SentiStrength 1.17(1.50) 28.29(24.02)\n2 AFINN 4.33 2 Semantria 2.83 61.02\n3 Opinion Lexicon 4.33 3 Sentiment140 4.17 36.49\n4 Semantria 4.50 4 Opinion Lexicon 6.50 71.59\n5 SO-CAL 5.17 5 LIWC15 6.67 65.80\n6 LIWC15 6.17 6 AFINN 7.00 74.21\n7 Umigon 9.50 7 SO-CAL 7.50 74.59\n8 Emolex 10.33 8 VADER 9.50(9.60) 81.98(85.34)\n9 Sentiment140 L 11.33 9 Umigon 10.50 57.87\n10 Stanford DM 11.67 10 Emoticons 11.83 4.99\n11 NRC Hashtag 12.00 11 Opinion Finder 13.00 55.66\n12 Pattern.en 12.67 12 SenticNet 13.00 95.28\n13 SenticNet 13.00 13 USent 14.00 45.66\n14 Opinion Finder 13.17 14 NRC Hashtag 14.67 93.43\n15 SentiWordNet 13.17 15 Emolex 15.00 69.69\n16 SASA 14.67 16 PANAS-t 15.50 5.10\n17 SentiStrength 15.17(19.00) 17 Stanford DM 15.67 84.43\n18 Sentiment140 15.50 18 Pattern.en 15.83 59.00\n19 USent 15.83 19 Sentiment140 L 15.83 92.30\n20 LIWC 17.67 20 SentiWordNet 17.00 63.32\n21 ANEW SUB 17.83 21 SASA 17.50 61.91\n22 Emoticons DS 22.67 22 LIWC 19.67 62.24\n23 PANAS-t 22.83 23 ANEW SUB 22.00 94.31\n24 Emoticons 23.17 24 Emoticons DS 23.67 99.31\nTable 13: Mean Rank Table for Datasets of Reviews\n3-Classes 2-Classes\nPos Method Mean Rank Pos Method Mean Rank Coverage (%)\n1 - - 1 Sentiment140 3.33 21.82\n2 - - 2 SenticNet 4.00 87.05\n3 - - 3 Semantria 4.33 66.04\n4 - - 4 SO-CAL 4.33 83.20\n5 - - 5 Opinion Lexicon 4.67 74.14\n6 - - 6 SentiStrength 5.00(5.00) 24.56(24.56)\n7 - - 7 Stanford DM 5.33 87.89\n8 - - 8 AFINN 8.67 69.77\n9 - - 9 VADER 9.67(11.00) 79.39(82.70)\n10 - - 10 Pattern.en 10.33 63.70\n11 - - 11 PANAS-t 11.00 2.80\n12 - - 12 Umigon 11.33 53.90\n13 - - 13 Emolex 13.33 69.47\n14 - - 14 LIWC15 13.67 62.90\n15 - - 15 USent 15.67 56.85\n16 - - 16 SentiWordNet 15.67 59.73\n17 - - 17 Sentiment140 L 16.00 91.71\n18 - - 18 NRC Hashtag 16.33 91.64\n19 - - 19 Opinion Finder 19.33 49.73\n20 - - 20 LIWC 20.00 62.75\n21 - - 21 SASA 20.33 61.22\n22 - - 22 ANEW SUB 21.33 96.05\n23 - - 23 Emoticons DS 23.00 99.71\n24 - - 24 Emoticons 23.33 0.04\nand 3-class experiments on datasets of comments whereas\nSentiment140, SenticNet, Semantria and SO-CAL showed\nto be the best for the 2-class experiments for the datasets\ncontaining short reviews. Note that for the last one, the 3-\nclass experiments have no results since datasets containing\nreviews have no neutral sentences nor a representative number\nof sentences without subjectivity.\nWe also calculated the Friedman’s value for each of these speci-\nfic contexts. Even after grouping the datasets, we still observe\nthat there are significant differences in the observed ranks\nacross the datasets. Although the values obtained for each\ncontext were quite smaller than Friedman’ global value, they\nare still above the critical value. Table 14 presents the results\nof Friedman’s test for the individual contexts in both experi-\nments, 2 and 3-class. Recall that for the 3-class experiments,\ndatasets with no neutral sentences or with an unrepresentative\nnumber of neutral sentences were not considered. For this rea-\nson, Friedman’s results for 3-class experiments in the Reviews\ncontext presents no values.\nConcluding Remarks\nRecent efforts to analyze the moods embedded in Web 2.0 con-\ntent have adopted various sentiment analysis methods, which\nwere originally developed in linguistics and psychology. Seve-\nral of these methods became widely used in their knowledge\nfields and have now been applied as tools to quantify mo-\nods in the context of unstructured short messages in online\nsocial networks. In this article, we present a thorough compa-\nrison of twenty-four popular sentence-level sentiment analysis\nmethods using gold standard datasets that span different types\nof data sources. Our effort quantifies the prediction perfor-\nmance of the twenty-four popular sentiment analysis methods\nacross eighteen datasets for two tasks: differentiating two clas-\nTable 14: Friedman’s Test Results per Contexts\nContext: Social Networks\n2-class experiments 3-class experiments\nFR 175.94 FR 124.16\nCritical Value 35.17 Critical Value 35.17\nReject null hypothesis Reject null hypothesis\nContext: Comments\n2-class experiments 3-class experiments\nFR 95.59 FR 96.41\nCritical Value 35.17 Critical Value 35.17\nReject null hypothesis Reject null hypothesis\nContext: Reviews\n2-class experiments 3-class experiments\nFR 60.52 FR -\nCritical Value 35.17 Critical Value -\nReject null hypothesis Reject null hypothesis\nses (positive and negative) and three classes (positive, negative,\nand neutral).\nAmong many findings, we highlight that although our results\nidentified a few methods able to appear among the best ones for\ndifferent datasets, we noted that the overall prediction perfor-\nmance still left a lot of space for improvements. More impor-\ntant, we show that the prediction performance of methods vary\nlargely across datasets. For example, LIWC 2007, is among\nthe most popular sentiment methods in the social network\ncontext and obtained a bad rank position in comparison with\nother datasets. This suggests that sentiment analysis methods\ncannot be used as “off-the-shelf” methods, specially for novel\ndatasets. We show that the same social media text can be inter-\npreted very differently depending on the choice of a sentiment\nmethod, suggesting that it is important that researchers and\ncompanies perform experiments with different methods before\napplying a method.\nAs a final contribution we open the datasets and codes used in\nthis paper for the research community. We also incorporated\nthem in a Web service from our research team called iFeel [4]\nthat allow users to easily compare the results of various senti-\nment analysis methods. We hope our effort can not only help\nresearchers and practitioners to compare a wide range of senti-\nment analysis techniques, but also help fostering new relevant\nresearch in this area with a rigorous scientific approach.\nREFERENCES\n1. Ahmed Abbasi, Ammar Hassan, and Milan Dhar. 2014.\nBenchmarking Twitter Sentiment Analysis Tools. In\nProceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC’14) (26-31),\nNicoletta Calzolari (Conference Chair), Khalid Choukri,\nThierry Declerck, Hrafn Loftsson, Bente Maegaard,\nJoseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios\nPiperidis (Eds.). European Language Resources\nAssociation (ELRA), Reykjavik, Iceland.\n2. Fotis Aisopos. 2014. Manually Annotated Sentiment\nAnalysis Twitter Dataset NTUA. (2014).\nwww.grid.ece.ntua.gr.\n3. Marc Brysbaert Amy Beth Warriner, Victor Kuperman.\n2013. Norms of valence, arousal, and dominance for\n13,915 English lemmas. Behavior research methods 45, 4\n(Dec. 2013), 1191–1207.\n4. Matheus Araujo, Joa˜o P. Diniz, Lucas Bastos, Elias\nSoares, Manoel Ju´nior, Miller Ferreira, Filipe Ribeiro,\nand Fabrı´cio Benevenuto. 2016. iFeel 2.0: A Multilingual\nBenchmarking System for Sentence-Level Sentiment\nAnalysis. In Proceedings of the International AAAI\nConference on Web-Blogs and Social Media. Cologne,\nGermany.\n5. Stefano Baccianella, Andrea Esuli, and Fabrizio\nSebastiani. 2010. SentiWordNet 3.0: An Enhanced\nLexical Resource for Sentiment Analysis and Opinion\nMining.. In LREC (2010-06-02), Nicoletta Calzolari,\nKhalid Choukri, Bente Maegaard, Joseph Mariani, Jan\nOdijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias\n(Eds.).\n6. Mark L. Berenson, David M. Levine, and Kathryn A.\nSzabat. 2014. Basic Business Statistics - Concepts and\nApplications (13 ed.). Pearson, USA. 840 pages.\n7. Celeste Biever. 2010. Twitter mood maps reveal\nemotional states of America. The New Scientist 207\n(2010). Issue 2771.\n8. Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2010.\nTwitter Mood Predicts the Stock Market. CoRR\nabs/1010.3003 (2010).\n9. Johan Bollen, Alberto Pepe, and Huina Mao. 2009.\nModeling Public Mood and Emotion: Twitter Sentiment\nand Socio-Economic Phenomena. CoRR abs/0911.1583\n(2009).\n10. M. M. Bradley and P. J. Lang. 1999. Affective norms for\nEnglish words (ANEW): Stimuli, instruction manual, and\naffective ratings. Technical Report. Center for Research\nin Psychophysiology, University of Florida, Gainesville,\nFlorida.\n11. Marc Brysbaert and Boris New. 2009. Moving beyond\nKucera and Francis: A Critical Evaluation of Current\nWord Frequency Norms and the Introduction of a New\nand Improved Word Frequency Measure for American\nEnglish. Behavior research methods 41, 4 (2009),\n977–990.\n12. E. Cambria, D. Olsher, and D. Rajagopal. 2014. Senticnet\n3: A common and common-sense knowledge base for\ncognition-driven sentiment analysis. In AAAI. Quebec\nCity, 1515–1521.\n13. Erik Cambria, Robert Speer, Catherine Havasi, and Amir\nHussain. 2010. SenticNet: A Publicly Available Semantic\nResource for Opinion Mining. In AAAI Fall Symposium\nSeries.\n14. Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto,\nand Krishna P. Gummadi. 2010. Measuring User\nInfluence in Twitter: The Million Follower Fallacy. In\nInternational AAAI Conference on Weblogs and Social\nMedia (ICWSM).\n15. Tom De Smedt and Walter Daelemans. 2012. Pattern for\npython. The Journal of Machine Learning Research 13, 1\n(2012), 2063–2067.\n16. N.A. Diakopoulos and D.A. Shamma. 2010.\nCharacterizing debate performance via aggregated twitter\nsentiment. In Proceedings of the 28th international\nconference on Human factors in computing systems.\nACM, 1195–1198.\n17. Peter Sheridan Dodds, Eric M. Clark, Suma Desu,\nMorgan R. Frank, Andrew J. Reagan, Jake Ryland\nWilliams, Lewis Mitchell, Kameron Decker Harris,\nIsabel M. Kloumann, James P. Bagrow, Karine\nMegerdoomian, Matthew T. McMahon, Brian F. Tivnan,\nand Christopher M. Danforth. 2015. Human language\nreveals a universal positivity bias. Proceedings of the\nNational Academy of Sciences 112, 8 (2015), 2389–2394.\nDOI:http://dx.doi.org/10.1073/pnas.1411678112\n18. Peter Sheridan Dodds and Christopher M Danforth. 2009.\nMeasuring the happiness of large-scale written\nexpression: songs, blogs, and presidents. Journal of\nHappiness Studies 11, 4 (2009), 441–456. DOI:\nhttp://dx.doi.org/10.1007/s10902-009-9150-9\n19. Esuli and Sebastiani. 2006. SentiWordNet: A Publicly\nAvailable Lexical Resource for Opinion Mining. In\nInternational Conference on Language Resources and\nEvaluation (LREC). 417–422.\n20. Ronen Feldman. 2013. Techniques and Applications for\nSentiment Analysis. Commun. ACM 56, 4 (April 2013),\n82–89. DOI:http://dx.doi.org/10.1145/2436256.2436274\n21. David Garcia, Antonios Garas, and Frank Schweitzer.\n2012. Positive words carry less information than negative\nwords. EPJ Data Science 1, 1 (2012), 1–12.\n22. Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter\nSentiment Classification using Distant Supervision.\nProcessing - (2009), 1–6.\n23. Namrata Godbole, Manjunath Srinivasaiah, and Steven\nSkiena. 2007. Large-Scale Sentiment Analysis for News\nand Blogs. In Proceedings of the International\nConference on Weblogs and Social Media (ICWSM).\n24. Pollyanna Gonc¸alves, Matheus Araujo, Fabrı´cio\nBenevenuto, and Meeyoung Cha. 2013a. Comparing and\nCombining Sentiment Analysis Methods. In Proceedings\nof the 1st ACM Conference on Online Social Networks\n(COSN’13). 12.\n25. Pollyanna Gonc¸alves, Fabrı´cio Benevenuto, and\nMeeyoung Cha. 2013b. PANAS-t: A Pychometric Scale\nfor Measuring Sentiments on Twitter. abs/1308.1857v1\n(2013).\n26. Aniko Hannak, Eric Anderson, Lisa Feldman Barrett,\nSune Lehmann, Alan Mislove, and Mirek Riedewald.\n2012. Tweetin’ in the Rain: Exploring societal-scale\neffects of weather on mood. In Int’l AAAI Conference on\nWeblogs and Social Media (ICWSM).\n27. Minqing Hu and Bing Liu. 2004. Mining and\nsummarizing customer reviews (KDD ’04). 168–177.\nhttp://doi.acm.org/10.1145/1014052.1014073\n28. CJ Hutto and Eric Gilbert. 2014. Vader: A parsimonious\nrule-based model for sentiment analysis of social media\ntext. In Eighth International AAAI Conference on\nWeblogs and Social Media (ICWSM).\n29. Raj Jain. 1991. The art of computer systems performance\nanalysis - techniques for experimental design,\nmeasurement, simulation, and modeling. (1 ed.). Wiley,\nCanada. 1–685 pages.\n30. Rie Johnson and Tong Zhang. 2015. Effective Use of\nWord Order for Text Categorization with Convolutional\nNeural Networks. In NAACL HLT 2015, The 2015\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Denver, Colorado, USA, May 31\n- June 5, 2015. 103–112.\n31. Nal Kalchbrenner, Edward Grefenstette, and Phil\nBlunsom. 2014. A Convolutional Neural Network for\nModelling Sentences. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Linguistics.\n32. Efthymios Kouloumpis, Theresa Wilson, and Johanna\nMoore. 2011. Twitter Sentiment Analysis: The Good the\nBad and the OMG!. In Int’l AAAI Conference on Weblogs\nand Social Media (ICWSM).\n33. Adam D I Kramer, Jamie E Guillory, and Jeffrey T\nHancock. 2014. Experimental evidence of massive-scale\nemotional contagion through social networks.\nProceedings of the National Academy of Sciences of the\nUnited States of America 111, 24 (June 2014), 8788–90.\nDOI:http://dx.doi.org/10.1073/pnas.1320040111\n34. J. Richard Landis and Gary G. Koch. 1977. The\nMeasurement of Observer Agreement for Categorical\nData. Biometrics 33, 1 (1977).\n35. Clement Levallois. 2013. Umigon: sentiment analysis for\ntweets based on terms lists and heuristics. In Second Joint\nConference on Lexical and Computational Semantics\n(*SEM), Volume 2: Proceedings of the Seventh\nInternational Workshop on Semantic Evaluation\n(SemEval 2013). Association for Computational\nLinguistics, Atlanta, Georgia, USA, 414–417.\nhttp://www.aclweb.org/anthology/S13-2068\n36. Lexalytics. 2015. Sentiment Extraction - Measuring the\nEmotional Tone of Content. Technical Report. Lexalytics.\n37. Bing Liu. 2012. Sentiment Analysis and Opinion Mining.\nSynthesis Lectures on Human Language Technologies 5, 1\n(May 2012), 1–167. DOI:\nhttp://dx.doi.org/10.2200/s00416ed1v01y201204hlt016\n38. George A. Miller. 1995. WordNet: a lexical database for\nEnglish. Commun. ACM 38, 11 (1995), 39–41.\n39. Saif Mohammad. 2012. #Emotional Tweets. In The First\nJoint Conference on Lexical and Computational\nSemantics - Volume 1: Proceedings of the main\nconference and the shared task, and Volume 2:\nProceedings of the Sixth International Workshop on\nSemantic Evaluation (SemEval 2012). Association for\nComputational Linguistics, Montre´al, Canada, 246–255.\nhttp://www.aclweb.org/anthology/S12-1033\n40. Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.\nGenerating High-coverage Semantic Orientation\nLexicons from Overtly Marked Words and a Thesaurus.\nIn Proceedings of the 2009 Conference on Empirical\nMethods in Natural Language Processing: Volume 2 -\nVolume 2 (EMNLP ’09). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 599–608.\nhttp://dl.acm.org/citation.cfm?id=1699571.1699591\n41. Saif Mohammad and Peter D. Turney. 2013.\nCrowdsourcing a Word-Emotion Association Lexicon.\nComputational Intelligence 29, 3 (2013), 436–465.\n42. Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan\nZhu. 2013. NRC-Canada: Building the State-of-the-Art\nin Sentiment Analysis of Tweets. In Proceedings of the\nseventh international workshop on Semantic Evaluation\nExercises (SemEval-2013). Atlanta, Georgia, USA.\n43. Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara\nRosenthal, Veselin Stoyanov, and Theresa Wilson. 2013.\nSemEval-2013 Task 2: Sentiment Analysis in Twitter.\n(2013).\n44. Sascha Narr, Michael Hu¨lfenhaus, and Sahin Albayrak.\n2012. Language-independent Twitter sentiment analysis.\nKnowledge Discovery and Machine Learning (KDML)\n(2012), 12–14.\n45. Finn rup Nielsen. 2011. A new ANEW: Evaluation of a\nword list for sentiment analysis in microblogs. arXiv\npreprint arXiv:1103.2903 (2011).\n46. Nuno Oliveira, Paulo Cortez, and Nelson Areal. 2013. On\nthe Predictability of Stock Market Behavior Using\nStockTwits Sentiment and Posting Volume. 8154 (2013),\n355–365.\n47. Bo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. In In Proceedings of the ACL.\n271–278.\n48. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up?: sentiment classification using\nmachine learning techniques. In ACL Conference on\nEmpirical Methods in Natural Language Processing.\n79–86.\n49. Nikolaos Pappas, Georgios Katsimpras, and Efstathios\nStamatatos. 2013. Distinguishing the Popularity Between\nTopics: A System for Up-to-date Opinion Retrieval and\nMining in the Web. In 14th International Conference on\nIntelligent Text Processing and Computational\nLinguistics.\n50. Nikolaos Pappas and Andrei Popescu-Belis. 2013.\nSentiment analysis of user comments for one-class\ncollaborative filtering over TED talks. In Proceedings of\nthe 36th international ACM SIGIR conference on\nResearch and development in information retrieval. ACM,\n773–776.\n51. R. Plutchik. 1980. A general psychoevolutionary theory\nof emotion. Academic press, New York, 3–33.\n52. Julio Reis, Fabricio Benevenuto, Pedro Vaz de Melo,\nRaquel Prates, Haewoon Kwak, and Jisun An. 2015.\nBreaking the News: First Impressions Matter on Online\nNews. In Proceedings of the 9th International AAAI\nConference on Web-Blogs and Social Media (ICWSM).\n53. Julio Reis, Pollyanna Goncalves, Pedro Vaz de Melo,\nRaquel Prates, and Fabricio Benevenuto. 2014. Magnet\nNews: You Choose the Polarity of What you Read. In\nInternational AAAI Conference on Web-Blogs and Social\nMedia.\n54. Niek Sanders. 2011. Twitter Sentiment Corpus by Niek\nSanders. (2011).\nhttp://www.sananalytics.com/lab/twitter-sentiment/.\n55. Rion Snow, Brendan O’Connor, Daniel Jurafsky, and\nAndrew Y. Ng. 2008. Cheap and Fast—but is It Good?:\nEvaluating Non-expert Annotations for Natural Language\nTasks. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP ’08).\n56. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,\nChristopher D. Manning, Andrew Y. Ng, and Christopher\nPotts. 2013. Recursive Deep Models for Semantic\nCompositionality Over a Sentiment Treebank. In 2013\nConference on Empirical Methods in Natural Language\nProcessing. 1631–1642.\n57. Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,\nand Daniel M. Ogilvie. 1966. The General Inquirer: A\nComputer Approach to Content Analysis. MIT Press,\nUSA.\n58. Carlo Strapparava and Rada Mihalcea. 2007.\nSemEval-2007 Task 14: Affective Text. In Proceedings of\nthe 4th International Workshop on Semantic Evaluations\n(SemEval ’07). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 70–74.\nhttp://dl.acm.org/citation.cfm?id=1621474.1621487\n59. Maite Taboada, Caroline Anthony, and Kimberly Voll.\n2006a. Methods for Creating Semantic Orientation\nDictionaries. In Conference on Language Resources and\nEvaluation (LREC). 427–432.\n60. Maite Taboada, Caroline Anthony, and Kimberly Voll.\n2006b. Methods for Creating Semantic Orientation\nDictionaries. In Conference on Language Resources and\nEvaluation (LREC). 427–432.\n61. Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly\nVoll, and Manfred Stede. 2011. Lexicon-based Methods\nfor Sentiment Analysis. Comput. Linguist. 37, 2 (June\n2011), 267–307.\n62. Acar Tamersoy, Munmun De Choudhury, and\nDuen Horng Chau. 2015. Characterizing Smoking and\nDrinking Abstinence from Social Media. In Proceedings\nof the 26th ACM Conference on Hypertext and Social\nMedia (HT).\n63. Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,\nand Bing Qin. 2014. Learning Sentiment-Specific Word\nEmbedding for Twitter Sentiment Classification. In\nProceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2014,\nJune 22-27, 2014, Baltimore, MD, USA, Volume 1: Long\nPapers. 1555–1565.\n64. Yla R. Tausczik and James W. Pennebaker. 2010. The\nPsychological Meaning of Words: LIWC and\nComputerized Text Analysis Methods. Journal of\nLanguage and Social Psychology 29, 1 (2010), 24–54.\n65. Mike Thelwall. 2013. Heart and soul: Sentiment strength\ndetection in the social web with SentiStrength. (2013).\nhttp://sentistrength.wlv.ac.uk/documentation/\nSentiStrengthChapter.pdf.\n66. Mikalai Tsytsarau and Themis Palpanas. 2012. Survey on\nMining Subjective Data on the Web. Data Min. Knowl.\nDiscov. 24, 3 (May 2012), 478–514. DOI:\nhttp://dx.doi.org/10.1007/s10618-011-0238-6\n67. Andranik Tumasjan, Timm O. Sprenger, Philipp G.\nSandner, and Isabell M. Welpe. 2010. Predicting\nElections with Twitter: What 140 Characters Reveal\nabout Political Sentiment. In International AAAI\nConference on Weblogs and Social Media (ICWSM).\n68. Ro Valitutti. 2004. WordNet-Affect: an Affective\nExtension of WordNet. In In Proceedings of the 4th\nInternational Conference on Language Resources and\nEvaluation. 1083–1086.\n69. Hao Wang, Dogan Can, Abe Kazemzadeh, Franc¸ois Bar,\nand Shrikanth Narayanan. 2012. A system for real-time\nTwitter sentiment analysis of 2012 U.S. presidential\nelection cycle. In ACL System Demonstrations. 115–120.\n70. D. Watson and L. Clark. 1985. Development and\nvalidation of brief measures of positive and negative\naffect: the PANAS scales. Journal of Personality and\nSocial Psychology 54, 1 (1985), 1063–1070.\n71. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating Expressions of Opinions and Emotions in\nLanguage. Language Resources and Evaluation 1, 2\n(2005), 0. http://www.cs.pitt.edu/˜\n72. Theresa Wilson, Paul Hoffmann, Swapna Somasundaran,\nJason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie,\nEllen Riloff, and Siddharth Patwardhan. 2005a.\nOpinionFinder: a system for subjectivity analysis. In\nHLT/EMNLP on Interactive Demonstrations. 34–35.\n73. Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.\n2005b. Recognizing Contextual Polarity in Phrase-Level\nSentiment Analysis. In ACL Conference on Empirical\nMethods in Natural Language Processing. 347–354.\n74. David H. Wolpert and William G. Macready. 1997. No\nfree lunch theorems for optimization. IEEE Transactions\non Evlutionary Computation 1, 1 (1997), 67–82.\n",
      "id": 24749806,
      "identifiers": [
        {
          "identifier": "oai:repositorio.ufop.br:123456789/9265",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:arxiv.org:1512.01818",
          "type": "OAI_ID"
        },
        {
          "identifier": "194925623",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.1140/epjds/s13688-016-0085-1",
          "type": "DOI"
        },
        {
          "identifier": "42661665",
          "type": "CORE_ID"
        },
        {
          "identifier": "1512.01818",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "2471350540",
          "type": "MAG_ID"
        },
        {
          "identifier": "478016225",
          "type": "CORE_ID"
        },
        {
          "identifier": "646587179",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:localhost:123456789/9265",
          "type": "OAI_ID"
        },
        {
          "identifier": "202415923",
          "type": "CORE_ID"
        }
      ],
      "title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment\n  analysis methods",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:repositorio.ufop.br:123456789/9265",
        "oai:localhost:123456789/9265",
        "oai:arxiv.org:1512.01818"
      ],
      "publishedDate": "2016-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "file:///data/remote/core/dit/data/Springer-OA/pdf/43c/aHR0cDovL2xpbmsuc3ByaW5nZXIuY29tLzEwLjExNDAvZXBqZHMvczEzNjg4LTAxNi0wMDg1LTEucGRm.pdf",
        "http://www.repositorio.ufop.br/jspui/bitstream/123456789/9265/1/ARTIGO_SentiBenchBencmark.pdf",
        "http://arxiv.org/abs/1512.01818"
      ],
      "updatedDate": "2025-06-14T06:02:30",
      "yearPublished": 2016,
      "journals": [
        {
          "title": "EPJ Data Science",
          "identifiers": [
            "issn:2193-1127",
            "2193-1127"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/478016225.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/478016225"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/478016225/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/478016225/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/24749806"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Bagheri, A"
        },
        {
          "name": "de Jong, F"
        },
        {
          "name": "Saraee, MH"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Human Media Interaction",
        "The Pennsylvania State University CiteSeerX Archives",
        "Faculty of Electrical Engineering, Mathematics & Computer Science",
        "Pervasive Systems"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/572108859",
        "https://api.core.ac.uk/v3/outputs/92026839"
      ],
      "createdDate": "2013-09-18T15:15:22",
      "dataProviders": [
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        },
        {
          "id": 130,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/130",
          "logo": "https://api.core.ac.uk/data-providers/130/logo"
        },
        {
          "id": 708,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/708",
          "logo": "https://api.core.ac.uk/data-providers/708/logo"
        },
        {
          "id": 363,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/363",
          "logo": "https://api.core.ac.uk/data-providers/363/logo"
        }
      ],
      "depositedDate": "2013-09-06T13:52:00",
      "abstract": "In recent years probabilistic topic models have gained tremendous attention in data mining and natural language processing research areas. In the field of information retrieval for text mining, a variety of probabilistic topic models have been used to analyse content of documents. A topic model is a generative model for documents, it specifies a probabilistic procedure by which documents can be generated. All topic models share the idea that documents are mixture of topics, where a topic is a probability distribution over words. In this paper we describe Latent Dirichlet Markov Allocation Model (LDMA), a new generative probabilistic topic model, based on Latent Dirichlet Allocation (LDA) and Hidden Markov Model (HMM), which emphasizes on extracting multi-word topics from text data. LDMA is a four-level hierarchical Bayesian model where topics are associated with documents, words are associated with topics and topics in the model can be presented with single- or multi-word terms. To evaluate performance of LDMA, we report results in the field of aspect detection in sentiment analysis, comparing to the basic LDA model",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/16412155.pdf",
      "fieldOfStudy": null,
      "fullText": "Latent Dirichlet Markov Allocation for Sentiment Analysis \nAyoub Bagheri Mohamad Saraee \n  \nIsfahan University of Technology, Isfahan, Iran \nUniversity of Salford, Manchester, \nUK \nIntelligent Database, Data Mining and Bioinformatics \nLab, Electrical and Computer Engineering Department \nSchool of Computing, Science and \nEngineering \na.bagheri@ec.iut.ac.ir m.saraee@salford.ac.uk \n  \n  \nFranciska de Jong \n \nUniversity of Twente, Enschede, Netherlands \nHuman Media Interaction, P.O. Box 217, 7500 AE \nf.m.g.dejong@utwente.nl \n \n \nAbstract. In recent years probabilistic topic models have gained tremendous attention in \ndata mining and natural language processing research areas. In the field of information \nretrieval for text mining, a variety of probabilistic topic models have been used to ana-\nlyse content of documents. A topic model is a generative model for documents, it speci-\nfies a probabilistic procedure by which documents can be generated. All topic models \nshare the idea that documents are mixture of topics, where a topic is a probability distri-\nbution over words. In this paper we describe Latent Dirichlet Markov Allocation Model \n(LDMA), a new generative probabilistic topic model, based on Latent Dirichlet Alloca-\ntion (LDA) and Hidden Markov Model (HMM), which emphasizes on extracting multi-\nword topics from text data. LDMA is a four-level hierarchical Bayesian model where \ntopics are associated with documents, words are associated with topics and topics in the \nmodel can be presented with single- or multi-word terms. To evaluate performance of \nLDMA, we report results in the field of aspect detection in sentiment analysis, comparing \nto the basic LDA model. \nKeywords: topic model, latent Dirichlet allocation (LDA), hidden markov model (HMM), Latent \nDirichlet Markov Allocation (LDMA), sentiment analysis. \n1 INTRODUCTION \nWith the explosion of web 2.0, user generated content in online reviews present a wealth of infor-\nmation that can be very helpful for manufactories, companies and other customers. Mining these \nonline reviews to extract and summarize users’ opinions is a challenging task in the field of data min-\ning and natural language processing. One main task in sentiment review analysis is to find aspects that \nusers evaluate in their reviews. Aspects are topics on which opinion are expressed about. In the field \nof sentiment analysis, other names for aspect are: features, product features or opinion targets [1-10]. \nAspects are important because without knowing them, the opinions expressed in a sentence or a re-\nview are of limited use. For example, in the review sentence “after using iPhone, I found the size to be \nperfect for carrying in a pocket”, “size” is the aspect for which an opinion is expressed. Likewise as-\npect detection is critical to sentiment analysis, because its effectiveness dramatically affects the per-\nformance of opinion word detection and sentiment orientation identification. Therefore, in this study \nwe concentrate on aspect detection for sentiment analysis. \nDifferent approaches have been proposed for aspect detection from reviews. Previous works like \ndouble propagation [4] and supervised learning methods have the limitation that they do not group \nsemantically related aspect expressions together [2]. Supervised methods, additionally, are not often \npractical due to the fact that building sufficient labeled data is often expensive and needs much human \nlabor. In contrast, the effectiveness of unsupervised topic modeling approaches has been shown in \nidentifying aspect words. Probabilistic topic models are a suite of algorithms whose aim is to extract \nlatent structure from large collection of documents. These models all share the idea that documents \nare mixtures of topics and each topic is a distribution over words [11-15]. We follow this promising \nline of research to extend existing topic models for aspect detection in sentiment analysis. Current \ntopic modeling approaches are computationally efficient and also seem to capture correlations be-\ntween words and topics but they have two main limitations: the first limitation is that they assume that \nwords are generated independently to each other, i.e. the bag of words assumption. In other words, \ntopic models only extract unigrams for topics in a corpus. We believe that a topic model considering \nunigrams and phrases is more realistic and would be more useful in applications. The second limit for \ncurrent topic modeling approaches is that of the assumption that the order of words can be ignored is \nan unrealistic oversimplification. Topic models assume the subsequent words in a document or a sen-\ntence have different topics, which is not a true assumption. In our model, in addition to extracting \nunigrams and phrases for topics we assume that the topics of words in a sentence form a Markov \nchain and that subsequent words are more likely to have the same topic. Therefore, in this paper we \npropose a new topic modeling approach that can automatically extract topics or aspects in sentiment \nreviews. We call the proposed model: Latent Dirichlet Markov Allocation Model (LDMA), which is a \ngenerative probabilistic topic model based on Latent Dirichlet Allocation (LDA) and Hidden Markov \nModel (HMM), which emphasizes on extracting multi-word topics from text data. In addition LDMA \nrelaxes the “bag of words” assumption from topic modeling approaches to yield to a better model in \nterms of extracting latent topics from text. \nWe proceed by reviewing the formalism of LDA. We then propose the LDMA model and its infer-\nence procedure to estimate parameters. We demonstrate the effectiveness of our model in our experi-\nments by comparing to basic LDA model for aspect detection. Finally, we conclude our work and \noutline future directions. \n2 LDA \nLDA, introduced by David Blei et al. [12], is a probabilistic generative topic model based on the \nassumption that each document is a mixture of various topics and each topic is a probability distribu-\ntion over different words.  \nA graphical model of LDA is shown in Figure 1, wherein nodes are random variables and edges in-\ndicate the dependence between nodes [12, 13]. As a directed graph, shaded and unshaded variables \nindicate observed and latent (i.e., unobserved) variables respectively, and arrows indicate conditional \ndependencies between variables while plates (the boxes in the figure) refer to repetitions of sampling \nsteps with the variable in the lower right corner referring to the number of samples. \nGiven a corpus with a collection of D documents, each document in the corpus is a sequence of W \nwords, each word in the document is an item from a vocabulary index with V distinct terms, and T is \nthe total number of topics. The procedure for generating a word in a document by LDA is as follows: \n1. for each topic z = 1…K, \nDraw z ~ Dirichlet(); \n2. for each document d = 1…D, \n(a) Draw topic distribution d ~ Dirichlet(); \n(b) for each word wi in document d, \ni. Draw a topic     \n \n~ d; \nii. Draw a word    \n \n~      ; \nThe goal of LDA is therefore to find a set of model parameters, topic proportions and topic-word \ndistributions. Standard statistical techniques can be used to invert the generative process of LDA, in-\nferring the set of topics that were responsible for generating a collection of documents. The exact in-\nference in LDA is generally intractable, and we have to appeal to approximate inference algorithms \nfor posterior estimation. The most common approaches that are used for approximate inference are \nEM, Gibbs Sampling and Variational method [12, 13, 15]. \n \n \n \nFigure 1 LDA Topic Model \n3 LDMA \nLDA model makes an assumption that words are generated independently to each other in a docu-\nment. Also LDA ignores the order or positions of words or simplicity. Here we propose LDMA, a \ngenerative probabilistic topic model based on Latent Dirichlet Allocation (LDA) and Hidden Markov \nModel (HMM), to relax the “bag of words” assumption from LDA to yield to a better model in terms \nof extracting latent topics from text. Figure 2 shows the graphical model corresponding to the LDMA \ngenerative model. \nLDMA, in addition to the two sets of random variables z and w, introduces a new set of variables x \nto detect an n-gram phrase from the text. LDMA assumes that the topics in a sentence form a Markov \nchain with a transition probability that depends on , a distribution zw, a random variable xi and the \ntopic of previous word zi-1. Random variable x denotes whether a bigram can be formed with previous \nterm or not. Therefore LDMA has the power to decide whether to generate a unigram, a bigram, a \ntrigram or etc. Here we only consider generating unigrams and bigrams from LDMA. If the model \nsets xi equal to one, it means that wi-1 and wi form a bigram and if it is equal to zero they do not.  \nThe generative process of the LDMA model can be described as follows: \n3. for z = 1…K, \nDraw multinomial z ~ Dirichlet(); \n4. for z = 1…K, \nfor w = 1…W, \n(a) Draw binomial zw ~ Dirichlet(); \n(b) Draw multinomial zw ~ Beta(); \n5. for d = 1…D, \n(c) Draw multinomial d ~ Dirichlet(); \n(d) for s = 1…S in document d, \n    for each word wi in sentence s, \n   i. Draw   \n  from binomial      \n     \n ; \n   ii. if(  \n  = 0) Draw   \n  from multinomial d; \n       else   \n      \n ; \n   iii. if(  \n  = 1) Draw  \n  from multinomial    \n     \n ; \n        else draw  \n  from multinomial    \n ; \n     \n \n \n\nD\nz\nW\n \nK\nw\n \n \nFigure 2 A graphical view of LDMA Topic Model \n \nLDMA model is sensitive to the order of the words, which it is not assumes that a document is a bag \nof words, i.e. successive words in LDMA tend to have the same topics. Therefore unlike LDA ap-\nproach, LDMA will not give the same topic to all appearances of the same word within a sentence, a \ndocument or corpus. \n4 INFERENCE \nThe exact inference of learning parameters in LDMA is intractable due to the large number of param-\neters in the model [13, 15]. Therefore approximate techniques are needed. We use Gibbs sampling to \napproximate the latent variables in the model. At each transition of Markov chain, the aspect (topic) \nof ith sentence, zi, and the n-gram variable xi are drawn from the following conditional probability: \n \nIf      then: \n  (     |                 )  \n               \n∑ (             \n)    \n(         ) (\n         \n∑ (       )\n \n   \n) \nIf      then: \n  (     |                 )  \n               \n∑ (             \n)    \n(         ) (\n             \n∑ (           )\n \n   \n) \n \nWhere     denotes the bigram status for all words except   ,     represents the topic (aspect) assign-\nments for all words except   ,     represents how many times word w is assigned to aspect z as a \nunigram,      represents how many times word v is assigned to aspect z as a second term of a bi-\ngram word given the previous word w,      denotes how many times the status variable x is set to k \ngiven the previous word w and the previous word’s aspect z, and     represents how many times a \nword is assigned to aspect z in sentence s of document d. \n\nzi - 1 zi ...\n\nzi + 1\nwi - 1 wi wi + 1\n\n...\nS\n\nK\n...\n...\nxi xi + 1\nD\n......\n\nKW\n \n5 EXPERIMENTS \nIn this section, we apply the proposed LDMA model to customer review datasets of digital cameras \nand compare the results with the original LDA model. These datasets of customer reviews [1] contain \ntwo different products: Canon G3 and Nikon Coolpix 4300. Table 1 shows the number of review sen-\ntences and the number of manually tagged product aspects for each product in the dataset. \nTable 1 \nSummary of customer review datasets. \nDataset Number of review sentences Number of \nmanual aspects \nCanon 597 100 \nNikon 346 74 \n \nWe first start by preprocessing review document from the datasets. We extract the sentences accord-\ning to the delimiters ‘.’, ‘,’, ‘!’, ‘?’, ‘;’. And then by removing Stopwords and words with frequency \nless than three we extract a feature vector to represent review documents. By applying LDMA and the \noriginal LDA models examples of most probable aspects (topics) extracted are shown in Tables 2 and \nTable 3. \nTable 2 \nExampled aspects discovered by LDMA and LDA form Canon reviews \nCanon \nLDMA LDA \ncanon \ncanon powershot \ncanon \npurchased \ndurability durability \nphotos \nbattery \ndigital zoom \neasy \nlife \nbattery \npicture \nsize \npicture quality \noptical zoom \nquality \npowershot \nzoom \nsize \nphotos quality picture \nbattery life digital \nTable 3 \nExampled aspects discovered by LDMA and LDA form Nikon reviews \nNikon \nLDMA LDA \neasy use \nauto mode \nBattery \nperfect \ncamera transfer \npictures camera \nnikon nikon \ntransfer cable Manual \nbattery \nmanual mode \nprint quality \nmacro \nsmall \nmode \n \nFrom the tables we can find that the LDMA model discovered more informative words for aspects or \ntopics. In addition to the unigrams, LDMA can extract phrases, hence the unigram and bigram list of \naspects are more pure in LDMA. Also LDMA associates words together to detect the multi-word as-\npects which are only highly probable in this model. Based on the results, LDMA can successfully find \naspects that consist of words that are consecutive in a review document. \n6 CONCLUSIONS \nManaging the explosion of digital content on the internet requires new tools for automatically mining, \nsearching, indexing and browsing large collections of text data. Recent research in data mining and \nstatistics has developed a new brand of techniques call probabilistic topic modeling for text. In this \npaper, we proposed LDMA model, a model which extends the original topic modeling approach, \nLDA, by considering the underlying structure of a document and order of words in document. LDMA \nignores the bag of words assumption of LDA to extract multi-word and n-gram aspects and topics \nfrom text data. We find that ignoring this basic assumption allows the model to learn more coherent \nand informatics topics. \nREFERENCES \n[1] Hu, M., Liu, B. (2004). Mining opinion features in customer reviews. In: Proceedings of 19th Na-tional Conference on Artificial \nIntelligence AAAI. \n[2] B. Liu, L. Zhang, (2012). A survey of opinion mining and sentiment analysis, Mining Text Data, 415-463. \n[3] C. Lin, Y. He, R. Everson, S. Ruger, (2012). Weakly supervised joint sentiment-topic detection from text, IEEE Trans. Knowl. Data \nEng. 24, no. 6, 1134-1145. \n[4] G. Qiu, B. Liu, J. Bu, C. Chen. (2011). Opinion word expansion and target extraction through double propagation, Computational \nlinguistics, 37(1), 9-27. \n[5] I. Titov, R. McDonald, (2008). A joint model of text and aspect ratings for sentiment summarization, in: Proceedings of the Annual \nMeeting on Association for Computational Linguistics and the Human Language Technology Conference (ACL-HLT). pp. 308–316. \n[6] S. Brody, N. Elhadad, (2010). An unsupervised aspect-sentiment model for online reviews, in: Proceedings of Annual Conference of \nthe North American Chapter of the Association for Computational Linguistics. Publishing, Association for Computational Linguistics, \npp. 804-812. \n[7] S. Moghaddam, M. Ester, (2011). ILDA: interdependent LDA model for learning latent aspects and their ratings from online product \nreviews, in: Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. \nPublishing, ACM, pp. 665-674. \n[8] X. Fu, G. Liu, Y. Guo, Z. Wang, (2013). Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling \nand HowNet lexicon, Knowledge-Based Systems,  37, 186-195. \n[9] Z. Zhai, B. Liu, H. Xu, P. Jia, (2011). Constrained LDA for grouping product features in opinion mining, in: Proceedings of 15th \nPacific-Asia Conference, Advances in Knowledge Discovery and Data Mining, pp 448-459. \n[10] Jo, Y., & Oh, A. H. (2011). Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM \ninternational conference on Web search and data mining (pp. 815-824). ACM. \n[11] C. Zhai, J. Lafferty, (2001). Model-based feedback in the language modeling approach to information retrieval, in: Proceedings of 10th \nInternational Conference on Information and knowledge management. Publishing, pp. 403-410. \n[12] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, \n3:993–1022. \n[13] Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. (2005). Integrating topics and syntax. In Lawrence K. \nSaul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press, \nCambridge, MA. \n[14] Hanna M. Wallach. (2006). Topic modeling: Beyond bag-of-words. In ICML ’06: 23rd International Conference on Machine Learn-\ning, Pittsburgh, Pennsylvania, USA. \n[15] Xuerui Wang and Andrew McCallum. (2005). A note on topical n-grams. Technical Report UM-CS-071, Department of Computer \nScience University of Massachusetts Amherst. \n",
      "id": 6795298,
      "identifiers": [
        {
          "identifier": "18296113",
          "type": "CORE_ID"
        },
        {
          "identifier": "16412155",
          "type": "CORE_ID"
        },
        {
          "identifier": "92026839",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:doc.utwente.nl:87440",
          "type": "OAI_ID"
        },
        {
          "identifier": "23567148",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:usir.salford.ac.uk:29460",
          "type": "OAI_ID"
        },
        {
          "identifier": "572108859",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:ris.utwente.nl:publications/531eadb2-76dc-4734-81bd-30774e99a480",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:citeseerx.psu:10.1.1.388.8641",
          "type": "OAI_ID"
        }
      ],
      "title": "Latent dirichlet markov allocation for sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:usir.salford.ac.uk:29460",
        "oai:doc.utwente.nl:87440",
        "oai:ris.utwente.nl:publications/531eadb2-76dc-4734-81bd-30774e99a480",
        "oai:citeseerx.psu:10.1.1.388.8641"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "The OR Society , Birmingham, UK",
      "pubmedId": null,
      "references": [
        {
          "id": 7223179,
          "title": "A joint model of text and aspect ratings for sentiment summarization, in:",
          "authors": [],
          "date": "2008",
          "doi": null,
          "raw": "I. Titov, R. McDonald, (2008). A joint model of text and aspect ratings for sentiment summarization, in: Proceedings of the Annual Meeting on Association for Computational Linguistics and the Human Language Technology Conference (ACL-HLT). pp. 308–316.",
          "cites": null
        },
        {
          "id": 7223192,
          "title": "A note on topical n-grams.",
          "authors": [],
          "date": "2005",
          "doi": "10.1109/icdm.2007.86",
          "raw": "Xuerui Wang and Andrew McCallum. (2005). A note on topical n-grams. Technical Report UM-CS-071, Department of Computer Science University of Massachusetts Amherst.",
          "cites": null
        },
        {
          "id": 7223175,
          "title": "A survey of opinion mining and sentiment analysis, Mining Text Data,",
          "authors": [],
          "date": "2012",
          "doi": "10.1007/978-1-4614-3223-4_13",
          "raw": "B. Liu, L. Zhang, (2012). A survey of opinion mining and sentiment analysis, Mining Text Data, 415-463.",
          "cites": null
        },
        {
          "id": 7223180,
          "title": "An unsupervised aspect-sentiment model for online reviews, in:",
          "authors": [],
          "date": "2010",
          "doi": null,
          "raw": "S. Brody, N. Elhadad, (2010). An unsupervised aspect-sentiment model for online reviews, in: Proceedings of Annual Conference of the North American Chapter of the Association for Computational Linguistics. Publishing, Association for Computational Linguistics, pp. 804-812.",
          "cites": null
        },
        {
          "id": 7223187,
          "title": "Aspect and sentiment unification model for online review analysis.",
          "authors": [],
          "date": "2011",
          "doi": "10.1145/1935826.1935932",
          "raw": "Jo, Y., & Oh, A. H. (2011). Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining (pp. 815-824). ACM.",
          "cites": null
        },
        {
          "id": 7223185,
          "title": "Constrained LDA for grouping product features in opinion mining, in:",
          "authors": [],
          "date": "2011",
          "doi": "10.1007/978-3-642-20841-6_37",
          "raw": "Z. Zhai, B. Liu, H. Xu, P. Jia, (2011). Constrained LDA for grouping product features in opinion mining, in: Proceedings of 15th Pacific-Asia Conference, Advances in Knowledge Discovery and Data Mining, pp 448-459.",
          "cites": null
        },
        {
          "id": 7223182,
          "title": "ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews, in:",
          "authors": [],
          "date": "2011",
          "doi": "10.1145/2009916.2010006",
          "raw": "S. Moghaddam, M. Ester, (2011). ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews, in: Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. Publishing, ACM, pp. 665-674.",
          "cites": null
        },
        {
          "id": 7223190,
          "title": "Integrating topics and syntax. In",
          "authors": [],
          "date": "2005",
          "doi": null,
          "raw": "Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. (2005). Integrating topics and syntax. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 537–544. MIT Press, Cambridge, MA.",
          "cites": null
        },
        {
          "id": 7223189,
          "title": "Latent Dirichlet allocation.",
          "authors": [],
          "date": "2003",
          "doi": null,
          "raw": "David M. Blei, Andrew Y. Ng, and Michael I. Jordan. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.",
          "cites": null
        },
        {
          "id": 7223174,
          "title": "Mining opinion features in customer reviews. In:",
          "authors": [],
          "date": "2004",
          "doi": "10.1145/1014052.1014073",
          "raw": "Hu, M., Liu, B. (2004). Mining opinion features in customer reviews. In: Proceedings of 19th Na-tional Conference on Artificial Intelligence AAAI.",
          "cites": null
        },
        {
          "id": 7223188,
          "title": "Model-based feedback in the language modeling approach to information retrieval, in:",
          "authors": [],
          "date": "2001",
          "doi": "10.1145/502585.502654",
          "raw": "C. Zhai, J. Lafferty, (2001). Model-based feedback in the language modeling approach to information retrieval, in: Proceedings of 10th International Conference on Information and knowledge management. Publishing, pp. 403-410.",
          "cites": null
        },
        {
          "id": 7223184,
          "title": "Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling and HowNet lexicon,",
          "authors": [],
          "date": "2013",
          "doi": "10.1016/j.knosys.2012.08.003",
          "raw": "X. Fu, G. Liu, Y. Guo, Z. Wang, (2013). Multi-aspect sentiment analysis for Chinese online social reviews based on topic modeling and HowNet lexicon, Knowledge-Based Systems,  37, 186-195.",
          "cites": null
        },
        {
          "id": 7223178,
          "title": "Opinion word expansion and target extraction through double propagation,",
          "authors": [],
          "date": "2011",
          "doi": "10.1162/coli_a_00034",
          "raw": "G. Qiu, B. Liu, J. Bu, C. Chen. (2011). Opinion word expansion and target extraction through double propagation, Computational linguistics, 37(1), 9-27.",
          "cites": null
        },
        {
          "id": 7223191,
          "title": "Topic modeling: Beyond bag-of-words.",
          "authors": [],
          "date": "2006",
          "doi": "10.1145/1143844.1143967",
          "raw": "Hanna M. Wallach. (2006). Topic modeling: Beyond bag-of-words. In ICML ’06: 23rd International Conference on Machine Learning, Pittsburgh, Pennsylvania, USA.",
          "cites": null
        },
        {
          "id": 7223176,
          "title": "Weakly supervised joint sentiment-topic detection from text,",
          "authors": [],
          "date": "2012",
          "doi": "10.1109/tkde.2011.48",
          "raw": "C. Lin, Y. He, R. Everson, S. Ruger, (2012). Weakly supervised joint sentiment-topic detection from text, IEEE Trans. Knowl. Data Eng. 24, no. 6, 1134-1145.",
          "cites": null
        }
      ],
      "sourceFulltextUrls": [
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.388.8641",
        "http://www.loc.gov/mods/v3",
        "http://doc.utwente.nl/87440/1/IMSIO_Latent_Dirichlet_Markov_Allocation.pdf",
        "https://ris.utwente.nl/ws/files/5511169/IMSIO_Latent_Dirichlet_Markov_Allocation.pdf"
      ],
      "updatedDate": "2023-07-14T13:54:28",
      "yearPublished": 2013,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/16412155.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/16412155"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/16412155/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/16412155/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/6795298"
        }
      ]
    },
    {
      "acceptedDate": "2018-01-08T00:00:00",
      "arxivId": "1707.02657",
      "authors": [
        {
          "name": "Bertaglia, Thales F. C."
        },
        {
          "name": "Brum, Henrico B."
        },
        {
          "name": "Corrêa Jr, Edilson A."
        },
        {
          "name": "Marinho, Vanessa Q."
        },
        {
          "name": "Santos, Leandro B. dos"
        },
        {
          "name": "Treviso, Marcos V."
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/466414045"
      ],
      "createdDate": "2017-07-23T18:37:55",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2017-10-01T00:00:00",
      "abstract": "The enormous amount of texts published daily by Internet users has fostered\nthe development of methods to analyze this content in several natural language\nprocessing areas, such as sentiment analysis. The main goal of this task is to\nclassify the polarity of a message. Even though many approaches have been\nproposed for sentiment analysis, some of the most successful ones rely on the\navailability of large annotated corpus, which is an expensive and\ntime-consuming process. In recent years, distant supervision has been used to\nobtain larger datasets. So, inspired by these techniques, in this paper we\nextend such approaches to incorporate popular graphic symbols used in\nelectronic messages, the emojis, in order to create a large sentiment corpus\nfor Portuguese. Trained on almost one million tweets, several models were\ntested in both same domain and cross-domain corpora. Our methods obtained very\ncompetitive results in five annotated corpora from mixed domains (Twitter and\nproduct reviews), which proves the domain-independent property of such\napproach. In addition, our results suggest that the combination of emoticons\nand emojis is able to properly capture the sentiment of a message.Comment: Accepted for publication in BRACIS 201",
      "documentType": "research",
      "doi": "10.1109/bracis.2017.45",
      "downloadUrl": "http://arxiv.org/abs/1707.02657",
      "fieldOfStudy": null,
      "fullText": "PELESent: Cross-domain polarity classification\nusing distant supervision\nEdilson A. Correˆa Jr, Vanessa Q. Marinho, Leandro B. dos Santos,\nThales F. C. Bertaglia, Marcos V. Treviso, Henrico B. Brum\nInstitute of Mathematics and Computer Science\nUniversity of Sa˜o Paulo (USP)\nSa˜o Carlos, Sa˜o Paulo, Brazil\nEmail: {edilsonacjr,vanessaqm,leandrobs,thales.bertaglia,marcostreviso,henrico.brum}@usp.br\nAbstract—The enormous amount of texts published daily by\nInternet users has fostered the development of methods to analyze\nthis content in several natural language processing areas, such\nas sentiment analysis. The main goal of this task is to classify the\npolarity of a message. Even though many approaches have been\nproposed for sentiment analysis, some of the most successful ones\nrely on the availability of large annotated corpus, which is an\nexpensive and time-consuming process. In recent years, distant\nsupervision has been used to obtain larger datasets. So, inspired\nby these techniques, in this paper we extend such approaches to\nincorporate popular graphic symbols used in electronic messages,\nthe emojis, in order to create a large sentiment corpus for\nPortuguese. Trained on almost one million tweets, several models\nwere tested in both same domain and cross-domain corpora.\nOur methods obtained very competitive results in five annotated\ncorpora from mixed domains (Twitter and product reviews),\nwhich proves the domain-independent property of such approach.\nIn addition, our results suggest that the combination of emoticons\nand emojis is able to properly capture the sentiment of a message.\nI. INTRODUCTION\nIn the last few years, Sentiment Analysis has become a\nprominent field in natural language processing (NLP), mostly\ndue to its direct application in several real-world scenarios [1],\nsuch as product reviews, government intelligence, and the\nprediction of the stock markets. One of the main tasks in Sen-\ntiment Analysis is the polarity classification, i.e., classifying\ntexts into categories according to the emotions expressed on\nthem. In general, the classes are positive, negative and neutral.\nA popular application of polarity classification is in social\nmedia content. Microblogging and social networks websites,\nsuch as Twitter, have been used to express personal thoughts.\nAccording to Twitter’s website1, more than 500 million short\nmessages, known as tweets, are posted each day. The analysis\nof this type of content is particularly challenging due to its\nspecific language, which is mostly informal, with spelling\nerrors, out of the vocabulary words, as well as the usage of\nemoticons and emojis to express ideas and sentiments.\nMachine learning methods have been widely applied to\npolarity classification tasks in the context of social networks.\nThis is particularly evident in shared tasks such as the SemEval\n1https://business.twitter.com/en/basics.html\nSentiment Analysis tasks [2], [3], where these methods usually\noutperform lexical-based approaches. However, a major draw-\nback of machine learning is its high dependency on large an-\nnotated corpora, and since manual annotation usually is time-\nconsuming and expensive [4], many non-English languages\nlack this type of resource or, when existing, are very limited\nand specific, as it is the case for Portuguese.\nIn this paper, we adapt a distant supervision approach [5] to\nannotate a large number of tweets in Portuguese and use them\nto train state-of-the-art methods for polarity classification. We\napplied these methods in manually annotated corpora from the\nsame domain (Twitter) and cross-domain (product reviews).\nThe obtained results indicate that the proposed approach is\nwell suited for both: same domain and cross-domain. More-\nover, it is a powerful alternative to produce sentiment analysis\ncorpora with less effort than manual annotation.\nThis paper is organized as follows. Section II gives a\nbrief overview of some approaches for sentiment analysis and\npresents some works that have applied distant supervision\nto this task. Our approach is described in Section III. The\nevaluation corpora, machine learning algorithms and results\nare given in Section IV. Finally, our conclusions are drawn in\nSection V.\nII. RELATED WORK\nCurrently, methods devised to perform sentiment analysis\nand, more specifically, polarity classification range from ma-\nchine learning to lexical-based approaches. While machine\nlearning methods have proved useful in scenarios where a large\namount of training data is available along with top quality\nNLP resources (such as taggers, parsers and others), they\nusually have low performance in opposite scenarios. Since\nmost non-English languages face resource limitations, for\nexample Portuguese, lexical-based approaches have become\nvery popular. Some works following this line are [6]–[8].\nAnother alternative for languages with fewer resources is\nthe use of hybrid systems, which combine machine learning\nand lexical-based methods. Avanc¸o et al. [9] showed that\nthis combination outperforms both individual approaches. This\nmay imply that the development of better individual elements\nwill lead to better results in the final combination.\nar\nX\niv\n:1\n70\n7.\n02\n65\n7v\n1 \n [c\ns.C\nL]\n  9\n Ju\nl 2\n01\n7\nMachine learning approaches rely on document representa-\ntions, normally vectorial ones with features like n-grams [1],\na simple example is the bag-of-words model. Once a repre-\nsentation has been chosen, several classification methods are\navailable, such as Support Vector Machines (SVM), Naive\nBayes (NB), Maximum Entropy (MaxEnt), Conditional Ran-\ndom Fields (CRF), and ensembles of classifiers [3].\nApart from the traditional features, such as n-grams, some\nresearchers have taken advantage of word embeddings, which\nare known to capture some linguistic properties, such as se-\nmantic and syntactic features. A well-known example of word\nembeddings is Word2Vec [10], [11]. Algebraic operations, such\nas sum or average, can be applied to convert word vectors\ninto a sentence or document vector [12], [13]. However, this\nrepresentation does not consider the order of the words in the\nsentence.\nParagraph vectors [14] (also known as Doc2Vec) can be\nunderstood as a generalization of Word2Vec for larger blocks\nof text, such as paragraphs or documents. This technique has\nobtained state-of-the-art results on sentiment analysis for two\ndatasets of movie reviews [14]. The main goal of these dense\nrepresentations is to predict the words in those blocks. Two\nmodels were proposed by Le and Mikolov [14], in which one\nof them accounts for the word order.\nIn addition, deep neural networks also consider the word\norder. Their methods have achieved good results in sentiment\nanalysis, as shown in [15]–[17] and in the SemEval Sentiment\nAnalysis Tasks [2], [3]. Nevertheless, these approaches need\nlarge datasets for training. Distant Supervision is a good\nalternative to obtain these datasets for the training/pre-training\nof deep neural networks [16], [18], [19].\nDistant supervision is an alternative to create large datasets\nwithout the need of manual annotation. Some works have\nreported the use of emoticons as semantic indicator for sen-\ntiment [5], [18], [20], [21], while others use emoticons and\nhashtags for the same purpose [22], [23]. Go et al. [5], the first\nwork to apply distant supervision to Twitter data, collected\napproximately 1.6 million of tweets containing positive and\nnegative emoticons – e.g. “:)” and “:(” – equally distributed\ninto two classes. They combined sets of features – unigrams,\nbigrams, part-of-speech (POS) tags – in order to train machine\nlearning algorithms (NB, MaxEnt and SVM) and evaluate\nthose in manually annotated datasets. The best accuracy was\nachieved using unigram and bigram as features for a MaxEnt\nclassifier.\nSeveryn and Moschitti [18] used Distant Supervision to pre-\ntrain a Convolutional Neural Network (CNN). An architecture\nsimilar to the one proposed by Kim [17]. The network is\ncomposed of a first layer to convert words in dense vec-\ntors, following a single convolutional layer with a non-linear\nactivation function, max pooling and soft-max. Deriu et al.\n[19] used a combination of 2 CNNs with a Random Forest\nclassifier. However, this approach did not obtain improvements\nwith distant supervision.\nDespite the numerous studies and investigations of different\ntechniques and methods for polarity classification, the problem\nTABLE I\nALL EMOTICONS USED TO REPRESENT EMOTION.\nPositive Negative\n:) :-) :D =) :( :-(\nof relying on large annotated corpora remains open and the\ndifficulty is intensified in non-English languages. In this paper,\nour contributions are the adapted framework for building\npolarity classification corpus to Portuguese, the built corpus\nitself and an evaluation on different state-of-the-art methods\nusing this corpus, for same domain and cross-domain corpora.\nIII. APPROACH\nFollowing the approach of Go et al. [5], we initially col-\nlected a large amount of tweets in order to create the distant\nsupervision corpus. Only tweets in Portuguese were crawled,\nand no specific queries were employed. In total, 41 million of\ntweets were collected.\nAfter collecting the tweets, the next step was to split them\ninto positive and negative classes. In order to do so, we\nused lists of emojis and emoticons selected according to the\nsentiment conveyed by them. Therefore, the polarity of a tweet\nis determined by the presence of emojis and emoticons in\nit – if it only contains positive ones (from the positive list),\nits polarity is assigned as positive. If a tweet contains both\npositive and negative elements, it is discarded since it is likely\nto be ambiguous. Following this idea, we used the same list of\nemoticons used by Go et al. [5], which is presented in Table I.\nGo et al. [5] did not use emojis, but these graphic symbols\nare also employed to convey ideas and sentiments [24]. In\ncontrast to the small set of emoticons, there are hundreds of\npossible emojis. Therefore, we selected a representative list\nwith positive and negative emojis. All the emojis conveying\npositive emotion are presented in Fig. 1. Fig. 2 illustrates the\nselected ones with negative emotion.\nAfter filtering the tweets by the aforementioned criteria, we\nobtained a labeled corpus comprising 554,623 positive tweets\nand 425,444 negative ones. This corpus was used to train the\nmachine learning methods. It is important to highlight that\nemojis and emoticons were removed from the tweets in the\nfinal corpus, so that their presence as a sentiment indicator is\nnot learned by the models.\nIn addition to the filtering process, some preprocessing\nsteps were performed to improve the corpus quality. Details\nabout the preprocessing steps are given in the Supplementary\nMaterial, Section A. After these steps, tweets containing less\nthan 4 tokens were discarded from the corpus. The com-\nplete framework (tweets collection, filtering and preprocessing\nmethods) along with all experimental evaluation will be made\navailable 2.\n2https://github.com/edilsonacjr/pelesent\n(a) (b) (c) (d) (e) (f) (g)\nFig. 1. All emojis used to represent positive emotion. Their respective\nunicodes are: (a) U+1F60A, (b) U+1F60B, (c) U+1F60D, (d) U+1F603, (e)\nU+1F606, (f) U+1F600, and (g) U+1F61D.\n(a) (b) (c) (d) (e) (f)\n(g) (h) (i) (j) (k) (l)\nFig. 2. All emojis used to represent negative emotion. Their respective\nunicodes are: (a) U+1F620, (b) U+1F627, (c) U+1F61E, (d) U+1F628,\n(e) U+1F626, (f) U+1F623, (g) U+1F614, (h) U+1F629, (i) U+1F612, (j)\nU+1F621, (k) U+2639, and (l) U+1F61F.\nIV. EXPERIMENTAL EVALUATION\nIn order to evaluate the quality of the corpus built using\ndistant supervision, we trained state-of-the-art methods for\npolarity classification and applied the learned models to 5 well\nknown manually annotated sentiment corpora. In the follow-\ning, we present these corpora along with the message polarity\nclassification methods, and finally, the obtained results.\nA. Corpora\nSentiment classifiers are usually trained on manually anno-\ntated corpora. Because sentiments may be expressed differ-\nently in different domains [4], it is common to create domain-\nspecific corpus. Since we intend to create a robust and generic\ncorpus that is not domain-specific, we selected 5 corpora for\nevaluation, 2 being from the same domain (Twitter) and 3 from\na different domain (product reviews). Below, we present the\ncorpora that were used.\na) Brazilian Presidential Election [25]: This dataset is\nformed by tweets about the Brazilian presidential election run\nin 2010. The corpus is divided in two parts, one referencing\nDilma Rousseff (BPE-Dilma) and the other Jose´ Serra (BPE-\nSerra), both being the most popular candidates in the election.\nThe corpora were manually annotated in positive and negative,\nand used to evaluate stream based sentiment analysis systems.\nb) Buscape´ [26]: This dataset is formed by product\nreviews extracted from Buscape´ website3. The documents were\nautomatically labeled based on two informations given by the\nusers. The first (Buscape-1) is based on a recommendation tag\nwhile the second (Buscape-2) is based on a 5-star scale (1-2\nstars for negative and 4-5 stars for positive). Both corpora\nare balanced between the two classes, even though there is a\nnotable difference on their sizes, possibly due to the low use\nof the recommendation tag.\n3http://www.buscape.com.br/\nc) Mercado Livre [9]: Similar to the Buscape´ dataset,\nthis corpus is formed by product reviews from the online\nmarketplace Mercado Livre4. The corpus was also automat-\nically annotated based on a 5-star scale given by the authors\nof the reviews. The dataset is balanced between the positive\nand negative classes.\nTable II presents a summary of the corpora.\nTABLE II\nDATASETS USED IN THE EVALUATION OF THE SYSTEM.\nDataset Total Positive Negative\nBPE-Dilma 66, 640 46, 805 19, 835\nBPE-Serra 9, 718 1, 371 8, 347\nBuscape-1 2, 000 1, 000 1, 000\nBuscape-2 13, 685 6, 873 6, 812\nMercado Livre 43, 318 21, 819 21, 499\nB. Machine Learning Methods\nMachine learning has dominated the area of sentiment\nanalysis, mostly because its high performance when manually\nannotated data is available. However, thanks to the great\nvariety of methods, there is no consensus about which method\nis the best in this scenario. In the last editions of SemEval Sen-\ntiment Analysis Task, most of the best methods/systems used\ndeep learning techniques [2], [3]. In this work, the evaluated\nmethods range from simple linear models for classification\nusing vector space models to hybrid (machine learning and\nlexical-based) and Deep Learning methods. The idea was to\nthoroughly evaluate the quality of the corpus regardless of\nthe technique being used for learning. Below, each method is\nbriefly described.\na) Logistic Regression (LR): Also known as logit re-\ngression, LR can be understood as a generalization of linear\nregression models to the binary classification scenario, where\na sigmoid function outputs the class probabilities [27]. In\nthis paper, the logistic regression model predicts the class\nprobabilities of a text, where the classes are ”positive” and\n”negative”. As input for this classifier, three text representa-\ntions were used: (1) a bag-of-words model (LR+tfidf), where\neach document (tweet or review) is represented by its set of\nwords weighted by tf-idf [28]; (2) a word embeddings based\nmodel (LR+w2v), where each document is represented by\nthe weighted average of the embedding vectors (generated by\nWord2Vec [10], [11]) of the words that compose the document,\nthe weights are defined by tf-idf ; (3) the Paragraph Vector\nmodel (LR+d2v), which uses a neural network to generate\nembeddings for words and documents simultaneously in an\nunsupervised manner. Only the vectorial representations of\ndocuments were used by the classifier.\nb) Convolutional Neural Networks (CNNs): With the\npopularity of deep learning, CNNs have been applied to many\ndifferent contexts, including several NLP tasks [29] and, more\nspecifically, sentiment analysis [16]–[19]. Our CNN is similar\n4http://www.mercadolivre.com.br/\nto the architecture proposed by Kim [17], which uses a single\nconvolutional layer. In this architecture, the network receives\nas input a matrix representing the document, and each word\nin the document is represented by a dense continuous vector.\nThe output of the network is the probability of a document\nbeing negative or positive.\nc) Recurrent Convolutional Neural Networks (RCNNs):\nThis deep neural architecture uses both convolutional and re-\ncurrent layers. Recently explored by many works in NLP [30]–\n[32], this architecture has been successfully applied to senti-\nment analysis [3], [32]. Our architecture consists of a slight\nmodification of the one used by Treviso et al. [31], where the\nfinal layer returns the probability for the whole document, in-\ndicating a positive/negative polarity. Using this combination of\nconvolutional and recurrent layers, we explored the principle\nthat nearby words have a greater influence in the classification,\nwhile distant words may also have some impact.\nd) Hybrid: This method is a combination of two clas-\nsifiers previously used for sentiment classification in cross-\ndomain corpora [9] and follows the same setting introduced\nby Avanc¸o [33]. The method consists of a SVM classifier\ncombined with a lexical-based approach. The documents are\nrepresented by arrays of features including a binary bag-\nof-words (presence/absence of terms), emoticons, sentiment\nwords and POS tags. Documents located near the separation\nhyperplane (in a threshold assumed as 0.5) learned by the\nSVM are considered to be uncertain. Those documents are\nthen classified with a lexical-based approach, that uses lin-\nguistic rules for polarity classification in Portuguese.\nFor all methods, well-known machine learning libraries\nwere used, such as Scikit-learn [34] and Keras [35]. Partic-\nularities such as parameters, details about the architecture,\ninitializations and others can be found in the Supplementary\nMaterial, Section B.\nC. Results and Discussion\nTo evaluate and compare the methods in each corpus, F1\nscore (macro-averaged), recall (macro-averaged) and accuracy\nwere chosen, mostly because of their traditional use in senti-\nment analysis [2], [3].\nThe main results are shown in Table III. Along with the\nresults of each polarity classification method, we present the\nstate-of-the-art (SotA) result reported for each corpus. Because\nthe BPE corpora were conceived for a different context, there\nare no SotA reported results for those corpora. We also ranked\neach evaluated method by its F1 score.\nThe differences between the best method (in bold) and\nthe SotA vary between 9.69% and 12.24%, very competitive\nresults given the fact that all SotA reported results were\nobtained by a 10-fold cross validation scheme and our methods\nused a corpus from a different domain for training. Of all the\nmethods, the Hybrid was the one that had the best performance\nin the corpora of product reviews. Such a result was due to the\nregularity of the language in this type of corpus, which makes\nlexical approaches highly effective. However, in domains such\nas Twitter, errors, abbreviations and slangs are very common,\nwhich decreases the effectiveness of lexical-based approaches.\nThis effect can be seen in the BPE-Dilma corpus.\nAn important aspect of Sentiment Analysis is the sensitivity\nof its methods to elements such as domain and temporality.\nIn our evaluation, both were present in the selected corpora,\nwhich demonstrates the robustness of the constructed corpus\nand its resilience to temporality and the non-regularity of the\nlanguage.\nRegarding the deep learning methods (CNN and RCNN),\nboth presented high rankings in almost all corpora. However,\nthere was no huge difference between deep and shallow\nmethods (logistic regression + document representation), indi-\ncating that large datasets decrease the performance difference\nbetween methods from different natures (even between simple\nand complex methods), a result commonly found in the big\ndata era [36].\nTABLE III\nRESULTS OBTAINED BY EACH METHOD TRAINED ON THE DISTANT\nSUPERVISION CORPUS.\nDataset Method F1 score Recall Accuracy\nBPE-Dilma\nLR + w2v 0.57395 0.6037 0.5952\nLR + tfidf 0.64771 0.6443 0.7128\nLR + d2v 0.61354 0.6071 0.7256\nCNN 0.63373 0.6295 0.7051\nRCNN 0.64442 0.6586 0.6816\nHybrid 0.52496 0.5855 0.5295\nSotA − − −\nBPE-Serra\nLR + w2v 0.35156 0.4398 0.3915\nLR + tfidf 0.41105 0.5546 0.4475\nLR + d2v 0.50553 0.6028 0.5915\nCNN 0.42404 0.5929 0.4558\nRCNN 0.52862 0.5975 0.6426\nHybrid 0.57451 0.6073 0.7344\nSotA − − −\nBuscape-1\nLR + w2v 0.72324 0.7250 0.7250\nLR + tfidf 0.74693 0.7480 0.7480\nLR + d2v 0.64276 0.6465 0.6465\nCNN 0.67135 0.6870 0.6870\nRCNN 0.76542 0.7654 0.7654\nHybrid 0.76681 0.7695 0.7695\nSotA 0.8892 − 0.8894\nBuscape-2\nLR + w2v 0.68146 0.6903 0.6910\nLR + tfidf 0.77252 0.7738 0.7742\nLR + d2v 0.70175 0.7027 0.7030\nCNN 0.70484 0.7115 0.7122\nRCNN 0.76563 0.7658 0.7657\nHybrid 0.79171 0.7930 0.7934\nSotA 0.8935 − 0.8935\nMercado Livre\nLR + w2v 0.68616 0.7048 0.7066\nLR + tfidf 0.83283 0.8329 0.8328\nLR + d2v 0.80894 0.8093 0.8097\nCNN 0.77455 0.7800 0.7813\nRCNN 0.85612 0.8561 0.8563\nHybrid 0.86141 0.8614 0.8614\nSotA 0.9583 − 0.9583\nV. CONCLUSION AND FUTURE WORK\nIn recent years, the polarity classification task has drawn\nthe attention of the scientific community, mainly due to its\ndirect application in scenarios such as social media content\nand product reviews. Even though machine learning methods\npresent themselves as high performance alternatives, they\nsuffer from the need of a large amount of data during their\ntraining phases. In this paper, we adapted a distant supervision\napproach to build a large sentiment corpus for Portuguese.\nState-of-the-art methods were trained on this corpus and\napplied to 5 selected corpora, from same domain and different\ndomain (cross-domain). Competitive results were obtained for\nall methods, although our best results did not outperform the\nbest ones reported for the same corpora.\nAs future works, we intend to explore ways to improve\nthe quality of the distant supervision corpus by applying\ntechniques to remove outliers and tweets that do not convey\nany sentiment or convey the wrong sentiment. We also intend\nto modify this framework to make it able to represent the\nneutral class.\nACKNOWLEDGMENT\nE.A.C.J. acknowledges financial support from Google\n(Google Research Awards in Latin America grant) and CAPES\n(Coordination for the Improvement of Higher Education Per-\nsonnel). V.Q.M. acknowledges financial support from FAPESP\n(grant no. 15/05676-8). L.B.S. acknowledges financial support\nfrom Google (Google Research Awards in Latin America\ngrant) and CNPq (National Council for Scientific and Tech-\nnological Development, Brazil). T.F.C.B., M.V.T., and H.B.B.\nacknowledge financial support from CNPq. In part of this work\na GPU donated by NVIDIA Corporation was used.\nREFERENCES\n[1] B. Pang, L. Lee et al., “Opinion mining and sentiment analysis,”\nFoundations and Trends in Information Retrieval, vol. 2, no. 1–2, pp.\n1–135, 2008.\n[2] S. Rosenthal, P. Nakov, S. Kiritchenko, S. M. Mohammad, A. Ritter, and\nV. Stoyanov, “Semeval-2015 task 10: Sentiment analysis in twitter,” in\nProceedings of the 9th international workshop on semantic evaluation\n(SemEval 2015), 2015, pp. 451–463.\n[3] P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, and V. Stoyanov,\n“Semeval-2016 task 4: Sentiment analysis in twitter,” Proceedings of\nthe 10th international workshop on semantic evaluation (SemEval 2016),\npp. 1–18, 2016.\n[4] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen, “Cross-domain\nsentiment classification via spectral feature alignment,” in Proceedings\nof the 19th international conference on World wide web. ACM, 2010,\npp. 751–760.\n[5] A. Go, R. Bhayani, and L. Huang, “Twitter sentiment classification using\ndistant supervision,” CS224N Project Report, Stanford, vol. 1, no. 12,\n2009.\n[6] M. Souza, R. Vieira, D. Busetti, R. Chishman, I. M. Alves et al.,\n“Construction of a portuguese opinion lexicon from multiple resources,”\nin 8th Brazilian Symposium in Information and Human Language\nTechnology, 2011, pp. 59–66.\n[7] P. P. Balage Filho, T. Pardo, and S. Aluı´sio, “An evaluation of the Brazil-\nian Portuguese LIWC dictionary for sentiment analysis,” in Proceedings\nof the 9th Brazilian Symposium in Information and Human Language\nTechnology (STIL), S. M. Aluı´sio and V. D. Feltrim, Eds. Fortaleza-\nCE, Brazil: Sociedade Brasileira de Computac¸a˜o, 21–23 Oct. 2013, pp.\n215–219.\n[8] L. V. Avanc¸o and M. d. G. V. Nunes, “Lexicon-based sentiment analysis\nfor reviews of products in brazilian portuguese,” in Intelligent Systems\n(BRACIS), 2014 Brazilian Conference on. IEEE, 2014, pp. 277–281.\n[9] L. V. Avanc¸o, H. B. Brum, and M. G. Nunes, “Improving opinion\nclassifiers by combining different methods and resources,” in XIII\nEncontro Nacional de Inteligeˆncia Artificial e Computacional (ENIAC),\n2016, pp. 25–36.\n[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of\nword representations in vector space,” arXiv preprint arXiv:1301.3781,\n2013.\n[11] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,\n“Distributed representations of words and phrases and their composi-\ntionality,” in Advances in neural information processing systems, 2013,\npp. 3111–3119.\n[12] Y. Zhou, Z. Zhang, and M. Lan, “Ecnu at semeval-2016 task 4: An\nempirical investigation of traditional nlp features and word embedding\nfeatures for sentence-level and topic-level sentiment analysis in twitter,”\nProceedings of SemEval, pp. 256–261, 2016.\n[13] E. A. Correˆa Jr, V. Q. Marinho, and L. B. d. Santos, “Nilc-usp at\nsemeval-2017 task 4: A multi-view ensemble for twitter sentiment\nanalysis,” Proceedings of the 11th International Workshop on Semantic\nEvaluation (SemEval’17), 2017.\n[14] Q. Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” in Proceedings of the 31st International Conference on\nMachine Learning (ICML-14), T. Jebara and E. P. Xing, Eds., 2014,\npp. 1188–1196.\n[15] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y.\nNg, C. Potts et al., “Recursive deep models for semantic composition-\nality over a sentiment treebank,” in Proceedings of the conference on\nempirical methods in natural language processing (EMNLP), vol. 1631.\nCiteseer, 2013, p. 1642.\n[16] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional\nneural network for modelling sentences,” in Proceedings of the 52nd\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers). Baltimore, Maryland: Association for\nComputational Linguistics, June 2014, pp. 655–665. [Online]. Available:\nhttp://www.aclweb.org/anthology/P14-1062\n[17] Y. Kim, “Convolutional neural networks for sentence classification,”\nin Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP). Doha, Qatar: Association\nfor Computational Linguistics, October 2014, pp. 1746–1751.\n[18] A. Severyn and A. Moschitti, “Unitn: Training deep convolutional neural\nnetwork for twitter sentiment classification,” in Proceedings of the\n9th International Workshop on Semantic Evaluation (SemEval 2015),\nAssociation for Computational Linguistics, Denver, Colorado, 2015, pp.\n464–469.\n[19] J. Deriu, M. Gonzenbach, F. Uzdilli, A. Lucchi, V. De Luca, and\nM. Jaggi, “Swisscheese at semeval-2016 task 4: Sentiment classifica-\ntion using an ensemble of convolutional neural networks with distant\nsupervision,” Proceedings of SemEval, pp. 1124–1128, 2016.\n[20] J. Read, “Using emoticons to reduce dependency in machine learning\ntechniques for sentiment classification,” in Proceedings of the ACL\nstudent research workshop. Association for Computational Linguistics,\n2005, pp. 43–48.\n[21] A. Pak and P. Paroubek, “Twitter as a corpus for sentiment analysis and\nopinion mining.” in LREc, vol. 10, no. 2010, 2010.\n[22] D. Davidov, O. Tsur, and A. Rappoport, “Enhanced sentiment learning\nusing twitter hashtags and smileys,” in Proceedings of the 23rd inter-\nnational conference on computational linguistics: posters. Association\nfor Computational Linguistics, 2010, pp. 241–249.\n[23] E. Kouloumpis, T. Wilson, and J. Moore, “Twitter sentiment analysis:\nThe good the bad and the omg,” in In The International Association\nfor the Advancement of Artificial Intelligence Conference on Weblogs\nand Social. Association for the Advancement of Artificial Intelligence,\n2011.\n[24] P. K. Novak, J. Smailovic´, B. Sluban, and I. Mozeticˇ, “Sentiment of\nemojis,” PloS one, vol. 10, no. 12, p. e0144296, 2015.\n[25] I. S. Silva, J. Gomide, A. Veloso, W. Meira Jr, and R. Ferreira, “Effective\nsentiment stream analysis with self-augmenting training and demand-\ndriven projection,” in Proceedings of the 34th international ACM SIGIR\nconference on Research and development in Information Retrieval.\nACM, 2011, pp. 475–484.\n[26] N. S. Hartmann, L. V. Avanc¸o, P. P. Balage Filho, M. S. Duran, M. D.\nG. V. Nunes, T. A. S. Pardo, S. M. Aluisio et al., “A large corpus of\nproduct reviews in portuguese: Tackling out-of-vocabulary words,” in\nInternational Conference on Language Resources and Evaluation, 9th.\nEuropean Language Resources Association-ELRA, 2014.\n[27] K. P. Murphy, Machine Learning: A Probabilistic Perspective. The\nMIT Press, 2012.\n[28] G. Salton, “Automatic text processing: The transformation, analysis, and\nretrieval of,” Reading: Addison-Wesley, 1989.\n[29] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\nP. Kuksa, “Natural language processing (almost) from scratch,” Journal\nof Machine Learning Research, vol. 12, no. Aug, pp. 2493–2537, 2011.\n[30] N. Kalchbrenner and P. Blunsom, “Recurrent convolutional neural net-\nworks for discourse compositionality,” arXiv preprint arXiv:1306.3584,\n2013.\n[31] M. V. Treviso, C. Shulby, and S. M. Aluı´sio, “Sentence segmentation\nin narrative transcripts from neuropsychological tests using recurrent\nconvolutional neural networks,” in Proceedings of the 15th Conference of\nthe European Chapter of the Association for Computational Linguistics:\nVolume 1, Long Papers, 2017, pp. 315–325.\n[32] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural\nnetworks for text classification.” in AAAI, vol. 333, 2015, pp. 2267–\n2273.\n[33] L. V. Avanc¸o, “Sobre normalizac¸a˜o e classificac¸a˜o de polaridade de\ntextos opinativos na web,” Master’s thesis, Universidade de Sa˜o Paulo,\n2015.\n[34] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,\n“Scikit-learn: Machine learning in python,” Journal of Machine Learning\nResearch, vol. 12, no. Oct, pp. 2825–2830, 2011.\n[35] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.\n[36] A. Halevy, P. Norvig, and F. Pereira, “The unreasonable effectiveness\nof data,” IEEE Intelligent Systems, vol. 24, no. 2, pp. 8–12, 2009.\n[37] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural networks\nfrom overfitting.” Journal of Machine Learning Research, vol. 15, no. 1,\npp. 1929–1958, 2014.\n[38] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\nSUPPLEMENTARY MATERIAL\nA. Preprocessing\nIn order to properly tokenize and preprocess the tweets, the\nfollowing steps were performed:\n• Punctuation marks forming an emoticon were considered\nas a single token (e.g. :-( and ;) )\n• Sequences of consecutive emojis were split so that each\nemoji formed a single token\n• Additional punctuation marks (not forming any emoticon)\nwere removed\n• Usernames (an @ symbol followed by up to 15 charac-\nters) were replaced by the tag USERNAME\n• Hashtags (a # symbol followed by any sequence of\ncharacters) were replaced by the tag HASHTAG\n• URLs were completely replaced by the tag URL\n• Numbers, including dates, telephone numbers and cur-\nrency values were replaced by the tag NUMBER\n• Subsequent character repetitions were limited to 3 – i.e.,\nall sequences of the same character were trimmed to fit\nthe limit of 3\nThe following tweet is used to illustrate the preprocessing\nsteps:\nOriginal: hj tenho aula de manha˜, tarde e noite. das 8h ate\n19h :(( #cansado\nPreprocessed: hj tenho aula de manha˜ tarde e noite das\nNUMBER ate NUMBER :(( HASHTAG\nB. Details about the machine learning methods\na) Logistic Regression: There is no additional informa-\ntion about this classifier.\nb) Convolutional Neural Networks: The complete archi-\ntecture is presented in Figure 3, where the input layer is a\nmatrix composed by n input words, and each word is a d\ndimensionality real vector. The convolutional layer receives\nthese vectors as input and it is responsible for the automatic\nextraction of nf features depending on a sliding window of\nlength h = {3, 4, 5}. The output from the convolutional layer\nis then passed to the max-overtime pooling layer, and the\nnew extracted features are concatenated. This results in a\nlarge dimensional vector that is passed to a fully connected\nlayer, where the softmax operation [27] is applied, returning\nthe probability of a document being negative or positive. In\nthe penultimate layer, we employed dropout with a constraint\non l2-norms of the weight vectors to reduce the chance of\noverfitting [37].\nN x d\ninput max-poolingCNN with multiple filter\nmaps\nwidths and features\nFig. 3. CNN architecture adapted from [17].\nc) Recurrent Convolutional Neural Networks: The com-\nplete architecture is illustrated in Figure 4. The architecture is\ncomposed by an input layer that has ϕ input features, and\neach feature has a dimensionality of d. The convolutional\nlayer is responsible for the automatic extraction of nf new\nfeatures depending on 3 neighboring words. Then, a max-\npooling operation is applied over time, looking at a region\nof hm = 3 elements to find the most significant features. The\nnew extracted features are fed into a recurrent bidirectional\nlayer which has nf units known as Long Short-Term Memory\n[38], which are able to learn over long dependencies between\nwords. Finally, the last recurrent state output is passed to a\ntotally connected layer, where the softmax operation [27] is\ncalculated, giving the probability of a document being negative\nor positive. Between these two layers, dropout is used to\nreduce the chance of overfitting [37].\nFor both neural network models (CNN and RCNN), we em-\nployed the early stopping strategy (p = 3) to avoid overfitting,\ni.e. the training phase finishes when the validation loss has\nstopped improving. Other experimental settings for CNN and\nRCNN (number of epochs, batch size, learning rate, etc.) can\nbe seen in their original papers [17], [31], respectively.\nd) Hybrid: The two methods combined to classify a\ndocument in polarity classes are described below:\nFig. 4. RCNN architecture adapted from [31].\n• SVM classifier: The SVM employed uses a RBF Ker-\nnel (gamma = 1/n features), C defined as 0.25, and L1\npenalty for regularization.\n• Lexical-based classifier: Each word present in a sentiment\nlexicon receives a value according to its polarity. Positive\nwords are valued as 1 and negative ones as −1. The\npresence of an intensification word (e.g. muito, demais) in\na window around the word multiplies its value by 3. The\npresence of a downtoner divides the current value by 3. A\nnegation multiplies the value of a word by −1, inverting\nits polarity. Whenever a negation is in the same window\nas an intensification, it becomes a downtoner (e.g. na˜o\nmuito), the same occurs with a downtoner (na˜o pouco).\nThe polarity values are then summed up to determine the\ndocument polarity.\n",
      "id": 43120027,
      "identifiers": [
        {
          "identifier": "1707.02657",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "info:doi/10.1109%2fbracis.2017.45",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1109/bracis.2017.45",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1707.02657",
          "type": "OAI_ID"
        },
        {
          "identifier": "2735427346",
          "type": "MAG_ID"
        },
        {
          "identifier": "84329379",
          "type": "CORE_ID"
        },
        {
          "identifier": "466414045",
          "type": "CORE_ID"
        }
      ],
      "title": "PELESent: Cross-domain polarity classification using distant supervision",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2735427346",
      "oaiIds": [
        "info:doi/10.1109%2fbracis.2017.45",
        "oai:arxiv.org:1707.02657"
      ],
      "publishedDate": "2017-07-09T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1707.02657"
      ],
      "updatedDate": "2024-02-26T18:26:18",
      "yearPublished": 2017,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1707.02657"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/43120027"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "1810.12897",
      "authors": [
        {
          "name": "Bhatia, Sumit"
        },
        {
          "name": "P, Deepak"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Alexandra",
        "Sumit"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/161509270",
        "https://api.core.ac.uk/v3/outputs/470618836"
      ],
      "createdDate": "2018-10-31T16:11:25",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 289,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/289",
          "logo": "https://api.core.ac.uk/data-providers/289/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Ideological leanings of an individual can often be gauged by the sentiment\none expresses about different issues. We propose a simple framework that\nrepresents a political ideology as a distribution of sentiment polarities\ntowards a set of topics. This representation can then be used to detect\nideological leanings of documents (speeches, news articles, etc.) based on the\nsentiments expressed towards different topics. Experiments performed using a\nwidely used dataset show the promise of our proposed approach that achieves\ncomparable performance to other methods despite being much simpler and more\ninterpretable.Comment: Presented at EMNLP Workshop on Computational Approaches to\n  Subjectivity, Sentiment & Social Media Analysis, 201",
      "documentType": "research",
      "doi": "10.18653/v1/w18-6212",
      "downloadUrl": "https://core.ac.uk/download/161509270.pdf",
      "fieldOfStudy": null,
      "fullText": "ar\nX\niv\n:1\n81\n0.\n12\n89\n7v\n1 \n [c\ns.C\nL]\n  3\n0 O\nct \n20\n18\nTopic-Specific Sentiment Analysis Can Help Identify Political\nIdeology\nSumit Bhatia\nIBM Research AI\nNew Delhi, India\nsumitbhatia@in.ibm.com\nDeepak P\nQueen’s University Belfast\nBelfast, UK\ndeepaksp@acm.org\nAbstract\nIdeological leanings of an individual can of-\nten be gauged by the sentiment one ex-\npresses about different issues. We pro-\npose a simple framework that represents a\npolitical ideology as a distribution of sen-\ntiment polarities towards a set of topics.\nThis representation can then be used to\ndetect ideological leanings of documents\n(speeches, news articles, etc.) based on the\nsentiments expressed towards different top-\nics. Experiments performed using a widely\nused dataset show the promise of our pro-\nposed approach that achieves comparable\nperformance to other methods despite be-\ning much simpler and more interpretable.\n1 Introduction\nThe ideological leanings of a person within\nthe left-right political spectrum are often re-\nflected by how one feels about different top-\nics and by means of preferences among vari-\nous choices on particular issues. For example,\na left-leaning person would prefer nationaliza-\ntion and state control of public services (such\nas healthcare) where privatization would be of-\nten preferred by people that lean towards the\nright. Likewise, a left-leaning person would of-\nten be supportive of immigration and will of-\nten talk about immigration in a positive man-\nner citing examples of benefits of immigration\non a country’s economy. A right-leaning per-\nson, on the other hand, will often have a neg-\native opinion about immigration.\nMost of the existing works on political\nideology detection from text have focused\non utilizing bag-of-words and other syntac-\ntic features to capture variations in lan-\nguage use (Sim et al., 2013; Biessmann, 2016;\nIyyer et al., 2014). We propose an alterna-\ntive mechanism for political ideology detection\nbased on sentiment analysis. We posit that ad-\nherents of a political ideology generally have\nsimilar sentiment toward specific topics (for\nexample, right wing followers are often posi-\ntive towards free markets, lower tax rates, etc.)\nand thus, a political ideology can be represented\nby a characteristic sentiment distribution over\ndifferent topics (Section 3). This topic-specific\nsentiment representation of a political ideol-\nogy can then be used for automatic ideology\ndetection by comparing the topic-specific sen-\ntiments as expressed by the content in a doc-\nument (news article, magazine article, collec-\ntion of social media posts by a user, utterances\nin a conversation, etc.).\nIn order to validate our hypothesis, we con-\nsider exploiting the sentiment information to-\nwards topics from archives of political debates\nto build a model for identifying political ori-\nentation of speakers as one of right or left\nleaning, which corresponds to republicans and\ndemocrats respectively, within the context of\nUS politics. This is inspired by our observa-\ntion that the political leanings of debators are\noften expressed in debates by way of speakers’\nsentiments towards particular topics. Parlia-\nmentary or Senate debates often bring the ide-\nological differences to the centre stage, though\nsomewhat indirectly. Heated debates in such\nforums tend to focus on the choices proposed\nby the executive that are in sharp conflict\nwith the preference structure of the opposi-\ntion members. Due to this inherent tendency\nof parliamentary debates to focus on topics of\ndisagreement, the sentiments exposited in de-\nbates hold valuable cues to identify the polit-\nical orientation of the participants.\nWe develop a simple classification model\nthat uses a topic-specific sentiment summa-\nrization for republican and democrat speeches\nseparately. Initial results of experiments con-\nducted using a widely used dataset of US\nCongress debates (Thomas et al., 2006) are\nencouraging and show that this simple model\ncompares well with classification models that\nemploy state-of-the-art distributional text rep-\nresentations (Section 4).\n2 Related Work\n2.1 Political Ideology Detection\nPolitical ideology detection has been a rela-\ntively new field of research within the NLP\ncommunity. Most of the previous efforts have\nfocused on capturing the variations in lan-\nguage use in text representing content of differ-\nent ideologies. Beissmann et al. (2016) employ\nbag-of-word features for ideology detection in\ndifferent domains such as speeches in German\nparliament, party manifestos, and facebook\nposts. Sim et al. (2013) use a labeled corpus\nof political writings to infer lexicons of cues\nstrongly associated with different ideologies.\nThese “ideology lexicons” are then used to an-\nalyze political speeches and identify their ide-\nological leanings. Iyyer at al. (2014) recently\nadopted a recursive neural network architec-\nture to detect ideological bias of single sen-\ntences. In addition, topic models have also\nbeen used for ideology detection by identify-\ning latent topic distributions across different\nideologies (Lin et al., 2008; Ahmed and Xing,\n2010). Gerrish and Blei (2011) connected text\nof the legislations to voting patterns of legis-\nlators from different parties.\n2.2 Sentiment Analysis for\nControversy Detection\nSentiment analysis has proved to be a use-\nful tool in detecting controversial topics as\nit can help identify topics that evoke differ-\nent feelings among people on opposite side\nof the arguments. Mejova et al. (2014) ana-\nlyzed language use in controversial news ar-\nticles and found that a writer may choose to\nhighlight the negative aspects of the opposing\nview rather than emphasizing the positive as-\npects of ones view. Lourentzou et al. (2015)\nutilize the sentiments expressed in social me-\ndia comments to identify controversial por-\ntions of news articles. Given a news article\nand its associated comments on social media,\nthe paper links comments with each sentence\nof the article (by using a sentence as a query\nand retrieving comments using BM25 score).\nFor all the comments associated with a sen-\ntence, a sentiment score is then computed, and\nsentences with large variations in positive and\nnegative comments are identified as controver-\nsial sentences. Choi et al. (2010) go one step\nfurther and identify controversial topics and\ntheir sub-topics in news articles.\n3 Using Topic Sentiments for\nIdeology Detection\nLet D = {. . . , d, . . .} be a corpus of political\ndocuments such as speeches or social media\npostings. Let L = {. . . , l, . . .} be the set of\nideology class labels. Typical scenarios would\njust have two class labels (i.e., |L| = 2), but we\nwill outline our formulation for a general case.\nFor document d ∈ D, ld ∈ L denotes the class\nlabel for that document. Our method relies\non the usage of topics, each of which are most\ncommonly represented by a probability distri-\nbution over the vocabulary. The set of topics\nover D, which we will denote using T , may be\nidentified using a topic modeling method such\nas LDA (Blei et al., 2003) unless a pre-defined\nset of handcrafted topics is available.\nGiven a document d and a topic t, our\nmethod relies on identifying the sentiment as\nexpressed by content in d towards the topic t.\nThe sentiment could be estimated in the form\nof a categorical label such as one of positive,\nnegative and neutral (Haney, 2013). Within\nour modelling, however, we adopt a more fine-\ngrained sentiment labelling, whereby the sen-\ntiment for a topic-document pair is a prob-\nability distribution over a plurality of ordi-\nnal polarity classes ranging from strongly pos-\nitive to strongly negative. Let sdt repre-\nsent the topic-sentiment polarity vector of\nd towards t such that sdt(x) represents the\nprobability of the polarity class x. Combin-\ning the topic-sentiment vectors for all top-\nics yields a document-specific topic-sentiment\nmatrix (TSM) as follows:\nSd,T =\n\n\n. . . sdt1(x) . . .\n. . . sdt2(x) . . .\n...\n...\n...\n\n (1)\nEach row in the matrix corresponds to a\ntopic within T , with each element quantifying\nthe probability associated with the sentiment\npolarity class x for the topic t within document\nd. The topic-sentiment matrix above may be\nregarded as a sentiment signature for the doc-\nument over the topic set T .\n3.1 Determining Topic-specific\nSentiments\nIn constructing TSMs, we make use of topic-\nspecific sentiment estimations as outlined\nabove. Typical sentiment analysis methods\n(e.g., NLTK Sentiment Analysis1) are de-\nsigned to determine the overall sentiment for\na text segment. Using such sentiment analysis\nmethods in order to determine topic-specific\nsentiments is not necessarily straightforward.\nWe adopt a simple keyword based approach for\nthe task. For every document-topic pair (t, d),\nwe extract the sentences from d that contain\nat least one of the top-k keywords associated\nwith the topic t. We then collate the sentences\nin the order in which they appear in d and\nform a mini-document dt. This document dt is\nthen passed on to a conventional sentiment an-\nalyzer that would then estimate the sentiment\npolarity as a probability distribution over sen-\ntiment polarity classes, which then forms the\nsdt(.) vector. We use k = 5 and the RNN\nbased sentiment analyzer (Socher et al., 2013)\nin our method.\n3.2 Nearest TSM Classification\nWe now outline a simple classification model\nthat uses summaries of TSMs. Given a la-\nbeled training set of documents, we would like\nto find the prototypical TSM corresponding\nto each label. This can be done by identify-\ning the matrix that minimizes the cumulative\ndeviation from those corresponding to the doc-\numents with the label.\nSl,T = argmin\nX\n∑\nd∈D∧ld=l\n||X − Sd,T ||\n2\nF (2)\nwhere ||M ||F denotes the Frobenius norm.\nIt turns out that such a label-specific signa-\nture matrix is simply the mean of the topic-\nsentiment matrices corresponding to docu-\n1http://text-processing.com/demo/sentiment/\nments that bear the respective label, which\nmay be computed using the below equation.\nSl,T =\n1\n|{d|d ∈ D ∧ ld = l}|\n∑\nd∈D∧ld=l\nSd,T\n(3)\nFor an unseen (test) document d′, we first\ncompute the TSM Sd′,T , and assign it the la-\nbel corresponding to the label whose TSM is\nmost proximal to Sd′,T .\nld′ = argmin\nl\n||Sd′,T − Sl,T ||\n2\nF (4)\n3.3 Logistic Regression Classification\nIn two class scenarios with label such as\n{left, right} or {democrat, republican} as we\nhave in our dataset, TSMs can be flattened\ninto a vector and fed into a logistic regression\nclassifier that learns weights - i.e., co-efficients\nfor each topic + sentiment polarity class com-\nbination. These weights can then be used to\nestimate the label by applying it to the new\ndocument’s TSM.\n4 Experiments\n4.1 Dataset\nWe used the publicly available Convote\ndataset2 (Thomas et al., 2006) for our exper-\niments. The dataset provides transcripts of\ndebates in the House of Representatives of the\nU.S Congress for the year 2005. Each file\nin the dataset corresponds to a single, unin-\nterrupted utterance by a speaker in a given\ndebate. We combine all the utterances of a\nspeaker in a given debate in a single file to\ncapture different opinions/view points of the\nspeaker about the debate topic. We call this\ndocument the view point document (VPD)\nrepresenting the speaker’s opinion about dif-\nferent aspects of the issue being debated. The\ndataset also provides political affiliations of all\nthe speakers – Republican (R), Democrat (D),\nand Independent (I). With there being only\nsix documents for the independent class (four\nin training, two in test), we excluded them\nfrom our evaluation. Table 1 summarizes the\nstatistics about the dataset and distribution\nof different classes. We obtained 50 topics\n2http://www.cs.cornell.edu/home/llee/data/convote.html\nTraining Set Test Set\nRepublican (R) 530 194\nDemocrat (D) 641 215\nTotal 1175 411\nTable 1: Distribution of different classes in the\nConVote dataset.\nMethod R D Total\nGloVe d2v 0.6391 0.6465 0.6430\nTSM-NC 0.6907 0.4558 0.5672\nTSM-LR 0.5258 0.7628 0.6504\nGloVe-d2v + TSM 0.5051 0.7023 0.6088\nTable 2: Results achieved by different methods on\nthe ideology classification task.\nusing LDA from Mallet3 run over the train-\ning dataset. The topic-sentiment matrix was\nobtained using the Stanford CoreNLP senti-\nment API4 (Manning et al., 2014) which pro-\nvides probability distributions over a set of five\nsentiment polarity classes.\n4.2 Methods\nIn order to evaluate our proposed TSM-based\nmethods - viz., nearest class (NC) and logistic\nregression (LR) - we use the following methods\nin our empirical evaluation.\n1. GloVe-d2v: We use pre-trained\nGloVe (Pennington et al., 2014) word\nembeddings to compute vector represen-\ntation of each VPD by averaging the\nGloVe vectors for all words in the doc-\nument. A logistic regression classifier is\nthen trained on the vector representations\nthus obtained.\n2. GloVe-d2v+TSM: A logistic regression\nclassifier trained on the GloVe features as\nwell as TSM features.\n4.3 Results\nTable 2 reports the classification results for\ndifferent methods described above. TSM-\nNC, the method that uses the TSMvectors\nand performs simple nearest class classifica-\ntion achieves an overall accuracy of 57%.\nNext, training a logistic regression classifier\n3http://mallet.cs.umass.edu/\n4https://nlp.stanford.edu/sentiment/code.html\ntrained on TSMvectors as features, TSM-LR,\nachieves significant improvement with an over-\nall accuracy of 65.04%. The word embed-\nding based baseline, the GloVe-d2v method,\nachieves slightly lower performance with an\noverall accuracy of 64.30%. However, we do\nnote that the per-class performance of GloVe-\nd2v method is more balanced with about\n64% accuracy for both classes. The TSM-LR\nmethod on the other hand achieves about 76%\nfor R class and only 52% for the D class. The\nresults obtained are promising and lend weight\nto out hypothesis that ideological leanings of\na person can be identified by using the fine-\ngrained sentiment analysis of the viewpoint a\nperson has towards different underlying topics.\n4.4 Discussion\nTowards analyzing the significance of the re-\nsults, we would like to start with drawing at-\ntention to the format of the data used in the\nTSM methods. The document-specific TSM\nmatrices do not contain any information about\nthe topics themselves, but only about the sen-\ntiment in the document towards each topic;\none may recollect that sdt(.) is a quantification\nof the strength of the sentiment in d towards\ntopic t. Thus, in contrast to distributional em-\nbeddings such as doc2vec, TSMs contain only\nthe information that directly relates to sen-\ntiment towards specific topics that are learnt\nfrom across the corpus. The results indicate\nthat TSM methods are able to achieve compa-\nrable performance to doc2vec-based methods\ndespite usage of only a small slice of informa-\ntiom. This points to the importance of senti-\nment information in determining the political\nleanings from text. We believe that leveraging\nTSMs along with distributional embeddings in\na manner that can combine the best of both\nviews would improve the state-of-the-art of po-\nlitical ideology detection.\nNext, we also studied if there are topics that\nare more polarizing than others and how differ-\nent topics impact classification performance.\nWe identified polarizing topics, i.e, topics that\ninvoke opposite sentiments across two classes\n(ideologies) by using the following equation.\ndist(t, R,D) = ||sR,t − sR,t||F (5)\nHere, sR,t and sD,t represent the sentiment\nMost polarizing topics\nH1: republican congress majority administration leadership n’t\nvote party republicans special\nH2: administration process vote work included find n’t true fix\ncarriers\nH3: health programs education funding million program cuts\ncare billion year\nH4: health insurance small care coverage businesses plans ahps\nemployees state\nH5: military center n’t students recruiters policy houston men\nuniversities colleges\nLeast polarizing topics\nL1: enter director march years response found letter criminal\npaid general\nL2: corps nuclear year energy projects committee project million\nfunding funds\nL3: osha safety workers commission health h.r employers\noccupational bills workplace\nL4: gun police industry lawsuits firearms dept chief\nmanufacturers dealers guns\nL5: medal gold medals individuals reagan history legislation\nronald king limiting\nTable 3: List of most polarizing (top) and least polarizing (bottom) topics as computed using equation 5.\nvectors for topic t for republican and democrat\nclasses. Note that these sentiment vectors are\nthe rows corresponding to topic t in TSMs for\nthe two classes, respectively.\nTable 3 lists the top five topics with most\ndistance, i.e., most polarizing topics (top) and\nfive topics with least distance, i.e.,least polar-\nizing topics (bottom) as computed by equa-\ntion 5. Note that the topics are represented us-\ning the top keywords that they contain accord-\ning to the probability distribution of the topic.\nWe observe that the most polarizing topics in-\nclude topics related to healthcare (H3, H4),\nmilitary programs (H5), and topics related to\nadministration processes (H1 and H2). The\nleast polarizing topics include topics related to\nworker safety (L3) and energy projects (L2).\nOne counter-intuitive observation is topic re-\nlated to gun control (L4) that is amongst the\nleast polarizing topics. This anomaly could\nbe attributed to only a few speeches related\nto this issue in the training set (only 23 out\nof 1175 speeches mention gun) that prevents\na reliable estimate of the probability distribu-\ntions. We observed similar low occurrences of\nother lower distance topics too indicating the\npotential for improvements in computation of\ntopic-specific sentiment representations with\nmore data. In fact, performing the nearest\nneighbor classification (TSM−NC) with only\ntop-10 most polarizing topics led to improve-\nments in classification accuracy from 57% to\n61% suggesting that with more data, better\nTSMrepresentations could be learned that are\nbetter at discriminating between different ide-\nologies.\n5 Conclusions\nWe proposed to exploit topic-specific senti-\nment analysis for the task of automatic ideol-\nogy detection from text. We described a sim-\nple framework for representing political ideolo-\ngies and documents as a matrix capturing sen-\ntiment distributions over topics and used this\nrepresentation for classifying documents based\non their topic-sentiment signatures. Empirical\nevaluation over a widely used dataset of US\nCongressional speeches showed that the pro-\nposed approach performs on a par with classi-\nfiers using distributional text representations.\nIn addition, the proposed approach offers sim-\nplicity and easy interpretability of results mak-\ning it a promising technique for ideology detec-\ntion. Our immediate future work will focus on\nfurther solidifying our observations by using a\nlarger dataset to learn better TSMs for differ-\nent ideologies. Further, the framework easily\nlends itself to be used for detecting ideological\nleanings of authors, social media users, news\nwebsites, magazines, etc. by computing their\nTSMs and comparing against the TSMs of dif-\nferent ideologies.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their valuable comments and sugges-\ntions that helped us improve the quality of this\nwork.\nReferences\nAmr Ahmed and Eric P. Xing. 2010. Staying in-\nformed: Supervised and semi-supervised multi-\nview topical analysis of ideological perspective.\nIn EMNLP, pages 1140–1150. ACL.\nFelix Biessmann. 2016. Automating political bias\nprediction. arXiv preprint arXiv:1608.02195.\nDavid M Blei, Andrew Y Ng, and Michael I Jor-\ndan. 2003. Latent dirichlet allocation. Journal\nof machine Learning research, 3(Jan):993–1022.\nYoonjung Choi, Yuchul Jung, and Sung-Hyon\nMyaeng. 2010. Identifying controversial issues\nand their sub-topics in news articles. In Intel-\nligence and Security Informatics, Pacific Asia\nWorkshop, PAISI 2010, Hyderabad, India, June\n21, 2010. Proceedings, volume 6122 of Lec-\nture Notes in Computer Science, pages 140–153.\nSpringer.\nSean Gerrish and David M. Blei. 2011. Predicting\nlegislative roll calls from text. In Proceedings of\nthe 28th International Conference on Machine\nLearning, ICML 2011, Bellevue, Washington,\nUSA, June 28 - July 2, 2011, pages 489–496.\nOmnipress.\nCarol Haney. 2013. Sentiment analysis: Provid-\ning categorical insight into unstructured textual\ndata. Social Media, Sociality, and Survey Re-\nsearch, pages 35–59.\nMohit Iyyer, Peter Enns, Jordan L. Boyd-Graber,\nand Philip Resnik. 2014. Political ideology de-\ntection using recursive neural networks. In ACL\n(1), pages 1113–1122. The Association for Com-\nputer Linguistics.\nWei-Hao Lin, Eric P. Xing, and Alexan-\nder G. Hauptmann. 2008. A joint topic\nand perspective model for ideological dis-\ncourse. In Machine Learning and Knowl-\nedge Discovery in Databases, European Confer-\nence, ECML/PKDD 2008, Antwerp, Belgium,\nSeptember 15-19, 2008, Proceedings, Part II,\nvolume 5212 of Lecture Notes in Computer Sci-\nence, pages 17–32. Springer.\nIsmini Lourentzou, Graham Dyer, Abhishek\nSharma, and ChengXiang Zhai. 2015. Hotspots\nof news articles: Joint mining of news text &\nsocial media to discover controversial points in\nnews. In Big Data, pages 2948–2950. IEEE.\nChristopher D. Manning, Mihai Surdeanu, John\nBauer, Jenny Rose Finkel, Steven Bethard, and\nDavid McClosky. 2014. The stanford corenlp\nnatural language processing toolkit. In ACL\n(System Demonstrations), pages 55–60. The As-\nsociation for Computer Linguistics.\nYelena Mejova, Amy X. Zhang, Nicholas Di-\nakopoulos, and Carlos Castillo. 2014. Contro-\nversy and sentiment in online news. CoRR,\nabs/1409.8152.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors\nfor word representation. In Proceedings of the\n2014 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2014, Oc-\ntober 25-29, 2014, Doha, Qatar, A meeting of\nSIGDAT, a Special Interest Group of the ACL,\npages 1532–1543. ACL.\nYanchuan Sim, Brice D. L. Acree, Justin H. Gross,\nand Noah A. Smith. 2013. Measuring ideological\nproportions in political speeches. In Proceedings\nof the 2013 Conference on Empirical Methods\nin Natural Language Processing, pages 91–101.\nAssociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Ja-\nson Chuang, Christopher D. Manning, Andrew\nNg, and Christopher Potts. 2013. Recursive\ndeep models for semantic compositionality over\na sentiment treebank. In Proceedings of the\n2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1631–1642, Seat-\ntle, Washington, USA. Association for Compu-\ntational Linguistics.\nMatt Thomas, Bo Pang, and Lillian Lee. 2006. Get\nout the vote: Determining support or opposition\nfrom congressional floor-debate transcripts. In\nEMNLP 2007, Proceedings of the 2006 Confer-\nence on Empirical Methods in Natural Language\nProcessing, 22-23 July 2006, Sydney, Australia,\npages 327–335. ACL.\n",
      "id": 54165030,
      "identifiers": [
        {
          "identifier": "186284280",
          "type": "CORE_ID"
        },
        {
          "identifier": "470618836",
          "type": "CORE_ID"
        },
        {
          "identifier": "161509270",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:arxiv.org:1810.12897",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:pure.qub.ac.uk/portal:publications/6539c915-cb4f-4072-ba7a-af542435daa5",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.18653/v1/w18-6212",
          "type": "DOI"
        },
        {
          "identifier": "1810.12897",
          "type": "ARXIV_ID"
        }
      ],
      "title": "Topic-Specific Sentiment Analysis Can Help Identify Political Ideology",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:arxiv.org:1810.12897",
        "oai:pure.qub.ac.uk/portal:publications/6539c915-cb4f-4072-ba7a-af542435daa5"
      ],
      "publishedDate": "2018-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://pure.qub.ac.uk/ws/files/158831815/wassa2018.pdf",
        "http://arxiv.org/abs/1810.12897"
      ],
      "updatedDate": "2022-11-12T04:17:27",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/161509270.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/161509270"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/161509270/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/161509270/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/54165030"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Klinger, Roman"
        },
        {
          "name": "Ruppenhofer, Josef"
        },
        {
          "name": "Sonntag, Jonathan"
        },
        {
          "name": "Struß, Julia Maria"
        },
        {
          "name": "Wiegand, Michael"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/619681847",
        "https://api.core.ac.uk/v3/outputs/621740672"
      ],
      "createdDate": "2018-02-10T05:13:03",
      "dataProviders": [
        {
          "id": 22756,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/22756",
          "logo": "https://api.core.ac.uk/data-providers/22756/logo"
        },
        {
          "id": 3352,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/3352",
          "logo": "https://api.core.ac.uk/data-providers/3352/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "We present the German Sentiment Analysis Shared Task (GESTALT) which consists of two main tasks: Source, Subjective Expression and Target Extraction from Political Speeches (STEPS) and Subjective Phrase and Aspect Extraction from Product Reviews (StAR). Both tasks focused on fine-grained sentiment analysis, extracting aspects and targets with their associated subjective expressions in the German language. STEPS focused on political discussions from a corpus of speeches in the Swiss parliament. StAR fostered the analysis of product reviews as they are available from the website Amazon.de. Each shared task led to one participating submission, providing baselines for future editions of this task and highlighting specific challenges. The shared task homepage can be found at https://sites.google.com/site/iggsasharedtask/",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/148067875.pdf",
      "fieldOfStudy": null,
      "fullText": "IGGSA Shared Tasks on German Sentiment Analysis (GESTALT)Josef Ruppenhofer‡, Roman Klinger∗†, Julia Maria Struß‡,Jonathan Sonntag§, Michael Wiegand◦‡ Dept. of Information Science and Language Technology, Hildesheim University† Institute for Natural Language Processing, University of Stuttgart∗ Semantic Computing Group, CIT-EC, Bielefeld University§ Computational Linguistics, Potsdam University◦ Spoken Language Systems, Saarland University{ruppenho,julia.struss}@uni-hildesheim.deroman.klinger@ims.uni-stuttgart.dejonathan.sonntag@yahoo.demichael.wiegand@lsv.uni-saarland.deAbstractWe present the German Sentiment Anal-ysis Shared Task (GESTALT) which con-sists of two main tasks: Source, Subjec-tive Expression and Target Extraction fromPolitical Speeches (STEPS) and SubjectivePhrase and Aspect Extraction from Prod-uct Reviews (StAR). Both tasks focused onfine-grained sentiment analysis, extractingaspects and targets with their associatedsubjective expressions in the German lan-guage. STEPS focused on political dis-cussions from a corpus of speeches in theSwiss parliament. StAR fostered the anal-ysis of product reviews as they are avail-able from the website Amazon.de. Eachshared task led to one participating sub-mission, providing baselines for future edi-tions of this task and highlighting specificchallenges. The shared task homepage canbe found at https://sites.google.com/site/iggsasharedtask/.1 IntroductionIn opinion mining, we are not only interestedin detecting the presence of opinions (or morebroadly, subjectivity) but determining particularattributes. We want to determine which valence orpolarity an opinion has (positive, negative or neu-tral), how strong it is (intensity), and also knowwhose opinion it is and what it is about. The lasttwo questions are what the task of opinion sourceThis work is licensed under a Creative Commons Attri-bution 4.0 International License (CC BY 4.0). Page numbersand proceedings footer are added by the organizers. Licensedetails: http://creativecommons.org/licenses/by/4.0/and target extraction is concerned with. Sourceand target extraction are capabilities needed forthe analysis of unrestricted language texts, wherethis kind of information cannot be derived frommeta-data and where opinions by multiple sourcesand about multiple, potentially related, targets ap-pear side by side.We present two shared tasks that ran under theauspices of the Interest Group of German Senti-ment Analysis1 (IGGSA). Maintask 1 on Source,Subjective Expression and Target Extraction fromPolitical Speeches (STEPS) constitutes the firstevaluation campaign for source and target ex-traction on German language data. Maintask 2on Subjective Phrase and Aspect Extraction fromProduct Reviews (StAR) focuses on the aspect ex-traction, which is understood as the target of asubjective phrase. For both tasks, publicly avail-able resources have been created, which serve asa reference corpus for the evaluation of opinionsource and target extraction in German.2 Task DescriptionsIn this section, we present the task setting, de-scribe the dataset, the annotation, the subtasks,the evaluation and results for each of the two maintasks (Section 2.1 and Section 2.2), respectively.2.1 Maintask 1Maintask 1 calls for the identification of subjec-tive expressions, sources and targets in parliamen-tary speeches. While these texts can be expectedto be opinionated, they pose the challenges that1https://sites.google.com/site/iggsahome/164sources other than the speaker may be relevantand that the targets, though constrained by topic,can vary widely. As in the case of Maintask 2,the dataset provided is the first one that providespublicly available expression-level annotations onrunning texts of this type for German.2.1.1 DatasetThe STEPS data set stems from the debatesof the Swiss parliament (Schweizer Bundesver-sammlung).2 This particular data set was selectedfor two reasons. First, the source data is opento the public and we can re-distribute it with ourannotations. We were not able to fully ascertainthe copyright situation for German parliamentaryspeeches, which we had also considered. Second,the text calls for annotation of multiple sourcesand targets.As the Swiss parliament is a multi-lingual in-stitution, we were careful to exclude not onlynon-German speeches but also German speechesthat constitute responses to, or comments on,speeches, heckling, and side questions in otherlanguages. This way, our annotators did not haveto label any German data whose correct under-standing might rely on material in a language thatthey might not be able to interpret correctly.Some potential linguistic difficulties consistedin peculiarities of Swiss German found in thedata. For instance, the vocabulary of Swiss Ger-man is different from standard German, often insubtle ways. For instance, the verb vorprellenis used in the following example instead of vor-preschen, which would be expected for Germanspoken in Germany:Es ist unglaublich: Weil die Aussen-ministerin vorgeprellt ist, kann man dasnicht mehr zuru¨cknehmen. (Hans Fehr,Fru¨hjahrsession 2008, Zweite Sitzung –04.03.2008)32The full task test data is available at https://sites.google.com/site/iggsasharedtask/home/testdata-maintask1-salto_tiger-xml.zip . The subtask test data for is at https://sites.google.com/site/iggsasharedtask/home/testdata-maintask1-subtasks-salto_tiger.xml.zip.3http://www.parlament.ch/ab/frameset/d/n/4802/263473/d_n_4802_263473_263632.htm‘It is incredible: because the foreignsecretary acted rashly, we cannot takethat back again.’In order to reduce any negative impact thatmight come from misreadings of the Swiss Ger-man by our annotators, who were German andAustrian rather than Swiss, we selected speechesabout what we deemed to be non-parochial issues.For instance, we picked texts on international af-fairs rather than ones about Swiss municipal gov-ernance.Technically, the STEPS data underwent thefollowing pre-processing pipeline. Sentencesegmentation and tokenization was done usingOpenNLP4, followed by lemmatization with theTreeTagger (Schmid, 1994), constituency pars-ing by the Berkeley parser (Petrov and Klein,2007), and final conversion of the parse treesinto TigerXML-Format using TIGER-tools (Lez-ius, 2002). To perform the annotation we used theSalto-Tool (Burchardt et al., 2006).2.1.2 AnnotationThrough our annotation scheme5, we provide an-notations at the expression level. No sentenceor document-level annotations are manually per-formed or automatically derived.There were no restrictions imposed on annota-tions. The subjective expressions could be verbs,nouns, adjectives or multi-words. The sourcesand targets could refer to any actor or issue as wedid not focus on anything in particular.The definition of subjective expressions (SE)that we used is broad and based on well-knownprototypes. It largely follows the model of whatWilson and Wiebe (2005) subsume under the um-brella term private state, as defined by Quirk etal. (1985): “As a result, the annotation schemeis centered on the notion of private state, a gen-eral term that covers opinions, beliefs, thoughts,feelings, emotions, goals, evaluations, and judg-ments.”:• evaluation (positive or negative):toll ‘great’, doof ‘stupid’4http://opennlp.apache.org/5See https://sites.google.com/site/iggsasharedtask/task-1/STEPS_guide.pdffor the the guidelines we used.165Name Source Target FrameSwissGerman not applicable 14RhetoricalDevices not applicable 64Inferred 344 (7.8%) 177 (3.9%) 97 (2.0%)Uncertain 61 (1.4%) 29 (0.6%) 58 (1.2%)Table 1: Flags annotated across all annotators and files of Maintask 1F1 Dice for true positivesSubjective Expression 63.32 0.92Sources∗ 68.70 0.99Targets∗ 80.63 0.85Table 2: Average inter-annotator agreement across allpairs of annotators on test data of Maintask 1 (F1 is basedon partial overlap; Dice quantifies the amount of overlapfor matches)• (un)certainty:zweifeln ‘doubt’, gewiss ‘certain’• emphasis:sicherlich/bestimmt ‘certainly’• speech acts:sagen ‘say’, anku¨ndigen ‘announce’• mental processes:denken ‘think’, glauben ‘believe’Beyond giving the prototypes, we did not seekto impose on our annotators any particular defini-tion of subjective or opinion expressions from thelinguistic, natural language processing or psycho-logical literature related to subjectivity, appraisal,emotion or related notions.In marking subjective expressions, the anno-tators were told to select minimal spans. Thisguidance was given because we had decided thatwithin the scope of this shared task we wouldforgo any treatment of polarity and intensity. Ac-cordingly, negation, intensifiers and attenuatorsand any other expressions that might affect a min-imal expression’s polarity or intensity could be ig-nored.When labeling sources and targets, annotatorswere asked to first consider syntactic and seman-tic dependents of the subjective expressions. Ifsources and targets were locally unrealized, theannotators could annotate other phrases in thecontext. Where a subjective expression repre-sented the view of the implicit speaker or textauthor, annotators could indicate this by settinga flag Sprecher ‘Speaker’ on the the source ele-ment.For all three types of labels, subjective expres-sions, sources, and targets, annotators had the op-tion of using two additional flags. The first flagwas intended to mark a label instance as Inferiert‘Inferred’. In the case of subjective expressions,this covers, for instance, cases where annotatorswere not sure if an expression constituted a po-lar fact or an inherently subjective expression. Inthe case of sources and targets, the ‘inferred’ labelapplies to cases where the referents cannot be an-notated as local dependents but have to be foundin the context. The second flag afforded annota-tors the ability to mark an annotation as Unsicher‘Uncertain’, if they were unsure whether the spanshould really be labeled with the relevant cate-gory.The annotators were asked to use a flagRhetorisches Stilmittel ‘Rhetorical device’ forsubjective expression instances where subjectiv-ity was conveyed through some kind of rhetoricaldevice such as repetition. Across all three annno-tators, 64 instances were labeled as ‘rhetorical de-166Run MeasureSubjectiveExpression Source Source SE Target Target SERun 3 Prec 63.42 48.55 74.89 56.25 79.71Rec 26.10 11.32 42.46 15.60 58.00F1 36.98 18.36 54.19 24.43 67.14Run 5 Prec 80.56 47.98 58.55 not applicableRec 29.97 10.44 32.65 not applicableF1 43.69 17.14 41.92 not applicableTable 3: Best participant runs for Maintask 1 (3 = rule-based system; 5 = translation-based system, which did not include Targer identification. Results suffixed with sub-jective expressions consider only cases where the system already matched the goldstandard on the subjective expression)vice’ in the data.Finally, the annotation guidelines gave annota-tors the option to mark particular subjective ex-pressions as Schweizerdeutsch ‘Swiss German’when they involved language usage that they werenot fully familiar with. Such cases could then beexcluded or weighted differently for the purposesof system evaluation. In our annotation, thesemarkings were in fact rare with only 14 of suchflag instances across all three annotators.Summing over all three annotators, our datasetcovers 1815 sentences. In total, 4935 subjectiveexpression frame instances were labeled by theannotators combined (2.7 frames/sentence). Re-lated to the frames, 8959 frame element (sourceor target) instances were annotated (1.8 frame el-ements/frame). Although the theory embodied byour guidelines calls for at least one source andtarget label per annotated subjective expressionframe, we find slightly less than one instance ofeach (4427 sources, 4532 targets). In Table 1, wesee that not many flags were annotated by our an-notators. The careful selection of our data withrespect to the topics treated seems to have workedwell. We have few instances of subjective expres-sions that were flagged as Swiss German formu-lations by our annotators. The most common typeof flag was the one for ‘inferred’ labels. Here, in-ference of sources was by far the most commoncase. Note, that fewer labels were marked ‘uncer-tain’ than were marked ‘inferred’. Inference didnot necessarily result in uncertainty.In Table 2, we present results on the inter-annotator agreement on the test data. Oneway of measuring the agreement uses theprecision/recall-framework of evaluation. We cal-culate the relevant numbers based on treating oneannotator as gold and another as system, and aver-aging the results for the three pairs of annotators.For F1, we counted a true positive when therewas partial span overlap. In addition, we presenta token-based multi-κ value (Davies and Fleiss,1982). Given that in our annotation scheme, asingle token can be e.g. a target of one subjectiveexpression while itself being a subjective expres-sion as well, we need to calculate three kappa val-ues covering the binary distinctions between pres-ence of each label and its absence. For subjectiveexpressions κ is 0.39, for sources 0.57, and fortargets 0.46.As exact matches on spans are relatively rare,the Dice coefficient is used to measure the over-lap between a system annotation and a gold stan-dard annotation (Dice, 1945). The Dice coef-ficient dc(S,G) is a similarity measure rangingfrom 0 to 1, wheredc(S,G) =2|S ∩G||S|+ |G| ,and G is the set of tokens in the gold annotationsand S the set of tokens the prediction (the systemlabel), respectively.2.1.3 SubtasksThe STEPS shared task offered a full task as wellas two subtasks:167Full task Identification of subjective expressionswith their respective sources and targets.Subtask 1 Participants are given the subjectiveexpressions and are only asked to identifyopinion sources.Subtask 2 Participants are given the subjectiveexpressions and are only asked to identifyopinion targets.Participants could choose any combination ofthe tasks. However, so as to not give an unfair ad-vantage, the full task was run and evaluated be-fore the gold information on subjective expres-sions was given out for the two subtasks, whichwere run concurrently.2.1.4 Evaluation MetricsThe runs that were submitted by the participantsof the shared task were evaluated on different lev-els, according to the task they chose to participatein. For the full task, there was an evaluation ofthe subjective expressions as well as the targetsand sources for subjective expressions, matchingthe system’s annotations against those in the goldstandard. For subtasks 1 and 2, only the sourcesand targets were evaluated, as the subjective ex-pressions were already given.In this first iteration of the STEPS task, weevaluated against each of our three annotatorsindividually rather than against a single gold-standard. Our intent behind this choice was toretain the variation between the annotators.We used recall to measure the proportion ofcorrect system annotations with respect to thegold standard annotations. Additionally, preci-sion was calculated so as to give the fraction ofcorrect system annotations relative to all the sys-tem annotations. As we did for inter-annotator-agreement, for recall and precision we counted amatch when there was partial span overlap. Sim-ilarly, we again used the Dice coefficient to as-sess the overlap between a system annotation anda gold standard annotation.The group that participated in our main tasksubmitted five different runs, based on two differ-ent system architectures. Table 3 shows the bestresult for each architecture. The scores representaverages across the comparisons relative to eachof the three annotators. The rule-based systemgenerally performed better than the translation-based one. However, the latter was much betterin its precision on recognizing subjective expres-sions in the full task. As is to be expected, whenthe system had already matched the gold standardon the subjective expressions, its performance onsource and target recognition, shown in columnsSource SE , Target SE, is much superior to per-formance in the general case.2.2 Maintask 2: Subjective Phrase andAspect Extraction from Product ReviewsMaintask 2 was designed to foster the develop-ment of systems to automatically extract sub-jective, evaluative phrases from German Ama-zon reviews, aspects described in the review andtheir relation, i.e., which evaluative phrase targetswhich aspect. In addition, another focus is cross-domain learning: The development corpus con-sists of reviews for various products while the testcorpus is from yet another product not known tothe participants before.2.2.1 DatasetFor this task, a data set was provided for train-ing parameters and developing the system. TheUSAGE Review Corpus for Fine Grained MultiLingual Opinion Analysis (Klinger and Cimiano,2014) was previously published and was fullyavailable to the participants from the start of thetask on. It consists of 611 German and 622 En-glish reviews for coffee machines, cutlery sets,microwaves, toasters, trashcans, vacuum clean-ers, and washers from which only the Germanpart has been used in this shared task. To con-struct the test corpus, 1646 reviews for the searchterm Wasserkocher ‘water boiler’ were retrieved.From these, 100 sampled reviews were annotatedand included in the test corpus. The training6 andtest7 data is freely available.2.2.2 AnnotationThe entity classes aspect and evaluative (subjec-tive) expression are annotated in the corpus. Eval-uative expressions are assigned a polarity (posi-6Maintask 2 training data: http://dx.doi.org/10.4119/unibi/citec.2014.147Maintask 2 test data: http://dx.doi.org/10.4119/unibi/2695161168tive, negative, neutral), which is not used in thisshared task, and a set of aspects they refer to. Theannotators were instructed to regard everything asan aspect that is part of a product or related to itand can influence the opinion about it, includingthe whole product itself. Evaluative phrases ex-press an opinion. Negations are not separately an-notated but are part of a phrase. Annotators wereasked to avoid overlapping annotations if possi-ble. The annotations should be as short as possi-ble, as long as the meaning is understandable ifonly the annotations were given (without the sen-tence itself).Every review in the training data is annotatedby two linguists, the test data is annotated by one(the information which of the training data anno-tation corresponds to the annotator of the test datais available).In the following examples, aspects are markedin blue and subjective phrases are marked in red:Ich hatte keine Probleme mit derRu¨ckgabe .I had no problems with the return .return is a target of no problems.no problems is positive.Die Waschmaschine selbst ist toll , derbeiliegende Schlauch ist Schrott.The washer itself is great , the includedhose is junk .washer is a target of great.hose is a target of junk.great is positive.junk is negative.Es sieht sehr hu¨bsch aus, wie einAufbewahrungsbeha¨lter , er ist leicht undeinfach zu benutzen .It looks very neat , like astorage container , and using it is verysimple and easy .– looks is a target of very neat.using is a target of simple and of easy.The inter-annotator agreement of the full train-ing corpus is κ = 0.65 (Cohen’s κ). The inter-annotator F1 measure is 0.71 for aspects, 0.55for subjective phrases and 0.42 for the relationsbetween both (including an error propagation ofhaving the exact same phrases annotated). Thesemeasures can be regarded as upper bounds formeaningful results of an automated approach.Table 4 presents the main statistics of the train-ing and testing corpora. Here, annotator 1 of thetraining corpus performed the annotation of thetest data. Obviously, the number of annotatedphrases is higher in the test data.The most frequent subjective phrases for thedifferent products are very similar. For instance,the phrases gut ‘good’ and sehr zufrieden ‘verysatisfied’ occurs in all top 10 lists of subjec-tive phrases. However, the most frequent aspectphrases are very different, as the product cate-gory itself is frequently used as an aspect (e.g.Kaffeemaschine ‘coffee maker’ or Besteck ‘cut-lery’). In addition, very product class-specificaspects are mentioned frequently, like Wasser‘water’, schneiden ‘cut’, or Edelstahl ‘stainlesssteel’. Some aspects are shared between productcategories, for instance Preis ‘price’ or Qualita¨t‘quality’.Clearly, the cross-domain inference task ismore challenging, as the mentioned aspects arenot as similar as the annotated subjective phrases.2.2.3 SubtasksThe three substasks to be addressed by the parti-cants were:Subtask 2a Identication of subjective phrases.Subtask 2b Identification of aspect phrases.Subtask 2c Identification of subjective phrasesand aspect phrases and indication for eachaspect phrase of which subjective phrase itis the target (if any).2.2.4 Evaluation metrics and BaselineapproachFor evaluation, the F1 measure of the exact matchof the predicted phrases in comparison to the an-notated phrases is taken into account. This isstraight-forward for Subtasks 2a and 2b. In 2c,a pair of aspect and subjective phrase was con-sidered to be correctly identified, if both phrases169Train Ann. 1 Train Ann. 2 TestNumber of reviews 611 100Number of products 127 100Number of Aspects 6340 5055 1662Number of Aspects/Review 10.4 8.3 16.6Number of positive Subj. 3840 3717 823Number of positive Subj./Review 6.3 6.1 8.2Number of negative Subj. 1094 1052 264Number of negative Subj./Review 1.8 1.7 2.6Target Rel. 4085 4643 1013Target Rel./Review 6.7 7.6 10.1Table 4: Statistics of the corpora used in Maintask 2predicted to be participating were identified cor-rectly (on the phrase level) as well as annotated asa pair.For comparison, as a baseline, a machinelearning-based system optimized for in-domaininference was applied8 (Klinger and Cimiano,2013a; Klinger and Cimiano, 2013b). A com-parison of the participant’s result and the baselineis shown in Table 5. It can be observed that thebaseline outperforms the subjective phrase detec-tion, but the result submitted by the participant issuperior in the more difficult cross-domain tasksof aspect extraction. The extraction of relationsclearly remains a challenge.3 Related WorkWhile quite a few shared tasks have addressed therecognition of subjective units of language and,possibly, the classification of their polarity (Se-mEval 2013 Task 2, Twitter Sentiment Analysis(Nakov et al., 2013); SemEval-2010 task 18: Dis-ambiguating sentiment ambiguous adjectives (Wuand Jin, 2010); SemEval-2007 Task 14: AffectiveText (Strapparava and Mihalcea, 2007) inter alia),few tasks have included the extraction of sourcesand targets.The prior work most relevant to the taskspresented here was done in the context of theJapanese NTCIR9 Project. In the NTCIR-6 Opin-8A high-recall combination of the joint configurationand the pipeline setting has been applied.9NII [National Institute of Informatics] Test Collectionion Analysis Pilot Task (Seki et al., 2007), whichwas offered for Chinese, Japanese and English,sources and targets had to be found relative towhole opinionated sentences rather than individ-ual subjective expressions. However, the task al-lowed for multiple opinion sources to be recordedfor a given sentence if there were multiple ex-pressions of opinion. The opinion source for asentence could occur anywhere in the document.In the evaluation, as necessary, co-reference in-formation was used to (manually) check whethera system response was part of the correct chainof co-referring mentions. The sentences in thedocument were judged as either relevant or non-relevant to the topic (=target). Polarity was deter-mined at the sentence level. For sentences withmore than one opinion expressed, the polarity ofthe main opinion was carried over to the sentenceas a whole. All sentences were annotated by threeraters, allowing for strict and lenient (by major-ity vote) evaluation. The subsequent Multilin-gual Opinion Analysis tasks NTCIR-7 (Seki et al.,2008) and NTCIR-8 (Seki et al., 2010) were basi-cally similar in their setup to NTCIR-6.While GESTALT shared tasks focussed onGerman, the most important difference to theshared tasks organized by NTCIR is that it definedthe source and target extraction task at the level ofindividual subjective expressions. There was nocomparable shared task annotating at the expres-sion level, rendering existing guidelines imprac-for IR Systems170Baseline ParticipantSubtask Prec Rec F1 Prec Rec F1Aspect Phrase 65.5 46.4 54.3 55.5 62.2 58.7Subjective Phrase 51.5 41.4 45.9 51.6 32.0 39.5Relation 15.9 8.3 10.9 12.6 13.8 13.2Table 5: Results of the baseline system and the participant’s best submission in Maintask 2.tical and necessitating the development of com-pletely new guidelines.Another more recent shared task related toGESTALT is the Sentiment Slot Filling track(SSF) that was part of the Shared Task for Knowl-edge Base Population of the Text Analysis Con-ference (TAC) organised by the National Instituteof Standards and Technology (NIST) (Mitchell,2013). The major distinguishing characteristicof that shared task, which is offered exclusivelyfor English language data, lies in its retrieval-like setup. Here, the task is to extract all possi-ble opinion sources and targets from a given text.By contrast, in SSF the task is to retrieve sourcesthat have some opinion towards a given target en-tity or targets of some given opinion sources. Inboth cases, the polarity of the underlying opin-ion is also specified within SSF. The given tar-gets or sources are considered a type of query.The opinion sources and targets are to be retrievedfrom a document collection.10 Unlike GESTALT,SSF uses heterogeneous text documents includingboth newswire and discussion forum data fromthe Web.This year’s SemEval-2014 Task 4 on AspectBased Sentiment Analysis (ABSA) on English re-view data for restaurant and laptop reviews (Pon-tiki et al., 2014) constitutes another related sharedtask. It focused on aspect-based polarity detec-tion. The main differences are that the aspect cat-egories were predefined and that the polarity as-signment did not include the detection of the eval-uative phrases. Therefore, the polarity assignmentwas on the aspect level and the relation betweena subjectivity-bearing word was implicit. An-other difference between ABSA and GESTALT(StAR, specifically) is that the number of products10In 2014, the text from which entities are to be retrievedis restricted to one document per query.taken into account is higher in StAR, motivatinga cross-domain inference challenge.4 Conclusion and OutlookWe reported on the first iteration of two sharedtasks for German sentiment analysis. Both tasksfocused on the discovery of subjective expres-sions and their related entities. In the case ofSTEPS, sources and targets had to be foundand linked to subjective expressions in politicalspeeches, in the case of StAR, aspects had tobe identified and tied to subjective expressions inAmazon reviews.Although a preliminary call for interest had in-dicated interest by 3–4 groups for each of thetasks, in the end each task had only one partic-ipant. We therefore solicited feedback from ac-tual and potential participants at the end of theIGGSA-GESTALT workshop in order to be ableto tailor the tasks better in a future iteration.Based on the discussion, both shared tasks planon including polarity in the evaluation for theirnext iteration. For both tasks, there was discus-sion what a suitable evaluation procedure wouldbe, in particular whether partial matches shouldbe the basis of the main measures or if exactmatches would be more desirable.Specific to STEPS, we are considering con-ducting the evaluation in alternative ways on a fu-ture iteration of the task. One direction to pur-sue is to derive new versions of the gold stan-dard based on the level of inter-annotator agree-ment on the labels. In a full-agreement mode, wewould only retain annotations of the gold stan-dard that had majority or even full agreement onthe subjective expression level for all three an-notators. Another alternative would consist inestablishing an expert-adjudicated gold-standard,after all. The benefit of any of these alterna-171tive evaluation modes would be that a clear ob-jective function can be learnt and that the up-per bound for system performance would againbe 100% precision/recall/F1-score, whereas it waslower for this iteration given that existing differ-ences between the annotators necessarily led tofalse positives and negatives.For the next iteration of GESTALT, we plan tomake a baseline system available, such that thebarrier to participation in the shared task is lowerand participants’ efforts can be focused on the ac-tual methods.AcknowledgmentsWe would like to thank Simon Clematide forhelping us get access to the Swiss data for theSTEPS task. For their support in preparingand carrying out the annotations of this data,we would like to thank Jasper Brandes, MelanieDick, Inga Hannemann, and Daniela Schneevogt.We thank the German Society for Computa-tional Linguistics for its financial support of theSTEPS annotation effort. For the annotationsused in the StAR task, we thank Luci Fillingerand Frederike Strunz. Roman Klinger was par-tially funded by the It’s OWL project (‘IntelligentTechnical Systems Ostwestfalen-Lippe’, http://www.its-owl.de/), a leading-edge tech-nology and research cluster funded by GermanMinistry of Education and Research (BMBF).This first and last author were partially supportedby the German Research Foundation (DFG) un-der grants RU 1873/2-1 and WI 4204/2-1, respec-tively.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, and Sebastian Pado. 2006. SALTO - AVersatile Multi-Level Annotation Tool. In Proceed-ings of the 5th Conference on Language Resourcesand Evaluation, pages 517–520.Mark Davies and Joseph L. Fleiss. 1982. Measur-ing agreement for multinomial data. Biometrics,38(4):1047–1051.Lee R. Dice. 1945. Measures of the Amount ofEcologic Association Between Species. Ecology,26(3):297–302.Roman Klinger and Philipp Cimiano. 2013a. Bi-directional inter-dependencies of subjective expres-sions and targets and their value for a joint model.In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 848–854, Sofia, Bulgaria,August. Association for Computational Linguistics.Roman Klinger and Philipp Cimiano. 2013b. Jointand pipeline probabilistic models for fine-grainedsentiment analysis: Extracting aspects, subjectivephrases and their relations. In Data Mining Work-shops (ICDMW), 2013 IEEE 13th InternationalConference on, pages 937–944, Dec.Roman Klinger and Philipp Cimiano. 2014. Theusage review corpus for fine grained multi lingualopinion analysis. In Nicoletta Calzolari (Confer-ence Chair), Khalid Choukri, Thierry Declerck,Hrafn Loftsson, Bente Maegaard, Joseph Mariani,Asuncion Moreno, Jan Odijk, and Stelios Piperidis,editors, Proceedings of the Ninth InternationalConference on Language Resources and Evalua-tion (LREC’14), Reykjavik, Iceland, may. EuropeanLanguage Resources Association (ELRA).Wolfgang Lezius. 2002. TIGERsearch - Ein Such-werkzeug fu¨r Baumbanken. In Stephan Buse-mann, editor, Proceedings of KONVENS 2002,Saarbru¨cken, Germany.Margaret Mitchell. 2013. Overview of the TAC2013Knowledge Base Population Evaluation: EnglishSentiment Slot Filling. In Proceedings of the TextAnalysis Conference (TAC), Gaithersburg, MD,USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wil-son. 2013. SemEval-2013 Task 2: SentimentAnalysis in Twitter. In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 312–320, Atlanta and Georgia and USA. As-sociation for Computational Linguistics.Slav Petrov and Dan Klein. 2007. Improved inferencefor unlexicalized parsing. In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404–411, Rochester, New York, April.Association for Computational Linguistics.Maria Pontiki, Dimitris Galanis, John Pavlopoulos,Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar. 2014. Semeval-2014 task 4:Aspect based sentiment analysis. In Proceedings ofthe 8th International Workshop on Semantic Eval-uation (SemEval 2014), pages 27–35, Dublin, Ire-land, August. Association for Computational Lin-guistics and Dublin City University.172Randolph Quirk, Sidney Greenbaum, Geoffry Leech,and Jan Svartvik. 1985. A comprehensive grammarof the English language. Longman.Helmut Schmid. 1994. Probabilistic part-of-speechtagging using decision trees. In Proceedings of In-ternational Conference on New Methods in Lan-guage Processing, Manchester, UK.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi.Chen, Noriko Kando, and Chin-Yew Lin. 2007.Overview of opinion analysis pilot task at ntcir-6. In Proceedings of NTCIR-6 Workshop Meeting,pages 265–278.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,Hsin-Hsi Chen, and Noriko Kando. 2008.Overview of multilingual opinion analysis task atNTCIR-7. In Proceedings of the 7th NTCIR Work-shop Meeting on Evaluation of Information Ac-cess Technologies: Information Retrieval, QuestionAnswering, and Cross-Lingual Information Access,pages 185–203.Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, andNoriko Kando. 2010. Overview of MultilingualOpinion Analysis Task at NTCIR-8: A Step TowardCross Lingual Opinion Analysis. In Proceedings ofthe 8th NTCIR Workshop Meeting on Evaluation ofInformation Access Technologies: Information Re-trieval, Question Answering and Cross-Lingual In-formation Access, pages 209–220.Carlo Strapparava and Rada Mihalcea. 2007.SemEval-2007 Task 14: Affective Text. In EnekoAgirre, Lluı´s Ma`rquez, and Richard Wicentowski,editors, Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007), pages 70–74. Association for ComputationalLinguistics.Theresa Wilson and Janyce Wiebe. 2005. Annotatingattributions and private states. In Proceedings of theWorkshop on Frontiers in Corpus Annotations II:Pie in the Sky, pages 53–60, Ann Arbor, Michigan,June. Association for Computational Linguistics.Yunfang Wu and Peng Jin. 2010. SemEval-2010 Task18: Disambiguating Sentiment Ambiguous Adjec-tives. In Katrin Erk and Carlo Strapparava, edi-tors, Proceedings of the 5th International Workshopon Semantic Evaluation, pages 81–85, Stroudsburgand PA and USA. Association for ComputationalLinguistics.173",
      "id": 80476409,
      "identifiers": [
        {
          "identifier": "oai:hilpub.uni-hildesheim.de:ubhi/16007",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:hildok.bsz-bw.de:295",
          "type": "OAI_ID"
        },
        {
          "identifier": "621740672",
          "type": "CORE_ID"
        },
        {
          "identifier": "148067875",
          "type": "CORE_ID"
        },
        {
          "identifier": "619681847",
          "type": "CORE_ID"
        }
      ],
      "title": "IGGSA Shared Tasks on German Sentiment Analysis (GESTALT)",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:hildok.bsz-bw.de:295",
        "oai:hilpub.uni-hildesheim.de:ubhi/16007"
      ],
      "publishedDate": "2014-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://hildok.bsz-bw.de/files/295/04_01.pdf"
      ],
      "updatedDate": "2024-11-20T20:02:14",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/148067875.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/148067875"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/148067875/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/148067875/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/80476409"
        }
      ]
    },
    {
      "acceptedDate": "2016-12-22T00:00:00",
      "arxivId": "1609.06772",
      "authors": [
        {
          "name": "Ekman P."
        },
        {
          "name": "Jiang Y.-G."
        },
        {
          "name": "Krizhevsky A."
        },
        {
          "name": "Resch B."
        },
        {
          "name": "You Q."
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Mohamed",
        "Yi"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/188789615"
      ],
      "createdDate": "2016-12-01T20:56:48",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2016-01-01T00:00:00",
      "abstract": "We perform spatio-temporal analysis of public sentiment using geotagged photo\ncollections. We develop a deep learning-based classifier that predicts the\nemotion conveyed by an image. This allows us to associate sentiment with place.\nWe perform spatial hotspot detection and show that different emotions have\ndistinct spatial distributions that match expectations. We also perform\ntemporal analysis using the capture time of the photos. Our spatio-temporal\nhotspot detection correctly identifies emerging concentrations of specific\nemotions and year-by-year analyses of select locations show there are strong\ntemporal correlations between the predicted emotions and known events.Comment: To appear in ACM SIGSPATIAL 201",
      "documentType": "research",
      "doi": "10.1145/2996913.2996978",
      "downloadUrl": "http://arxiv.org/abs/1609.06772",
      "fieldOfStudy": null,
      "fullText": "Spatio-Temporal Sentiment Hotspot Detection Using\nGeotagged Photos\nYi Zhu and Shawn Newsam\nElectrical Engineering & Computer Science\nUniversity of California at Merced\nyzhu25,snewsam@ucmerced.edu\nABSTRACT\nWe perform spatio-temporal analysis of public sentiment us-\ning geotagged photo collections. We develop a deep learning-\nbased classifier that predicts the emotion conveyed by an\nimage. This allows us to associate sentiment with place.\nWe perform spatial hotspot detection and show that differ-\nent emotions have distinct spatial distributions that match\nexpectations. We also perform temporal analysis using the\ncapture time of the photos. Our spatio-temporal hotspot\ndetection correctly identifies emerging concentrations of spe-\ncific emotions and year-by-year analyses of select locations\nshow there are strong temporal correlations between the pre-\ndicted emotions and known events.\nCCS Concepts\n•Computing methodologies → Scene understanding;\nNeural networks; •Human-centered computing → Geo-\ngraphic visualization;\nKeywords\nHotspot detection, emotion recognition, geotagged photos,\nspatio-temporal geographic analysis, deep learning\n1. INTRODUCTION\nSpatio-temporal hotspot detection is an important com-\nponent of making cities smart, especially for tasks such as\nmonitoring, early warning, resource allocation, and sustain-\nable management. Hotspot analysis is typically conducted\nby mapping crime rates, monitoring disease outbreaks, lo-\ncating traffic accidents, etc. In this paper, we focus instead\non determining the emotional states of a city’s inhabitants\nas conveyed through their photos as a step towards creating\nan affect-aware city.\nEmotions play important roles in everyone’s daily life. No\nmatter what you do, you will have feelings associated with\nyour activities. Services like Twitter, Facebook, Flickr, or\nSnapchat are great platforms for people to share their emo-\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full cita-\ntion on the first page. Copyrights for components of this work owned by others than\nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-\npublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGSPATIAL’16, October 31-November 03, 2016, Burlingame, CA, USA\nc© 2016 ACM. ISBN 978-1-4503-4589-7/16/10. . . $15.00\nDOI: http://dx.doi.org/10.1145/2996913.2996978\ntional states by posting words/pictures/videos. With geo-\ntagged and timestamped social multimedia, we can associate\nsentiment with geographical locations over time.\nThere exists work on detecting emotions from text such\nas in Twitter posts [12], but much less work on using im-\nage/video data. The reason is simple, words can determin-\nistically express people’s emotions, like “This is awesome!”,\n“I feel blue”, “So upset”. However, predicting emotions in vi-\nsual data is much more subtle and difficult. Luckily, the field\nof computer vision has made great advances recently in high-\nlevel image understanding thanks in large part to deep learn-\ning. With respect to our problem, large-scale visual datasets\nfor emotion recognition [11, 14, 5] have been created allowing\ndeep neural networks to be trained and achieve respectable\nperformance on emotion recognition over the last five years.\nThis has opened the opportunity for work such as ours to ex-\nploit these advances for performing spatio-temporal hotspot\ndetection of public emotion using geo-referenced photos.\nThe major contributions of our work include: (i) We con-\nduct the first investigation into sentiment hotspot detection\nin space and time via geotagged photos. (ii) The spatial\nhotspots for the different emotions have distinct spatial dis-\ntributions and agree with expectations. (iii) Our temporal\nhotspot analysis is able to detect emerging concentrations\nof emotions. And, a year-by-year analysis of specific re-\ngions finds strong correlations between emotions and tem-\nporal events, such as between the level of joy and the success\nof the San Francisco Giants at AT&T park, and between the\nlevel of disgust and the increase in gentrification in the Mis-\nsion residential neighborhood.\n2. RELATEDWORK\nOur work is related to several lines of research.\nGeo-Referenced Multimedia The exponential growth of\npublicly available geo-referenced multimedia has created a\nrange of interesting opportunities to learn about our world.\nAt the intersection of geographic information science and\ncomputer vision, large collections of geotagged photos have\nbeen used to map world phenomena [1], classify land use [15],\ngeolocate photos [4], recognize and model landmarks [13],\nperform smart city and urban planning [10], etc. Although\nonline photo collections represent a wealth of information,\nthey present challenges due to how noisy and diverse they\nare. The challenges in using them for geographic discovery\ninclude inaccurate location information, uneven spatial dis-\ntribution, and varying photographer intent. We are mindful\nof these and recognize they likely temper our results.\nDeep Learning Deep learning has advanced a number of\npattern recognition and machine learning areas including\nar\nX\niv\n:1\n60\n9.\n06\n77\n2v\n1 \n [c\ns.C\nV]\n  2\n1 S\nep\n 20\n16\ncomputer vision in which it debuted as deep convolutional\nneural networks (ConvNets) in 2012 [6]. Since then, re-\nsearchers have applied deep ConvNets to a range of vision\nproblems, obtaining state-of-the-art results. Key to Con-\nvNets’ performance is their ability to learn high-level or\nsemantic features from the data as opposed to the hand-\ncrafted low- to mid-level features traditionally used in image\nanalysis. This level of image analysis, or understanding, is\nimportant for our task since detecting the emotion conveyed\nin an image is a high-level and abstract task.\nEmotion Recognition Emotions represent higher intelli-\ngence and so being able to recognize them is key to artifi-\ncial intelligence. For example, real-time emotion recognition\nduring a customer service phone call can lead to a more sat-\nisfactory experience; analyzing a Twitter user’s emotional\nstate can help detect an emotional crisis; and, a chatting\nrobot who is able to recognize emotions can have better in-\nteraction with users.\nEmotions can be conveyed and therefore detected, at least\nin principle, in various multimedia sources such as text, im-\nages, and videos. We develop our own deep learning based\nsystem to detect the emotions conveyed in geotagged images.\nThis then allows us to associate sentiment with place.\n3. METHODOLOGY\nWe first describe our approach to detecting the emotion\nconveyed by an image. We then describe our spatial and\nspatio-temporal hotspot detection using the Gi* statistic [9]\nand Mann-Kendall test [8].\n3.1 Emotion Recognition\nAs mentioned above, emotion can be conveyed by a num-\nber of multimedia sources such as text, audio, image, videos,\netc. Visual emotion analysis is appealing since vision, as the\nrichest sense, is arguably the most effective at conveying\nemotion. Existing work on visual emotion analysis can be\nclassified into two approaches, dimensional models [7] and\ncategorical models [11, 14]. We focus on categorical analysis\nusing Ekman’s six basic emotions [3]: anger, disgust, fear,\njoy, sadness, and surprise.\nOur goal is using geotagged photos for sentiment hotspot\ndetection. The foundation of our approach is assigning each\nphoto one of the six emotions. We therefore design a per-\nimage emotion classifier using ConvNets. This is motivated\nby the finding of You et al. [14] that ConvNets outperform\ntraditional hand-crafted low-level features on most classes\nin a visual emotion analysis task. Specifically, we start with\na VGG-16 network that has been pre-trained on ImageNet\n[2], and then fine-tune it using the Emotion6 dataset [11].\nOnce trained, our classifier achieves an average accuracy of\n61.95%, which is reasonable and performs much better than\nrandom guess (which is 16.67%).\n3.2 Spatial Hotspot Detection\nOnce we have labeled the geotagged images, we can map\nand start to investigate the spatial distribution of public sen-\ntiment. To simplify the analysis, we divide our study area,\nthe city of San Francisco, into a 1000×1000 grid and assign\neach image to the closest bin center. The resulting quantiza-\ntion of the image locations does not affect our results since\neach bin measures less than approximately 12 × 14 meters\nwhich is finer than the scale of our analysis. All the spa-\ntial analysis below is based on the grid instead of the point\nlocations of the photos.\nOur data can now be considered a 1000×1000×6 datacube\nin which the third dimension is the number of images labeled\nwith a particular emotion. We normalize for the uneven\nspatial distribution of the images by computing the ratio of\neach emotion in each bin. That is, for each emotion, we\ncompute a 1000× 1000 grid where each bin is assigned\nratioek =\nnumber of photos in bin k of emotion e\nnumber of photos in bin k\n, (1)\nwhere k is the spatial index of the bin, and e is the emotion\nclass. Each value in a bin indicates the percentage of a\nparticular emotion evoked at the bin’s location. Hence, for\neach location, the third dimension should sum to 1.\nWe use the Getis-Ord Gi* statistic [9] to find where high\nand low emotion ratios cluster spatially. Note that, for each\nemotion, only bins that contain photos are considered and\nnothing is computed for bins that do not have any photos.\n3.3 Spatio-Temporal Hotspot Detection\nOur geotagged photos have timestamps which enables us\nto perform temporal analysis. These timestamps indicate\nwhen the photo was taken. We temporally bin the photos\nat yearly intervals. Our photos span ten years so we now\nhave a 1000 × 1000 × 10 × 6 datacube in which each bin is\nthe ratio of images with a particular emotion to the total\nimages for a particular location for a particular year. This\nnow allows us to perform spatio-temporal hotspot detection.\nGlobal Detection We perform spatio-temporal hotspot de-\ntection using the emerging hotspot analysis tool1 in ArcGIS.\nWe perform this analysis for each emotion separately. First,\nthe Gi* statistic is computed spatially for each year. This is\nthen followed by a Mann-Kendall test [8] to detect temporal\ntrends at each spatial location. This test essentially looks for\ncorrelations between a spatial location’s Gi* value and time.\nThe emerging hotspot analysis tool classifies each spatial lo-\ncation into one of 17 categories: new hot (cold) spot, consec-\nutive hot (cold) spot, intensifying hot (cold) spot, persistent\nhot (cold) spot, diminishing hot (cold) spot, sporadic hot\n(cold) spot, oscillating hot (cold) spot, historical hot (cold)\nspot, and no trend detected.\nLocal Temporal Analysis The emerging hotspot analysis\ntool identifies spatio-temporal hotspots but does not pro-\nvide detailed information on the year-to-year changes. We\ntherefore perform local analysis at a few locations with the\ngoal of relating the changes to known temporal events. We\nexplore a region’s emotional trend over time by computing\nthe emotion ratio for a bounding-box at yearly intervals:\nratioey =\nNumber of photos locally of emotion e in year y\nNumber of photos locally in year y\n.\n(2)\n4. EXPERIMENTS AND RESULTS\nWe first describe the training and performance of our emo-\ntion classifier. We then present the results of the hotspot\ndetection both in time and space.\n4.1 Datasets\nEmotion6 We use the Emotion6 [11] image dataset to fine-\ntune our emotion classifier. This dataset contains 1,980 im-\nages evenly divided into six emotion classes: anger, disgust,\nfear, joy, sadness, and surprise. The images were collected\n1https://desktop.arcgis.com/en/arcmap/latest/tools/space-\ntime-pattern-mining-toolbox/emerginghotspots.htm\nMission\nDistrict\nMessy Streets &\nRestaurants\n(a)\nAT&T\nPark\nBotanical Garden\nCandlestick\nPark\nAlamoSquare\nPark\nLake\nMerced\n(b)\nAT&T\nPark\nFishermans\nWharf\nPier 70\nAlamoSquare\nPark\nConservatory\t\r  \nof\t\r  Flowers\nCity Hall\nCivic Center Plaza\nUnion Square\nWestfieldCenter\nFerry\n(c)\nFigure 1: Spatial hotspot detection: (a) disgust; (b) joy. Red, yellow, blue represent hot, not significant and\ncold spots, respectively. Spatio-temporal hotspot detection: (c) joy. See the text for more details.\nfrom Flickr by using the class labels and synonyms as search\nterms. We randomly split the dataset into training and val-\nidation subsets in the ratio 8 : 2. All images are resized to\n256× 256 pixels for input to the classifier.\nGeotagged Photos We download geotagged photos from\nFlickr for San Francisco city for the ten year period from\n2006 to 2015. These are the images we label with our emo-\ntion classifier. The total number of images is around 1.9\nmillion. However, some of the images are too dark/light,\ntoo small, or just a placeholder in Flickr, and so we perform\na simple filtering step to remove these images. The dataset\nafter filtering contains 1,753,903 images. The distribution\nby year and predicted emotion can be seen in figure 2.\nFigure 2(a) conveys a sense of popularity of the Flickr\nplatform over the ten year period. The number of uploaded\nphotos reaches a peak of 259,741 in 2011 and then falls each\nyear after that. This decline is interesting although we leave\nit to the reader to stipulate on its cause. Figure 2(b) shows\nthe distribution of emotions as predicted by our classifier.\nThere are more joy and sadness images than other emotions.\nYear\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nN\num\nbe\nr o\nf P\nho\nto\ns\n×10 5\n0\n0.5\n1\n1.5\n2\n2.5\nEmotion Classes\nanger disgust fear joy sadness surprise\nN\num\nbe\nr o\nf P\nho\nto\ns\n×10 5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFigure 2: Number of geo-referenced photos in San\nFrancisco area. Left: per year; right: per emotion.\n4.2 Spatial Hotspot Analysis\nWe now present the results of our spatial hotspot detec-\ntion. We use the optimized hotspot analysis tool2 in ArcGIS\nto compute and visualize our results.\nOne of the challenges of our work is that there is no\nground-truth for evaluation. Nonetheless, we make the fol-\nlowing qualitative observations from the results in Fig. 1:\n(i) Different emotions have distinct spatial patterns (we only\nvisualize the results of emotions joy and disgust for illustra-\ntion). This indicates that our emotion classifier is detecting\nconsistent signals in the geotagged photos. (ii) The detected\nhotspots make sense. For example, Fig. 1(b) shows that joy\nhotspots are detected at the San Francisco botanical garden,\n2http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-\nstatistics-toolbox/optimized-hot-spot-analysis.htm\nAlamo square park, AT&T park, Candlestick park, and the\nFort Mason chapel. Further, these locations are detected\nas coldspots or not being significant for the other emotions.\n(iii) Some places are a mix of all emotions. These places,\nsuch as downtown San Francisco, have a wide variety of\nscenes which results in a relatively balanced distribution.\n4.3 Spatio-Temporal Hotspot Detection\nThe goal here is to identify locations that are significiant in\nboth space and time. We first conduct global detection and\nthen perform temporal analysis for select locations.\nGlobal Detection Fig. 1(c) shows the spatio-temporal\nhotspots detected for the north-east part of San Francisco.\nWe make the following observations: (i) Pier 70 (blue box)\nis detected as a new hotspot which means that is a sta-\ntistically significant hot spot for the final time step (2015)\nbut has never been a statistically significant hot spot before.\nThis makes sense since the Pier 70 buildings were recently\nrenovated in 2014 to host large corporate parties, concert\nevents, expositions, etc. (ii) Tourist destinations and pub-\nlic spaces are detected as intensifying hotspots which means\nthey have been a statistically significant hot spot for ninety\npercent of the time-step intervals, including the final time\nstep, and the intensity of clustering of high counts in each\ntime step is increasing overall and that increase is statisti-\ncally significant. The fact that these are intensifying and not\njust persistent hotspots is interesting. We postulate that it is\ndue to the economic recovery that has occurred during the\nlatter part of our time period which especially affects the\ntourist and leisure industry. (iii) Many locations are de-\ntected as consecutive hotspots which means there is a single\nuninterrupted run of statistically significant hot spot bins\nin the final time-step intervals but the location has never\nbeen a statistically significant hot spot prior to the final hot\nspot run and less than ninety percent of all time-steps are\nstatistically significant hot spots. These locations also tend\nto be detected as cold spots or as having no significance in\nthe spatial hopspot results in Fig. 1(b). Taken together,\nthese results indicate that these locations have recently be-\ncome hotspots. This again could be the result of the im-\nproved economy. It could also be the result of more photos\nbeing captured with GPS-enabled smartphones recently and\nthus having more accurate location information. This would\nmake the photos more concentrated.\nLocal Analysis AT&T park shows up as a spatio-temporal\nhotspot with respect to joy. It is also a location whose sen-\ntiment one might expect to be correlated with the perfor-\nmance of the professional baseball team that plays there, the\nYear\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nJo\ny \nRa\ntio\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n3\n5\n4\n3\n1\n2 1\n4\n2 2\nYear\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nD\nis\ngu\nst\n R\nat\nio\n0\n0.005\n0.01\n0.015\n0.02\n2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\nYear\n0\n2\n4\n6\n8\n10\n12\nM\ned\nia\nn \nSa\nle\ns \nPr\nic\ne \nin\n M\nis\nsi\non\n ($\n)\n×105\nFigure 3: Observed temporal trends for select regions. Left: Joy ratio computed yearly for AT&T park.\nShown above the bars are the end of season rankings of the SF Giants who play at the park. Notice the\nstrong correlation. Middle: Disgust ratio computed yearly for the Mission neighborhood. Notice the steady\nincrease since 2008 which is about when it started to become very popular with young professionals in the\ntech industry. Right: The average house price in the Mission neighborhood from a real estate website3.\nNotice the correlation with the disgust ratio.\nSF Giants. To investigate this, we calculate the joy ratio per\nyear for a window centered on the park. These values are\nplotted in Fig. 3(a). Shown above each bar is the end-of-\nseason ranking of the Giants for each year. The joy ratio\nand ranking are clearly correlated demonstrating that we\nare able to detect public sentiment from geotagged photos.\nWe also perform this local temporal analysis for another\nlocation, the Mission, for the emotion disgust. The Mis-\nsion is one of the less expensive residential neighborhoods\nin San Francisco and is shown to exhibit a relatively large\nnumber of disgust spatial hotspots as shown Fig. 1(a) (this\nfigure also delineates the neighborhood). We compute the\nper year disgust ratio in a window centered on the Mission\nand plot the results in Fig. 3(b). There is a clear increasing\ntrend since 2008 which is about when the Mission started\nto become very popular with young professionals in the tech\nindustry. These were not the traditional Mission residents\nand the detected increase in disgust could be a result of\ntheir reaction to the dirtiness, etc. of the streets. In fact,\nthe yearly disgust ratio is strongly correlated with the aver-\nage home price for the Mission, shown in Fig. 3(c)3. This\nincrease in housing prices is likely also a result of the new\ndemographic.\n5. CONCLUSIONS\nWe conduct the first investigation into using geotagged\nsocial multimedia for spatio-temporal sentiment hotspot de-\ntection. We leverage deep ConvNets to develop an emotion\nclassifier to predict the emotions conveyed in geotagged pho-\ntos. This allows us to associate sentiment with place. We\napply the Getis-Ord Gi* statistic to detect spatial hotspots,\nand show that different emotions have distinct spatial dis-\ntributions that match expectations. We detect emerging\nconcentrations of emotions through spatio-temporal hotspot\ndetection and show that year-by-year analyses of select lo-\ncations are correlated with known events.\n6. ACKNOWLEDGMENTS\nWe gratefully acknowledge the support of NVIDIA Cor-\nporation through the donation of the Titan X GPU used in\nthis work. This work was funded in part by a National Sci-\nence Foundation CAREER grant, #IIS-1150115, and a seed\ngrant from the Center for Information Technology in the In-\n3http://www.trulia.com/real estate/Mission-\nSan Francisco/1436/market-trends/\nterest of Society (CITRIS). We would like to thank the UC\nMerced Spatial Analysis and Research Center (SpARC) for\nhelp with the hotspot analysis.\n7. REFERENCES\n[1] D. Crandall et al. Mapping the World’s Photos. In\nWWW, 2009.\n[2] J. Deng et al. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR, 2009.\n[3] P. Ekman et al. What Emotion Categories or\nDimensions can Observers Judge from Facial\nBehavior. Emotion in the Human Face, 1982.\n[4] J. Hays and A. A. Efros. IM2GPS: Estimating\nGeographic Information from a Single Image. In\nCVPR, 2008.\n[5] Y.-G. Jiang et al. Speech Emotion Recognition Using\nDeep Neural Network and Extreme Learning Machine.\nIn AAAI, 2014.\n[6] A. Krizhevsky et al. ImageNet Classification with\nDeep Convolutional Neural Networks. In NIPS, 2012.\n[7] X. Lu et al. On Shape and the Computability of\nEmotions. In ACM MM, 2012.\n[8] H. B. Mann. Nonparametric Tests Against Trend.\nEconometrica, 1945.\n[9] J. Ord and A. Getis. Local Spatial Autocorrelation\nStatistics: Distributional Issues and an Application.\nGeographical Analysis, 1995.\n[10] S. Paldino et al. Urban Magnetism Through The Lens\nof Geo-tagged Photography. arXiv preprint\narXiv:1503.05502, 2015.\n[11] K.-C. Peng et al. A Mixed Bag of Emotions: Model,\nPredict, and Transfer Emotion Distributions. In\nCVPR, 2015.\n[12] B. Resch et al. Citizen-Centric Urban Planning\nthrough Extracting Emotion Information from\nTwitter in an Interdisciplinary Space-Time-Linguistics\nAlgorithm. Urban Planning, 2016.\n[13] N. Snavely et al. Modeling the World from Internet\nPhoto Collections. IJCV, 2008.\n[14] Q. You et al. Building a Large Scale Dataset for Image\nEmotion Recognition: The Fine Print and The\nBenchmark. In ACM MM, 2016.\n[15] Y. Zhu and S. Newsam. Land Use Classification Using\nConvolutional Neural Networks Applied to\nGround-Level Images. In ACM SIGSPATIAL, 2015.\n",
      "id": 37623147,
      "identifiers": [
        {
          "identifier": "10.1145/2996913.2996978",
          "type": "DOI"
        },
        {
          "identifier": "73378914",
          "type": "CORE_ID"
        },
        {
          "identifier": "2554034627",
          "type": "MAG_ID"
        },
        {
          "identifier": "188789615",
          "type": "CORE_ID"
        },
        {
          "identifier": "1609.06772",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:1609.06772",
          "type": "OAI_ID"
        }
      ],
      "title": "Spatio-Temporal Sentiment Hotspot Detection Using Geotagged Photos",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2554034627",
      "oaiIds": [
        "oai:arxiv.org:1609.06772"
      ],
      "publishedDate": "2016-09-21T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1609.06772"
      ],
      "updatedDate": "2021-08-04T20:06:47",
      "yearPublished": 2016,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1609.06772"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/37623147"
        }
      ]
    },
    {
      "acceptedDate": "2018-11-15T00:00:00",
      "arxivId": "1804.05276",
      "authors": [
        {
          "name": "Deb, Ashok"
        },
        {
          "name": "Ferrara, Emilio"
        },
        {
          "name": "Lerman, Kristina"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/540163085",
        "https://api.core.ac.uk/v3/outputs/201626531"
      ],
      "createdDate": "2018-04-18T20:35:28",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 22080,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/22080",
          "logo": "https://api.core.ac.uk/data-providers/22080/logo"
        },
        {
          "id": 645,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/645",
          "logo": "https://api.core.ac.uk/data-providers/645/logo"
        }
      ],
      "depositedDate": "2018-11-15T00:00:00",
      "abstract": "Recent high-profile cyber attacks exemplify why organizations need better\ncyber defenses. Cyber threats are hard to accurately predict because attackers\nusually try to mask their traces. However, they often discuss exploits and\ntechniques on hacking forums. The community behavior of the hackers may provide\ninsights into groups' collective malicious activity. We propose a novel\napproach to predict cyber events using sentiment analysis. We test our approach\nusing cyber attack data from 2 major business organizations. We consider 3\ntypes of events: malicious software installation, malicious destination visits,\nand malicious emails that surpassed the target organizations' defenses. We\nconstruct predictive signals by applying sentiment analysis on hacker forum\nposts to better understand hacker behavior. We analyze over 400K posts\ngenerated between January 2016 and January 2018 on over 100 hacking forums both\non surface and Dark Web. We find that some forums have significantly more\npredictive power than others. Sentiment-based models that leverage specific\nforums can outperform state-of-the-art deep learning and time-series models on\nforecasting cyber attacks weeks ahead of the events",
      "documentType": "research",
      "doi": "10.3390/info9110280",
      "downloadUrl": "http://arxiv.org/abs/1804.05276",
      "fieldOfStudy": "computer science",
      "fullText": "Predicting Cyber Events by Leveraging Hacker Sentiment\nAshok Deb, Kristina Lerman, Emilio Ferrara\nUSC Information Sciences Institute, Marina del Rey, CA\nEmail: {ashok, lerman, ferrarae}@isi.edu\nAbstract— Recent high-profile cyber attacks exemplify why\norganizations need better cyber defenses. Cyber threats are\nhard to accurately predict because attackers usually try to\nmask their traces. However, they often discuss exploits and\ntechniques on hacking forums. The community behavior of the\nhackers may provide insights into groups’ collective malicious\nactivity. We propose a novel approach to predict cyber events\nusing sentiment analysis. We test our approach using cyber\nattack data from 2 major business organizations. We consider\n3 types of events: malicious software installation, malicious\ndestination visits, and malicious emails that surpassed the target\norganizations’ defenses. We construct predictive signals by\napplying sentiment analysis on hacker forum posts to better\nunderstand hacker behavior. We analyze over 400K posts\ngenerated between January 2016 and January 2018 on over\n100 hacking forums both on surface and Dark Web. We find\nthat some forums have significantly more predictive power than\nothers. Sentiment-based models that leverage specific forums\ncan outperform state-of-the-art deep learning and time-series\nmodels on forecasting cyber attacks weeks ahead of the events.\nI. INTRODUCTION\nRecent high-profile cyber attacks—the massive denial of\nservice attack using Mirai botnet, infections of comput-\ners word-wide with WannaCry and Petya ransomware, the\nEquifax data breach—highlight the need for organizations\nto develop cyber crime defenses. Cyber threats are hard\nto identify and predict because the hackers that conduct\nthese attacks often obfuscate their activity and intentions.\nHowever, they may still use publicly accessible forums dis-\ncuss vulnerabilities and share tradecraft about how to exploit\nthem. The behavior of the hacker community, as expressed\nin such venues, may provide insights into group’s malicious\nintent. It has been shown that computational models based on\nvarious behavior learning theories can help in cyber security\nsituational awareness [1]. While cyber situation awareness\n[2], [3] is critical for defending networks, it is focused\non detecting cyber events. In this paper, we describe a\ncomputational method that analyzes discussions on hacker\nforums to predict cyber attacks.\nOpinion mining or sentiment analysis can be linked all\nthe way back to Freud’s 1901 paper on how slips of the\ntongue can reveal a person’s hidden intentions [4]. While\nsentiment analysis was originally developed in the field of\nlinguistics and psychology, it has recently been applied to\na number of other fields with the first seminal work in the\ncomputational sciences being Pang 2002 [5]. Historically, the\ncontext it has been applied to are social networks, comments\n(such as on news sites) and reviews (either for products or\nmovies). In this work, we apply sentiment analysis to posts\non Dark Web forums with the purpose of forecasting cyber\nattacks. The Dark Web consists of websites that are not\nindexed nor searchable by standard search engine and can\nonly be accessed using a special browser service.\nWe further explore the link between community behavior\nand malicious activity. The connection between security\nand human behavior has been studied in designing new\ntechnology [6], here we look to reverse engineer by mapping\nthe malicious events to hacker behavior. Social media has\nbeen shown to be a source of useful data on human behavior\nand used to predict real world events [7], [8], [9]. Here, we\ninspect the ability of hacker forums to predict cyber events.\nWe consider each individual forum, applying sentiment anal-\nysis to each post in the forum. After computing a daily\naverage per forum and a 7 day running average sentiment\nsignal per forum, we test these signals against ground truth\ndata. We determine some forums have significantly more\npredictive power and these isolated forums can beat the\nevaluation models in 36% of the months under study using\nprecision and recall of predictions within a 39-hour window\nof the event.\nII. DATA\nA. Hacker Forum Texts\nWe look at hacking forums from both the Surface Web\nand the Dark Web from 1 January 2016 to 31 January 2018.\nThe Dark Web refers to sites accessible through The Onion\nRouter private network platform [10]. The Surface Web\nrefers to the World Wide Web accessible through standard\nbrowsers. In this paper, we focus only on English posts from\n113 forums which were identified based on cyber security\nkeywords consisting of 432,060 posts. The text from these\nforums were accessed using the methods proposed in [11],\n[12].\nB. Ground truth data\nWe use ground truth data of cyber attacks from 2 major\norganizations in the Defense Industrial Base (DIB) industry.\nHenceforth, we will refer to them as Organization A and\nOrganization B for anonymity. The ground truth comprises\n3 event types:\n• endpoint-malware: a malicious software installation\nsuch as ransomware, spyware and adware is discovered\non a company endpoint device.\nar\nX\niv\n:1\n80\n4.\n05\n27\n6v\n1 \n [c\ns.C\nL]\n  1\n4 A\npr\n 20\n18\n• malicious-destination a visit by a user to a URL or IP\naddress that is malicious in nature or a compromised\nwebsite.\n• malicious-email receipt of an email that contains a\nmalicious email attachment and/or a link to a known\nmalicious destination.\nIII. SENTIMENT ANALYSIS\nThe first effective use of sentiment analysis in a predictive\nsense was by Pang et. al. [5] in assessing movie reviews.\nSince then, sentiment analysis has expanded to other fields.\nSentiment analysis can be done with or without supervision\n(label training data). Supervised methods can be adapted to\ncreate trained models for specific purposes and contexts. The\ndrawback is that labeled data may be highly costly and often\nresearchers end up using AMT - Amazon Mechanical Turk.\nThe alternative is to use lexical-based methods that do not\nrely on labeled data; however, it is hard to create a unique\nlexical-based dictionary to be used for all different contexts.\nDeep learning methods allow for additional functions like\ntaking into account order of words in a sentence like the\nStanford Recursive Deep Model. Methods can either be 2\nway (positive or negative) or 3 way (positive, neutral, neg-\native). Furthermore, dictionary based sentiment algorithms\nare either polarity-based where sentiment is based only of\nthe frequency of positive or negative words whereas valence-\nbased methods factor the intensity of the words into polarity.\nThere are a number of issues with sentiment analysis which\ninclude: word pairs, word tuples, emoticons, slang, sarcasm,\nirony, questions, URLs, code, domain specific use of words\n(shoot an email, dead link), and inversions (small is good\nfor portable electronics) which are difficult for computerized\ntext analysis to handle.\nStudies have found that a methods prediction performance\nvaries considerably from one dataset to another. VADER\nworks well for some tweets, but not for others, depending\non the context. SentiStrength has good Macro F1 values, but\nhas low coverage because it tends to classify a high number\nof instances as neutral.\nThe choice of a sentiment analysis is highly dependent\non the data and application, therefore you need to take\ninto account prediction performance and coverage. There\nis no single method that always achieves a consistent rank\nposition for different datasets. Therefore, in this paper we\ntest multiple methods for sentiment analysis. Most languages\nthemselves are biased positive and if a lexicon is built on\ndata, the positive bias that data can lead to a bias in the\nlexicon. This is why most methods are better at classifying\npositive than neutral or negative methods meaning that they\nare biased, neutral are the hardest to detect [13].\nA. Vader\nVADER: Valence Aware Dictionary for sEntiment Rea-\nsoning [14] is a rule-based sentiment model that has both a\ndictionary and associated intensity measures. It’s dictionary\nhas been tuned for microblog-like contexts and they incor-\nporate 5 generalizable rules that goes beyond pure dictionary\nlookups:\n1) Increase intensity due to exclamation point\n2) Increase intensity due to all caps in the presence of\nother non-all cap words\n3) Increase intensity with degree modifiers i.e. extremely\n4) Negate sentiment with contrastive conjunction i.e. but\n5) Examine the preceding tri-gram to identify cases where\nnegation flips the polarity of the text.\nTherefore, VADER not only captures positive or negative,\nbut also how positive and how negative beyond simple words\ncounts. It is made further robust by the additional rules. It’s\n”gold standard” lexicon was developed manually and with\nAmazon Mechanical Turk. Vader scores range from 0.0 to\n1.0.\nB. LIWC\nLinguistic Inquiry and Word Count (LIWC) [15] was a\npioneer in the computerized text analysis field with the first\nmajor iteration in 2007, we used the updated version LIWC\n2015. It has two components: the processing component and\nthe dictionaries. The heart of LIWC are the dictionaries that\ncontain the lookup words in psychometric categories which is\nable to resolve content words from style words. LIWC counts\nthe inputted words in psychologically meaningful categories\nwhich produces close to 100 dimensions for any given text\nbeing analyzed. For the purposes of this research, we are\nonly focused on Tone which bests maps to sentiment as we\nhave defined it. The Tone scores range from 0 to 100. LIWC\nalso ignores context, irony, sarcasm, and idioms.\nC. SentiStrength\nSentiStrength [16] is another lexicon-based sentiment clas-\nsifier which leverages dictionaries and non-lexical linguistics\ninformation to detect sentiment. SentiStrength focuses on the\nstrength of the sentiment and uses weights for the words\nin its dictionaries. Additionally, positive sentiment strength\nand negative sentiment strength is scored separately. Each\nis scored from 1 to 5, with 5 being the greatest strength.\nFor our purposes, we seek overall sentiment so we subtract\nthe negative sentiment from the positive sentiment so that\nstrongly positive (5,1) becomes 4, neutral (1,1) becomes\n0 and strongly negative (1,5) becomes -4. Therefore, Sen-\ntiStrength scorese range from -4 to 4. SentiStrength is\ndesigned to do better with social media; however, it can’t\nexploit indirect indicators of sentiment. It is also weaker for\npositive sentiment in news-related discussions.\nIV. METHODOLOGY\nIn this section we document the methodology used and\nprocess workflow from the data processing to signal gener-\nation through warning generation and signal testing. Three\ncases studies are used to illustrate the process via example\nand Figure X provides a visual reference.\nA. Processing the Data\nWorking with researchers at Arizona State University, we\nwere able to develop a database of posts from forums on\nboth the Dark Web and Surface Web which discuss computer\nsecurity and network vulnerability topics. To protect the\nfuture utility of these sources, each forum has been coded\nwith a number (forumid) from 1 to 350. The data consist\nof the forumid, date the post was made, and the text of the\npost. The data in this study was from 1 January 2016 to 31\nJanuary 2018. The data was collected by ASU and we used\nan API to pull and store the data in a local server and access\nit via Apache Lucene’s Elastic Search engine.\nB. Evaluating Sentiment Analysis\nAfter a review of the sentiment analysis methods in Sen-\ntiBench [13], we decided to use Vader[14], SentiStrenght[16]\nand LIWC15[15]. For social networks, VADER and LIWC15\nwere found to be the best method for 3-class classification\nand SentiStrength was the winner for 2-class classification.\n[13] These three methods were used because they Vader has\na Python module, SentiStrenght has a Java implementation\nand LIWC15 is a stand-alone program.\nC. Computing Daily Averages\nA sentiment score for each forum post was computed using\nthe three sentiment methods outlined above. Since there can\nbe multiple posts on a forum for a day, we characterization\nthe overall sentiment of the day with a daily average. There\ncan be a wide range of sentiment scores for any given day,\nespecially if there are a lot of posts from on a popular forum.\nIn order to understand the trend of sentiment over time, we\ncompute running averages.\nD. Computing Running Averages\nA running daily average was computed in order to assess\nthe trend of sentiment over time. The more days in the run-\nning average, the smoother the curve and the harder to detect\na change. Whereas no using a running average or making it\nonly 1 or 2 days would have many jump discontinuities and\nswings. We looked at adjusting the running average from 1\nto 30 days and settled on 7 days primarily because that was\nour original prediction window. Figure 1 shows the average\nF1 score various signals computed with running averages of\n3, 7, 10 and 14 days.\nE. Standardizing the Score\nTo make the 3 sentiment scores more comparable, their\nscores were standardized. As previously mentioned, VADER\ngenerates sentiment scores on a scale of 0 to 1, SentiStrength\ngoes from -4 to 4, and LIWC goes from 0 to 100 for Tone.\nWhile standardizing the scores do not affect the correlation\nany forum would have with the ground truth from our\ntarget organizations, it will be necessary when we potentially\ncombine signals from various forums and sentiment methods\nto find more powerful predictors.\nFig. 1: Average F1 Scores by Signal using Different Running\nAverages\nF. Compute Correlations to Find Potential Signals\nAs previously mention, we have ground truth events\nfrom 2 defense industrial base organizations of 3 different\ncyber event types. The event types are endpoint-malware,\nmalicious-destination and malicious-email. Correlations were\ncomputed between all forum-sentiments against all event\ntypes from both organizations. Additionally, since we are\nlooking for predictive signals, we computed correlations with\na negative lag from 0 to 30 days with a lag of -30 meaning\noffset the sentiment signal 30 days before the organization’s\nevent occurrence. A number of signals stood out as being\nmore correlated than others against certain event types as\nseen in Figure I. This shows the LIWC sentiment on Forum\n84 against Organization B’s endpoint-malware events. The\nfact that multiple, consecutive lags have low p-values gives\nsome indication that this might be a useful signal.\nG. Forecasting Models\nWe also apply widely-used ARIMA model for forecasting\nevents. ARIMA stands for autoregressive integrated mov-\ning average. The key idea is that the number of current\nevents (yt) depends on the past counts and forecast errors.\nFormally, ARIMA(p,d,q) defines an autoregressive model\nwith p autoregressive lags, d difference operations, and q\nmoving average lags (see [17]). Given the observed series of\nevents Y = (y1, y2, . . . , yT ), ARIMA(p,d,q) applies d (≥ 0)\ndifference operations to transform Y to a stationary series Y ′.\nThen the predicted value y′t at time point t can be expressed\nin terms of past observed values and forecasting errors which\nis as follows:\ny′t = µy ++\np∑\ni=1\nαiy\n′\nt−i +\nq∑\nj=1\nβjet−j + et (1)\nHere µy is a constant, αi is the autoregressive (AR)\ncoefficient at lag i, βj is the moving average (MA) coefficient\nat lag j, et−j = y′t−j− yˆ′t−j is the forecast error at lag j, and\net is assumed to be the white noise (et ∼ N (0, σ2)). The\nAR model is essentially an ARIMA model without moving\naverage terms.\nTABLE I: Best Signals for Organization B’s Events\nForum# Sent Lag Correlation p Value Events\n84 LIWC -11 0.2170 0.000055 EP-Mal\n84 LIWC -12 0.2221 0.000037 EP-Mal\n84 LIWC -14 0.2185 0.000052 EP-Mal\n219 Vader -18 -0.2329 0.000079 EP-Mal\n264 LIWC -10 0.2472 0.000040 EP-Mal\n264 LIWC -12 0.2362 0.000095 EP-Mal\n264 LIWC -15 0.2380 0.000091 EP-Mal\n159 Senti -14 0.8498 0.000008 Mal-Email\n266 Senti -14 -0.5517 0.000058 Mal-Email\n261 LIWC -3 0.2173 0.000043 Mal-Dest\n266 Senti -27 -0.6243 0.000080 Mal-Dest\nWe use maximum likelihood estimation for learning the\nparameters; more specifically, parameters are optimized with\nLBFGS method [18]. These models assume that (p, d, q) are\nknown and the series is weakly stationary. To select the\nvalues for (p, d, q) we employ grid search over the values\nof (p, d, q) and select the one with minimum AIC score.\nH. Testing Signals with ARIMAX\nAgain, Table I shows the signals that are better correlated\nwith Organization B’s ground truth events. The next step\nis to test these signals to see if they have any predictive\npower. To do this, the ARIMA model is used with the ground\ntruth events to develop a baseline model from which to\ncompare potential signals for the potential to have predictive\npower. Additionally, 4 other methods were used for com-\nparison: Dark-Mentions, Deep-Exploit [19], ARIMAX with\nabuse.ch and a daywise-hourly-baserate model. Using ground\ntruth events from both Organization A and Organization\nB, sentiment signals from the various forums, computed\nwith the different methodologies were tested. Testing was\ndone across the 3 event types for both Organizations with\nPrecision, Recall and F1 computed to evaluate the signal. The\ntimeseries of the sentiment for a given forum and sentiment\nmethod was used as the input to the timeseries forecasting\nmodel to predict future events. The model was trained on\ndata from April 1, 2016 to May 31, 2017, in order to\nstart generating warnings for the month of June 2017. After\npredictions were made for the month of June, they were\nscored against the actual ground truth and then the model\nwas ran again to predict warnings for August 2017. This\nwas done for all the way through January 2018.\nI. Scoring\nTo determine how well the signals under study performed,\na matching algorithm was used to compare the date occur-\nrence of the predicted events with the actual events that\noccurred. Using the matching algorithm, we could consis-\ntently score which predicted events should be mapped to\nactual events and which predicted events did not occur as\nwell as which actual events were not predicted. There is a\nwindow around the actual events which varies based on the\nevent type. Endpoint-maleware has to be within 0.875 days,\nmalicious-destination within 1.625 days and malicious-email\nwithin 1.375 days.\nJ. External Signals\nCurrently, there are other external signals that the data\nprovider Organizations are currently evaluating for predictive\npotential. Again, external signals are timeseries information\nderived from open sources that are not based on informa-\ntion system network data. The other external signals under\nevaluation are:\n• ARIMAX: is the same model outlined in §4.7; however,\ntime series counts of malicious activity are acquired\nfrom https://abuse.ch and used in conjunction\nwith historical data.\n• Baseline: is the exact same model in §4.7 with no\nexternal signal and using only historical ground truth\ndata to predict the future rate of attack.\n• Daywise-Baserate: is the same as the ARIMAX model\nmentioned above; however, the model takes day of the\nweek into consideration assuming that the event rate for\neach day of the week is not the same.\n• Deep-Exploit: is an ARIMA model that is based on the\nvulnerability analysis determined by [20]. This method\nreferred to as DarkEmbed learns the embeddings of\nDark Web posts and then uses a trained exploit classifier\nto predicted which vulnerabilities in Dark Web posts\nmight be exploited.\n• Dark-Mentions: Is an extension of [21] which predicts\nif a disclosed vulnerability will be exploited based on\na variety of data sources in addition to the Dark Web\nusing methods still being developed. These predictions\nare used to construct a rule based forecasting method\nbased on keyword mentions in Dark Web forums and\nmarketplaces.\nV. RESULTS\nAfter generating ARIMAX models with each potential\nsignal, they were scored as mentioned above for each month\nfrom July 2017 to January 2018. The following tables show\nthe results for the months under study. By month, you can see\nthe number of actual ground truth events (Evt), the number of\nwarnings generated by each signal (Warn), and the precision\n(P), recall (R) and F1 score for each. The table is sorted by\nlargest F1 score for each month with only the top five signals\nlisted. Signals generated by sentiment analysis that were part\nof the top five for each month are highlighted in light blue.\nA. Organization A\nTable II shows Organization A’s endpoint-malware where\nsentiment signals dominated July, September and November\nand did reasonable well in the remaining months. Every\nmonth a sentiment signal beat at least on evaluation model.\nMalicious-Destination (Table III) had periodic performance\nJuly, September, November and January but the case is\nnot as strong as Endpoint-Malware. Lastly, Table IV shows\nMalicious-Email results which illustrate that sentiment sig-\nnals did well in July to September with waning results for\nthe later months. Upon further inspection this is believed to\nbe due to some key forums going offline toward the end of\nthe year.\nTABLE II: Results from Organization A’s Endpoint-Malware\nMonth Evt Warn Signal P R F1\nJuly 15 14 forum211-Senti 0.57 0.53 0.55\nJuly 15 29 forum196-LIWC 0.41 0.80 0.55\nJuly 15 27 forum89-Senti 0.41 0.73 0.52\nJuly 15 12 forum111-LIWC 0.58 0.47 0.52\nJuly 15 9 baseline 0.67 0.40 0.50\nAugust 19 14 baseline 0.71 0.53 0.61\nAugust 19 11 forum111-LIWC 0.82 0.47 0.60\nAugust 19 35 forum8-Vader 0.46 0.84 0.59\nAugust 19 8 daywise-baserate 1.00 0.42 0.59\nAugust 19 23 forum230-Senti 0.52 0.63 0.57\nSeptember 18 16 forum111LIWC 0.69 0.61 0.65\nSeptember 18 32 forum250LIWC 0.50 0.89 0.64\nSeptember 18 35 forum211vader 0.46 0.89 0.60\nSeptember 18 41 forum147LIWC 0.41 0.94 0.58\nSeptember 18 41 forum194LIWC 0.41 0.94 0.58\nOctober 6 14 daywise-baserate 0.29 0.67 0.40\nOctober 6 35 baseline 0.17 1.00 0.29\nOctober 6 29 forum8vader 0.17 0.83 0.29\nOctober 6 37 forum111LIWC 0.16 1.00 0.28\nOctober 6 43 forum211vader 0.14 1.00 0.24\nNovember 27 38 forum6senti 0.63 0.89 0.74\nNovember 27 42 forum147LIWC 0.60 0.93 0.72\nNovember 27 40 forum111LIWC 0.60 0.89 0.72\nNovember 27 41 forum211senti 0.59 0.89 0.71\nNovember 27 43 forum121LIWC 0.56 0.89 0.69\nDecember 13 18 arimax 0.33 0.46 0.39\nDecember 13 16 dark-mentions 0.31 0.38 0.34\nDecember 13 80 forum121LIWC 0.16 1.00 0.28\nDecember 13 73 forum194LIWC 0.16 0.92 0.28\nDecember 13 10 deep-exploit 0.30 0.23 0.26\nJanuary 1 15 dark-mentions 0.07 1.00 0.13\nJanuary 1 37 forum6senti 0.03 1.00 0.05\nJanuary 1 61 forum147LIWC 0.02 1.00 0.03\nJanuary 1 64 baseline 0.02 1.00 0.03\nJanuary 1 19 arimax 0.00 0.00 0.00\nB. Organization B\nTable V shows that sentiment signals do best for July\nand October for Malicious-Destination. While baseline and\ndaywise-baserate dominate the other months, sentiment sig-\nnals perform better than the other evaluation models. Similar\nto Organization A, the Malicious-Destination for Organiza-\ntion B (Table VI) does the best early in July in August\nand moderately well in September to November until de-\ngrading to below all evaluation models in December and\nJanuary. This may be due the few number of events and\nperhaps sentiment signals do not perform the best under low\nfrequency conditions. The performance for Malicious-Email\n(Table VII) is oddly cyclical; however, sentiment signals\ndominated December and beat at least one evaluation model\nfor every month.\nVI. RELATED WORK\nGiven the serious nature of cyber attacks, naturally there\nare a number of other research efforts to predict such attacks.\nAs it relates to our efforts, the three main areas of research\nare sentiment analysis in cyber security, predictive methods\nfor cyber attacks and leveraging dark web data in cyber\nsecurity.\nTABLE III: Results from Organization A’s Malicious-\nDestination\nMonth Evt Warn Signal P R F1\nJuly 4 5 baseline 0.40 0.50 0.44\nJuly 4 3 daywise-baserate 0.33 0.25 0.29\nJuly 4 17 dark-mentions 0.12 0.50 0.19\nJuly 4 42 forum266-LIWC 0.05 0.50 0.09\nJuly 4 0 arimax 0.00 0.00 0.00\nAugust 10 6 baseline 1.00 0.60 0.75\nAugust 10 10 daywise-baserate 0.60 0.60 0.60\nAugust 10 8 dark-mentions 0.50 0.40 0.44\nAugust 10 0 arimax 0.00 0.00 0.00\nAugust 10 0 deep-exploit 0.00 0.00 0.00\nSeptember 4 15 forum194LIWC 0.20 0.75 0.32\nSeptember 4 15 forum210LIWC 0.20 0.75 0.32\nSeptember 4 15 forum264LIWC 0.20 0.75 0.32\nSeptember 4 15 forum6senti 0.20 0.75 0.32\nSeptember 4 15 forum194LIWC 0.20 0.75 0.32\nOctober 2 0 arimax 0.00 0.00 0.00\nOctober 2 0 dark-mentions 0.00 0.00 0.00\nOctober 2 5 daywise-baserate 0.00 0.00 0.00\nOctober 2 0 deep-exploit 0.00 0.00 0.00\nNovember 1 5 daywise-baserate 0.20 1.00 0.33\nNovember 1 6 forum111LIWC 0.17 1.00 0.29\nNovember 1 6 forum147LIWC 0.17 1.00 0.29\nNovember 1 30 forum210senti 0.03 1.00 0.06\nNovember 1 0 arimax 0.00 0.00 0.00\nDecember 1 10 daywise-baserate 0.10 1.00 0.18\nDecember 1 11 dark-mentions 0.09 1.00 0.17\nDecember 1 0 arimax 0.00 0.00 0.00\nDecember 1 0 deep-exploit 0.00 0.00 0.00\nJanuary 2 24 forum111LIWC 0.08 1.00 0.15\nJanuary 2 0 arimax 0.00 0.00 0.00\nJanuary 2 10 dark-mentions 0.00 0.00 0.00\nJanuary 2 9 daywise-baserate 0.00 0.00 0.00\nJanuary 2 0 deep-exploit 0.00 0.00 0.00\nA. Sentiment Analysis in Cyber Security\nThe closest work which has applied sentiment analysis\nof hacker forums to cyber security is [22]. While much\nresearch has investigated the specifics of cyber attacks, [22]\ninvestigates the actual cyber actors via their communication\nactivities. Their focus was the cyber-physical systems related\nto critical infrastructure and they developed an automated\nanalysis tool to identify potential threats against such in-\nfrastructure. Despite recognizing that there are over 140\nhacker forums on the public web, the authors chose only one\nforum to scrape the complete forum once. They leveraged\nthe Open Discussion Forum Crawler to do the scrapping\nand then used OpenNLP to tag parts of speech, filtering on\nnouns. Those nouns were cross referenced with three list\nof malicious keywords to identify posts whose sentiment\nwould be determined with SentiStrength. Contextual analysis\nof keyword pairings with sentiment scores allowed them to\nconfirm current statistics about critical infrastructure cyber\nattacks. The main differences illustrated in our work is\nthat we use looked at over 100 forums, not just one from\nboth the Dark Web and Surface Web. In collecting posts\nfor over a two year period, we found the sentiment of all\nposts applying Vader and LIWC for sentiment in addition to\njust SentiStrength. Furthermore, we were able to model our\ndata against ground truth events from companies making our\napproach predictive in nature.\nTABLE IV: Results from Organization A’s Malicious-Email\nMonth Evt Warn Signal P R F1\nJuly 26 21 forum210-LIWC 0.76 0.62 0.68\nJuly 26 27 forum250-LIWC 0.67 0.69 0.68\nJuly 26 19 forum147-LIWC 0.74 0.54 0.62\nJuly 26 36 forum159-Senti 0.53 0.73 0.61\nJuly 26 17 forum28-LIWC 0.76 0.50 0.60\nAugust 11 17 forum179-Vader 0.59 0.91 0.71\nAugust 11 15 forum250-LIWC 0.60 0.82 0.69\nAugust 11 7 daywise-baserate 0.86 0.55 0.67\nAugust 11 18 forum210-Senti 0.50 0.82 0.62\nAugust 11 25 forum159-Senti 0.44 1.00 0.61\nSeptember 15 36 forum264LIWC 0.36 0.87 0.51\nSeptember 15 17 daywise-baserate 0.47 0.53 0.50\nSeptember 15 18 forum210senti 0.44 0.53 0.48\nSeptember 15 45 forum147LIWC 0.31 0.93 0.47\nSeptember 15 46 forum6senti 0.28 0.87 0.43\nOctober 11 14 daywise-baserate 0.50 0.64 0.56\nOctober 11 8 deep-exploit 0.50 0.36 0.42\nOctober 11 42 forum264LIWC 0.17 0.64 0.26\nOctober 11 51 forum194LIWC 0.16 0.73 0.26\nOctober 11 102 forum8vader 0.11 1.00 0.19\nNovember 50 16 daywise-baserate 0.69 0.22 0.33\nNovember 50 4 deep-exploit 0.75 0.06 0.11\nNovember 50 0 arimax 0.00 0.00 0.00\nNovember 50 0 dark-mentions 0.00 0.00 0.00\nDecember 17 22 daywise-baserate 0.55 0.71 0.62\nDecember 17 10 deep-exploit 0.80 0.47 0.59\nDecember 17 5 dark-mentions 0.80 0.24 0.36\nDecember 17 0 arimax 0.00 0.00 0.00\nJanuary 40 18 daywise-baserate 0.94 0.43 0.59\nJanuary 40 8 deep-exploit 0.75 0.15 0.25\nJanuary 40 6 dark-mentions 0.83 0.13 0.22\nJanuary 40 0 arimax 0.00 0.00 0.00\nBiSAL [23] did sentiment analysis in English and Arabic\non Dark Web forums with slight modification to cyber\nsecurity terms. Other work such as [24] use sentiment in\nmeasuring radicalization. Remaining research in sentiment\nanalysis, not specific to cyber security was presented earlier.\nB. Predicting Cyber Attack\nThe issue of predicting cyber attacks is not new and\ntheir has been a considerable research effort in this field.\nThe efforts split along two categories, using network traffic\nor non-network traffic. Forecasting methods such as [25],\n[26], [27] analyze network traffic. Where [25] is specific\nto predicting attacks using IPV4 packet traffic, and [26]\nlooks at various network sensors at different layers to prevent\nunwanted Internet traffic, whereas [27] combines DNS traffic\nwith security metadata such as number of policy violations\nand the number of clients in the network. Many researchers\nsuch as [28] based cyber prediction on open source infor-\nmation. They use the National Vulnerability Database. They\nhighlight the difficulty in using public sources for building\neffective models. Other work has focused on detecting cyber\nbullying using graph detection models [29] with success, but\nis limited in malicious activity and not a predictive model.\nThe closest to our research is Gandotra et al [30] who\noutlined a number cyber prediction efforts using statistical\nmodeling and algorithmic modeling. They highlight several\nsignificant challenges that we tried to address. The first\nchallenge is that open source ground truth is incomplete\nTABLE V: Results from Organization B’s Endpoint-Malware\nMonth Evt Warn Signal P R F1\nJuly 18 47 forum264LIWC 0.38 1.00 0.55\nJuly 18 50 forum250LIWC 0.36 1.00 0.53\nJuly 18 43 baseline 0.37 0.89 0.52\nJuly 18 35 forum8senti 0.37 0.72 0.49\nJuly 18 50 forum111LIWC 0.32 0.89 0.47\nAugust 28 39 baseline 0.67 0.93 0.78\nAugust 28 31 forum264LIWC 0.65 0.71 0.68\nAugust 28 32 forum121LIWC 0.63 0.71 0.67\nAugust 28 35 forum211vader 0.60 0.75 0.67\nAugust 28 33 forum194LIWC 0.61 0.71 0.66\nSeptember 31 40 baseline 0.60 0.77 0.68\nSeptember 31 38 forum210senti 0.61 0.74 0.67\nSeptember 31 37 forum121LIWC 0.57 0.68 0.62\nSeptember 31 46 forum219vader 0.50 0.74 0.60\nSeptember 31 30 forum194LIWC 0.60 0.58 0.59\nOctober 53 44 forum210LIWC 0.77 0.64 0.70\nOctober 53 47 baseline 0.74 0.66 0.70\nOctober 53 41 forum264LIWC 0.78 0.60 0.68\nOctober 53 39 forum250LIWC 0.74 0.55 0.63\nOctober 53 40 forum8vader 0.73 0.55 0.62\nNovember 37 52 daywise-baserate 0.62 0.86 0.72\nNovember 37 49 forum121LIWC 0.57 0.76 0.65\nNovember 37 53 forum147LIWC 0.55 0.78 0.64\nNovember 37 50 forum111LIWC 0.56 0.76 0.64\nNovember 37 50 forum194LIWC 0.56 0.76 0.64\nDecember 35 30 daywise-baserate 0.67 0.57 0.62\nDecember 35 27 baseline 0.63 0.49 0.55\nDecember 35 23 forum250LIWC 0.65 0.43 0.52\nDecember 35 28 forum194LIWC 0.57 0.46 0.51\nDecember 35 29 forum147LIWC 0.55 0.46 0.50\nJanuary 43 42 baseline 0.60 0.58 0.59\nJanuary 43 37 daywise-baserate 0.59 0.51 0.55\nJanuary 43 35 forum219vader 0.60 0.49 0.54\nJanuary 43 37 forum111LIWC 0.57 0.49 0.53\nJanuary 43 37 forum147LIWC 0.57 0.49 0.53\nand should be compiled from multiple sources and analysis\ndoesn’t scale to real world scenarios. We were able to get\nground truth data from 2 companies that operate in the\ndefense industrial base, this ground truth is across three\ndifferent attack vectors and is over a two year time period.\nThe additional challenges in [30] focus on the volume, speed\nand heterogeneity of network data which we avoid since we\nare attempting to prevent cyber events specifically with non-\nnetwork data. They also present two modeling approaches\nof statistical modeling and algorithmic modeling. We used\nstatistical models not unlike what they present as classical\ntime series models with auto-regressive, integrated moving\naverage with historical data and external signals.\nC. Dark Web Research\nThere has been a lot of research recently concerning the\nDark Web or websites not indexed by major search engines.\nTypically the Dark Web refers to the TOR [10] network\nwhich is only accessible via specialized browsers. It has been\nshown by [12] that from an overall cyber security threat\nperspective, the Dark Web provides a valuable source of\ninformation for malicious activity. They developed a system\nthat scrapes hacker forum and marketplace sites on the Dark\nWeb to develop threat warnings for cyber defenders. We\nleverage the same data source but perform sentiment analysis\nTABLE VI: Results from Organization B’s Malicious-\nDestination\nMonth Evt Warn Signal P R F1\nJuly 6 8 forum130vader 0.63 0.83 0.71\nJuly 6 8 forum8senti 0.63 0.83 0.71\nJuly 6 8 forum111LIWC 0.50 0.67 0.57\nJuly 6 12 forum194LIWC 0.42 0.83 0.56\nJuly 6 9 forum210senti 0.44 0.67 0.53\nAugust 8 6 forum210senti 0.67 0.50 0.57\nAugust 8 17 daywise-baserate 0.35 0.75 0.48\nAugust 8 13 forum211senti 0.38 0.63 0.48\nAugust 8 5 forum210LIWC 0.60 0.38 0.46\nAugust 8 21 forum8vader 0.29 0.75 0.41\nSeptember 6 11 daywise-baserate 0.55 1.00 0.71\nSeptember 6 9 forum210LIWC 0.56 0.83 0.67\nSeptember 6 10 forum250LIWC 0.30 0.50 0.37\nSeptember 6 11 forum121LIWC 0.27 0.50 0.35\nSeptember 6 1 forum147LIWC 1.00 0.17 0.29\nOctober 9 8 daywise-baserate 0.25 0.22 0.24\nOctober 9 2 forum121LIWC 0.50 0.11 0.18\nOctober 9 114 forum210senti 0.03 0.33 0.05\nOctober 9 0 arimax 0.00 0.00 0.00\nOctober 9 0 dark-mentions 0.00 0.00 0.00\nNovember 4 14 daywise-baserate 0.29 1.00 0.44\nNovember 4 5 forum210LIWC 0.20 0.25 0.22\nNovember 4 21 forum219vader 0.10 0.50 0.16\nNovember 4 9 forum211vader 0.11 0.25 0.15\nNovember 4 13 forum210senti 0.08 0.25 0.12\nDecember 3 12 daywise-baserate 0.17 0.67 0.27\nDecember 3 0 arimax 0.00 0.00 0.00\nDecember 3 0 dark-mentions 0.00 0.00 0.00\nDecember 3 0 deep-exploit 0.00 0.00 0.00\nJanuary 5 18 daywise-baserate 0.22 0.80 0.35\nJanuary 5 0 arimax 0.00 0.00 0.00\nJanuary 5 0 dark-mentions 0.00 0.00 0.00\nJanuary 5 0 deep-exploit 0.00 0.00 0.00\nto not only predict future threats, but to predict actual attacks.\nThey also leverage the Deep Net which is the portion of the\nSurface Web not indexed by standard search engines.\nWhile not using sentiment analysis, [31] offers insight to\nthe trust establishment between participants in Dark Web\nforums. There may be behavioral patterns of malicious actors\nthat provide insight to future activity. Dark Web conversa-\ntions were shown to provide earlier insights than Surface\nWeb conversations by [19] indicating potential predictive\npower for cyber events. [19] highlight two cases with a major\nDDoS attack and the Mirai attack. There may also be early\ninsights on the Surface Web in many of the social media\nsites as illustrated in [32]. Our work focused only on forums\nwhere it was likely that computer security items would be\ndiscussed, but does contain a mix of Dark Web and Surface\nWeb. There has been work using natural language processing\non Dark Web text for predictive method such as [20]. Other\npredictive approaches such as Cyber Attacker Model Profile\n(CAMP) [33], focus on the macro level of a country and\nfinancial cyber crimes, where we look at a wider range of\nmalicious activity against specific target organizations.\nVII. CONCLUSIONS\nMalicious activity can be very devastating to national\nsecurity, economies, businesses and personal lives. As such,\ncyber security professionals working with major organiza-\ntions and nation states could use all the help they can get\nTABLE VII: Results from Organization B’s Malicious-Email\nMonth Evt Warn Signal P R F1\nJuly 24 49 forum210LIWC 0.33 0.67 0.44\nJuly 24 56 forum210senti 0.30 0.71 0.43\nJuly 24 75 baseline 0.23 0.71 0.34\nJuly 24 81 daywise-baserate 0.21 0.71 0.32\nJuly 24 81 forum130vader 0.21 0.71 0.32\nAugust 57 55 forum111LIWC 0.55 0.53 0.54\nAugust 57 70 baseline 0.49 0.60 0.54\nAugust 57 91 daywise-baserate 0.43 0.68 0.53\nAugust 57 107 forum147LIWC 0.39 0.74 0.51\nAugust 57 153 forum6senti 0.33 0.88 0.48\nSeptember 179 70 daywise-baserate 0.76 0.30 0.43\nSeptember 179 102 forum210senti 0.58 0.33 0.42\nSeptember 179 180 forum210LIWC 0.40 0.40 0.40\nSeptember 179 100 forum147LIWC 0.54 0.30 0.39\nSeptember 179 76 baseline 0.57 0.24 0.34\nOctober 71 125 daywise-baserate 0.50 0.87 0.63\nOctober 71 118 baseline 0.49 0.82 0.61\nOctober 71 90 forum211senti 0.53 0.68 0.60\nOctober 71 142 forum194LIWC 0.44 0.89 0.59\nOctober 71 150 forum210senti 0.42 0.89 0.57\nNovember 426 104 daywise-baserate 0.67 0.16 0.26\nNovember 426 205 forum264LIWC 0.39 0.19 0.25\nNovember 426 118 baseline 0.55 0.15 0.24\nNovember 426 251 forum210LIWC 0.31 0.18 0.23\nNovember 426 579 forum210senti 0.20 0.27 0.23\nDecember 51 69 forum210LIWC 0.30 0.41 0.35\nDecember 51 329 forum147LIWC 0.09 0.55 0.15\nDecember 51 313 forum111LIWC 0.08 0.51 0.14\nDecember 51 249 forum194LIWC 0.08 0.41 0.14\nDecember 51 284 forum211senti 0.08 0.45 0.14\nJanuary 10 12 deep-exploit 0.25 0.30 0.27\nJanuary 10 103 daywise-baserate 0.10 1.00 0.18\nJanuary 10 186 baseline 0.05 1.00 0.10\nJanuary 10 226 forum111LIWC 0.04 1.00 0.08\nin preventing malicious activity. We present a methodology\nto predict malicious cyber events by exploiting malicious\nactor’s behavior via sentiment analysis of posts on hacker\nforums. These forums on both Surface Web and Dark Web\nhave some predictive power to be used as signals external to\nthe network for forecasting attacks using time series models.\nUsing ground truth data from two major organizations in the\nDefense Industrial Base across three different cyber event\ntypes, we show that sentiment signals can be more predictive\nthan a baseline time series model. Additionally, they will\noften beat other state of the art external signals, in the 7\nmonths under study across the 3 event types from the 2\norganizations, sentiment signals performed the best 15 out\nof 42 times or 36%. The signal parameters need to be tuned\nover significant historical data and the source forum could be\nshut off or taken down at any time; however, an automated\nimplementation of this system would still be value added.\nACKNOWLEDGMENT\nThis work has been partly funded by the Intelligence Advanced Research\nProjects Activity (IARPA). The views and conclusions contained herein are\nthose of the authors and should not be interpreted as necessarily representing\nthe official policies, either expressed or implied, of IARPA, or the U.S.\nGovernment. The U.S. Government had no role in study design, data\ncollection and analysis, decision to publish, or preparation of the manuscript.\nThe U.S. Government is authorized to reproduce and distribute reprints for\ngovernmental purposes notwithstanding any copyright annotation therein.\nREFERENCES\n[1] V. Dutt, Y.-S. Ahn, and C. Gonzalez, “Cyber situation awareness:\nmodeling detection of cyber attacks with instance-based learning\ntheory,” Human Factors, vol. 55, no. 3, pp. 605–618, 2013.\n[2] S. Jajodia, P. Liu, V. Swarup, and C. Wang, Cyber situational aware-\nness. Springer, 2009.\n[3] U. Franke and J. Brynielsson, “Cyber situational awareness–a system-\natic review of the literature,” Computers & Security, vol. 46, pp. 18–31,\n2014.\n[4] S. Freud, “1960. the psychopathology of everyday life,” The standard\nedition of the complete psychological works of Sigmund Freud, vol. 6,\n1901.\n[5] B. Pang, L. Lee, and S. Vaithyanathan, “Thumbs up?: sentiment\nclassification using machine learning techniques,” in Proceedings of\nthe ACL-02 conference on Empirical methods in natural language\nprocessing-Volume 10, pp. 79–86, Association for Computational\nLinguistics, 2002.\n[6] S. L. Pfleeger and D. D. Caputo, “Leveraging behavioral science to\nmitigate cyber security risk,” Computers & security, vol. 31, no. 4,\npp. 597–611, 2012.\n[7] S. Agarwal and A. Sureka, “Applying social media intelligence for\npredicting and identifying on-line radicalization and civil unrest ori-\nented threats,” arXiv preprint arXiv:1511.06858, 2015.\n[8] S. Asur and B. A. Huberman, “Predicting the future with social\nmedia,” in Proceedings of the 2010 IEEE/WIC/ACM International\nConference on Web Intelligence and Intelligent Agent Technology-\nVolume 01, pp. 492–499, IEEE Computer Society, 2010.\n[9] E. Kalampokis, E. Tambouris, and K. Tarabanis, “Understanding the\npredictive power of social media,” Internet Research, vol. 23, no. 5,\npp. 544–559, 2013.\n[10] R. Dingledine, N. Mathewson, and P. Syverson, “Tor: The second-\ngeneration onion router,” tech. rep., Naval Research Lab Washington\nDC, 2004.\n[11] J. Robertson, A. Diab, E. Marin, E. Nunes, V. Paliath, J. Shakarian, and\nP. Shakarian, Darkweb Cyber Threat Intelligence Mining. Cambridge\nUniversity Press, 2017.\n[12] E. Nunes, A. Diab, A. Gunn, E. Marin, V. Mishra, V. Paliath,\nJ. Robertson, J. Shakarian, A. Thart, and P. Shakarian, “Darknet\nand deepnet mining for proactive cybersecurity threat intelligence,”\nin Intelligence and Security Informatics (ISI), 2016 IEEE Conference\non, pp. 7–12, IEEE, 2016.\n[13] R. et al, “Sentibench - a benchmark comparison of state-of-the-practice\nsentiment analysis methods,” in EPJ Data Science, pp. 5–23, 2016.\n[14] C. Hutto and E. Gilbert, “Vader: a parsimonious rule-based model for\nsentiment analysis of social media text,” in 8th international AAAI\nconference on weblogs and social media (ICWSM), AAAI, 2014.\n[15] J. W. Pennebaker, M. E. Francis, and R. J. Booth, “Linguistic inquiry\nand word count: Liwc 2001,” Mahway: Lawrence Erlbaum Associates,\nvol. 71, no. 2001, p. 2001, 2001.\n[16] “Heart and soul: sentiment strength detection in the social web with\nsentistrength,” in Journal of Language and Social Psychology, pp. 24–\n54, 2010.\n[17] R. H. Shumway and D. S. Stoffer, Time series analysis and its\napplications: with R examples. Springer Science & Business Media,\n2010.\n[18] S. Seabold and J. Perktold, “Statsmodels: Econometric and statistical\nmodeling with python,” in Proceedings of the 9th Python in Science\nConference, vol. 57, p. 61, 2010.\n[19] A. Sapienza, A. Bessi, S. Damodaran, P. Shakarian, K. Lerman, and\nE. Ferrara, “Early warnings of cyber threats in online discussions,”\nin 2017 IEEE International Conference on Data Mining Workshops\n(ICDMW), pp. 667–674, IEEE, 2017.\n[20] N. Tavabi, P. Goyal, M. Almukaynizi, P. Shakarian, and K. Lerman,\n“Darkembed: Exploit prediction with neural language models,” in\nIAAI 2018: Thirtieth Annual Conference on Innovative Applications\nof Artificial Intelligence, 2018.\n[21] M. Almukaynizi, E. Nunes, K. Dharaiya, M. Senguttuvan, J. Shakar-\nian, and P. Shakarian, “Proactive identification of exploits in the wild\nthrough vulnerability mentions online,” in Cyber Conflict (CyCon US),\n2017 International Conference on, pp. 82–88, IEEE, 2017.\n[22] M. Macdonald, R. Frank, J. Mei, and B. Monk, “Identifying digital\nthreats in a hacker web forum,” in Advances in Social Networks Analy-\nsis and Mining (ASONAM), 2015 IEEE/ACM International Conference\non, pp. 926–933, IEEE, 2015.\n[23] K. Al-Rowaily, M. Abulaish, N. A.-H. Haldar, and M. Al-Rubaian,\n“Bisal–a bilingual sentiment analysis lexicon to analyze dark web\nforums for cyber security,” Digital Investigation, vol. 14, pp. 53–62,\n2015.\n[24] H. Chen, “Sentiment and affect analysis of dark web forums: Measur-\ning radicalization on the internet,” in Intelligence and Security Infor-\nmatics, 2008. ISI 2008. IEEE International Conference on, pp. 104–\n109, IEEE, 2008.\n[25] H. Park, S.-O. D. Jung, H. Lee, and H. P. In, “Cyber weather forecast-\ning: Forecasting unknown internet worms using randomness analysis,”\nin IFIP International Information Security Conference, pp. 376–387,\nSpringer, 2012.\n[26] E. Pontes, A. E. Guelfi, S. T. Kofuji, and A. A. Silva, “Applying\nmulti-correlation for improving forecasting in cyber security,” in\nDigital Information Management (ICDIM), 2011 Sixth International\nConference on, pp. 179–186, IEEE, 2011.\n[27] N. O. Leslie, R. E. Harang, L. P. Knachel, and A. Kott, “Statistical\nmodels for the number of successful cyber intrusions,” The Journal of\nDefense Modeling and Simulation, p. 1548512917715342, 2017.\n[28] S. Zhang, X. Ou, and D. Caragea, “Predicting cyber risks through na-\ntional vulnerability database,” Information Security Journal: A Global\nPerspective, vol. 24, no. 4-6, pp. 194–206, 2015.\n[29] V. Nahar, S. Unankard, X. Li, and C. Pang, “Sentiment analysis for\neffective detection of cyber bullying,” in Asia-Pacific Web Conference,\npp. 767–774, Springer, 2012.\n[30] E. Gandotra, D. Bansal, and S. Sofat, “Computational techniques for\npredicting cyber threats,” in Intelligent Computing, Communication\nand Devices, Advance in Intelligent Systems and Computing, pp. 247–\n253, 2015.\n[31] D. Lacey and P. M. Salmon, “Its dark in there: Using systems\nanalysis to investigate trust and engagement in dark web forums,” in\nInternational Conference on Engineering Psychology and Cognitive\nErgonomics, pp. 117–128, Springer, 2015.\n[32] C. Sabottke, O. Suciu, and T. Dumitras, “Vulnerability disclosure in\nthe age of social media: Exploiting twitter for predicting real-world\nexploits.,” in USENIX Security Symposium, pp. 1041–1056, 2015.\n[33] P. A. Watters, S. McCombie, R. Layton, and J. Pieprzyk, “Charac-\nterising and predicting cyber attacks using the cyber attacker model\nprofile (camp),” Journal of Money Laundering Control, vol. 15, no. 4,\npp. 430–441, 2012.\n",
      "id": 50753704,
      "identifiers": [
        {
          "identifier": "oai:mdpi.com:/2078-2489/9/11/280/",
          "type": "OAI_ID"
        },
        {
          "identifier": "2796638516",
          "type": "MAG_ID"
        },
        {
          "identifier": "oai:arxiv.org:1804.05276",
          "type": "OAI_ID"
        },
        {
          "identifier": "1804.05276",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "154990569",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:doaj.org/article:79ddf15cbe4a494f961731da71c82339",
          "type": "OAI_ID"
        },
        {
          "identifier": "201626531",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.3390/info9110280",
          "type": "DOI"
        },
        {
          "identifier": "540163085",
          "type": "CORE_ID"
        }
      ],
      "title": "Predicting Cyber Events by Leveraging Hacker Sentiment",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2796638516",
      "oaiIds": [
        "oai:arxiv.org:1804.05276",
        "oai:mdpi.com:/2078-2489/9/11/280/",
        "oai:doaj.org/article:79ddf15cbe4a494f961731da71c82339"
      ],
      "publishedDate": "2018-04-14T00:00:00",
      "publisher": "'MDPI AG'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1804.05276"
      ],
      "updatedDate": "2023-02-11T01:28:27",
      "yearPublished": 2018,
      "journals": [
        {
          "title": "Information",
          "identifiers": [
            "2078-2489"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1804.05276"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/50753704"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Basili, Roberto"
        },
        {
          "name": "Castellucci, Giuseppe"
        },
        {
          "name": "Croce, Danilo"
        },
        {
          "name": "VANZO, ANDREA"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Basili, Roberto",
        "Croce, Danilo",
        "Lenci, Alessandro",
        "Vanzo, Andrea",
        "Magnini, Bernardo",
        "Castellucci, Giuseppe"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/54528697"
      ],
      "createdDate": "2016-11-12T07:32:51",
      "dataProviders": [
        {
          "id": 1084,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1084",
          "logo": "https://api.core.ac.uk/data-providers/1084/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Studi recenti per la Sentiment\r\nAnalysis in Twitter hanno tentato di creare\r\nmodelli per caratterizzare la polarit´a di\r\nun tweet osservando ciascun messaggio\r\nin isolamento. In realt`a, i tweet fanno\r\nparte di conversazioni, la cui natura pu`o\r\nessere sfruttata per migliorare la qualit`a\r\ndell’analisi da parte di sistemi automatici.\r\nIn (Vanzo et al., 2014) `e stato proposto un\r\nmodello basato sulla classificazione di sequenze\r\nper la caratterizzazione della polarit`\r\na dei tweet, che sfrutta il contesto in\r\ncui il messaggio `e immerso. In questo lavoro,\r\nsi vuole verificare l’applicabilit`a di\r\ntale metodologia anche per la lingua Italiana.Recent works on Sentiment\r\nAnalysis over Twitter leverage the idea\r\nthat the sentiment depends on a single\r\nincoming tweet. However, tweets are\r\nplunged into streams of posts, thus making\r\navailable a wider context. The contribution\r\nof this information has been recently\r\ninvestigated for the English language by\r\nmodeling the polarity detection as a sequential\r\nclassification task over streams of\r\ntweets (Vanzo et al., 2014). Here, we want\r\nto verify the applicability of this method\r\neven for a morphological richer language,\r\ni.e. Italian",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/54528697.pdf",
      "fieldOfStudy": null,
      "fullText": "A context based model for Sentiment Analysis in Twitterfor the Italian LanguageAndrea Vanzo(†), Giuseppe Castellucci(‡), Danilo Croce(†) and Roberto Basili(†)(†)Department of Enterprise Engineering(‡)Department of Electronic EngineeringUniversity of Roma, Tor VergataVia del Politecnico 1, 00133 Roma, Italy{vanzo,croce,basili}@info.uniroma2.it, castellucci@ing.uniroma2.itAbstractEnglish. Recent works on SentimentAnalysis over Twitter leverage the ideathat the sentiment depends on a singleincoming tweet. However, tweets areplunged into streams of posts, thus makingavailable a wider context. The contribu-tion of this information has been recentlyinvestigated for the English language bymodeling the polarity detection as a se-quential classification task over streams oftweets (Vanzo et al., 2014). Here, we wantto verify the applicability of this methodeven for a morphological richer language,i.e. Italian.Italiano. Studi recenti per la SentimentAnalysis in Twitter hanno tentato di crearemodelli per caratterizzare la polarita´ diun tweet osservando ciascun messaggioin isolamento. In realta`, i tweet fannoparte di conversazioni, la cui natura puo`essere sfruttata per migliorare la qualita`dell’analisi da parte di sistemi automatici.In (Vanzo et al., 2014) e` stato proposto unmodello basato sulla classificazione di se-quenze per la caratterizzazione della po-larita` dei tweet, che sfrutta il contesto incui il messaggio e` immerso. In questo la-voro, si vuole verificare l’applicabilita` ditale metodologia anche per la lingua Ital-iana.1 IntroductionWeb 2.0 and Social Networks allow users to writeabout their life and personal experiences. Thishuge amount of data is crucial in the study of theinteractions and dynamics of subjectivity on theWeb. Sentiment Analysis (SA) is the computa-tional study and automatic recognition of opinionsand sentiments. Twitter is a microblogging ser-vice that counts about a billion of active users. InTwitter, SA is traditionally treated as any other textclassification task, as proved by most systems par-ticipating to the Sentiment Analysis in Twitter taskin SemEval-2013 (Nakov et al., 2013). A MachineLearning (ML) setting allows to induce detectionfunctions from real world labeled examples. How-ever, the shortness of the message and the resultingsemantic ambiguity represent a critical limitation,thus making the task very challenging. Let us con-sider the following message between two users:Benji: @Holly sono completamente d’accordo con teThe tweet sounds like to be a reply to the previ-ous one. Notice how no lexical or syntactic prop-erty allows to determine the polarity. Let’s looknow at the entire conversation:Benji : @Holly con un #RigoreAl90 vinci facile!!Holly : @Benji Lui vince sempre pero` :) accantoa chiunque.. Nessuno regge il confronto!Benji : @Holly sono completamente d’accordo con teThe first is clearly a positive tweet, followed bya positive one that makes the third positive aswell. Thus, through the conversation we can dis-ambiguate even a very short message. We wantto leverage on this to define a context-sensitiveSA model for the Italian language, in line with(Vanzo et al., 2014). The polarity detection of atweet is modeled as a sequential classification taskthrough the SVMhmm learning algorithm (Altun etal., 2003), as it allows to classify an instance (i.e.a tweet) within an entire sequence. First experi-mental evaluations confirm the effectiveness of theproposed sequential tagging approach combinedwith the adopted contextual information even inthe Italian language.A survey of the existing approaches is presentedin Section 2. Then, Section 3 provides an ac-count of the context-based model. The experimen-tal evaluation is presented in Section 4.2 Related WorkThe spread of microblog services, where userspost real-time opinions about “everything”, posesdifferent challenges in Sentiment Analysis. Clas-sical approaches (Pang et al., 2002; Pang and Lee,2008) are not directly applicable to tweets: theyfocus on relatively large texts, e.g. movie or prod-uct reviews, while tweets are short and informaland a finer analysis is required. Recent works triedto model the sentiment in tweets (Go et al., 2009;Davidov et al., 2010; Bifet and Frank, 2010; Zan-zotto et al., 2011; Croce and Basili, 2012; Si etal., 2013). Specific approaches, e.g. probabilis-tic paradigms (Pak and Paroubek, 2010) or Kernelbased (Barbosa and Feng, 2010; Agarwal et al.,2011; Castellucci et al., 2013), and features, e.g.n-grams, POS tags, polarity lexicons, have beenadopted in the tweet polarity recognition task.In (Mukherjee and Bhattacharyya, 2012) con-textual information, in terms of discourse rela-tions is adopted, e.g. the presence of conditionalsand semantic operators like modals and negations.However, these features are derived by consider-ing a tweet in isolation. The approach in (Vanzoet al., 2014) considers a tweet within its context,i.e. the stream of related posts. In order to ex-ploit this information, a Markovian extension of aKernel-based categorization approach is there pro-posed and it is briefly described in the next section.3 A Context Based Model for SAAs discussed in (Vanzo et al., 2014), contextualinformation about one tweet stems from variousaspects: an explicit conversation, the user attitudeor the overall set of recent tweets about a topic(for example a hashtag like #RigoreAl90). Inthis work, we concentrate our analysis only on theexplicit conversation a tweet belongs to. In linewith (Vanzo et al., 2014), a conversation is a se-quence of tweets, each represented as vectors offeatures characterizing different semantic proper-ties. The Sentiment Analysis task is thus modeledas a sequential classification function that asso-ciates tweets, i.e. vectors, to polarity classes.3.1 Representing TweetsThe proposed representation makes use of differ-ent representations that allow to model differentaspects within a Kernel-based paradigm.Bag of Word (BoWK). The simplest Kernel func-tion describes the lexical overlap between tweets,thus represented as a vector, whose dimensionscorrespond to the presence or not of a word. Evenif very simple, the BoW model is one of the mostinformative representation in Sentiment Analysis,as emphasized since (Pang et al., 2002).Lexical Semantic Kernel (LSK). In order to gen-eralize the BoW model, we provide a furtherrepresentation. A vector for each word is ob-tained from a co-occurrence Word Space built ac-cording to the Distributional Analysis technique(Sahlgren, 2006). A word-by-context matrix M isbuilt through large scale corpus analysis and thenprocessed through Latent Semantic Analysis (Lan-dauer and Dumais, 1997). Dimensionality reduc-tion is applied to M through Singular Value De-composition (Golub and Kahan, 1965): the origi-nal statistical information about M is captured bythe new k-dimensional space, which preserves theglobal structure while removing low-variance di-mensions, i.e. distribution noise. A word can beprojected in the reduced Word Space: the distancebetween vectors surrogates the notion of paradig-matic similarity between represented words, e.g.the most similar words of vincere are perdere andpartecipare. A vector for each tweet is representedthrough the linear combination of its word vectors.Whenever the different representations areavailable, we can combine the contribution of bothvector simply through a juxtaposition, in order toexploit both lexical and semantic properties.3.2 SA as a Sequential Tagging ProblemContextual information is embodied by the streamof tweets in which a message ti is immersed. Astream gives rise to a sequence on which sequencelabeling can be applied: the target tweet is here la-beled within the entire sequence, where contextualconstraints are provided by the preceding tweets.Let formally define a conversational context.Conversational context. For every tweet ti ∈ T ,let r(ti) : T → T be a function that returns eitherthe tweet to which ti is a reply to, or null if ti isnot a reply. Then, the conversational context ΛC,liof tweet ti (i.e., the target tweet) is the sequenceof tweet iteratively built by applying r(·), until ltweets have been selected or r(·) = null.A markovian approach. The sentiment predic-tion of a target tweet can be seen as a sequen-tial classification task over a context, and theSVMhmm algorithm can be applied. Given an in-put sequence x = (x1 . . . xl) ⊆ X , where x is atweet context, i.e. the conversational context pre-viously defined, and xi is a feature vector rep-resenting a tweet, the model predicts a tag se-quence y = (y1 . . . yl) ∈ Y+ after learning a lin-ear discriminant function F : P(X ) × Y+ → Rover input/output pairs. The labeling f(x) is de-fined as: f(x) = arg maxy∈Y+ F (x,y;w). Inthese models, F is linear in some combined fea-ture representation of inputs and outputs Φ(x,y),i.e. F (x,y;w) = 〈w,Φ(x,y)〉. As Φ extractsmeaningful properties from an observation/labelsequence pair (x,y), in SVMhmm it is modeledthrough two types of features: interactions be-tween attributes of the observation vectors xi anda specific label yi (i.e. emissions of a tweet w.r.t.a polarity class) as well as interactions betweenneighboring labels yi along the chain (i.e. transi-tions of polarity labels in a conversation context.).Thus, through SVMhmm the label for a target tweetis made dependent on its context history. Themarkovian setting acquires patterns across tweetsequences to recognize sentiment even for trulyambiguous tweets. Further details about the mod-eling and the SVMhmm application to tweet label-ing can be found in (Vanzo et al., 2014).4 Experimental EvaluationThe aim of the experiments is to verify the appli-cability of the model proposed in (Vanzo et al.,2014) in a different language, i.e. Italian. Inorder to evaluate the models discussed above inan Italian setting, an appropriate dataset has beenbuilt by gathering1 tweets from Twitter servers.By means of Twitter APIs2, we retrieved thewhole corpus by querying several Italian hot top-ics, i.e. expo, mose, renzi, prandelli,mondiali, balotelli and commonly usedemoticons, i.e. :) and :( smiles. Each tweet tiand its corresponding conversation ΛC,li have beenincluded into the dataset if and only if the con-versation itself was available (i.e. |ΛC,li | > 1).Then, three annotators labeled each tweet witha sentiment polarity label among positive,negative, neutral and conflicting3,obtaining a inter-annotator agreement of 0.829,measured as the mean accuracy computed betweenannotators pairs.1The process has been run during June-July 20142http://twitter4j.org/3A tweet is said to be conflicting when it expresses both apositive and negative polarityAs about 1,436 tweets, including conversa-tions, were gathered from Twitter, a static split of64%/16%/20% in Training/Held-out/Test respec-tively, has been carried out as reported in Table 1.train dev testPositive 212 61 69Negative 211 42 92Neutral 387 72 87Conflicting 129 26 48939 201 296Table 1: Dataset compositionTweets have been analyzed through the Chaosnatural language parser (Basili et al., 1998). Anormalization step is previously applied to eachmessage: fully capitalized words are converted inlowercase; reply marks, hyperlinks and hashtagsare replaced with the pseudo-tokens, and emoti-cons have been classified with respect to 13 differ-ent classes. LSK vectors are obtained from a WordSpace derived from a corpus of about 3 milliontweets, downloaded during July and September2013. The methodology described in (Sahlgren,2006) with the setting discussed in (Croce and Pre-vitali, 2010) has been applied.Performance scores are reported in terms of Pre-cision, Recall and F-Measure. We also report boththe F pnn1 score as the arithmetic mean between theF1s of positive, negative and neutral classes, andthe F pnnc1 considering even the conflicting class.It is worth noticing that a slightly different set-ting w.r.t. (Vanzo et al., 2014) has been used. Inthis work we manually labeled every tweet in eachconversation and performance measures considersall the tweets. On the contrary in (Vanzo et al.,2014) only the last tweet of the conversation ismanually labeled and considered in the evaluation.4.1 Experimental ResultsExperiments are meant to verify the ability of acontext-based model in the Italian setting. Asa baseline we considered a multi-class classifierwithin the SVMmulticlass framework (Tsochan-taridis et al., 2004). Each tweet in a conversationis classified considering it in isolation, i.e. withoutusing contextual information. In Table 2, perfor-mances of the Italian dataset are reported, whileTable 3 shows the outcomes of experiments overthe English dataset (Vanzo et al., 2014). Here, w/oconv results refer to a baseline computed with theSVMmulticlass algorithm, while w/ conv results re-fer to the application of the model described in thePrecision Recall F1 Fpnn1 Fpnnc1pos neg neu conf pos neg neu conf pos neg neu confBoWKw/o conv .705 .417 .462 .214 .449 .109 .690 .438 .549 .172 .553 .288 .425 .390w conv .603 .580 .379 .375 .507 .435 .701 .063 .551 .497 .492 .107 .513 .412BoWK+LSKw/o conv .507 .638 .416 .000 .493 .402 .793 .000 .500 .493 .545 .000 .513 .385w conv .593 .560 .432 .368 .464 .457 .736 .146 .520 .503 .545 .209 .523 .444Table 2: Evaluation results of the Italian setting.Precision Recall F1 Fpnn1pos neg neu pos neg neu pos neg neuBoWKw/o conv .713 .496 .680 .649 .401 .770 .679 .444 .723 .615w/ conv .723 .511 .722 .695 .472 .762 .709 .491 .741 .647BoWK+LSKw/o conv .754 .595 .704 .674 .486 .804 .712 .535 .751 .666w/ conv .774 .554 .717 .682 .542 .791 .725 .548 .752 .675Table 3: Evaluation results on the English language from (Vanzo et al., 2014)previous sections with the SVMhmm algorithm. Inthe last setting, the whole conversational contextof each tweet is considered.Firstly, all w/o conv models beneficiate by thelexical generalization provided by the Word Spacein the LSA model. In fact, the information derivedfrom the Word Space seems beneficial in its rela-tive improvement with respect to the simple BoWKernel accuracy, up to an improvement of 20.71%of Fpnn1 , from .425 to .513. However, it is not al-ways true, in particular w.r.t. the conflicting classwhere the smoothing provided by the generaliza-tion negatively impact on the classifiers, that arenot able to discriminate the contemporary pres-ence of positive and negative polarity.Most importantly, the contribution of conver-sations is confirmed in all context-driven models,i.e. w/conv improves w.r.t. their w/o conv coun-terpart. Every polarity category benefits from theintroduction of contexts, although many tweets an-notated with the conflicting (conf ) class are notcorrectly recognized: contextual information un-balances the output of a borderline tweet with thepolarity of the conversations. The impact of con-versational information contribute to a statisticallysignificant improvement of 20.71% in the BoWKsetting, and of 1.95% in the BoWK+LSK setting.In (Vanzo et al., 2014) a larger dataset (10,045examples) has been used for the evaluation of con-textual models in an English setting. The datasetis provided by ACL SemEval-2013 (Nakov et al.,2013). Results are thus not directly comparable,as in this latter dataset, where even tweets with-out a conversational contexts are included, onlythe target tweet is manually labeled and the labelsof remaining tweets have been automatically pre-dicted in a semi supervised fashion, as discussedin (Vanzo et al., 2014). Additionally, the conflict-ing class, where a lexical overlap is observed withboth positive and negative classes, is not consid-ered. However, results in Table 3 show that theBoWK setting benefits by the introduction of thelexical generalization, given by the LSK, with aperformance improvement of 8.29%. When thefocus is held within the same Kernel setting, inboth BoWK and BoWK+LSK, the conversationalinformation seems to be beneficial as increases of5.20% and 1.35%, respectively, are observed.5 ConclusionsIn this work, the role of contextual information insupervised Sentiment Analysis over Twitter is in-vestigated for the Italian language. Experimentalresults confirm the empirical findings presented in(Vanzo et al., 2014) for the English language. Al-though the size of the involved dataset is still lim-ited, i.e. about 1,400 tweets, the importance ofcontextual information is emphasized within theconsidered markovian approach: it is able to takeadvantage of the dependencies that exist betweendifferent tweets in a conversation. The approachis also largely applicable as all experiments havebeen carried out without the use of any manualcoded resource, but mainly exploiting unannotatedmaterial within the distributional method. A largerexperiment, eventually on an oversized dataset,such as SentiTUT4, will be carried out.4http://www.di.unito.it/~tutreeb/sentiTUT.htmlReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau. 2011. Sentimentanalysis of twitter data. In Proceedings of theWorkshop on Languages in Social Media, LSM ’11,pages 30–38, Stroudsburg, PA, USA. Associationfor Computational Linguistics.Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003.Hidden Markov support vector machines. In Pro-ceedings of the International Conference on Ma-chine Learning.Luciano Barbosa and Junlan Feng. 2010. Robust sen-timent detection on twitter from biased and noisydata. In Chu-Ren Huang and Dan Jurafsky, editors,COLING (Posters), pages 36–44. Chinese Informa-tion Processing Society of China.Roberto Basili, Maria Teresa Pazienza, and Fabio Mas-simo Zanzotto. 1998. Efficient parsing for informa-tion extraction. In Proc. of the European Conferenceon Artificial Intelligence, pages 135–139.Albert Bifet and Eibe Frank. 2010. Sentiment knowl-edge discovery in twitter streaming data. In Pro-ceedings of the 13th International Conference onDiscovery Science, DS’10, pages 1–15, Berlin, Hei-delberg. Springer-Verlag.Giuseppe Castellucci, Simone Filice, Danilo Croce,and Roberto Basili. 2013. Unitor: Combiningsyntactic and semantic kernels for twitter sentimentanalysis. In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 369–374, Atlanta, Georgia, USA, June. Asso-ciation for Computational Linguistics.Danilo Croce and Roberto Basili. 2012. Grammaticalfeature engineering for fine-grained ir tasks. In Gi-ambattista Amati, Claudio Carpineto, and GiovanniSemeraro, editors, IIR, volume 835 of CEUR Work-shop Proceedings, pages 133–143. CEUR-WS.org.Danilo Croce and Daniele Previtali. 2010. Mani-fold learning for the semi-supervised induction offramenet predicates: An empirical investigation. InProceedings of the 2010 Workshop on GEometricalModels of Natural Language Semantics, GEMS ’10,pages 7–16, Stroudsburg, PA, USA. Association forComputational Linguistics.Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.Enhanced sentiment learning using twitter hashtagsand smileys. In Chu-Ren Huang and Dan Jurafsky,editors, COLING (Posters), pages 241–249. ChineseInformation Processing Society of China.Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-ter sentiment classification using distant supervision.Processing, pages 1–6.G. Golub and W. Kahan. 1965. Calculating the singu-lar values and pseudo-inverse of a matrix. Journal ofthe Society for Industrial and Applied Mathematics,2(2).T. Landauer and S. Dumais. 1997. A solution to plato’sproblem: The latent semantic analysis theory ofacquisition, induction and representation of knowl-edge. Psychological Review, 104(2):211–240.Subhabrata Mukherjee and Pushpak Bhattacharyya.2012. Sentiment analysis in twitter with lightweightdiscourse analysis. In Proceedings of COLING,pages 1847–1864.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013. Semeval-2013 task 2: Sentiment analysisin twitter. In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 312–320, Atlanta, Georgia, USA, June. Asso-ciation for Computational Linguistics.Alexander Pak and Patrick Paroubek. 2010. Twit-ter as a corpus for sentiment analysis and opinionmining. In Proceedings of the 7th Conference onInternational Language Resources and Evaluation(LREC’10), Valletta, Malta, May. European Lan-guage Resources Association (ELRA).Bo Pang and Lillian Lee. 2008. Opinion mining andsentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002. Thumbs up? sentiment classification us-ing machine learning techniques. In Proceedings ofEMNLP, pages 79–86.Magnus Sahlgren. 2006. The Word-Space Model.Ph.D. thesis, Stockholm University.Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,Huayi Li, and Xiaotie Deng. 2013. Exploiting topicbased twitter sentiment for stock prediction. In ACL(2), pages 24–29.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun. 2004. Support vectormachine learning for interdependent and structuredoutput spaces. In Proceedings of the twenty-first in-ternational conference on Machine learning, ICML’04, pages 104–, New York, NY, USA. ACM.Andrea Vanzo, Danilo Croce, and Roberto Basili.2014. A context-based model for sentiment anal-ysis in twitter. In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics, pages 2345–2354, Dublin, Ireland, Au-gust. Dublin City University and Association forComputational Linguistics.Fabio M. Zanzotto, Marco Pennaccchiotti, and KostasTsioutsiouliklis. 2011. Linguistic Redundancy inTwitter. In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 659–669, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.",
      "id": 30335914,
      "identifiers": [
        {
          "identifier": "oai:iris.uniroma1.it:11573/871178",
          "type": "OAI_ID"
        },
        {
          "identifier": "54528697",
          "type": "CORE_ID"
        }
      ],
      "title": "A context based model for sentiment analysis in twitter for the italian language",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:iris.uniroma1.it:11573/871178"
      ],
      "publishedDate": "2014-01-01T00:00:00",
      "publisher": "Pisa University Press srl",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://iris.uniroma1.it/retrieve/handle/11573/871178/230822/Vanzo_A-context-based_Postprint_2014.pdf"
      ],
      "updatedDate": "2022-12-08T12:12:03",
      "yearPublished": 2014,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/54528697.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/54528697"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/54528697/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/54528697/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/30335914"
        }
      ]
    },
    {
      "acceptedDate": "2013-10-16T00:00:00",
      "arxivId": "1305.6143",
      "authors": [
        {
          "name": "Arora, Ishan"
        },
        {
          "name": "Bhatia, Arjun"
        },
        {
          "name": "Narayanan, Vivek"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/456302185"
      ],
      "createdDate": "2014-10-24T19:18:29",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        }
      ],
      "depositedDate": "2013-01-01T00:00:00",
      "abstract": "We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.Comment: 8 pages, 2 figure",
      "documentType": "research",
      "doi": "10.1007/978-3-642-41278-3_24",
      "downloadUrl": "http://arxiv.org/abs/1305.6143",
      "fieldOfStudy": null,
      "fullText": "Fast and accurate sentiment classification using an \nenhanced Naive Bayes model. \nVivek Narayanan1, Ishan Arora2, Arjun Bhatia3 \nDepartment of Electronics Engineering, \nIndian Institute of Technology (BHU), Varanasi, India \n1\nvivek.narayanan.ece09@iitbhu.ac.in \n2ishan.arora.ece09@iitbhu.ac.in \n3\narjun.bhatia.ece09@iitbhu.ac.in\nAbstract. We have explored different methods of improving the accuracy of a \nNaive Bayes classifier for sentiment analysis. We observed that a combination \nof methods like effective negation handling, word n-grams and feature selection \nby mutual information results in a significant improvement in accuracy. This \nimplies that a highly accurate and fast sentiment classifier can be built using a \nsimple Naive Bayes model that has linear training and testing time complexi-\nties. We achieved an accuracy of 88.80% on the popular IMDB movie reviews \ndataset. The proposed method can be generalized to a number of text categori-\nzation problems for improving speed and accuracy. \n \nKeywords :- Sentiment classification, Negation Handling, Mutual Information, \nFeature Selection, n-grams \n1 Introduction \nAmong the most researched topics of natural language processing is sentiment analy-\nsis. Sentiment analysis involves extraction of subjective information from documents \nlike online reviews to determine the polarity with respect to certain objects. It is use-\nful for identifying trends of public opinion in the social media, for the purpose of \nmarketing and consumer research. It has its uses in getting customer feedback about \nnew product launches, political campaigns and even in financial markets [14]. It aims \nto determine the attitude of a speaker or a writer with respect to some topic or simply \nthe contextual polarity of a document. Early work in this area was done by Turney and \nPang ([2], [7]) who applied different methods for detecting the polarity of product and \nmovie reviews. \n \nSentiment analysis is a complicated problem but experiments have been done using \nNaive Bayes, maximum entropy classifiers and support vector machines. Pang et al. \nfound the SVM to be the most accurate classifier in [2]. In this paper we present a \nsupervised sentiment classification model based on the Naïve Bayes algorithm.  \nNaïve Bayes is a very simple probabilistic model that tends to work well on text \nclassifications and usually takes orders of magnitude less time to train when com-\npared to models like support vector machines. We will show in this paper that a high \ndegree of accuracy can be obtained using Naïve Bayes model, which is comparable to \nthe current state of the art models in sentiment classification. \n2 Data \nWe used a publicly available dataset of movie reviews from the Internet Movie Data-\nbase (IMDb) [1] which was compiled by Andrew Maas et al. It is a set of 25,000 \nhighly polar movie reviews for training, and 25,000 for testing. Both the training and \ntest sets have an equal number of positive and negative reviews. We chose movie \nreviews as our data set because it covers a wide range of human emotions and cap-\ntures most of the adjectives relevant to sentiment classification. Also, most existing \nresearch on sentiment classification uses movie review data for benchmarking.  \nWe used the 25,000 documents in the training set to build our supervised learning \nmodel. The other 25,000 were used for evaluating the accuracy of our classifier. \n3 Naïve Bayes Classifier \nA Naive bayes classifier is a simple probabilistic model based on the Bayes rule along \nwith a strong independence assumption. \n \nThe Naïve Bayes model involves a simplifying conditional independence assumption. \nThat is given a class (positive or negative), the words are conditionally independent of \neach other. This assumption does not affect the accuracy in text classification by \nmuch but makes really fast classification algorithms applicable for the problem. Ren-\nnie et al discuss the performance of Naïve Bayes on text classification tasks in their \n2003 paper. [6] \nIn our case, the maximum likelihood probability of a word belonging to a particu-\nlar class is given by the expression: \n\u0001\u0002\u0003\u0004| \u0007) =\n\n\u000b\f\r\u000e \u000b\u000f \u0003\u0004 \u0010\r \u0011\u000b\u0007\f\u0012\u0013\r\u000e\u0014 \u000b\u000f \u0007\u0015\u0016\u0014\u0014 \u0007 \n\u0017\u000b\u000e\u0016\u0015 \r\u000b \u000b\u000f \u0018\u000b\u0019\u0011\u0014 \u0010\r \u0011\u000b\u0007\f\u0012\u0013\r\u000e\u0014 \u000b\u000f \u0007\u0015\u0016\u0014\u0014 \u0007\n \n                         (1) \nThe frequency counts of the words are stored in hash tables during the training \nphase. \nAccording to the Bayes Rule, the probability of a particular document belonging to \na class ci is given by, \n\u0001\u0002\u0007\u0004|\u0011) =  \n\u0001\u0002\u0011 |\u0007\u0004) ∗ \u0001\u0002\u0007\u0004)\n\u0001\u0002\u0011)\n     \u00022) \n3 \n \nIf we use the simplifying conditional independence assumption, that given a class \n(positive or negative), the words are conditionally independent of each other. Due to \nthis simplifying assumption the model is termed as “naïve”. \n\u0001\u0002\u0007\u0004|\u0011) =  \n\u0002∏ \u0001\u0002\u0003\u0004|\u0007\u001d)) ∗ \u0001\u0002\u0007\u001d)\n\u0001\u0002\u0011)\n    \u00023) \nHere the xi s are the individual words of the document. The classifier outputs the \nclass with the maximum posterior probability. \nWe also remove duplicate words from the document, they don’t add any additional \ninformation; this type of naïve bayes algorithm is called Bernoulli Naïve Bayes. In-\ncluding just the presence of a word instead of the count has been found to improve \nperformance marginally, when there is a large number of training examples. \n4 Laplacian Smoothing \nIf the classifier encounters a word that has not been seen in the training set, the \nprobability of both the classes would become zero and there won’t be anything to \ncompare between. This problem can be solved by Laplacian smoothing \n \n\u0001\u0002\u0003\u0004|\u0007\u001d) =\n\n\u000b\f\r\u000e\u0002\u0003\u0004) +   \n\u0002 + 1) ∗ \u0002\"\u000b \u000b\u000f \u0018\u000b\u0019\u0011\u0014 \u0010\r \u0007\u0015\u0016\u0014\u0014 \u0007\u001d)\n     \u00024) \n \nUsually, k is chosen as 1. This way, there is equal probability for the new word to \nbe in either class. Since Bernoulli Naïve Bayes is used, the total number of words in a \nclass is computed differently. For the purpose of this calculation, each document is \nreduced to a set of unique words with no duplicates. \n5 Negation Handling \nNegation handling was one of the factors that contributed significantly to the accuracy \nof our classifier. A major problem faced during the task of sentiment classification is \nthat of handling negations. Since we are using each word as feature, the word “good” \nin the phrase “not good” will be contributing to positive sentiment rather that negative \nsentiment as the presence of “not” before it is not taken into account.  \nTo solve this problem we devised a simple algorithm for handling negations using \nstate variables and bootstrapping. We built on the idea of using an alternate represen-\ntation of negated forms as shown by Das & Chen in [3]. Our algorithm uses a state \nvariable to store the negation state. It transforms a word followed by a not or n’t into \n“not_” + word.  Whenever the negation state variable is set, the words read are treated \nas “not_” + word. The state variable is reset when a punctuation mark is encountered \nor when there is double negation. The pseudo code of the algorithm is described be-\nlow: \nPSEUDO CODE:.  \nnegated := False \nfor each word in document: \n   if negated = True: \n       Transform word to “not_” + word. \n   if word is “not” or “n’t”: \n       negated := not negated \n   if a punctuation mark is encountered \n       negated := False. \nSince the number of negated forms might not be adequate for correct classifications. \nIt is possible that many words with strong sentiment occur only in their normal forms \nin the training set. But their negated forms would be of strong polarity. \nWe addressed this problem by adding negated forms to the opposite class along \nwith normal forms of all the features during the training phase. That is to say if we \nencounter the word “good” in a positive document during the training phase, we in-\ncrement the count of “good” in the positive class and also increment the count of \n“not_good” for the negative class.  This is to ensure that the number of “not_” forms \nare sufficient for classification. This modification resulted in a significant improve-\nment in classification accuracy (about 1%) due to bootstrapping of negated forms \nduring training. This form of negation handling can be applied to a variety of text \nrelated applications. \n6 n - grams \nGenerally, information about sentiment is conveyed by adjectives or more specifically \nby certain combinations of adjectives with other parts of speech.  \nThis information can be captured by adding features like consecutive pairs of \nwords (bigrams), or even triplets of words (trigrams). Words like \"very\" or \"definite-\nly\" don't provide much sentiment information on their own, but phrases like \"very \nbad\" or \"definitely recommended\" increase the probability of a document being nega-\ntively or positively biased. By including bigrams and trigrams, we were able to cap-\nture this information about adjectives and adverbs. Using bigrams and trigrams re-\nquire a substantial amount of data in the training set, but this is not a problem as our \ntraining set had 25,000 reviews. But the data may not be enough to add 4-grams, as \nthis may over-fit the training set. The counts of the n-grams were stored in a hash \ntable along with the counts of unigrams. \n7 Feature Selection \n \nFeature selection is the process of removing redundant features, while retaining those \nfeatures that have high disambiguation capabilities.  \n  \nThe use of higher dimensional features like bigrams and trigrams pr\nlem, that of the number of feature\nMost of these features are redundant and noisy in nature. Including them would affect \nboth efficiency and accuracy. A basic filtering step of removing the features/terms \nwhich occur only once is performed. Now\n1,500,000 features. The features are then further filtered on the basis of mutual info\nmation [3]. \n7.1    Mutual Information\nMutual information is a quantity that measures the mutual dependence of the two \nrandom variables. Formally, the mutual information of two discrete random v\nriables X and Y can be defined as:\nWhere p(x,y) is the joint probability distribution function of \nand P(Y)  are the marginal probability distribution functions of \nHere X is an individual feature which can take two values, the feature is present or \nabsent and Y is the class, positive or negative. We selected the top \nmaximum mutual information. By plotting a graph between accuracy and number of \nfeatures, the optimal value for \n \nA plot of Accuracy versus Number of features\nFig. 1. Plot of Accuracy v/s No\n5 \nesents a pro\ns increasing from 300,000 to about 11,000,000. \n the number of features is reduced to about \n \n \n(5) \n \nX and Y, and P(X)  \nX and Y respectively. \nk features with \nk was found out to be 32,000.  \n is shown in Fig 1: \n \n of features selected on Validation set \nb-\nr-\na-\n8 Results \nWe implemented the classifier in Python using hash tables to store the counts of \nwords in their respective classes. The code is available at [13].  Training involved \npreprocessing data and applying negation handling before counting the words. Since \nwe were using Bernoulli Naive Bayes, each word is counted only once per document. \nOn a laptop running an Intel Core 2 Duo processor at 2.1 GHz, training took around 1 \nminute 30 seconds and used about 700 megabytes of memory. The memory usage \nstems largely from bigrams and trigrams prior to feature selection. \nThe optimal number of features was chosen by using a validation set of 1000 doc-\numents, the plot of accuracy v/s number of features is shown in Fig 1. Then the accu-\nracy was measured on the entire test set of 25000 documents.  The time for feature \nselection was about 3 minutes.  \n8.1   Performance and Comparison \nWe obtained an overall classification accuracy of 88.80% on the test set of 25000 \nmovie reviews. The running time of our algorithm is O(n + V lg V) for training and \nO(n) for testing, where n is the number of words in the documents (linear) and V the \nsize of the reduced vocabulary. It is much faster than other machine learning algo-\nrithms like Maxent classification or Support Vector Machines which take a long time \nto converge to the optimal set of weights.  The accuracy is comparable to that of the \ncurrent state-of-the-art algorithms used for sentiment classification on movie reviews. \nIt achieved a better or similar accuracy when compared to more complicated models \nlike SVMs, autoencoders, contextual valence shifters, matrix factorisation, appraisal \ngroups etc used in [2], [7], [8], [9], [10], [11] on the dataset of IMDb movie reviews. \n8.2   Results Timeline \nThe table and graph illustrate the evolution of the accuracy of our classifier and how \nthe inclusion of certain features helped. \nTable 1.RESULTS TIMELINE \n \nFeature Added Accuracy on test set \nOriginal Naive Bayes \nalgorithm with Laplacian \nSmoothing \n73.77% \nHandling negations 82.80% \nBernoulli  Naive Bayes 83.66% \nBigrams and trigrams 85.20% \nFeature Selection 88.80% \n \n7 \n \n \nFig. 2. Evolution of classification accuracy. \n9 Conclusion  \nOur results show that a simple Naive Bayes classifier can be enhanced to match the \nclassification accuracy of more complicated models for sentiment analysis by choos-\ning the right type of features and removing noise by appropriate feature selection. \nNaive Bayes classifiers due to their conditional independence assumptions are ex-\ntremely fast to train and can scale over large datasets. They are also robust to noise \nand less prone to overfitting. Ease of implementation is also a major advantage of \nNaive Bayes. They were thought to be less accurate than their more sophisticated \ncounterparts like support vector machines and logistic regression but we have shown \nthrough this paper that a significantly high accuracy can be achieved. The ideas used \nin this paper can also be applied to the more general domain of text classification.  \n \nAcknowledgement \n \nWe would like to express our sincere gratitude to our mentor, Professor R. R. Das \nfor sparing his valuable time and guiding us during the project. We are also thankful \nto Prof. P. K. Mukherjee for his support and encouragement. Lastly, we would like to \nthank all the faculty members and support staff of the Department of Electronics En-\ngineering, IIT (BHU). \n \nReferences \n1. Large Movie Review Dataset. (n.d.). Retrieved from \nhttp://ai.stanford.edu/~amaas/data/sentiment/ \n50\n60\n70\n80\n90\n100\nAccuracy\n1. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. \"Thumbs up?: sentiment classifica-\ntion using machine learning techniques.\" Proceedings of the ACL-02 conference on Empir-\nical methods in natural language processing-Volume 10. Association for Computational \nLinguistics, 2002. \n2.  Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. Introduction to in-\nformation retrieval. Vol. 1. Cambridge: Cambridge University Press, 2008. \n3. Das, Sanjiv, and Mike Chen. \"Yahoo! for Amazon: Sentiment parsing from small talk on \nthe web.\" EFA 2001 Barcelona Meetings. 2001. \n4.  Pauls, Adam, and Dan Klein. \"Faster and smaller n-gram language models.\"Proceedings \nof the 49th annual meeting of the Association for Computational Linguistics: Human Lan-\nguage Technologies. Vol. 1. 2011. \n5.      Rennie, Jason D., et al. \"Tackling the poor assumptions of naive bayes text classifi-\ners.\" MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-. \nVol. 20. No. 2. 2003. \n6. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and \nChristopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th An-\nnual Meeting of the Association for Computational Linguistics (ACL 2011). \n7.  Kennedy, Alistair, and Diana Inkpen. \"Sentiment classification of movie reviews using \ncontextual valence shifters.\" Computational Intelligence 22.2 (2006): 110-125. \n8.  Li, Tao, Yi Zhang, and Vikas Sindhwani. \"A non-negative matrix tri-factorization ap-\nproach to sentiment classification with lexical prior knowledge.\" Proceedings of the Joint \nConference of the 47th Annual Meeting of the ACL and the 4th International Joint Confe-\nrence on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association \nfor Computational Linguistics, 2009. \n9.  Matsumoto, S., Takamura, H., & Okumura, M. (2005). Sentiment classification using \nword sub-sequences and dependency sub-trees. In PAKDD 2005 (pp. 301–311). \n10. Springer-Verlag: Berlin, Heidelberg. \n11.  Whitelaw, Casey, Navendu Garg, and Shlomo Argamon. \"Using appraisal groups for sen-\ntiment analysis.\" Proceedings of the 14th ACM international conference on Information \nand knowledge management. ACM, 2005. \n12. Socher, Richard, et al. \"Semi-supervised recursive autoencoders for predicting sentiment \ndistributions.\" Proceedings of the Conference on Empirical Methods in Natural Language \nProcessing. Association for Computational Linguistics, 2011. \n13.  Source code of classifier developed for this paper –[Online] \nhttp://github.com/vivekn/sentiment \n14.  Devitt, Ann, and Khurshid Ahmad. \"Sentiment polarity identification in financial news: A \ncohesion-based approach.\" ANNUAL MEETING-ASSOCIATION FOR \nCOMPUTATIONAL LINGUISTICS. Vol. 45. No. 1. 2007. \n15. Peng, Fuchun, and Dale Schuurmans. \"Combining naive Bayes and n-gram language mod-\nels for text classification.\" Advances in Information Retrieval. Springer Berlin Heidelberg, \n2003. 335-350. \n",
      "id": 17125007,
      "identifiers": [
        {
          "identifier": "24935500",
          "type": "CORE_ID"
        },
        {
          "identifier": "2106095291",
          "type": "MAG_ID"
        },
        {
          "identifier": "info:doi/10.1007%2f978-3-642-41278-3_24",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.1007/978-3-642-41278-3_24",
          "type": "DOI"
        },
        {
          "identifier": "oai:arxiv.org:1305.6143",
          "type": "OAI_ID"
        },
        {
          "identifier": "1305.6143",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "456302185",
          "type": "CORE_ID"
        }
      ],
      "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes\n  model",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": "2106095291",
      "oaiIds": [
        "info:doi/10.1007%2f978-3-642-41278-3_24",
        "oai:arxiv.org:1305.6143"
      ],
      "publishedDate": "2013-01-01T00:00:00",
      "publisher": "'Springer Science and Business Media LLC'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://arxiv.org/abs/1305.6143"
      ],
      "updatedDate": "2024-03-03T00:12:13",
      "yearPublished": 2013,
      "journals": [
        {
          "title": null,
          "identifiers": [
            "0302-9743"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/1305.6143"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/17125007"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "De las Heras-Pedrosa, Carlos"
        },
        {
          "name": "Pelaez-Sanchez, Jose Ignacio"
        },
        {
          "name": "Sánchez-Núñez, Pablo"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/288162334",
        "https://api.core.ac.uk/v3/outputs/618762668",
        "https://api.core.ac.uk/v3/outputs/540307207",
        "https://api.core.ac.uk/v3/outputs/323326686",
        "https://api.core.ac.uk/v3/outputs/597637488"
      ],
      "createdDate": "2020-03-22T00:09:33",
      "dataProviders": [
        {
          "id": 22080,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/22080",
          "logo": "https://api.core.ac.uk/data-providers/22080/logo"
        },
        {
          "id": 1730,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1730",
          "logo": "https://api.core.ac.uk/data-providers/1730/logo"
        },
        {
          "id": 2072,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2072",
          "logo": "https://api.core.ac.uk/data-providers/2072/logo"
        },
        {
          "id": 681,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/681",
          "logo": "https://api.core.ac.uk/data-providers/681/logo"
        },
        {
          "id": 11082,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11082",
          "logo": "https://api.core.ac.uk/data-providers/11082/logo"
        },
        {
          "id": 1114,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1114",
          "logo": "https://api.core.ac.uk/data-providers/1114/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Opinion mining and sentiment analysis has become ubiquitous in our society, with\r\napplications in online searching, computer vision, image understanding, artificial intelligence and\r\nmarketing communications (MarCom). Within this context, opinion mining and sentiment analysis\r\nin marketing communications (OMSAMC) has a strong role in the development of the field by\r\nallowing us to understand whether people are satisfied or dissatisfied with our service or product\r\nin order to subsequently analyze the strengths and weaknesses of those consumer experiences. To\r\nthe best of our knowledge, there is no science mapping analysis covering the research about opinion\r\nmining and sentiment analysis in the MarCom ecosystem. In this study, we perform a science\r\nmapping analysis on the OMSAMC research, in order to provide an overview of the scientific work\r\nduring the last two decades in this interdisciplinary area and to show trends that could be the basis\r\nfor future developments in the field. This study was carried out using VOSviewer, CitNetExplorer\r\nand InCites based on results from Web of Science (WoS). The results of this analysis show the\r\nevolution of the field, by highlighting the most notable authors, institutions, keywords,\r\npublications, countries, categories and journals.The research was funded by Programa Operativo FEDER Andalucía 2014‐2020, grant number “La\r\nreputación de las organizaciones en una sociedad digital. Elaboración de una Plataforma Inteligente para la\r\nLocalización, Identificación y Clasificación de Influenciadores en los Medios Sociales Digitales (UMA18‐\r\nFEDERJA‐148)” and The APC was funded by the same research gran",
      "documentType": "research",
      "doi": "10.3390/socsci9030023.",
      "downloadUrl": "https://core.ac.uk/download/288162334.pdf",
      "fieldOfStudy": null,
      "fullText": " Soc. Sci. 2020, 9, 23; doi:10.3390/socsci9030023  www.mdpi.com/journal/socsci Article Opinion  Mining  and  Sentiment  Analysis  in Marketing  Communications:  A  Science  Mapping Analysis in Web of Science (1998–2018) Pablo Sánchez‐Núñez 1,*, Carlos de las Heras‐Pedrosa 2 and José Ignacio Peláez 3 1  Doctorate Program in Communication by Universidad de Cádiz, Universidad de Huelva, Universidad de Málaga and Universidad de Sevilla, 29071 Malaga, Spain; psancheznunez@uma.es 2  Department of Audiovisual Communication and Advertising, Faculty of Communication Sciences, Universidad de Málaga, 29071 Malaga, Spain; cheras@uma.es 3  Department of Languages and Computer Sciences, Higher Technical School of Computer Engineering, Universidad de Málaga, 29071 Malaga, Spain; jipelaez@uma.es *  Correspondence: psancheznunez@uma.es Received: 20 January 2020; Accepted: 24 February 2020; Published: 28 February 2020 Abstract:  Opinion mining  and  sentiment  analysis  has  become  ubiquitous  in  our  society,  with applications in online searching, computer vision, image understanding, artificial intelligence and marketing communications (MarCom). Within this context, opinion mining and sentiment analysis in marketing  communications  (OMSAMC) has a  strong  role  in  the development of  the  field by allowing us to understand whether people are satisfied or dissatisfied with our service or product in order to subsequently analyze the strengths and weaknesses of those consumer experiences. To the best of our knowledge, there is no science mapping analysis covering the research about opinion mining  and  sentiment  analysis  in  the MarCom  ecosystem.  In  this  study, we perform  a  science mapping analysis on the OMSAMC research, in order to provide an overview of the scientific work during the last two decades in this interdisciplinary area and to show trends that could be the basis for future developments in the field. This study was carried out using VOSviewer, CitNetExplorer and  InCites based on  results  from Web of Science  (WoS). The  results of  this analysis  show  the evolution  of  the  field,  by  highlighting  the  most  notable  authors,  institutions,  keywords, publications, countries, categories and journals. Keywords: sentiment analysis; opinion mining; advertising; marketing; science mapping analysis; Web of Science (WoS); bibliometric indicators; scientific collaboration  1. Introduction Sentiment analysis and opinion mining are an automatic mass classification of textual and visual information,  which  focuses  on  cataloguing  and  classifying  data  according  to  the  polarity—the positive  or negative  connotation—of  the  language used  in  them  (Pang & Lee,  2008; Prabowo & Thelwall, 2009). These positive or negative connotations of the  language are reflected  in opinions, attitudes and  emotions  expressed by  Internet users  (Mostafa 2013)  in online mentions on digital ecosystems (Kennedy 2012; Mäntylä et al. 2018). Knowing what others  think and  feel can be  fundamental  to most of us during  the decision‐making process  (Bericat 2016; Saaty and Vargas 2012). User opinions not only help people make informed decisions, but also help organizations identify customer opinions, attitudes and emotions about the products and services they offer (Peláez et al. 2019). In  this  context,  Opinion  Mining  and  Sentiment  Analysis  in  Marketing  Communications (OMSAMC) are extremely important when it comes to analyzing consumer buying patterns (Peláez Soc. Sci. 2020, 9, 23  2  of  22  et al. 2018; Sebastian 2014), collecting customer feedback from social media, websites or online forms (Liu and Ji 2018), as well as knowing what types of stimuli impact people (Baraybar‐Fernández et al. 2017), understanding the reasons that motivates people to like a product/service (Peláez et al. 2019), conducting market research (Wereda and Woźniak 2019), categorizing customer service requests and predicting consumer behavior, among others (Baron et al. 2017). The way of researching in this field has varied considerably over the last few years. There are many different approaches  in  the  field of opinion mining and  sentiment analysis. Some of  them provide new frameworks for measuring customer‐specific variables (Kang and Park 2014), as well as systematic reviews with bibliometric indicators of productivity, impact and collaboration (Martínez‐López et al. 2018), Altmetrics (Thelwall et al. 2013) or scientific mapping analysis (Piryani et al. 2017). In this last case, the study of scientific mapping allows us to study the impact and visibility of scientific publications. The study of scientific collaboration plays a decisive role  in  the expansion, visibility, specialization, consolidation and emergence of the results of scientific production. Being able to identify their topological structure is a key and disruptor element for the study of the reception and transmission of knowledge (Newman 2000). This  article  aims  to  trace  the  evolution  of OMSAMC  by  asking  the  following  questions  in different academic research scenarios:  How has scientific research at OMSAMC progressed from 1998 to 2018?  In which countries and organizations has most of the research on OMSAMC been carried out?  What are the main sources of publication (journals) that publish research on OMSAMC?    Who are the most productive and cited authors in OMSAMC research during the study period?  What is the degree of international scientific collaboration in OMSAMC research?  What type of authoring patterns are observed in the results of OMSAMC research?  What are the main concepts that appear in OMSAMC research publications?  What are the main themes, approaches and methods of OMSAMC?  What are the main areas of application of OMSAMC research? This work offers a science mapping analysis that studies the development of OMSAMC during the period 1998–2018. We have used both manual and computational analysis for this purpose. The collected data obtained from Web of Science (WoS) database is analyzed computationally with the aim to identify the year‐wise number and rate of growth of proceedings, articles, reviews and book chapters,  as well  as  the  different  types  of  authorships  on OMSAMC,  collaboration  and  citation patterns,  the most productive  authors,  journals, keywords,  institutions  and  countries during  the period. Thereafter a detailed manual analysis of the research publication data is performed to identify popular trends, patterns, approaches and possible application areas of OMSAMC. To  the best of our knowledge,  this  is  the  first work of  its kind, differing  from  the rest of  the science mapping analysis works and scientometric studies in opinion mining and sentiment analysis (Piryani et al. 2017) due to the research focused in marketing communications (MarCom).   This  perspective  change  from  previous  works  makes  this  study  more  targeted  towards marketing communications scholars, therefore allowing a more precise analysis on the topic while taking into consideration  its particularities. This results  in a set of scientific network and research productivity clusters that describe very concisely the current interactions and research areas within this field. For this reason, this study may be useful for experts in information processing and scientific communication, specialists in research policy formulation as well as for students and researchers in the fields of communication, computer science, psychology, marketing or artificial intelligence. The research has been organized as follows: Section 2 is the results section and presents in an analytical way  the data obtained  from  the  analysis of  scientific maps on  sentiment  analysis  and opinion mining in marketing communications. Section 3 is the discussion and debates the results, presenting the different approaches and levels of OMSAMC, the main sources of data, the areas of application as well as the possible future lines of research. Section 4 is the Materials and Methods section, describing the data collection and the methodology.   Soc. Sci. 2020, 9, 23  3  of  22   2. Results In this section, we present the science mapping and scientometric indicators computed through computational  analysis of  the data. The  subsections below present details of different  indicators computed and figures and tables illustrating the resultant values of the analysis.   The  results  are derived  from bibliometric  analysis obtained  from  the Web of  Science  (WoS) database. To date, the 845 studies (1998–2018) that were found and that will form the basis of this analysis  is  presented.  In  addition,  it  limited  the  analyzing  to  the  document  types  “Articles”, “Reviews”, “Book Chapter” and “Proceedings paper” written in English.   2.1. Distribution of Documents by Year (1998–2018) The distribution of publications during the period 1998–2018 is shown in Figure 1. During the first decade (1998–2007) of studies, a sustained growth of publications (51) is observed while in the second decade  (2008–2018)  it was detected  that OMSAMC has  shown exponential growth  in  the number of investigations (794).    Figure 1. Publication years and record count. 2.2. Citation Report The  total publications  retrieved  (Table  1)  combined had  a  sum  of  9557  citations  (Figure  2), making an average of 11.31 citations per paper. The H‐index  is 49, which means that there are 49 studies that have received at least 49 citations. Table 1. Citation report. Citation Report Total publications  845 Sum of times cited  9557 Average citations per item    11.31 H‐index  49  Soc. Sci. 2020, 9, 23  4  of  22   Figure 2. Citations by years and record count. 2.3. Top 25 Co‐Authorship Analysis (Authors and Record Count) The 25 most productive  international collaborations can be seen  in Table 2, which presents a ranking of  the 25 most  influential authors  for OMSAMC, along with  the number of  international collaboration documents produced, the sum of citations and the total link strength.   Table 2. Top 25 Co‐authorship analysis (authors). The relatedness of items is determined based on their  number  of  co‐authored  documents. Minimum  number  of  documents  of  an  author  (3)  and minimum number of citations of an author (1). Of the 2505 authors, 38 met the thresholds. Ranking  Author  Documents  Citations  Total Link Strength 1.   Pieters, R  6  775  6 2.   Wedel, M  6  775  6 3.   Cambria, Erik  8  306  4 4.   Wedel, Michel  8  292  6 5.   Pieters, Rik  6  288  6 6.   Poria, Soujanya  3  205  3 7.   Wojdynski, Bartosz W.  4  123  3 8.   Bang, Hyejin  3  35  3 9.   Bodendorf, Freimut  4  31  3 10.   Kaiser, Carolin  3  31  3 11.   Holmberg, Nils  3  18  3 12.   Sandberg, Helena  3  18  3 13.   Recupero, Diego Reforgiato  3  10  2 14.   Dragoni, Mauro  4  7  1 15.   Farkas, Richard  3  5  3 16.   Hangya, Viktor  3  5  3 17.   Oliveira, Eugenio  3  5  9 18.   Reis, Luis Paulo  3  5  9 19.   Teixeira, Jorge  3  5  9 20.   Vinhas, Vasco  3  5  9 21.   Lu, Hanqing  3  4  1 22.   Xu, Changsheng  3  4  1 23.   Kincl, Tomas  3  2  6 24.   Novak, Michal  4  2  6 25.   Pribil, Jiri  3  2  6  Soc. Sci. 2020, 9, 23  5  of  22  A citation network (1998–2018) is shown in Figure 3, where we find four clusters: a 1st group of 136 publications, 2nd group of 55 publications, 3rd group of 21 publications and 4th group of 15 publications. Additionally, there are 618 publications that do not belong to any cluster. The citation visualization network is based in 60 publications (based on their citation score).   The authors Beijer and Crundall are founded but they are isolated from the rest of the literature. Pieter’s publication is not only included in the main network but also originates it.    Figure 3. Citation network (1998–2018). 2.4. Group Authors and Record Count A Group Author is an organization or institution that is credited with authorship of an article by the source publication. OMSAMC Group Authors and Record Count are shown in Figure 4. We observed that in the Top 5 Group Authors, IEEE has the biggest number of corporate authors with 140, followed by ACM with 14, Association for Computer Machinery with 3 and towards the end ASME and DEStech Publications, Inc., both with 3.  Figure 4. Group Authors and Record Count (669 records (79.172%) do not contain data in the field being analyzed). Soc. Sci. 2020, 9, 23  6  of  22  2.5. Top 25 Co‐Authorship Analysis (Countries/Regions and Record Count) This section presents the results of the most influential Co‐Authorship countries in OMSAMC publications (Figure 5a), as well as the document and citation details of the first Top 10 countries in OMSAMC Co‐Authorship. The  first place  is occupied by  the USA  (211 documents and 4939 citations),  followed by The Netherlands  (36 documents and 1623 citations),  Italy  (50 documents and 824 citations), China  (90 documents and 670 citations), England (45 documents and 590 citation), Germany (44 documents and 460 citations), France (24 documents and 452 citations), Australia (32 documents and 374 citations), Singapore (25 documents and 361 citations) and towards the end by Spain (37 documents and 344 citations). InCites regional analysis (Figure 5b) allows us to view and compare indicators at the country level, seeing the geographical distribution of the top producing regions, authors in publications in each  research area and  identification  research  trends  in countries of  interest  (Fonseca et al. 2016; Luukkonen  et  al.  1992).  InCites Regional  Indicators measure  Productivity  (% Documents  in Q1 Journals,  count  of  documents  in  Q1  Journals),  Collaboration  (%  International  Collaborations, percentage of publications that have international co‐authors) and Impact (Times‐Cited, number of times the set of publications has been cited).  (a)  Soc. Sci. 2020, 9, 23  7  of  22   (b) Figure 5. (a) Top 25 Co‐authorship analysis (countries). The relatedness of items is determined based on their number of co‐authored documents. Minimum number of documents of a country (5) and minimum number of citations of a country (5). (b) InCites Top 25 Locations and Record Count. 2.6. Top 25 Co‐Authorship Analysis (Organizations and Record Count) Table  3  presents  a  ranking  of  the  most  influential  international  collaborations  through universities along with  several documents and  sum  citations  indicators;  two  indicators of global university ranking according to the 2019 Quacquarelli Symonds (QS) World University Rankings and 2019 Academic Ranking of World Universities (ARWU) that allow us to measure the relative position in which we find the most influential institutions in OMSAMC. Table 3. Top 25 International Collaborations/Organizations and Record Count. Minimum number of documents of a country (5) and minimum number of citations of a country (5). Ranking  Organization  Documents Citations Total Link Strength ARWU 2019 QS 2019 1.  Tilburg University  14  1085  14  501–600  319 2.  University of Michigan  7  739  8  20  20 3.  University of Pennsylvania  5  461  1  17  19 4.  The University of Maryland  12  372  9  46  126 5.  Nanyang Technological University  10  302  3  73  12 6.  City University of Hong Kong  7  286  2  201–300  55 7.  Copenhagen Business School  5  193  0  701–800  ‐ 8.  University of Nottingham  7  158  0  101–150  82 Soc. Sci. 2020, 9, 23  8  of  22  9.  University of California San Diego  7  151  5  18  41 10.  The University of Georgia  9  150  6  201–300  431 11.  Michigan State University  6  138  5  101–150  141 12.  Pennsylvania State University  6  129  7  98  95 13.  The University of Arizona  6  100  4  101–150  246 14.  Korea Advanced Institute of Science and Technology  5  85  1  201–300  40 15.  University of Amsterdam  5  74  0  101–150  57 16.  University of Florida  8  58  5  95  180 17.  University of Minnesota  5  48  5  41  156 18.  Microsoft Research Asia  5  42  1  ‐  ‐ 19.  University of Pittsburgh  6  39  1  89  136 20.  The Australian National University  6  34  5  76  24 21.  Zhejiang University  5  34  1  70  68 22.  University of Malaya  5  22  1  301–400  87 23.  National University of Singapore  7  19  3  67  11 24.  Chinese Academy of Sciences  8  18  2  ‐  ‐ 25.  Politecnico di Milano  5  17  1  201–300  156  Within  the  first 10 universities, 50% are  in  the United States,  followed by  institutions  in The Netherlands (1), Singapore (5), Hong Kong (6), Denmark (7) and United Kingdom (8). Further down the rankings are other institutions in Korea, China, Australia, Malaysia and Italy. The first institution in  the  ranking  in  terms  of  international  collaboration  is  Tilburg  University  with  a  total  of  14 documents published in OMSAMC, where 14 of these studies have received 1085 citations. As for the relative position of the University, Tilburg University is located within the first 501–600 ARWU 2019 and  319 QS  2019.  The  following  university  is University  of Michigan, with  a  total  of  7  articles published, of which 7 have been  cited  at  least 739  times. The  following one  is  the University of Pennsylvania, with 5 papers published and a ratio of 461 citation.   Only 12 of the Top 25 rankings of universities are in the Top 100 ranking according to ARWU: University  of  Michigan,  University  of  Pennsylvania,  The  University  of  Maryland,  Nanyang Technological  University,  University  of  California  San  Diego,  Pennsylvania  State  University, University of Florida, University of Minnesota, University of Pittsburgh, The Australian National University, Zhejiang University and National University of Singapore. Of these, 8 are in the United States while only 8 are part of the Top 100 according to QS: University of Michigan, University of Pennsylvania, Nanyang Technological University, University of California San Diego, Pennsylvania State University, The Australian National University, Zhejiang University and National University of Singapore. 2.7. Top 25 OMSAMC Funding Agencies and Top 25 InCites Funding Agencies and Record Count In  this  section we  analyze  the  impact  of OMSAMC  research  that  has  been  funded  and/or published by funding agencies. We assess this by leveraging unified funding acknowledgment data from the Web of Science (Figure 6a), with the aim to understand whether these funds were spent in studies that made a disruptive scientific advance (Álvarez‐Bornstein et al. 2017). InCites Funding  Indicators  (Figure  6b) measure Productivity  (% Documents  in Q1  Journals, count of documents in Q1 Journals) and Impact (Citation Impact, average number of citations per paper).    Soc. Sci. 2020, 9, 23  9  of  22   (a)  Soc. Sci. 2020, 9, 23  10  of  22   (b) Figure 6.  (a) Top 25 OMSAMC Funding Agencies and Record Count.  (b)  InCites Top 25 Funding Agencies and Record Count. 2.8. Research Areas and Record Count There is a total of 5 general categories on the Web of Science (Arts and Humanities, Life Sciences and  Biomedicine,  Physical  Sciences,  Social  Sciences  and  Technology).  Within  these  5  general categories there are 71 different sub‐categories. Figure 7 shows the abovementioned sub‐categories. Among  the Top 10 most representative categories  in OMSAMC we  find  the  following: Computer Science (450 registers and 53.254% of 845), Engineering (235 registers and 27.811% of 845), Business Economics  (158  registers  and  18.698%  of  845),  Psychology  (57  registers  and  6.746%  of  845), Telecommunications  (55 registers and 6509% of 845), Communication  (34 registers and 4.024% of 845), Information Science and Library Science (26 registers and 3.077% of 845), Operations Research and Management  Science  (26  registers  and  3.077%  of  845),  Social  Science  and Other  Topics  (25 registers and 2.959% of 845); towards the end we find Imaging Science and Photographic Technology (22 registers and 2.604% of 845).   Soc. Sci. 2020, 9, 23  11  of  22   Figure 7. Research areas and record count. 2.9. Top 25 Co‐Citation Analysis (Sources) As shown in Table 4, the most cited journals in OMSAMC have a clear focus on marketing since they prominently cited marketing magazines. Journal of Consumer Research (1st ranked with a sum of 521 citations), Journal of Marketing Research (2nd ranked with a sum of 371 citations), Journal of Marketing (3rd ranked with a sum of 316 citations), Lecture Notes in Computer Science LNCS (4th ranked with  a  sum  of  296  citations)  and  Journal  of Advertising  (5th  ranked with  a  sum  of  279 citations) are the most cited journals in OMSAMC. The first, second and third are usually regarded as the three most influential magazines in consumerism and marketing, while the fourth and fifth magazine  shows  its  clear  thematic  connection  (computer  science  and  advertising  journals) with OMSAMC.   Soc. Sci. 2020, 9, 23  12  of  22  Table 4. The Top 25 Co‐Citation analysis (sources). The relatedness of items is determined based on the number of times they are cited together. Minimum number of citations of a source (75). Of the 11,186 sources, 34 met the threshold. Ranking  Source  Citations  Total Link Strength 1.   Journal of Consumer Research  521  10,340 2.   Journal of Marketing Research  371  9258 3.   Journal of Marketing  316  7353 4.  Lecture Notes in Computer Science LNCS 296  1707 5.   Journal of Advertising  279  5370 6.  Journal of Advertising Research 276  6244 7.  IEEE Transactions on Pattern Analysis and Machine Intelligence 249  1748 8.   Expert Systems with Applications  246  6128 9.   Marketing Science  239  7223 10.   Decision Support Systems  213  5198 11.  IEEE Conference on Computer Vision and Pattern Recognition 161  1015 12.   Management Science  155  5277 13.  Journal of Interactive Marketing 138  3716 14.   Psychology & Marketing  138  3651 15.   Psychological Bulletin  135  2116 16.   Vision Research  125  1835 17.  Journal of Personality and Social Psychology 121  2377 18.   Computers in Human Behavior  120  2656 19.   Journal of Business Research  114  4262 20.   Journal of the Association for  112  880 Soc. Sci. 2020, 9, 23  13  of  22  Information Science and Technology 21.  International Journal of Research in Marketing 101  2990 22.   Psychological Review  100  1739 23.   Advances in Consumer Research  92  2420 24.  Journal of Experimental Psychology: Human Perception and Performance 92  1731 25.   Tourism Management  92  1592 2.10. Co‐Occurrence Analysis The use of cooccurrence data is very common in scientometric and cooccurrences of words may be used to construct so‐called co‐word maps, which are maps that provide a visual representation of the structure of a scientific field (Van Eck and Waltman 2009). In Figure 8 is shown the relatedness of items determined based on the number of documents in which they occur together (all keywords). The minimum  number  of  occurrences  of  a  keyword was  3. Of  the  3094  keywords,  332 met  the threshold. We identified 10 different clusters:  Cluster  1  Social  Media,  Machine  Learning  and  Artificial  Intelligence:  algorithms,  antecedents, behavior analysis, big data, blogosphere, brand monitoring, business intelligence, classification, cluster  analysis,  clustering,  community,  deep  neural  networks,  emotions,  engagement,  eye tracking  technology,  face  recognition,  Facebook,  framework,  identification,  image,  image classification,  in‐game advertising, influence,  information technology, Instagram,  intelligence, k‐means,  literature  review,  marketing  intelligence,  models,  naive  Bayes,  network,  object detection, opinion, pattern recognition, power, retrieval, satisfaction, sentiment, small business, social  influence,  social  media,  social  media  analytics,  social  media  mining,  social  media monitoring,  social  network,  social  network  analysis,  support  vector  machines,  systems, technology, text classification, tourism, trust, twitter, visual analytics, visualization, words.    Cluster 2 Product Design: advertisement, advertising, alcohol, allocation, avoidance, behavior, bias,  body  dissatisfaction,  brand  recall,  brands,  commercials,  communication,  consumption, drivers,  experience,  exposure,  eye,  eye  fixation,  eye movement,  eye  tracking,  health,  health warnings,  interactivity,  labels,  media,  messages,  nonsmokers,  online,  patterns,  perceptions, perspective, pictorial, pictures, products, recall, road safety, smokers, television, smoking, text, tobacco, trends, tv, united states, video, visual attention, warning labels, working memory.    Cluster  3  Design  Techniques:  advertisement,  algorithm,  attitudes,  augmented  reality,  color, computer vision, customer satisfaction, design, digital image processing, food, gender, grading, hidden  Markov  model,  image  processing,  image  segmentation,  integration,  knowledge, machine, machine vision, methodology, model, neural network, objects, optimization, orange, prediction,  purchase  intention,  quality,  regression,  segmentation,  sex  differences,  sorting, support vector machines, surface area, system, user experience, vision, website.  Cluster 4 Advertising and Message Impact: ads, animation, attention, attitude, capacity, capture, children, cognitive load, context, contextual advertising, distraction, features, human‐computer interaction,  information,  internet,  internet  use,  involvement,  looking,  mechanisms,  motion, movement, older drivers, online advertising, people, performance, persuasion, saliency, search, Soc. Sci. 2020, 9, 23  14  of  22  selective  attention,  strategies,  surveillance,  tracking, visual  attention, visual behavior, visual saliency, visual search, web design, websites, world wide web.  Cluster 5 Advertising and Neuroscience: advertising effectiveness, affective computing, arousal, biometrics, brain, consumer, cortex, cues, destination, digital signage, EEG, emotion recognition, face, familiarity, feature extraction, galvanic skin response, gaze, gender classification, images, interferences, memory, object recognition, perception, preference, recognition, representation, responses, scale, scenes, selection, stimuli, television commercials, valence, warnings, young.      Cluster  6  Social Network Analysis  (SNA):  approximation,  artificial  intelligence,  convolutional neural  networks,  corpus,  cross  domain,  customer  reviews,  data  mining,  deep  learning,  e‐commerce,  emotion  analysis,  feature  selection,  image  recognition,  information  extraction, lexicon  based, machine  learning, marketing,  natural  language  processing,  networks,  neural networks, NLP, ontology, opinion mining, product review, security, semantic web, sentiment analysis, sentiment classification, text analysis, text categorization, topic modelling, tweets, web, word embedding.    Cluster  7  Consumer  behavior:  analytics,  brand  image,  communities,  community  detection, congruity,  consumer  reviews,  dynamics,  experience,  field,  helpfulness,  hospitality  impact, knowledge discovery,  lexicon, management, moderating  role, news, online products  review, online  reputation management,  online  reviews, persuasion  knowledge,  product,  reputation, saccades,  sales,  semantics,  sentiments,  social  media  marketing,  support,  text  mining,  user‐generated content, word of mouth.    Cluster  8  Brand Analysis  and  Retail:  brand,  brand  attention,  brand  choice,  branding,  choice, consumer behavior, consumer choice, consumer neuroscience, decision making, display, eye tracking,  eye  movement,  facial  expression,  fixations,  gaze  bias,  implicit  memory,  in‐store decision making, mere exposure, neuromarketing, nutrition information, print advertisement, promotion, selective visual attention, time, time‐pressure, willingness to pay.    Cluster 9 Social Networks and Spam Detection: crowdsourcing, location, retailing, social networks, spam detection, web 2.0, web mining.    Cluster 10 Consumer Relationship Management: consumer, customer engagement, Hadoop, CRM, social media.  Figure 8. Co‐Occurrence analysis. The relatedness of  items  is determined based on  the number of documents  in which  they  occur  together  (all  keywords). Minimum  number  of  occurrences  of  a keyword was 3. Of the 3094 keywords, 332 met the threshold. Soc. Sci. 2020, 9, 23  15  of  22  2.11. Top 25 Times‐Cited Works in Opinion Mining and Sentiment Analysis in Marketing Communication The distribution of the most cited articles by year in OMSAMC is shown in Table 5. The first 25 studies are done according to their year publication, author, study title, total number of citations and ranking. There are 19 studies that have been cited at least 100 times. Table 5. The Top 25 Times‐Cited Works analysis. The relatedness of items is determined based on the number of times they are cited (publications). Title  Authors  Source  Publication Years  Citations Average Citation/Year 1. In the Eye of the Beholder: A Survey of Models for Eyes and Gaze Hansen, Dan Witzner; Ji, Qiang IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2010  565  56.5 2. Attention capture and transfer in advertising: Brand, pictorial, and text‐size effects Pieters, R; Wedel, M  JOURNAL OF MARKETING  2004  304  19 3. Does In‐Store Marketing Work? Effects of the Number and Position of Shelf Facings on Brand Attention and Evaluation at the Point of Purchase Chandon, Pierre; Hutchinson, J. Wesley; Bradlow, Eric T.; Young, Scott H. JOURNAL OF MARKETING  2009  249  22.64 4. More than words: Social networksʹ text mining for consumer brand sentiments Mostafa, Mohamed M. EXPERT SYSTEMS WITH APPLICATIONS  2013  188  26.86 5. Designing Ranking Systems for Hotels on Travel Search Engines by Mining User‐Generated and Crowdsourced Content Ghose, Anindya; Ipeirotis, Panagiotis G.; Li, Beibei MARKETING SCIENCE  2012  177  22.13 6. Eye fixations on advertisements and memory for brands: A model and findings Wedel, M; Pieters, R  MARKETING SCIENCE  2000  173  8.65 7. User generated content: the use of blogs for tourism organisations and tourism consumers Akehurst, Gary  SERVICE BUSINESS  2009  168  15.27 8. Breaking through the clutter: Benefits of advertisement originality and familiarity for brand attention and memory Pieters, R; Warlop, L; Wedel, M MANAGEMENT SCIENCE  2002  143  7.94 9. Understanding Transit Scenes: A Survey on Human Behavior‐Recognition Algorithms Candamo, Joshua; Shreve, Matthew; Goldgof, Dmitry B.; Sapper, Deborah B.; Kasturi, Rangachar IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS 2010  140  14 10. Shape Analysis of Agricultural Products: A Review of Recent Research Advances and Costa, Corrado; Antonucci, Francesca; Pallottino, Federico; Aguzzi, Jacopo; Sun, FOOD AND BIOPROCESS TECHNOLOGY 2011  139  15.44 Soc. Sci. 2020, 9, 23  16  of  22  Potential Application to Computer Vision Da‐Wen; Menesatti, Paolo 11. Sentic patterns: Dependency‐based rules for concept‐level sentiment analysis Poria, Soujanya; Cambria, Erik; Winterstein, Gregoire; Huang, Guang‐Bin KNOWLEDGE‐BASED SYSTEMS  2014  132  22 12. Using Twitter to Examine Smoking Behavior and Perceptions of Emerging Tobacco Products Myslin, Mark; Zhu, Shu‐Hong; Chapman, Wendy; Conway, Mike JOURNAL OF MEDICAL INTERNET RESEARCH 2013  125  17.86 13. Branding the brain: A critical review and outlook Plassmann, Hilke; Ramsoy, Thomas Zoega; Milosavljevic, Milica JOURNAL OF CONSUMER PSYCHOLOGY 2012  121  15.13 14. The impact of social and conventional media on firm equity value: A sentiment analysis approach Yu, Yang; Duan, Wenjing; Cao, Qing DECISION SUPPORT SYSTEMS  2013  118  16.86 15. Survey on mining subjective data on the web Tsytsarau, Mikalai; Palpanas, Themis DATA MINING AND KNOWLEDGE DISCOVERY 2012  118  14.75 16. A flexible model of consumer country‐of‐origin perceptions ‐ A cross‐cultural investigation Knight, GA; Calantone, RJ INTERNATIONAL MARKETING REVIEW  2000  116  5.8 17. Affective News: The Automated Coding of Sentiment in Political Texts Young, Lori; Soroka, Stuart POLITICAL COMMUNICATION  2012  113  14.13 18. Mining comparative opinions from customer reviews for Competitive Intelligence Xu, Kaiquan; Liao, Stephen Shaoyi; Li, Jiexun; Song, Yuxia DECISION SUPPORT SYSTEMS  2011  113  12.56 19. Visual attention to repeated print advertising: A test of scanpath theory Pieters, R; Rosbergen, E; Wedel, M JOURNAL OF MARKETING RESEARCH 1999  103  4.9 20. Building models for marketing decisions: Past, present and future Leeflang, PSH; Wittink, DR INTERNATIONAL JOURNAL OF RESEARCH IN MARKETING 2000  92  4.6 21. Goal control of attention to advertising: The Yarbus implication Pieters, Rik; Wedel, Michel JOURNAL OF CONSUMER RESEARCH 2007  90  6.92 22. What Do You See When Youʹre Surfing? Using Eye Tracking to Predict Salient Regions of Web Pages Buscher, Georg; Cutrell, Edward; Morris, Meredith Ringel CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1–4 2009  89  8.09 23. Going Native: Effects of Disclosure Position and Language on the Recognition and Evaluation of Online Native Advertising Wojdynski, Bartosz W.; Evans, Nathaniel J. JOURNAL OF ADVERTISING  2016  88  22 Soc. Sci. 2020, 9, 23  17  of  22  24. H‐ATLAS: PACS imaging for the Science Demonstration Phase Ibar, Edo; Ivison, R. J.; Cava, A.; Rodighiero, G.; Buttiglione, S.; Temi, P.; Frayer, D.; Fritz, J.; Leeuw, L.; Baes, M.; Rigby, E.; Verma, A.; Serjeant, S.; Mueller, T.; Auld, R.; Dariush, A.; Dunne, L.; Eales, S.; Maddox, S.; Panuzzo, P.; Pascale, E.; Pohlen, M.; Smith, D.; de Zotti, G.; Vaccari, M.; Hopwood, R.; Cooray, A.; Burgarella, D.; Jarvis, M. MONTHLY NOTICES OF THE ROYAL ASTRONOMICAL SOCIETY 2010  87  8.7 25. Predicting consumer sentiments from online text Bai, Xue  DECISION SUPPORT SYSTEMS  2011  85  9.44 The study with the most citations (565) of OMSAMC is “In the Eye of the Beholder: A Survey of Models for Eyes and Gaze”, published in 2010, and reviews the current progress in and state‐of‐the‐art of video‐based eye detection and tracking in order to identify promising techniques as well as issues  to  be  further  addressed.  The  study  presents  a  detailed  review  of  recent  eye models  and techniques for eye detection and tracking and survey methods for gaze estimation, comparing them based on their geometric properties and reported accuracies. The ratio of number of citations per year is approximately 56.9 citations.   “Attention capture and transfer in advertising: Brand, pictorial, and text‐size effects” published in 2004, follows, and because of the paper studies how the three key ad elements (brand, pictorial and text) each have unique superiority effects on attention to advertisements, which are on par with many commonly held ideas in marketing practice—the main conclusion of an analysis of 1363 print advertisements tested with infrared eye‐tracking methodology on more than 3600 consumers. The pictorial  is superior  in capturing attention,  independent of  its size. The authors discuss how  their findings can be used to render more effective decisions in advertising. This study has been cited 304 times and has a ratio equal to 19 citations per year.   Within the Top 3 is “Does In‐Store Marketing Work? Effects of the Number and Position of Shelf Facings on Brand Attention and Evaluation at the Point of Purchase”, published in 2009, which, like other studies within the table, covers the topic of interplay between in‐store and out‐of‐store factors on consumer attention to and evaluation of brands displayed on supermarket shelves. The results underscore the importance of combining eye‐tracking and purchase data to obtain a full picture of the effects of in‐store and out‐of‐store marketing at the point of purchase. This study has been cited 249 times and has a ratio equal to 22.64 citations per year.   The following studies cover other  issues such as text mining for consumer brand sentiments, design for ranking systems, user generated content and advertising or advertising and memory. 3. Discussion Sentiment  analysis  and  opinion  mining  in  marketing  communications  (OMSAMC)  is  a promising and growing research field. OMSAMC has been acquiring a crucial role in both research and commercial applications because of their probable applicability to numerous diverse fields, such as the identification of brand awareness, reputation and popularity at a specific moment or over time, the tracking of consumer reception of new products or features, the pinpoint targeting of an audience or the evaluation performance success of a marketing campaign. OMSAMC has experienced an exponential growth in the number of investigations (794) in the recent years, with a sum of citations of 9557 and with an average of 11.31 citations per paper. The H‐Soc. Sci. 2020, 9, 23  18  of  22  index  reveals  a  result  of  49  (49  studies  that  have  received  at  least  49  citations). To  explain  this development  there are  two main  topics that are  illustrious:  the  increase of researchers worldwide (United Nations, 2015) and the development of information technology and the Internet (Jasanoff and Pinch  2019)  that  permits  one  to  rapidly  acquire  a  greater  volume  of  information  connected  to OMSAMC and all global issues. The  study  shows which  are  the most prolific  authors  in  the  field  of OMSAMC  in  terms  of scientific collaboration: Pieters, R. (6 documents, 775 citations and total link strength of 6), Wedel, M. (6 documents, 775 citations and total link strength of 6) and Cambria, E. (8 documents, 306 citations and total link strength of 4). When analyzing the citation network, it was shown that although there are two distinct clusters, there is no real strong cluster structure within that citation network except for a group of more recent publications. Most of the group authors belongs to IEEE with a sum of 140 corporate authors. The  research  study  has  shown  that  in  terms  of  OMSAMC  international  co‐authorship productivity  the  first  place  is  for  USA  (211  documents  and  4939  citations),  followed  by  The Netherlands (36 documents and 1623 citations) and Italy (50 documents and 824 citations).   It  is  paradoxical  because  if  we  study  international  scientific  collaboration  in  all  areas  of knowledge, we observe that the United States is one of the most productive countries in terms of quality publications  (Q1  journals),  but  it  is  one  of  the  least productive  in  terms  of  international collaborations, since its tendency continues to be towards national collaborations. It is also interesting to talk about countries like the United Kingdom, Spain, Italy, France, Holland, Finland or Israel, with a lower volume of publications in Q1 journals, which are the countries that make more international collaborations.   The  study  of  international  collaboration  in  organizations  has  shown  that  the University  of Tilburg  (The Netherlands),  followed by  the University of Michigan  (USA)  and  the University of Pennsylvania (USA), are the three  institutions  that carry out research  in OMSAMC that have had more documents and citations.   The  funding  agencies  that have  funded more  studies  in OMSAMC have  been  the National Natural Science Foundation of China, followed by the National Institute of Health of NIH‐USA and the United States Department of Health Human Services. If we compare the funding agencies that have invested in research in OMSAMC with the funding agencies that have invested in research in all industries and scientific areas we see that the United States is the country that has produced the most documents that are located in Q1 journals and with the highest citation impact while China has produced many papers located in Q1 but has not had the same influence in terms of citation impact.   Results  prove  that  the  three most  representative  categories  of OMSAMC works  in Web  of Science  are Computer Science  (450  registers  and  53.254% of  845), Engineering  (235  registers  and 27.811%  of  845)  and Business Economics  (158  registers  and  18.698%  of  845),  and  the most  cited journals  in OMSAMC has a clear focus on marketing since it mostly cites marketing  journals. We have the Journal of Consumer Research (1st ranked with a sum of 521 citations), Journal of Marketing Research (2nd ranked with a sum of 371 citations) and Journal of Marketing (3rd ranked with a sum of 316 citations).   The results of the keyword analysis determine different clusters in which we find that the major areas of expertise in OMSAMC are opinion mining and sentiment analysis in computer science and neuroscience research areas. The  most  used  computational  intelligence  techniques  to  analyze  sentiment  and  opinion  in marketing are k‐means algorithms, Bayesian networks, clustering techniques, deep neural networks, convolutional neural networks, support vector machines, hidden Markov models as well as natural language  processing  and  ontology  developments.  From  the  neuroscience  field,  the  most  used techniques are eye tracking, EEG and galvanic skin response. On the one hand, there is a deep interest in the study and monitoring of brands, corporate reputation as well as the design of visual content and  advertising message. Several  studies  reveal  that  there  is new  research  in  areas of  consumer experience such as the impact and effect of alcohol and tobacco on buyers. On the other hand, the Soc. Sci. 2020, 9, 23  19  of  22  studies reflect that the most studied and analyzed social networks in OMSAMC research have been Instagram, Facebook and Twitter.   Finally, the research demonstrates that the studies that have had the greatest repercussion on OMSAMC in terms of times cited and impact citation have been “In the Eye of the Beholder: A Survey of Models for Eyes and Gaze “(565), which reviews the current progress in and the state‐of‐the‐art of video‐based eye detection and tracking in order to identify promising techniques as well as issues to be further addressed. “Attention capture and transfer in advertising: Brand, pictorial, and text‐size effects” (304) follows, and this paper studies how the three key ad elements (brand, pictorial, and text) each have unique superiority effects on attention to advertisements, which are on par with many commonly held ideas in marketing practice. And towards the end, “Does In‐Store Marketing Work?” (249), which  covers  the  topic of  interplay between  in‐store  and out‐of‐store  factors on  consumer attention to and evaluation of brands displayed on supermarket shelves. In future lines of research, it would be interesting to continue deepening in the OMSAMC cluster analysis, the dynamization of synergies in OMSAMC scientific collaboration networks, the increase of  the  research  in OMSAMC by  alternative metrics,  the  study  of opinion mining  and  sentiment analysis  in brand monitoring, political management, or sectorial analysis, as well as  the study of OMSAMC distribution resources and industrial collaborations by countries and organizations. 4. Materials and Methods   We  gathered  research  publications  indexed  in  Web  of  Science  (WoS)  on  OMSAMC  for  a significantly  large  timespan  of  20  years  (1998–2018),  which  almost  covers  the  whole  period  of beginning and development of OMSAMC research. We downloaded data for publications (article, book chapter, proceeding or review) on OMSAMC written in English. Table 6 illustrates the query, the selected  inclusion and exclusion criteria used, and  the  indexes, timespan and date of  the data downloaded. We  obtained  a  total  of  845  papers  as  a  result  of  the  query. Keywords  and  terms associated with opinion mining, sentiment analysis and emotion understanding were utilized in the subject search in blend with derived terms of advertising/marketing. Table 6. Details of dataset. Indexes  Timespan  Search  Results  Date Web of Science Core Collection: SCI‐EXPANDED, SSCI, A&HCI, CPCI‐S, CPCISSH, BKCI‐S, BKCI‐SSH, ESCI. 1998–2018 ((((((TS = (((“Sentiment Analysis”) OR (“Sentiment of Images”) OR (“Sentiment Classification”) OR (“Opinion Mining”) OR (“Opinion Classification”) OR (“Image Sentiment”) OR (“Image Emotion”) OR (“Image Processing”) OR (“Image Recognition”) OR (“Mining sentiment”) OR (“Visual Content”) OR (“Visual Attention”) OR (“Object Recognition”) OR (“Object Detection”) OR (“Image Classification”) OR (“Affect Analysis”) OR (“Affective Computing”)) AND (Advert* OR “Marketing”)))))))) AND LANGUAGES: (English) AND DOCUMENT TYPES: (Article OR Book Chapter OR Proceedings Paper OR Review) 845  03.12.2019 The  methodology  followed  the  science  mapping  analysis  approach  (Chen  2017),  a  generic process of domain analysis and scientometric visualization  (Egghe 1994). The scope of a scientific mapping study can be a research field, a scientific discipline or a thematic area related to specific investigation  issues  (Katz and Hicks 1997; Moya‐Anegón et al. 2013). The  study presents  several components, a selection of highlighted scientific works, a set of scientometric and visual mapping analytical  tools,  some  indicators and metrics  that can highlight potentially  significant  trends and patterns,  and  theories  of  scientific  change  that  can  lead  the  interpretation  and  exploration  of visualized dynamic patterns  and  intellectual  structures  (Ahlgren  et  al.  2013; Beaver  2001; De  las Heras‐Pedrosa et al. 2018; Glänzel 2001). All the publications were assessed in terms of following aspects: distribution of languages and publication year, distribution of countries, co‐authorship relations among countries, distribution of Soc. Sci. 2020, 9, 23  20  of  22  journals and subject categories, distribution of author keywords, distribution analysis of authors and institutions, authorship pattern analysis and co‐authorship relations among authors. All analyses and data visualizations referring to the document type, language, journal, country, institutes and author were performed using:    VOSviewer, a software tool for constructing and visualizing bibliometric networks (including individual publications, researchers, journals); being those constructed based on co‐authorship relations, co‐citation, bibliographic coupling, citation and co‐occurrence networks of important terms extracted from a body of scientific literature (Van Eck and Waltman 2010).    CitNetExplorer,  a  software  tool  for visualizing  and  analyzing  citation networks of  scientific publications by allowing us  to  identify clusters of closely  related publications  (Van Eck and Waltman 2014) .  InCites|Clarivate  Analytics  is  a  bibliometric  analysis  tool  that  gathers  all  the  scientific production of an institution included in the Web of Science from 1981 to the present. The tool allows  to analyze  the productivity of an  institution, compare  the performance of  researchers with other scientists in the world as well as determine emerging trends in research. Author Contributions: Conceptualization, Pablo Sánchez‐Núñez; Data curation, Pablo Sánchez‐Núñez; Formal analysis,  Pablo  Sánchez‐Núñez;  Funding  acquisition, Carlos de  las Heras‐Pedrosa  and  José  Ignacio  Peláez; Investigation, Pablo Sánchez‐Núñez; Methodology, Pablo Sánchez‐Núñez; Project administration, Carlos de las Heras‐Pedrosa  and  José  Ignacio  Peláez;  Resources,  Carlos  de  las  Heras‐Pedrosa  and  José  Ignacio  Peláez; Software, Carlos de las Heras‐Pedrosa and José Ignacio Peláez; Supervision, Carlos de las Heras‐Pedrosa and José  Ignacio  Peláez; Validation, Carlos  de  las Heras‐Pedrosa  and  José  Ignacio  Peláez; Visualization,  Pablo Sánchez‐Núñez; Writing – original draft, Pablo Sánchez‐Núñez; Writing – review & editing, Carlos de las Heras‐Pedrosa and José Ignacio Peláez. All authors have read and agreed to the published version of the manuscript. Funding: The research was  funded by Programa Operativo FEDER Andalucía 2014‐2020, grant number “La reputación de  las organizaciones en una sociedad digital. Elaboración de una Plataforma  Inteligente para  la Localización,  Identificación  y  Clasificación  de  Influenciadores  en  los  Medios  Sociales  Digitales  (UMA18‐FEDERJA‐148)” and The APC was funded by the same research grant. Conflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results. References Ahlgren, P., Persson, O., & Tijssen, R. (2013). Geographical distance in bibliometric relations within epistemic communities. Scientometrics, 95(2), 771–784. https://doi.org/10.1007/s11192‐012‐0819‐1 Álvarez‐Bornstein, B., Morillo, F., & Bordons, M.  (2017). Funding acknowledgments  in  the Web of Science: completeness  and  accuracy  of  collected  data.  Scientometrics,  112(3),  1793–1812. https://doi.org/10.1007/s11192‐017‐2453‐4 Baraybar‐Fernández, A., Baños‐González, M., Barquero‐Pérez, Ó., Goya‐Esteban, R., & De‐la‐Morena‐Gómez, A.  (2017).  Evaluation  of  Emotional  Responses  to  Television  Advertising  through  Neuromarketing. Comunicar, 25(52), 19–28. https://doi.org/10.3916/C52‐2017‐02 Baron, A. S., Zaltman, G., & Olson, J. (2017). Barriers to advancing the science and practice of marketing. Journal of Marketing Management, 33(11–12), 893–908. https://doi.org/10.1080/0267257X.2017.1323839 Beaver,  D.  D.  B.  (2001).  Reflections  on  scientific  collaboration  (and  its  study):  Past,  present,  and  future. Scientometrics, 52(3), 365–377. https://doi.org/10.1023/A:1014254214337 Bericat,  E.  (2016).  The  sociology  of  emotions:  Four  decades  of  progress.  Current  Sociology,  64(3),  491–513. https://doi.org/10.1177/0011392115588355 Chen, C. (2017). Science Mapping: A Systematic Review of the Literature. Journal of Data and Information Science, 2(2), 1–40. https://doi.org/10.1515/jdis‐2017‐0006 Soc. Sci. 2020, 9, 23  21  of  22  de las Heras‐Pedrosa, C., Martel‐Casado, T., & Jambrino‐Maldonado, C. (2018). Análisis de las redes académicas y tendencias científicas de la comunicación en las universidades españolas. Prisma Social, (22), 229–246. https://doi.org/http://doi.org/10.5281/zenodo.2603078 Eck, N. J. van, & Waltman, L. (2009). How to normalize cooccurrence data? An analysis of some well‐known similarity measures. Journal of the American Society for Information Science and Technology, 60(8), 1635–1651. https://doi.org/10.1002/asi.21075 Egghe,  L.  (1994).  Little  science,  big  science...  and  beyond.  Scientometrics,  30(2–3),  389–392. https://doi.org/10.1007/BF02018109 Fonseca, B. de P. F. e, Sampaio, R. B., Fonseca, M. V. de A., & Zicker, F. (2016). Co‐authorship network analysis in  health  research:  method  and  potential  use.  Health  Research  Policy  and  Systems,  14(1),  34. https://doi.org/10.1186/s12961‐016‐0104‐5 Glänzel, W.  (2001). National  characteristics  in  international  scientific  co‐authorship  relations.  Scientometrics, 51(1), 69–115. https://doi.org/10.1023/A:1010512628145 Jasanoff, S., & Pinch, T. (2019). Springer Handbook of Science and Technology Indicators. (W. Glänzel, H. F. Moed, U. Schmoch, & M. Thelwall, Eds.). Cham: Springer  International Publishing. https://doi.org/10.1007/978‐3‐030‐02511‐3 Kang, D., & Park, Y. (2014). Review‐based measurement of customer satisfaction in mobile service: Sentiment analysis  and  VIKOR  approach.  Expert  Systems  with  Applications,  41(4),  1041–1050. https://doi.org/10.1016/j.eswa.2013.07.101 Katz,  J.  S.,  &  Hicks,  D.  (1997).  How  much  is  a  collaboration  worth?  A  calibrated  bibliometric  model. Scientometrics, 40(3), 541–554. https://doi.org/10.1007/BF02459299 Kennedy, H. (2012). Perspectives on Sentiment Analysis. Journal of Broadcasting & Electronic Media, 56(4), 435–450. https://doi.org/10.1080/08838151.2012.732141 Liu, W., & Ji, R. (2018). Examining the Role of Online Reviews in Chinese Online Group Buying Context: The Moderating  Effect  of  Promotional  Marketing.  Social  Sciences,  7(8),  141. https://doi.org/10.3390/socsci7080141 Luukkonen,  T.,  Persson,  O.,  &  Sivertsen,  G.  (1992).  Understanding  Patterns  of  International  Scientific Collaboration.  Science,  Technology,  &  Human  Values,  17(1),  101–126. https://doi.org/10.1177/016224399201700106 Mäntylä, M. V., Graziotin, D., & Kuutila, M. (2018). The evolution of sentiment analysis—A review of research topics,  venues,  and  top  cited  papers.  Computer  Science  Review,  27,  16–32. https://doi.org/10.1016/j.cosrev.2017.10.002 Martínez‐López, F. J., Merigó, J. M., Valenzuela‐Fernández, L., & Nicolás, C. (2018). Fifty years of the European Journal  of  Marketing:  a  bibliometric  analysis.  European  Journal  of  Marketing,  52(1–2),  439–468. https://doi.org/10.1108/EJM‐11‐2017‐0853 Mostafa, M. M. (2013). More than words: Social networks’ text mining for consumer brand sentiments. Expert Systems with Applications, 40(10), 4241–4251. https://doi.org/10.1016/j.eswa.2013.01.019 Moya‐Anegón, F., Guerrero‐Bote, V. P., Bornmann, L., & Moed, H. F. (2013). The research guarantors of scientific papers  and  the  output  counting:  a  promising  new  approach.  Scientometrics,  97(2),  421–434. https://doi.org/10.1007/s11192‐013‐1046‐0 Newman,  M.  E.  J.  (2000).  The  structure  of  scientific  collaboration  networks,  2000. https://doi.org/https://doi.org/10.1073/pnas.98.2.404 Pang, B., & Lee, L.  (2008). Opinion Mining  and  Sentiment Analysis. Foundations  and Trends®  in  Information Soc. Sci. 2020, 9, 23  22  of  22  Retrieval, 2(1–2), 1–135. https://doi.org/10.1561/1500000011 Peláez,  J.  I.,  Martínez,  E.  A., &  Vargas,  L. G.  (2019).  Products  and  services  valuation  through  unsolicited information from social media. Soft Computing, 3. https://doi.org/10.1007/s00500‐019‐04005‐3 Peláez, José I., Martínez, E. A., & Vargas, L. G. (2019). Decision making in social media with consistent data. Knowledge‐Based Systems, 172, 33–41. https://doi.org/10.1016/j.knosys.2019.02.009 Peláez, Jose Ignacio, Cabrera, F. E., & Vargas, L. G. (2018). Estimating the importance of consumer purchasing criteria  in  digital  ecosystems.  Knowledge‐Based  Systems,  162(March),  252–264. https://doi.org/10.1016/j.knosys.2018.07.023 Piryani, R., Madhavi, D., & Singh, V. K. (2017). Analytical mapping of opinion mining and sentiment analysis research  during  2000–2015.  Information  Processing  &  Management,  53(1),  122–150. https://doi.org/10.1016/j.ipm.2016.07.001 Prabowo, R., & Thelwall, M. (2009). Sentiment analysis: A combined approach. Journal of Informetrics, 3(2), 143–157. https://doi.org/10.1016/j.joi.2009.01.003 Saaty, T. L., & Vargas, L. G. (2012). Models, Methods, Concepts & Applications of the Analytic Hierarchy Process (Vol. 175). Boston, MA: Springer US. https://doi.org/10.1007/978‐1‐4614‐3597‐6 Sebastian, V.  (2014). New Directions  in Understanding  the Decision‐making  Process: Neuroeconomics  and Neuromarketing.  Procedia  ‐  Social  and  Behavioral  Sciences,  127,  758–762. https://doi.org/10.1016/j.sbspro.2014.03.350 Thelwall, M., Haustein, S., & Larivie, V. (2013). Do Altmetrics Work ? Twitter and Ten Other Social Web Services, 8(5), 1–7. https://doi.org/https://doi.org/10.1371/journal.pone.0064841 United  Nations.  (2015).  UNESCO  Science  Report:  towards  2030.  Retrieved  from https://en.unesco.org/unescosciencereport van  Eck,  N.  J., &  Waltman,  L.  (2010).  Software  survey: VOSviewer,  a  computer  program  for  bibliometric mapping. Scientometrics, 84(2), 523–538. https://doi.org/10.1007/s11192‐009‐0146‐3 van Eck, N. J., & Waltman, L. (2014). CitNetExplorer: A new software tool for analyzing and visualizing citation networks. Journal of Informetrics, 8(4), 802–823. https://doi.org/10.1016/j.joi.2014.07.006 Wereda, W., & Woźniak, J. (2019). Building Relationships with Customer 4.0 in the Era of Marketing 4.0: The Case  Study  of  Innovative  Enterprises  in  Poland.  Social  Sciences,  8(6),  177. https://doi.org/10.3390/socsci8060177   © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).   ",
      "id": 7653684,
      "identifiers": [
        {
          "identifier": "oai:e-archivo.uc3m.es:10016/39404",
          "type": "OAI_ID"
        },
        {
          "identifier": "10.3390/socsci9030023",
          "type": "DOI"
        },
        {
          "identifier": "oai:mdpi.com:/2076-0760/9/3/23/",
          "type": "OAI_ID"
        },
        {
          "identifier": "597637488",
          "type": "CORE_ID"
        },
        {
          "identifier": "540307207",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:dnet:riuma_______::d2a79740937b8612c995e531113c6ce1",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:rabida.uhu.es:10272/23932",
          "type": "OAI_ID"
        },
        {
          "identifier": "486309680",
          "type": "CORE_ID"
        },
        {
          "identifier": "10.3390/socsci9030023.",
          "type": "DOI"
        },
        {
          "identifier": "323326686",
          "type": "CORE_ID"
        },
        {
          "identifier": "618762668",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:riuma.uma.es:10630/19377",
          "type": "OAI_ID"
        },
        {
          "identifier": "288162334",
          "type": "CORE_ID"
        }
      ],
      "title": "Opinion mining and sentiment analysis in marketing communications: a science mapping analysis in Web of Science (1998–2018)",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:dnet:riuma_______::d2a79740937b8612c995e531113c6ce1",
        "oai:mdpi.com:/2076-0760/9/3/23/",
        "oai:e-archivo.uc3m.es:10016/39404",
        "oai:rabida.uhu.es:10272/23932",
        "oai:riuma.uma.es:10630/19377"
      ],
      "publishedDate": "2020-01-01T00:00:00",
      "publisher": "'MDPI AG'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://rabida.uhu.es/dspace/bitstream/10272/23932/2/socsci-09-00023-v2.pdf",
        "https://riuma.uma.es/xmlui/bitstream/10630/19377/1/Opinion%20Mining%20and%20Sentiment%20Analysis%20in%20Marketing%20Communications-%20A%20Science%20Mapping%20Analysis%20in%20Web%20of%20Science%20%281998%e2%80%932018%29.pdf",
        "http://dx.doi.org/10.3390/socsci9030023",
        "https://e-archivo.uc3m.es/bitstream/10016/39404/2/opinion_SS_2020.pdf"
      ],
      "updatedDate": "2024-10-04T15:13:56",
      "yearPublished": 2020,
      "journals": [
        {
          "title": "Social Sciences",
          "identifiers": [
            "issn:2076-0760",
            "2076-0760"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/288162334.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/288162334"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/288162334/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/288162334/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/7653684"
        }
      ]
    },
    {
      "acceptedDate": "2016-07-14T00:00:00",
      "arxivId": null,
      "authors": [
        {
          "name": "Al-Ayyoub, Mahmoud"
        },
        {
          "name": "Al-Smadi, Mohammad"
        },
        {
          "name": "Androutsopoulos, Ion"
        },
        {
          "name": "Apidianaki, Marianna"
        },
        {
          "name": "Bel Rafecas, Núria"
        },
        {
          "name": "Bel, Nuria"
        },
        {
          "name": "De Clercq, Orphée"
        },
        {
          "name": "Eryiğit, Gülşen"
        },
        {
          "name": "Galanis, Dimitris"
        },
        {
          "name": "Hoste, Véronique"
        },
        {
          "name": "Jiménez-Zafra, Salud María"
        },
        {
          "name": "Kotelnikov, Evgeniy"
        },
        {
          "name": "Loukachevitch, Natalia"
        },
        {
          "name": "Manandhar, Suresh"
        },
        {
          "name": "Papageorgiou, Haris"
        },
        {
          "name": "Pontiki, Maria"
        },
        {
          "name": "Qin, Bing"
        },
        {
          "name": "Tannier, Xavier"
        },
        {
          "name": "Zhao, Yanyan"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur (LIMSI) ; Université Paris-Sud - Paris 11 (UP11)-Sorbonne Université - UFR d'Ingénierie (UFR 919) ; Sorbonne Université (SU)-Sorbonne Université (SU)-Université Paris-Saclay-Centre National de la Recherche Scientifique (CNRS)-Université Paris Saclay (COmUE)",
        "Maria",
        "Steven"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/275785886",
        "https://api.core.ac.uk/v3/outputs/74663966",
        "https://api.core.ac.uk/v3/outputs/661702534",
        "https://api.core.ac.uk/v3/outputs/207821446",
        "https://api.core.ac.uk/v3/outputs/627173036",
        "https://api.core.ac.uk/v3/outputs/160076403"
      ],
      "createdDate": "2017-02-14T19:08:33",
      "dataProviders": [
        {
          "id": 320,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/320",
          "logo": "https://api.core.ac.uk/data-providers/320/logo"
        },
        {
          "id": 4786,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/4786",
          "logo": "https://api.core.ac.uk/data-providers/4786/logo"
        },
        {
          "id": 1493,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/1493",
          "logo": "https://api.core.ac.uk/data-providers/1493/logo"
        },
        {
          "id": 2314,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2314",
          "logo": "https://api.core.ac.uk/data-providers/2314/logo"
        },
        {
          "id": 2973,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2973",
          "logo": "https://api.core.ac.uk/data-providers/2973/logo"
        }
      ],
      "depositedDate": "2016-01-01T00:00:00",
      "abstract": "International audienceThis paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams",
      "documentType": "research",
      "doi": "10.18653/v1/s16-1002",
      "downloadUrl": "https://core.ac.uk/download/74663966.pdf",
      "fieldOfStudy": null,
      "fullText": "Proceedings of SemEval-2016, pages 19–30,\nSan Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics\nSemEval-2016 Task 5: Aspect Based Sentiment Analysis\nMaria Pontiki∗1, Dimitrios Galanis1, Haris Papageorgiou1, Ion Androutsopoulos1,2,\nSuresh Manandhar3,Mohammad AL-Smadi4,Mahmoud Al-Ayyoub4, Yanyan Zhao5,\nBing Qin5, Orphée De Clercq6, Véronique Hoste6,Marianna Apidianaki7,\nXavier Tannier7, Natalia Loukachevitch8, Evgeny Kotelnikov9,\nNuria Bel10, Salud María Jiménez-Zafra11, Gülşen Eryiğit12\n1Institute for Language and Speech Processing, Athena R.C., Athens, Greece,\n2Dept. of Informatics, Athens University of Economics and Business, Greece,\n3Dept. of Computer Science, University of York, UK,\n4Computer Science Dept., Jordan University of Science and Technology Irbid, Jordan,\n5Harbin Institute of Technology, Harbin, Heilongjiang, P.R. China,\n6LT3, Ghent University, Ghent, Belgium,\n7LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay, Orsay, France,\n8Lomonosov Moscow State University, Moscow, Russian Federation,\n9Vyatka State University, Kirov, Russian Federation,\n10Universitat Pompeu Fabra, Barcelona, Spain,\n11Dept. of Computer Science, Universidad de Jaén, Spain,\n12Dept. of Computer Engineering, Istanbul Technical University, Turkey\nAbstract\nThis paper describes the SemEval 2016 shared\ntask on Aspect Based Sentiment Analysis\n(ABSA), a continuation of the respective tasks\nof 2014 and 2015. In its third year, the task\nprovided 19 training and 20 testing datasets\nfor 8 languages and 7 domains, as well as a\ncommon evaluation procedure. From these\ndatasets, 25 were for sentence-level and 14 for\ntext-level ABSA; the latter was introduced for\nthe first time as a subtask in SemEval. The task\nattracted 245 submissions from 29 teams.\n1 Introduction\nMany consumers use the Web to share their experi-\nences about products, services or travel destinations\n(Yoo and Gretzel, 2008). Online opinionated texts\n(e.g., reviews, tweets) are important for consumer\ndecision making (Chevalier and Mayzlin, 2006) and\nconstitute a source of valuable customer feedback\nthat can help companies to measure satisfaction and\nimprove their products or services. In this setting,\nAspect Based Sentiment Analysis (ABSA) - i.e.,\nmining opinions from text about specific entities and\ntheir aspects (Liu, 2012) - can provide valuable in-\nsights to both consumers and businesses. An ABSA\n∗*Corresponding author: mpontiki@ilsp.gr.\nFigure 1: Table summarizing the average sentiment for each\naspect of an entity.\nmethod can analyze large amounts of unstructured\ntexts and extract (coarse- or fine-grained) informa-\ntion not included in the user ratings that are available\nin some review sites (e.g., Fig. 1).\nSentiment Analysis (SA) touches every aspect\n(e.g., entity recognition, coreference resolution,\nnegation handling) of Natural Language Processing\n(Liu, 2012) and as Cambria et al. (2013) mention “it\nrequires a deep understanding of the explicit and im-\nplicit, regular and irregular, and syntactic and se-\nmantic language rules”. Within the last few years\nseveral SA-related shared tasks have been organized\nin the context of workshops and conferences focus-\n19\ning on somewhat different research problems (Seki\net al., 2007; Seki et al., 2008; Seki et al., 2010;\nMitchell, 2013; Nakov et al., 2013; Rosenthal et al.,\n2014; Pontiki et al., 2014; Rosenthal et al., 2015;\nGhosh et al., 2015; Pontiki et al., 2015; Moham-\nmad et al., 2016; Recupero and Cambria, 2014; Rup-\npenhofer et al., 2014; Loukachevitch et al., 2015).\nSuch competitions provide training datasets and the\nopportunity for direct comparison of different ap-\nproaches on common test sets.\nCurrently, most of the available SA-related\ndatasets, whether released in the context of shared\ntasks or not (Socher et al., 2013; Ganu et al., 2009),\nare monolingual and usually focus on English texts.\nMultilingual datasets (Klinger and Cimiano, 2014;\nJiménez-Zafra et al., 2015) provide additional ben-\nefits enabling the development and testing of cross-\nlingual methods (Lambert, 2015). Following this di-\nrection, this year the SemEval ABSA task provided\ndatasets in a variety of languages.\nABSAwas introduced as a shared task for the first\ntime in the context of SemEval in 2014; SemEval-\n2014 Task 41 (SE-ABSA14) provided datasets of\nEnglish reviews annotated at the sentence level with\naspect terms (e.g., “mouse”, “pizza”) and their po-\nlarity for the laptop and restaurant domains, as well\nas coarser aspect categories (e.g., “food”) and their\npolarity only for restaurants (Pontiki et al., 2014).\nSemEval-2015 Task 122 (SE-ABSA15) built upon\nSE-ABSA14 and consolidated its subtasks into a\nunified framework in which all the identified con-\nstituents of the expressed opinions (i.e., aspects,\nopinion target expressions and sentiment polarities)\nmeet a set of guidelines and are linked to each other\nwithin sentence-level tuples (Pontiki et al., 2015).\nThese tuples are important since they indicate the\npart of text within which a specific opinion is ex-\npressed. However, a user might also be interested\nin the overall rating of the text towards a partic-\nular aspect. Such ratings can be used to estimate\nthe mean sentiment per aspect from multiple re-\nviews (McAuley et al., 2012). Therefore, in addition\nto sentence-level annotations, SE-ABSA163 accom-\nmodated also text-level ABSA annotations and pro-\nvided the respective training and testing data. Fur-\n1http://alt.qcri.org/semeval2014/task4/\n2http://alt.qcri.org/semeval2015/task12/\n3http://alt.qcri.org/semeval2016/task5/\nthermore, the SE-ABSA15 annotation framework\nwas extended to new domains and applied to lan-\nguages other than English (Arabic, Chinese, Dutch,\nFrench, Russian, Spanish, and Turkish).\nThe remainder of this paper is organized as fol-\nlows: the task set-up is described in Section 2. Sec-\ntion 3 provides information about the datasets and\nthe annotation process, while Section 4 presents the\nevaluation measures and the baselines. General in-\nformation about participation in the task is provided\nin Section 5. The evaluation scores of the participat-\ning systems are presented and discussed in Section\n6. The paper concludes with an overall assessment\nof the task.\n2 Task Description\nThe SE-ABSA16 task consisted of the following\nsubtasks and slots. Participants were free to choose\nthe subtasks, slots, domains and languages they\nwished to participate in.\nSubtask 1 (SB1): Sentence-level ABSA. Given\nan opinionated text about a target entity, identify all\nthe opinion tuples with the following types (tuple\nslots) of information:\n• Slot1: Aspect Category. Identification of the\nentity E and attribute A pairs towards which\nan opinion is expressed in a given sentence. E\nand A should be chosen from predefined in-\nventories4 of entity types (e.g., “restaurant”,\n“food”) and attribute labels (e.g., “price”,\n“quality”).\n• Slot2: Opinion Target Expression (OTE).\nExtraction of the linguistic expression used in\nthe given text to refer to the reviewed entity E\nof each E#A pair. The OTE is defined by its\nstarting and ending offsets. When there is no\nexplicit mention of the entity, the slot takes the\nvalue “null”. The identification of Slot2 val-\nues was required only in the restaurants, hotels,\nmuseums and telecommunications domains.\n• Slot3: Sentiment Polarity. Each identified\nE#A pair has to be assigned one of the following\npolarity labels: “positive”, “negative”, “neu-\ntral” (mildly positive or mildly negative).\n4The full inventories of the aspect category labels for each\ndomain are provided in Appendix A.\n20\nLang. Domain Subtask Train Test#Texts #Sent. #Tuples #Texts #Sent. #Tuples\nEN REST SB1 350 2000 2507 90 676 859\nEN REST SB2 335 1950 1435 90 676 404\nEN LAPT SB1 450 2500 2909 80 808 801\nEN LAPT SB2 395 2375 2082 80 808 545\nAR HOTE SB1 1839 4802 10509 452 1227 2604\nAR HOTE SB2 1839 4802 8757 452 1227 2158\nCH PHNS SB1 140 6330 1333 60 3191 529\nCH CAME SB1 140 5784 1259 60 2256 481\nDU REST SB1 300 1711 1860 100 575 613\nDU REST SB2 300 1711 1247 100 575 381\nDU PHNS SB1 200 1389 1393 70 308 396\nFR REST SB1 335 1733 2530 120 696 954\nFR MUSE SB3 - - - 162 686 891\nRU REST SB1 302 3490 4022 103 1209 1300\nRU REST SB2 302 3490 1545 103 1209 500\nES REST SB1 627 2070 2720 286 881 1072\nES REST SB2 627 2070 2121 286 881 881\nTU REST SB1 300 1104 1535 39 144 159\nTU REST SB2 300 1104 972 39 144 108\nTU TELC SB1 - 3000 4082 - 310 336\nTable 1: Datasets provided for SE-ABSA16.\nAn example of opinion tuples with Slot1-3 values\nfrom the restaurants domain is shown below: “Their\nsake list was extensive, but we were looking for Pur-\nple Haze, which wasn’t listed but made for us upon\nrequest!” → {cat: “drinks#style_options”, trg:\n“sake list”, fr: “6”, to: “15”, pol: “positive”}, {cat:\n“service#general”, trg: “null”, fr: “0”, to: “0”,\npol: “positive”}. The variable cat indicates the as-\npect category (Slot1), pol the polarity (Slot3), and\ntrg the ote (Slot2); fr, to are the starting/ending\noffsets of ote.\nSubtask 2 (SB2): Text-level ABSA. Given a cus-\ntomer review about a target entity, the goal was\nto identify a set of {cat, pol} tuples that summa-\nrize the opinions expressed in the review. cat can\nbe assigned the same values as in SB1 (E#A tu-\nple), while pol can be set to “positive”, “negative”,\n“neutral”, or “conflict”. For example, for the re-\nview text “The So called laptop Runs to Slow and\nI hate it! Do not buy it! It is the worst laptop\never ”, a system should return the following opin-\nion tuples: {cat: “laptop#general”, pol: “nega-\ntive”}, {cat: “laptop#operation_performance”,\npol: “negative”} .\nSubtask 3 (SB3): Out-of-domain ABSA. In SB3\nparticipants had the opportunity to test their systems\nin domains for which no training data was made\navailable; the domains remained unknown until the\nstart of the evaluation period. Test data for SB3 were\nprovided only for the museums domain in French.\n3 Datasets\n3.1 Data Collection and Annotation\nA total of 39 datasets were provided in the context of\nthe SE-ABSA16 task; 19 for training and 20 for test-\ning. The texts were from 7 domains and 8 languages;\nEnglish (en), Arabic (ar), Chinese (ch), Dutch (du),\nFrench (fr), Russian (ru), Spanish (es) and Turk-\nish (tu). The datasets for the domains of restaurants\n(rest), laptops (lapt), mobile phones (phns), digital\ncameras (came), hotels (hote) and museums (muse)\nconsist of customer reviews, whilst the telecommu-\nnication domain (telc) data consists of tweets. A to-\ntal of 70790 manually annotated ABSA tuples were\nprovided for training and testing; 47654 sentence-\nlevel annotations (SB1) in 8 languages for 7 do-\nmains, and 23136 text-level annotations (SB2) in 6\nlanguages for 3 domains. Table 1 provides more in-\nformation on the distribution of texts, sentences and\nannotated tuples per dataset.\nThe rest, hote, and lapt datasets were annotated\n21\nat the sentence-level (SB1) following the respective\nannotation schemas of SE-ABSA15 (Pontiki et al.,\n2015). Below are examples5 of annotated sentences\nfor the aspect category “service#general” in en\n(1), du (2), fr (3), ru (4), es (5), and tu (6) for the\nrest domain and in ar (7) for the hote domain:\n1. Service was slow, but the people were friendly.\n→ {trg: “Service”, pol: “negative”}, {trg:\n“people”, pol: “positive”}\n2. Snelle bediening en vriendelijk personeel moet\nook gemeld worden!! → {trg: “bediening”,\npol: “positive”}, {trg: “personeel”, pol: “posi-\ntive”}\n3. Le service est impeccable, personnel agréable.\n→ {trg: “service” , pol: “positive”}, {trg: “per-\nsonnel”, pol: “positive”}\n4. Про сервис ничего негативного не скажешь-\nбыстро подходят, все улябаются, подходят\nспрашивают, всё ли нравится. → {trg:\n“сервис”, pol: “neutral” }\n5. También la rapidez en el servicio. → {trg: “ser-\nvicio”, pol: “positive” }\n6. Servisi hızlı valesi var. → {trg: “Servisi”, pol:\n“positive”}\n7. .. ﺔﻌﯾﺮﺳ و اﺪﺟ ةﺪﯿﺟ ﺔﻣﺪﺨﻟا → {trg: ”ﺔﻣﺪﺨﻟا“ , pol:\n“positive”}\nThe lapt annotation schema was extended to two\nother domains of consumer electronics, came and\nphns. Examples of annotated sentences in the lapt\n(en), phns (du and ch) and came (ch) domains are\nshown below:\n1. It is extremely portable and easily connects to\nWIFI at the library and elsewhere. → {cat:\n“laptop#portability”, pol: “positive”} , {cat:\n“laptop#connectivity”, pol: “positive”}\n2. Apps starten snel op en werken\nvlot, internet gaat prima. → {cat:\n“software#operation_performance”, pol:\n“positive”}, {cat: “phone#connectivity”,\npol: “positive”}\n5The offsets of the opinion target expressions are omitted.\n3. 当然屏幕这么好→{cat: “display#quality”,\npol: “positive”}\n4. 更 轻 便 的 机 身 也 便 于 携 带。→ {cat:\n“camera# portability”, pol: “positive”}\nIn addition, the SE-ABSA15 framework was ex-\ntended to two new domains for which annotation\nguidelines were compiled: telc for tu and muse for\nfr. Below are two examples:\n1. #Internet kopuyor sürekli :( @turkcell→ {cat:\n“internet#coverage”, trg: “Internet”, pol:\n“positive”}\n2. 5€ pour les étudiants, ça vaut le coup. → {cat:\n“museum#prices”, “null”, “positive”}\nThe text-level (SB2) annotation task was based\non the sentence-level annotations; given a customer\nreview about a target entity (e.g., a restaurant) that\nincluded sentence-level annotations of ABSA tu-\nples, the goal was to identify a set of {cat, pol}\ntuples that summarize the opinions expressed in it.\nThis was not a simple summation/aggregation of the\nsentence-level annotations since an aspect may be\ndiscussed with different sentiment in different parts\nof the review. In such cases the dominant sentiment\nhad to be identified. In case of conflicting opin-\nions where the dominant sentiment was not clear, the\n”conflict” label was assigned. In addition, each re-\nview was assigned an overall sentiment label about\nthe target entity (e.g., “restaurant#general”,\n“laptop#general”), even if it was not included in\nthe sentence-level annotations.\n3.2 Annotation Process\nAll datasets for each language were prepared by one\nor more research groups as shown in Table 2. The\nen, du, fr, ru and es datasets were annotated using\nbrat (Stenetorp et al., 2012), a web-based annota-\ntion tool, which was configured appropriately for the\nneeds of the task. The tu datasets were annotated us-\ning a customized version of turksent (Eryigit et al.,\n2013), a sentiment annotation tool for social media.\nFor the ar and the ch data in-house tools6 were used.\n6The ar annotation tool was developed by the technical\nteam of the Advanced Arabic Text Mining group at Jordan Uni-\nversity of Science and Technology. The ch tool was developed\nby the Research Center for Social Computing and Information\nRetrieval at Harbin Institute of Technology.\n22\nLang. Research team(s)\nEnglish Institute for Language and Speech Processing, Athena R.C., Athens, GreeceDept. of Informatics, Athens University of Economics and Business, Greece\nArabic Computer Science Dept., Jordan University of Science and Technology Irbid, Jordan\nChinese Harbin Institute of Technology, Harbin, Heilongjiang, P.R. China\nDutch LT3, Ghent University, Ghent, Belgium\nFrench LIMSI, CNRS, Univ. Paris-Sud, Université Paris-Saclay, Orsay, France\nRussian Lomonosov Moscow State University, Moscow, Russian FederationVyatka State University, Kirov, Russian Federation\nSpanish Universitat Pompeu Fabra, Barcelona, SpainSINAI, Universidad de Jaén, Spain\nTurkish Dept. of Computer Engineering, Istanbul Technical University, Turkey\nTurkcell Global Bilgi, Turkey\nTable 2: Research teams that contributed to the creation of the datasets for each language.\nBelow are some further details about the annotation\nprocess for each language.\nEnglish. The SE-ABSA15 (Pontiki et al., 2015)\ntraining and test datasets (with some minor correc-\ntions) were merged and provided for training (rest\nand lapt domains). New data was collected and an-\nnotated from scratch for testing. In a first phase, the\nrest test data was annotated by an experienced7 lin-\nguist (annotator A), and the lapt data by 5 under-\ngraduate computer science students. The resulting\nannotations for both domains were then inspected\nand corrected (if needed) by a second expert linguist,\none of the task organizers (annotator B). Borderline\ncases were resolved collaboratively by annotators A\nand B.\nArabic. The hote dataset was annotated in re-\npeated cycles. In a first phase, the data was annotated\nby three native Arabic speakers, all with a computer\nscience background; then the output was validated\nby a senior researcher, one of the task organizers. If\nneeded (e.g. when inconsistencies were found) they\nwere given back to the annotators.\nChinese. The datasets presented by Zhao et al.\n(2015) were re-annotated by three native Chinese\nspeakers according to the SE-ABSA16 annotation\nschema and were provided for training and testing\n(phns and came domains).\nDutch. The rest and phns datasets (De Clercq\nand Hoste, 2016) were initially annotated by a\ntrained linguist, native speaker of Dutch. Then,\nthe output was verified by another Dutch linguist\nand disagreements were resolved between them. Fi-\n7Also annotator for SE-ABSA14 and 15.\nnally, the task organizers inspected collaboratively\nall the annotated data and corrections were made\nwhen needed.\nFrench. The train (rest) and test (rest, muse)\ndatasets were annotated from scratch by a linguist,\nnative speaker of French. When the annotator was\nnot confident, a decision was made collaboratively\nwith the organizers. In a second phase, the task or-\nganizers checked all the annotations for mistakes and\ninconsistencies and corrected them, when necessary.\nFor more information on the French datasets consult\nApidianaki et al. (2016).\nRussian. The rest datasets of the SentiRuEval-\n2015 task (Loukachevitch et al., 2015) were auto-\nmatically converted to the SE-ABSA16 annotation\nschema; then a linguist, native speaker of Russian,\nchecked them and added missing information. Fi-\nnally, the datasets were inspected by a second lin-\nguist annotator (also native speaker of Russian) for\nmistakes and inconsistencies, which were resolved\nalong with one of the task organizers.\nSpanish. Initially, 50 texts (134 sentences) from\nthe whole available data were annotated by 4 annota-\ntors. The inter-anotator agreement (IAA) in terms of\nF-1 was 91% for the identification of OTE, 88% for\nthe aspect category detection (E#A pair), and 80%\nfor opinion tuples extraction (E#A, OTE, polarity).\nProvided that the IAA was substantially high for all\nslots, the rest of the data was divided into 4 parts and\neach one was annotated by a different native Spanish\nspeakers (2 linguists and 2 software engineers). Sub-\nsequently, the resulting annotations were validated\nand corrected (if needed) by the task organizers.\n23\nTurkish. The telc dataset was based on the data\nused in (Yıldırım et al., 2015), while the rest dataset\nwas created from scratch. Both datasets were anno-\ntated simultaneously by two linguists. Then, one of\nthe organizers validated/inspected the resulting an-\nnotations and corrected them when needed.\n3.3 Datasets Format and Availability\nSimilarly to SE-ABSA14 and SE-ABSA15, the\ndatasets8 of SE-ABSA16 were provided in an XML\nformat and they are available under specific license\nterms through META-SHARE9, a repository de-\nvoted to the sharing and dissemination of language\nresources (Piperidis, 2012).\n4 Evaluation Measures and Baselines\nThe evaluation ran in two phases. In the first phase\n(Phase A), the participants were asked to return\nseparately the aspect categories (Slot1), the OTEs\n(Slot2), and the {Slot1, Slot2} tuples for SB1. For\nSB2 the respective text-level categories had to be\nidentified. In the second phase (Phase B), the gold\nannotations for the test sets of Phase A were pro-\nvided and participants had to return the respective\nsentiment polarity values (Slot3). Similarly to SE-\nABSA15, F-1 scores were calculated for Slot1, Slot2\nand {Slot1, Slot2} tuples, by comparing the anno-\ntations that a system returned to the gold annota-\ntions (using micro-averaging). For Slot1 evaluation,\nduplicate occurrences of categories were ignored in\nboth SB1 and SB2. For Slot2, the calculation for\neach sentence considered only distinct targets and\ndiscarded “null” targets, since they do not corre-\nspond to explicit mentions. To evaluate sentiment\npolarity classification (Slot3) in Phase B, we calcu-\nlated the accuracy of each system, defined as the\nnumber of correctly predicted polarity labels of the\n(gold) aspect categories, divided by the total num-\nber of the gold aspect categories. Furthermore, we\nimplemented and provided baselines for all slots of\nSB1 and SB2. In particular, the SE-ABSA15 base-\nlines that were implemented for the English language\n8The data are available at: http://metashare.ilsp.\ngr:8080/repository/search/?q=semeval+2016\n9META-SHARE (http://www.metashare.org/) was\nimplemented in the framework of the META-NET Network of\nExcellence (http://www.meta-net.eu/).\n(Pontiki et al., 2015), were adapted for the other lan-\nguages by using appropriate stopword lists and to-\nkenization functions. The baselines are briefly dis-\ncussed below:\nSB1-Slot1: For category (E#A) extraction, a Sup-\nport Vector Machine (SVM) with a linear kernel is\ntrained. In particular, n unigram features are ex-\ntracted from the respective sentence of each tuple\nthat is encountered in the training data. The cat-\negory value (e.g., “service#general”) of the tu-\nple is used as the correct label of the feature vec-\ntor. Similarly, for each test sentence s, a fea-\nture vector is built and the trained SVM is used\nto predict the probabilities of assigning each possi-\nble category to s (e.g., {“service#general”, 0.2},\n{“restaurant#general”, 0.4}. Then, a thresh-\nold10 t is used to decide which of the categories will\nbe assigned11 to s. As features, we use the 1,000\nmost frequent unigrams of the training data exclud-\ning stopwords.\nSB1-Slot2: The baseline uses the training\nreviews to create for each category c (e.g.,\n“service#general”) a list of OTEs (e.g.,\n“service#general” → {“staff”, “waiter”}).\nThese are extracted from the (training) opinion\ntuples whose category value is c . Then, given a test\nsentence s and an assigned category c, the baseline\nfinds in s the first occurrence of each OTE of c’s\nlist. The OTE slot is filled with the first of the target\noccurrences found in s. If no target occurrences are\nfound, the slot is assigned the value “null”.\nSB1-Slot3: For polarity prediction we trained a\nSVM classifier with a linear kernel. Again, as in\nSlot1, n unigram features are extracted from the re-\nspective sentence of each tuple of the training data.\nIn addition, an integer-valued feature12 that indicates\nthe category of the tuple is used. The correct label\nfor the extracted training feature vector is the corre-\nsponding polarity value (e.g., “positive”). Then, for\neach tuple {category, OTE} of a test sentence s, a\nfeature vector is built and classified using the trained\nSVM.\nSB2-Slot1: The sentence-level tuples returned by\nthe SB1 baseline are copied to the text level and du-\nplicates are removed.\n10The threshold t was set to 0.2 for all datasets.\n11We use the –b 1 option of LibSVM to obtain probabilities.\n12Each E#A pair has been assigned a distinct integer value.\n24\nLang./ Slot1 Slot2 {Slot1,Slot2} Slot3\nDom. F-1 F-1 F-1 Acc.\nEN/ NLANG./U/73.031 NLANG./U/72.34 NLANG./U/52.607 XRCE/C/88.126\nREST NileT./U/72.886 AUEB-./U/70.441 XRCE/C/48.891 IIT-T./U/86.729\nBUTkn./U/72.396 UWB/U/67.089 NLANG./C/45.724 NileT./U/85.448\nAUEB-./U/71.537 UWB/C/66.906 TGB/C/43.081* IHS-R./U/83.935\nBUTkn./C/71.494 GTI/U/66.553 bunji/U/41.113 ECNU/U/83.586\nSYSU/U/70.869 Senti./C/66.545 UWB/C/41.108 AUEB-./U/83.236\nXRCE/C/68.701 bunji/U/64.882 UWB/U/41.088 INSIG./U/82.072\nUWB/U/68.203 NLANG./C/63.861 DMIS/U/39.796 UWB/C/81.839\nINSIG./U/68.108 DMIS/C/63.495 DMIS/C/38.976 UWB/U/81.723\nESI/U/67.979 XRCE/C/61.98 basel./C/37.795 SeemGo/C/81.141\nUWB/C/67.817 AUEB-./C/61.552 IHS-R./U/35.608 bunji/U/81.024\nGTI/U/67.714 UWate./U/57.067 IHS-R./U/34.864 TGB/C/80.908*\nAUEB-./C/67.35 KnowC./U/56.816* UWate./U/34.536 ECNU/C/80.559\nNLANG./C/65.563 TGB/C/55.054* SeemGo/U/30.667 UWate./U/80.326\nLeeHu./C/65.455 BUAP/U/50.253 BUAP/U/18.428 INSIG./C/80.21\nTGB/C/63.919* basel./C/44.071 DMIS/C/79.977\nIIT-T./U/63.051 IHS-R./U/43.808 DMIS/U/79.627\nDMIS/U/62.583 IIT-T./U/42.603 IHS-R./U/78.696\nDMIS/C/61.754 SeemGo/U/34.332 Senti./U/78.114\nIIT-T./C/61.227 LeeHu./C/78.114\nbunji/U/60.145 basel./C/76.484\nbasel./C/59.928 bunji/C/76.251\nUFAL/U/59.3 SeemGo/U/72.992\nINSIG./C/58.303 AKTSKI/U/71.711\nIHS-R./U/55.034 COMMI./C/70.547\nIHS-R./U/53.149 SNLP/U/69.965\nSeemGo/U/50.737 GTI/U/69.965\nUWate./U/49.73 CENNL./C/63.912\nCENNL./C/40.578 BUAP/U/60.885\nBUAP/U/37.29\nTable 3: English REST results for SB1.\nSB2-Slot3: For each text-level aspect category c\nthe baseline traverses the predicted sentence-level\ntuples of the same category returned by the respec-\ntive SB1 baseline and counts the polarity labels (pos-\nitive, negative, neutral). Finally, the polarity label\nwith the highest frequency is assigned to the text-\nlevel category c. If there are no sentence-level tuples\nfor the same c, the polarity label is determined based\non all tuples regardless of c.\nThe baseline systems and evaluation scripts are\nimplemented in Java and are available for down-\nload from the SE-ABSA16 website13. The LibSVM\npackage14 (Chang and Lin, 2011) is used for SVM\ntraining and prediction. The scores of the baselines\n13http://alt.qcri.org/semeval2016/task5/index.\nphp?id=data-and-tools\n14http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nin the test datasets are presented in Section 6 along\nwith the system scores.\n5 Participation\nThe task attracted in total 245 submissions from 29\nteams. The majority of the submissions (216 runs)\nwere for SB1. The newly introduced SB2 attracted\n29 submissions from 5 teams in 2 languages (en and\nsp). Most of the submissions (168) were runs for\nthe rest domain. This was expected, mainly for two\nreasons; first, the rest classification schema is less\nfine-grained (complex) compared to the other do-\nmains (e.g., lapt). Secondly, this domain was sup-\nported for 6 languages enabling also multilingual or\nlanguage-agnostic approaches. The remaining sub-\nmissions were distributed as follows: 54 in lapt, 12\nin phns, 7 in came and 4 in hote.\n25\nLang./ Slot1 Slot2 {Slot1,Slot2} Slot3\nDom. F-1 F-1 F-1 Acc.\nES/ GTI/U/70.588 GTI/C/68.515 TGB/C/41.219* IIT-T./U/83.582\nREST GTI/C/70.027 GTI/U/68.387 basel./C/36.379 TGB/C/82.09*\nTGB/C/63.551* IIT-T./U/64.338 UWB/C/81.343\nUWB/C/61.968 TGB/C/55.764* INSIG./C/79.571\nINSIG./C/61.37 basel./C/51.914 basel./C/77.799\nIIT-T./U/59.899\nIIT-T./C/59.062\nUFAL/U/58.81\nbasel./C/54.686\nFR/ XRCE/C/61.207 IIT-T./U/66.667 XRCE/C/47.721 XRCE/C/78.826\nREST IIT-T./U/57.875 XRCE/C/65.316 basel./C/33.017 UWB/C/75.262\nIIT-T./C/57.033 basel./C/45.455 UWB/C/74.319\nINSIG./C/53.592 INSIG./C/73.166\nbasel./C/52.609 IIT-T./U/72.222\nUFAL/U/49.928 basel./C/67.4\nRU/ UFAL/U/64.825 basel./C/49.308 basel./C/39.441 MayAnd/U/77.923\nREST INSIG./C/62.802 Danii./U/33.472 Danii./U/22.591 INSIG./C/75.077\nIIT-T./C/62.689 Danii./C/30.618 Danii./C/22.107 IIT-T./U/73.615\nIIT-T./C/58.196 Danii./U/73.308\nbasel./C/55.882 Danii./C/72.538\nDanii./C/39.601 basel./C/71\nDanii./U/38.692\nDU/ TGB/C/60.153* IIT-T./U/56.986 TGB/C/45.167* TGB/C/77.814*\nREST INSIG./C/56 TGB/C/51.775* basel./C/30.916 IIT-T./U/76.998\nIIT-T./U/55.247 basel./C/50.64 INSIG./C/75.041\nIIT-T./C/54.98 basel./C/69.331\nUFAL/U/53.876\nbasel./C/42.816\nTU/ UFAL/U/61.029 basel./C/41.86 basel./C/28.152 IIT-T./U/84.277\nREST basel./C/58.896 INSIG./C/74.214\nIIT-T./U/56.627 basel./C/72.327\nIIT-T./C/55.728\nINSIG./C/49.123\nAR/ INSIG./C/52.114 basel./C/30.978 basel./C/18.806 INSIG./C/82.719\nHOTE UFAL/U/47.302 IIT-T./U/81.72\nbasel./C/40.336 basel./C/76.421\nTable 4: REST and HOTE results for SB1.\nAn interesting observation is that, unlike SE-\nABSA15, Slot1 (aspect category detection) attracted\nsignificantly more submissions than Slot2 (OTE ex-\ntraction); this may indicate a shift towards concept-\nlevel approaches. Regarding participation per lan-\nguage, the majority of the submissions (156/245)\nwere for en; see more information in Table 5. Most\nteams (20) submitted results only for one language\n(18 for en and 2 for ru). Of the remaining teams,\n3 submitted results for 2 languages, 5 teams submit-\nted results for 3-7 languages, while only one team\nparticipated in all languages.\n6 Evaluation Results\nThe evaluation results are presented in Tables 3\n(SB1: rest-en), 4 (SB1: rest-es, fr, ru, du, tu\n& hote-ar), 6 (SB1: lapt, came, phns), and 7\n(SB2)15. Each participating team was allowed to\nsubmit up to two runs per slot and domain in each\nphase; one constrained (C), where only the provided\ntraining data could be used, and one unconstrained\n(U), where other resources (e.g., publicly available\n15No submissions were made for sb3-muse-fr & sb1-telc-\ntu.\n26\nLanguage Teams Submissions\nEnglish 27 156\nArabic 3 4\nChinese 3 14\nDutch 4 16\nFrench 5 13\nRussian 5 15\nSpanish 6 21\nTurkish 3 6\nAll 29 245\nTable 5: Number of participating teams and submitted runs per\nlanguage.\nlexica) and additional data of any kind could be used\nfor training. In the latter case, the teams had to re-\nport the resources used. Delayed submissions (i.e.,\nruns submitted after the deadline and the release of\nthe gold annotations) are marked with “*”.\nAs revealed by the results, in both SB1 and SB2\nthe majority of the systems surpassed the baseline\nby a small or large margin and, as expected, the un-\nconstrained systems achieved better results than the\nconstrained ones. In SB1, the teams with the high-\nest scores for Slot1 and Slot2 achieved similar F-1\nscores (see Table 3) in most cases (e.g., en/rest,\nes/rest, du/rest, fr/rest), which shows that the\ntwo slots have a similar level of difficulty. How-\never, as expected, the {Slot1, Slot2} scores were sig-\nnificantly lower since the linking of the target ex-\npressions to the corresponding aspects is also re-\nquired. The highest scores in SB1 for all slots (Slot1,\nSlot2, {Slot1, Slot2}, Slot3) were achieved in the\nen/rest; this is probably due to the high participation\nand to the lower complexity of the rest annotation\nschema compared to the other domains. If we com-\npare the results for SB1 and SB2, we notice that the\nSB2 scores for Slot1 are significantly higher (e.g.,\nen/lapt, en/rest, es/rest) even though the respec-\ntive annotations are for the same (or almost the same)\nset of texts. This is due to the fact that it is easier to\nidentify whether a whole text discusses an aspect c\nthan finding all the sentences in the text discussing\nc . On the other hand, for Slot3, the SB2 scores are\nlower (e.g., en/rest, es/rest, ru/rest, en/lapt) than\nthe respective SB1 scores. This is mainly because an\naspect may be discussed at different points in a text\nand often with different sentiment. In such cases a\nsystem has to identify the dominant sentiment, which\nLang./ Slot1 Slot3\nDom. F-1 Acc.\nEN/ NLANG./U/51.937 IIT-T./U/82.772\nLAPT AUEB-./U/49.105 INSIG./U/78.402\nSYSU/U/49.076 ECNU/U/78.152\nBUTkn./U/48.396 IHS-R./U/77.903\nUWB/C/47.891 NileT./U/77.403\nBUTkn./C/47.527 AUEB-./U/76.904\nUWB/U/47.258 LeeHu./C/75.905\nNileT./U/47.196 Senti./U/74.282\nNLANG./C/46.728 INSIG./C/74.282\nINSIG./U/45.863 UWB/C/73.783\nAUEB-./C/45.629 UWB/U/73.783\nIIT-T./U/43.913 SeemGo/C/72.16\nLeeHu./C/43.754 UWate./U/71.286\nIIT-T./C/42.609 bunji/C/70.287\nSeemGo/U/41.499 bunji/U/70.162\nINSIG./C/41.458 ECNU/C/70.037\nbunji/U/39.586 basel./C/70.037\nIHS-R./U/39.024 COMMI./C/67.541\nbasel./C/37.481 GTI/U/67.291\nUFAL/U/26.984 BUAP/U/62.797\nCENNL./C/26.908 CENNL./C/59.925\nBUAP/U/26.787 SeemGo/U/40.824\nCH/ UWB/C/36.345 SeemGo/C/80.457\nCAME INSIG./C/25.581 INSIG./C/78.17\nbasel./C/18.434 UWB/C/77.755\nSeemGo/U/17.757 basel./C/74.428\nSeemGo/U/73.181\nCH/ UWB/C/22.548 SeemGo/C/73.346\nPHNS basel./C/17.03 INSIG./C/72.401\nINSIG./C/16.286 UWB/C/72.023\nSeemGo/U/10.43 basel./C/70.132\nSeemGo/U/65.406\nDU/ INSIG./C/45.551 INSIG./C/83.333\nPHNS IIT-T./U/45.443 IIT-T./U/82.576\nIIT-T./C/45.047 basel./C/80.808\nbasel./C/33.55\nTable 6: LAPT, CAME, and PHNS results for SB1.\nusually is not trivial.\n7 Conclusions\nIn its third year, the SemEval ABSA task provided\n19 training and 20 testing datasets, from 7 domains\nand 8 languages, attracting 245 submissions from\n29 teams. The use of the same annotation guide-\nlines for domains addressed in different languages\ngives the opportunity to experiment also with cross-\nlingual or language-agnostic approaches. In addi-\ntion, SE-ABSA16 included for the first time a text-\n27\nLang./ Slot1 Slot3\nDom. F-1 Acc.\nEN/ GTI/U/83.995 UWB/U/81.931\nREST UWB/C/80.965 ECNU/U/81.436\nUWB/U/80.163 UWB/C/80.941\nbunji/U/79.777 ECNU/C/78.713\nbasel./C/78.711 basel./C/74.257\nSYSU/U/68.841 bunji/U/70.545\nSYSU/U/68.841 bunji/C/66.584\nGTI/U/64.109\nES/ GTI/C/84.192 UWB/C/77.185\nREST GTI/U/84.044 basel./C/74.548\nbasel./C/74.548\nUWB/C/73.657\nRU/ basel./C/84.792 basel./C/70.6\nREST\nRU/ basel./C/84.792 basel./C/70.6\nREST\nDU/ basel./C/70.323 basel./C/73.228\nREST\nTU/ basel./C/72.642 basel./C/57.407\nREST\nAR/ basel./C/42.757 basel./C/73.216\nHOTE\nEN/ UWB/C/60.45 ECNU/U/75.046\nLAPT UWB/U/59.721 UWB/U/75.046\nbunji/U/54.723 UWB/C/74.495\nbasel./C/52.685 basel./C/73.028\nSYSU/U/48.889 ECNU/C/67.523\nSYSU/U/48.889 bunji/C/62.202\nbunji/U/60\nGTI/U/58.349\nTable 7: Results for SB2.\nlevel subtask. Future work will address the cre-\nation of datasets in more languages and domains and\nthe enrichment of the annotation schemas with other\ntypes of SA-related information like topics, events\nand figures of speech (e.g., irony, metaphor).\nAcknowledgments\nThe authors are grateful to all the annotators and\ncontributors for their valuable support to the task:\nKonstantina Papanikolaou, Juli Bakagianni, Omar\nQwasmeh, Nesreen Alqasem, AreenMagableh, Saja\nAlzoubi, Bashar Talafha, Zekui Li, Binbin Li,\nShengqiu Li, Aaron Gevaert, Els Lefever, Cécile\nRichart, Pavel Blinov, Maria Shatalova, M. Teresa\nMartín-Valdivia, Pilar Santolaria, Fatih Samet Çetin,\nEzgi Yıldırım, Can Özbey, Leonidas Valavanis,\nStavros Giorgis, Dionysios Xenos, Panos Theodor-\nakakos, and Apostolos Rousas. The work described\nin this paper is partially funded by the projects EOX\nGR07/3712 and “Research Programs for Excellence\n2014-2016 / CitySense-ATHENA R.I.C.”. The Ara-\nbic track was partially supported by the Jordan Uni-\nversity of Science and Technology, Research Grant\nNumber: 20150164. The Dutch track has been\npartly funded by the PARIS project (IWT-SBO-\nNr. 110067). The French track was partially sup-\nported by the French National Research Agency un-\nder project ANR-12-CORD-0015/TransRead. The\nRussian track was partially supported by the Rus-\nsian Foundation for Basic Research (RFBR) accord-\ning to the research projects No. 14-07-00682a, 16-\n07-00342a, and No. 16-37-00311mol_a. The Span-\nish track has been partially supported by a grant\nfrom theMinisterio de Educación, Cultura y Deporte\n(MECD - scholarship FPU014/00983) and REDES\nproject (TIN2015-65136-C2-1-R) from the Minis-\nterio de Economía y Competitividad. The Turk-\nish track was partially supported by TUBITAK-\nTEYDEB (The Scientific and Technological Re-\nsearch Council of Turkey – Technology and Inno-\nvation Funding Programs Directorate) project (grant\nnumber: 3140671).\nReferences\nMarianna Apidianaki, Xavier Tannier, and Cécile\nRichart. 2016. A Dataset for Aspect-Based Sentiment\nAnalysis in French. In Proceedings of the Interna-\ntional Conference on Language Resources and Eval-\nuation.\nErik Cambria, Björn W. Schuller, Yunqing Xia, and\nCatherine Havasi. 2013. New Avenues in Opinion\nMining and Sentiment Analysis. IEEE Intelligent Sys-\ntems, 28(2):15–21.\nChih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:\nA library for support vector machines. ACM TIST,\n2(3):27.\nJudith Chevalier and Dina Mayzlin. 2006. The effect\nof word of mouth on sales: Online book reviews. J.\nMarketing Res, pages 345–354.\nOrphée De Clercq and Véronique Hoste. 2016. Rude\nwaiter but mouthwatering pastries! An exploratory\nstudy into Dutch Aspect-Based Sentiment Analysis. In\nProceedings of the 10th International Conference on\nLanguage Resources and Evaluation.\nGülsen Eryigit, Fatih Samet Cetin, Meltem Yanık, Turk-\ncell Global Bilgi, Tanel Temel, and Ilyas Ciçekli.\n28\n2013. TURKSENT: A Sentiment Annotation Tool for\nSocial Media. In Proceedings of the 7th Linguistic\nAnnotation Workshop and Interoperability with Dis-\ncourse.\nGayatree Ganu, Noemie Elhadad, and Amélie Marian.\n2009. Beyond the Stars: Improving Rating Predic-\ntions using Review Text Content. In Proceedings of\nWebDB.\nAniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso,\nEkaterina Shutova, John Barnden, and Antonio Reyes.\n2015. SemEval-2015 Task 11: Sentiment Analysis of\nFigurative Language in Twitter. In Proceedings of the\n9th International Workshop on Semantic Evaluation,\nDenver, Colorado.\nSaludM. Jiménez-Zafra, Giacomo Berardi, Andrea Esuli,\nDiego Marcheggiani, María Teresa Martín-Valdivia,\nand Alejandro Moreo Fernández. 2015. A Multi-\nlingual Annotated Dataset for Aspect-Oriented Opin-\nion Mining. In Proceedings of Empirical Methods in\nNatural Language Processing, pages 2533–2538.\nRoman Klinger and Philipp Cimiano. 2014. The USAGE\nReview Corpus for Fine Grained Multi Lingual Opin-\nion Analysis. In Proceedings of the Ninth Interna-\ntional Conference on Language Resources and Eval-\nuation, Reykjavik, Iceland.\nPatrik Lambert. 2015. Aspect-Level Cross-lingual Sen-\ntiment Classification with Constrained SMT. In Pro-\nceedings of the Association for Computational Linguis-\ntics and the International Joint Conference on Natu-\nral Language Processing, 2015, Beijing, China, pages\n781–787.\nBing Liu. 2012. Sentiment Analysis and Opinion Mining.\nSynthesis Lectures onHuman Language Technologies.\nMorgan & Claypool Publishers.\nNatalia Loukachevitch, Pavel Blinov, Evgeny Kotel-\nnikov, Yulia Rubtsova, Vladimir Ivanov, and Elena\nTutubalina. 2015. SentiRuEval: Testing Object-\noriented Sentiment Analysis Systems in Russian. In\nProceedings of International Conference Dialog.\nJulian J. McAuley, Jure Leskovec, and Dan Jurafsky.\n2012. Learning Attitudes and Attributes from Multi-\naspect Reviews. In 12th IEEE International Confer-\nence on Data Mining, ICDM 2012, Brussels, Belgium,\nDecember 10-13, 2012, pages 1020–1025.\nMargaret Mitchell. 2013. Overview of the TAC2013\nKnowledge Base Population Evaluation English Senti-\nment Slot Filling. In Proceedings of the 6th Text Anal-\nysis Conference, Gaithersburg, Maryland, USA.\nSaif M Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, andColin Cherry. 2016. SemEval-\n2016 Task 6: Detecting Stance in Tweets. In Proceed-\nings of the 10th International Workshop on Semantic\nEvaluation, San Diego, California.\nPreslav Nakov, Sara Rosenthal, Zornitsa Kozareva,\nVeselin Stoyanov, Alan Ritter, and Theresa Wilson.\n2013. SemEval-2013 Task 2: Sentiment Analysis in\nTwitter. In Proceedings of the 7th International Work-\nshop on Semantic Evaluation, Atlanta, Georgia.\nStelios Piperidis. 2012. The META-SHARE Language\nResources Sharing Infrastructure: Principles, Chal-\nlenges, Solutions. In Proceedings of the 8th Interna-\ntional Conference on Language Resources and Evalu-\nation.\nMaria Pontiki, Dimitrios Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. SemEval-2014 Task 4: Aspect\nBased Sentiment Analysis. In Proceedings of the\n8th International Workshop on Semantic Evaluation,\nDublin, Ireland.\nMaria Pontiki, Dimitrios Galanis, Harris Papageorgiou,\nSuresh Manandhar, and Ion Androutsopoulos. 2015.\nSemEval-2015 Task 12: Aspect Based Sentiment\nAnalysis. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation, Denver, Colorado.\nDiego Reforgiato Recupero and Erik Cambria. 2014.\nEswc’14 challenge on concept-level sentiment analy-\nsis. In Semantic Web Evaluation Challenge - SemWe-\nbEval 2014 at ESWC 2014, Anissaras, Crete, Greece,\npages 3–20.\nSara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin\nStoyanov. 2014. SemEval-2014 Task 4: Sentiment\nAnalysis in Twitter. In Proceedings of the 8th Interna-\ntional Workshop on Semantic Evaluation, Dublin, Ire-\nland.\nSara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,\nSaif M Mohammad, Alan Ritter, and Veselin Stoy-\nanov. 2015. SemEval-2015 Task 10: Sentiment Anal-\nysis in Twitter. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation, Denver, Colorado.\nJosef Ruppenhofer, Roman Klinger, Julia Maria Struß,\nJonathan Sonntag, and Michael Wiegand. 2014. IG-\nGSA Shared Tasks on German Sentiment Analysis\n(GESTALT). In Workshop Proceedings of the 12th\nEdition of the KONVENS Conference, pages 164–173.\nYohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi\nChen, Noriko Kando, and Chin-Yew Lin. 2007.\nOverview of Opinion Analysis Pilot Task at NTCIR-\n6. In Proceedings of the 6th NTCIR Workshop, Tokyo,\nJapan.\nYohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,\nHsin-Hsi Chen, and Noriko Kando. 2008. Overview\nof Multilingual Opinion Analysis Task at NTCIR-7.\nIn Proceedings of the 7th NTCIR Workshop, Tokyo,\nJapan.\nYohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and\nNoriko Kando. 2010. Overview of Multilingual Opin-\nion Analysis Task at NTCIR-8: A Step Toward Cross\n29\nLingual Opinion Analysis. In Proceedings of the 8th\nNTCIR Workshop, Tokyo, Japan, pages 209–220.\nRichard Socher, Alex Perelygin, JeanWu, Jason Chuang,\nChristopher D. Manning, Andrew Y. Ng, and Christo-\npher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of Empirical Methods in Natural\nLanguage Processing, pages 1631–1642, Stroudsburg,\nPA.\nPontus Stenetorp, Sampo Pyysalo, Goran Topic, Tomoko\nOhta, Sophia Ananiadou, and Jun’ichi Tsujii. 2012.\nBRAT: AWeb-based Tool for NLP-Assisted Text An-\nnotation. In Proceedings of the European Chapter of\nthe Association for Computational Linguistics, pages\n102–107.\nEzgi Yıldırım, Fatih Samet Çetin, Gülşen Eryiğit,\nand Tanel Temel. 2015. The impact of nlp\non turkish sentiment analysis. TÜRKİYE BİLİŞİM\nVAKFI BİLGİSAYARBİLİMLERİ veMÜHENDİSLİĞİ\nDERGİSİ, 7(1 (Basılı 8).\nKyung Hyan Yoo and Ulrike Gretzel. 2008. What Moti-\nvates Consumers to Write Online Travel Reviews? J.\nof IT & Tourism, 10(4):283–295.\nYanyan Zhao, Bing Qin, and Ting Liu. 2015. Creating a\nFine-Grained Corpus for Chinese Sentiment Analysis.\nIEEE Intelligent Systems, 30(1):36–43.\nAppendix A. Aspect inventories for all domains\nEntity Labels\nlaptop, display, keyboard, mouse, motherboard,\ncpu, fans_cooling, ports, memory, power_supply\noptical_drives, battery, graphics, hard_disk,\nmultimedia_devices, hardware, software, os,\nwarranty, shipping, support, company\nAttribute Labels\ngeneral, price, quality, design_features,\noperation_performance, usability, portability,\nconnectivity, miscellaneous\nTable 8: Laptops.\nEntity Labels\nphone, display, keyboard, cpu, ports, memory,\npower_supply, hard_disk, multimedia_devices,\nbattery, hardware, software, os, warranty,\nshipping, support, company\nAttribute Labels\nSame as in Laptops (Table 8) with the exception of\nportability that is included in the design_features\nlabel and does not apply as a separate attribute type.\nTable 9: Mobile Phones.\nEntity Labels\ncamera, display, keyboard, cpu, ports, memory,\npower_supply, battery, multimedia_devices,\nhardware, software, os, warranty, shipping,\nsupport, company, lens, photo, focus\nAttribute Labels\nSame as in Laptops (Table 8).\nTable 10: Digital Cameras.\nEntity Labels\nrestaurant, food, drinks, ambience,\nservice, location\nAttribute Labels\ngeneral, prices, quality,\nstyle_options, miscellaneous\nTable 11: Restaurants.\nEntity Labels\nhotel, rooms, facilities, room_amenities,\nservice, location, food_drinks\nAttribute Labels\ngeneral, price, comfort, cleanliness, quality,\nstyle_options, design_features, miscellaneous\nTable 12: Hotels.\nEntity Labels\ntelecom operator, device, internet,\ncustomer_services, application_service\nAttribute Labels\ngeneral, price_invoice, coverage,\nspeed, campaign_advertisement, miscellaneous\nTable 13: Telecommunications.\nEntity Labels\nmuseum, collections, facilities, service,\ntour_guiding, location\nAttribute Labels\ngeneral, prices, comfort, activities,\narchitecture, interest, set up, miscellaneous\nTable 14: Museums.\n30\n",
      "id": 37978496,
      "identifiers": [
        {
          "identifier": "10.18653/v1/s16-1002",
          "type": "DOI"
        },
        {
          "identifier": "661702534",
          "type": "CORE_ID"
        },
        {
          "identifier": "627173036",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:archive.ugent.be:8131987",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:repositori.upf.edu:10230/68305",
          "type": "OAI_ID"
        },
        {
          "identifier": "74663966",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:repositori-api.upf.edu:10230/68305",
          "type": "OAI_ID"
        },
        {
          "identifier": "oai:hal:hal-02407165v1",
          "type": "OAI_ID"
        },
        {
          "identifier": "160076403",
          "type": "CORE_ID"
        },
        {
          "identifier": "207821446",
          "type": "CORE_ID"
        },
        {
          "identifier": "oai:hal:hal-01838537v1",
          "type": "OAI_ID"
        },
        {
          "identifier": "624090453",
          "type": "CORE_ID"
        },
        {
          "identifier": "275785886",
          "type": "CORE_ID"
        },
        {
          "identifier": "627655992",
          "type": "CORE_ID"
        }
      ],
      "title": "SemEval-2016 task 5 : aspect based sentiment analysis",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:archive.ugent.be:8131987",
        "oai:hal:hal-02407165v1",
        "oai:repositori.upf.edu:10230/68305",
        "oai:repositori-api.upf.edu:10230/68305",
        "oai:hal:hal-01838537v1"
      ],
      "publishedDate": "2016-01-01T00:00:00",
      "publisher": "'Association for Computational Linguistics (ACL)'",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://hal.science/hal-01838537v1/document",
        "https://biblio.ugent.be/publication/8131987/file/8502211.pdf",
        "https://hal.science/hal-02407165v1/document"
      ],
      "updatedDate": "2025-07-13T18:00:08",
      "yearPublished": 2016,
      "journals": [
        {
          "title": null,
          "identifiers": [
            "issn:0736-587X",
            "0736-587x"
          ]
        }
      ],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/74663966.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/74663966"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/74663966/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/74663966/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/37978496"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": "cs/0409058",
      "authors": [
        {
          "name": "Lee, Lillian"
        },
        {
          "name": "Pang, Bo"
        }
      ],
      "citationCount": 0,
      "contributors": [
        "The Pennsylvania State University CiteSeerX Archives"
      ],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/24682462"
      ],
      "createdDate": "2012-04-13T14:21:49",
      "dataProviders": [
        {
          "id": 144,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/144",
          "logo": "https://api.core.ac.uk/data-providers/144/logo"
        },
        {
          "id": 145,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/145",
          "logo": "https://api.core.ac.uk/data-providers/145/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "Sentiment analysis seeks to identify the viewpoint(s) underlying a text span;\nan example application is classifying a movie review as \"thumbs up\" or \"thumbs\ndown\". To determine this sentiment polarity, we propose a novel\nmachine-learning method that applies text-categorization techniques to just the\nsubjective portions of the document. Extracting these portions can be\nimplemented using efficient techniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence contextual constraints.Comment: Data available at\n  http://www.cs.cornell.edu/people/pabo/movie-review-data",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "http://arxiv.org/abs/cs/0409058",
      "fieldOfStudy": null,
      "fullText": "ar\nX\niv\n:c\ns/0\n40\n90\n58\nv1\n  [\ncs\n.C\nL]\n  2\n9 S\nep\n 20\n04\nA Sentimental Education: Sentiment Analysis Using Subjectivity\nSummarization Based on Minimum Cuts\nBo Pang and Lillian Lee\nDepartment of Computer Science\nCornell University\nIthaca, NY 14853-7501\n{pabo,llee}@cs.cornell.edu\nAbstract\nSentiment analysis seeks to identify the view-\npoint(s) underlying a text span; an example appli-\ncation is classifying a movie review as “thumbs up”\nor “thumbs down”. To determine this sentiment po-\nlarity, we propose a novel machine-learning method\nthat applies text-categorization techniques to just\nthe subjective portions of the document. Extracting\nthese portions can be implemented using efficient\ntechniques for finding minimum cuts in graphs; this\ngreatly facilitates incorporation of cross-sentence\ncontextual constraints.\nPublication info: Proceedings of the ACL, 2004.\n1 Introduction\nThe computational treatment of opinion, sentiment,\nand subjectivity has recently attracted a great deal\nof attention (see references), in part because of its\npotential applications. For instance, information-\nextraction and question-answering systems could\nflag statements and queries regarding opinions\nrather than facts (Cardie et al., 2003). Also, it has\nproven useful for companies, recommender sys-\ntems, and editorial sites to create summaries of peo-\nple’s experiences and opinions that consist of sub-\njective expressions extracted from reviews (as is\ncommonly done in movie ads) or even just a re-\nview’s polarity — positive (“thumbs up”) or neg-\native (“thumbs down”).\nDocument polarity classification poses a sig-\nnificant challenge to data-driven methods, re-\nsisting traditional text-categorization techniques\n(Pang, Lee, and Vaithyanathan, 2002). Previous ap-\nproaches focused on selecting indicative lexical fea-\ntures (e.g., the word “good”), classifying a docu-\nment according to the number of such features that\noccur anywhere within it. In contrast, we propose\nthe following process: (1) label the sentences in\nthe document as either subjective or objective, dis-\ncarding the latter; and then (2) apply a standard\nmachine-learning classifier to the resulting extract.\nThis can prevent the polarity classifier from consid-\nering irrelevant or even potentially misleading text:\nfor example, although the sentence “The protagonist\ntries to protect her good name” contains the word\n“good”, it tells us nothing about the author’s opin-\nion and in fact could well be embedded in a negative\nmovie review. Also, as mentioned above, subjectiv-\nity extracts can be provided to users as a summary\nof the sentiment-oriented content of the document.\nOur results show that the subjectivity extracts\nwe create accurately represent the sentiment in-\nformation of the originating documents in a much\nmore compact form: depending on choice of down-\nstream polarity classifier, we can achieve highly sta-\ntistically significant improvement (from 82.8% to\n86.4%) or maintain the same level of performance\nfor the polarity classification task while retaining\nonly 60% of the reviews’ words. Also, we ex-\nplore extraction methods based on a minimum cut\nformulation, which provides an efficient, intuitive,\nand effective means for integrating inter-sentence-\nlevel contextual information with traditional bag-of-\nwords features.\n2 Method\n2.1 Architecture\nOne can consider document-level polarity classi-\nfication to be just a special (more difficult) case\nof text categorization with sentiment- rather than\ntopic-based categories. Hence, standard machine-\nlearning classification techniques, such as sup-\nport vector machines (SVMs), can be applied to\nthe entire documents themselves, as was done by\nPang, Lee, and Vaithyanathan (2002). We refer to\nsuch classification techniques as default polarity\nclassifiers.\nHowever, as noted above, we may be able to im-\nprove polarity classification by removing objective\nsentences (such as plot summaries in a movie re-\nview). We therefore propose, as depicted in Figure\n1, to first employ a subjectivity detector that deter-\nmines whether each sentence is subjective or not:\ndiscarding the objective ones creates an extract that\nshould better represent a review’s subjective content\nto a default polarity classifier.\ns1\ns2\ns3\ns4\ns_n\n+/-\ns4\ns1\nsu\nbje\ncti\nvit\ny\nde\nte\nct\nor\nyes\nno\nno\nyes\nn-sentence review\nsubjective\nsentence? m-sentence extract\n(m<=n) review?\npositive or negative\nde\nfa\nul\nt\ncl\nas\nsif\nie\nr\npo\nla\nrit\ny\nsubjectivity extraction\nFigure 1: Polarity classification via subjectivity detec-\ntion.\nTo our knowledge, previous work has not\nintegrated sentence-level subjectivity detec-\ntion with document-level sentiment polarity.\nYu and Hatzivassiloglou (2003) provide methods\nfor sentence-level analysis and for determining\nwhether a document is subjective or not, but do not\ncombine these two types of algorithms or consider\ndocument polarity classification. The motivation\nbehind the single-sentence selection method of\nBeineke et al. (2004) is to reveal a document’s\nsentiment polarity, but they do not evaluate the\npolarity-classification accuracy that results.\n2.2 Context and Subjectivity Detection\nAs with document-level polarity classification, we\ncould perform subjectivity detection on individual\nsentences by applying a standard classification algo-\nrithm on each sentence in isolation. However, mod-\neling proximity relationships between sentences\nwould enable us to leverage coherence: text spans\noccurring near each other (within discourse bound-\naries) may share the same subjectivity status, other\nthings being equal (Wiebe, 1994).\nWe would therefore like to supply our algorithms\nwith pair-wise interaction information, e.g., to spec-\nify that two particular sentences should ideally re-\nceive the same subjectivity label but not state which\nlabel this should be. Incorporating such informa-\ntion is somewhat unnatural for classifiers whose\ninput consists simply of individual feature vec-\ntors, such as Naive Bayes or SVMs, precisely be-\ncause such classifiers label each test item in isola-\ntion. One could define synthetic features or fea-\nture vectors to attempt to overcome this obstacle.\nHowever, we propose an alternative that avoids the\nneed for such feature engineering: we use an ef-\nficient and intuitive graph-based formulation rely-\ning on finding minimum cuts. Our approach is in-\nspired by Blum and Chawla (2001), although they\nfocused on similarity between items (the motiva-\ntion being to combine labeled and unlabeled data),\nwhereas we are concerned with physical proximity\nbetween the items to be classified; indeed, in com-\nputer vision, modeling proximity information via\ngraph cuts has led to very effective classification\n(Boykov, Veksler, and Zabih, 1999).\n2.3 Cut-based classification\nFigure 2 shows a worked example of the concepts\nin this section.\nSuppose we have n items x1, . . . , xn to divide\ninto two classes C1 and C2, and we have access to\ntwo types of information:\n• Individual scores indj(xi): non-negative esti-\nmates of each xi’s preference for being in Cj based\non just the features of xi alone; and\n• Association scores assoc(xi, xk): non-negative\nestimates of how important it is that xi and xk be in\nthe same class.1\nWe would like to maximize each item’s “net hap-\npiness”: its individual score for the class it is as-\nsigned to, minus its individual score for the other\nclass. But, we also want to penalize putting tightly-\nassociated items into different classes. Thus, after\nsome algebra, we arrive at the following optimiza-\ntion problem: assign the xis to C1 and C2 so as to\nminimize the partition cost∑\nx∈C1\nind2(x)+\n∑\nx∈C2\nind1(x)+\n∑\nxi∈C1,\nxk∈C2\nassoc(xi, xk).\nThe problem appears intractable, since there are\n2n possible binary partitions of the xi’s. How-\never, suppose we represent the situation in the fol-\nlowing manner. Build an undirected graph G with\nvertices {v1, . . . , vn, s, t}; the last two are, respec-\ntively, the source and sink. Add n edges (s, vi), each\nwith weight ind1(xi), and n edges (vi, t), each with\nweight ind2(xi). Finally, add\n(\nn\n2\n)\nedges (vi, vk),\neach with weight assoc(xi, xk). Then, cuts in G\nare defined as follows:\nDefinition 1 A cut (S, T ) of G is a partition of its\nnodes into sets S = {s} ∪ S′ and T = {t} ∪ T ′,\nwhere s 6∈ S′, t 6∈ T ′. Its cost cost(S, T ) is the sum\n1Asymmetry is allowed, but we used symmetric scores.\n[]\ns t\nY\nM\nN\n2ind (Y) [.2]1ind (Y) [.8]\n2ind (M) [.5]1ind (M) [.5]\n[.1]assoc(Y,N)\n2ind (N) [.9]1ind (N)\nassoc(M,N)\nassoc(Y,M)\n[.2]\n[1.0]\n[.1]\nC1 Individual Association Cost\npenalties penalties\n{Y,M} .2 + .5 + .1 .1 + .2 1.1\n(none) .8 + .5 + .1 0 1.4\n{Y,M,N} .2 + .5 + .9 0 1.6\n{Y} .2 + .5 + .1 1.0 + .1 1.9\n{N} .8 + .5 + .9 .1 + .2 2.5\n{M} .8 + .5 + .1 1.0 + .2 2.6\n{Y,N} .2 + .5 + .9 1.0 + .2 2.8\n{M,N} .8 + .5 + .9 1.0 + .1 3.3\nFigure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to\nbe probabilities. Based on individual scores alone, we would put Y (“yes”) in C1, N (“no”) in C2, and be undecided\nabout M (“maybe”). But the association scores favor cuts that put Y and M in the same class, as shown in the table.\nThus, the minimum cut, indicated by the dashed line, places M together with Y in C1.\nof the weights of all edges crossing from S to T . A\nminimum cut of G is one of minimum cost.\nObserve that every cut corresponds to a partition of\nthe items and has cost equal to the partition cost.\nThus, our optimization problem reduces to finding\nminimum cuts.\nPractical advantages As we have noted, formulat-\ning our subjectivity-detection problem in terms of\ngraphs allows us to model item-specific and pair-\nwise information independently. Note that this is\na very flexible paradigm. For instance, it is per-\nfectly legitimate to use knowledge-rich algorithms\nemploying deep linguistic knowledge about sen-\ntiment indicators to derive the individual scores.\nAnd we could also simultaneously use knowledge-\nlean methods to assign the association scores. In-\nterestingly, Yu and Hatzivassiloglou (2003) com-\npared an individual-preference classifier against a\nrelationship-based method, but didn’t combine the\ntwo; the ability to coordinate such algorithms is\nprecisely one of the strengths of our approach.\nBut a crucial advantage specific to the uti-\nlization of a minimum-cut-based approach is\nthat we can use maximum-flow algorithms with\npolynomial asymptotic running times — and\nnear-linear running times in practice — to ex-\nactly compute the minimum-cost cut(s), despite\nthe apparent intractability of the optimization\nproblem (Cormen, Leiserson, and Rivest, 1990;\nAhuja, Magnanti, and Orlin, 1993).2 In con-\ntrast, other graph-partitioning problems\nthat have been previously used to formu-\n2Code available at http://www.avglab.com/andrew/soft.html.\nlate NLP classification problems3 are NP-\ncomplete (Hatzivassiloglou and McKeown, 1997;\nAgrawal et al., 2003; Joachims, 2003).\n3 Evaluation Framework\nOur experiments involve classifying movie re-\nviews as either positive or negative, an appeal-\ning task for several reasons. First, as mentioned\nin the introduction, providing polarity informa-\ntion about reviews is a useful service: witness\nthe popularity of www.rottentomatoes.com. Sec-\nond, movie reviews are apparently harder to clas-\nsify than reviews of other products (Turney, 2002;\nDave, Lawrence, and Pennock, 2003). Third, the\ncorrect label can be extracted automatically from\nrating information (e.g., number of stars). Our data4\ncontains 1000 positive and 1000 negative reviews\nall written before 2002, with a cap of 20 reviews per\nauthor (312 authors total) per category. We refer to\nthis corpus as the polarity dataset.\nDefault polarity classifiers We tested support vec-\ntor machines (SVMs) and Naive Bayes (NB). Fol-\nlowing Pang et al. (2002), we use unigram-presence\nfeatures: the ith coordinate of a feature vector is\n1 if the corresponding unigram occurs in the input\ntext, 0 otherwise. (For SVMs, the feature vectors\nare length-normalized). Each default document-\nlevel polarity classifier is trained and tested on the\nextracts formed by applying one of the sentence-\nlevel subjectivity detectors to reviews in the polarity\ndataset.\n3Graph-based approaches to general clustering problems\nare too numerous to mention here.\n4Available at www.cs.cornell.edu/people/pabo/movie-\nreview-data/ (review corpus version 2.0).\nSubjectivity dataset To train our detectors, we\nneed a collection of labeled sentences. Riloff and\nWiebe (2003) state that “It is [very hard] to ob-\ntain collections of individual sentences that can be\neasily identified as subjective or objective”; the\npolarity-dataset sentences, for example, have not\nbeen so annotated.5 Fortunately, we were able\nto mine the Web to create a large, automatically-\nlabeled sentence corpus6. To gather subjective\nsentences (or phrases), we collected 5000 movie-\nreview snippets (e.g., “bold, imaginative, and im-\npossible to resist”) from www.rottentomatoes.com.\nTo obtain (mostly) objective data, we took 5000 sen-\ntences from plot summaries available from the In-\nternet Movie Database (www.imdb.com). We only\nselected sentences or snippets at least ten words\nlong and drawn from reviews or plot summaries of\nmovies released post-2001, which prevents overlap\nwith the polarity dataset.\nSubjectivity detectors As noted above, we can use\nour default polarity classifiers as “basic” sentence-\nlevel subjectivity detectors (after retraining on the\nsubjectivity dataset) to produce extracts of the orig-\ninal reviews. We also create a family of cut-based\nsubjectivity detectors; these take as input the set of\nsentences appearing in a single document and de-\ntermine the subjectivity status of all the sentences\nsimultaneously using per-item and pairwise rela-\ntionship information. Specifically, for a given doc-\nument, we use the construction in Section 2.2 to\nbuild a graph wherein the source s and sink t cor-\nrespond to the class of subjective and objective sen-\ntences, respectively, and each internal node vi cor-\nresponds to the document’s ith sentence si. We can\nset the individual scores ind1(si) to PrNBsub (si) and\nind2(si) to 1 − PrNBsub (si), as shown in Figure 3,\nwhere PrNBsub (s) denotes Naive Bayes’ estimate of\nthe probability that sentence s is subjective; or, we\ncan use the weights produced by the SVM classi-\nfier instead.7 If we set all the association scores\nto zero, then the minimum-cut classification of the\n5We therefore could not directly evaluate sentence-\nclassification accuracy on the polarity dataset.\n6Available at www.cs.cornell.edu/people/pabo/movie-\nreview-data/ , sentence corpus version 1.0.\n7We converted SVM output di, which is a signed distance\n(negative=objective) from the separating hyperplane, to non-\nnegative numbers by\nind1(si)\ndef\n=\n{\n1 di > 2;\n(2 + di)/4 −2 ≤ di ≤ 2;\n0 di < −2.\nand ind2(si) = 1 − ind1(si). Note that scaling is employed\nonly for consistency; the algorithm itself does not require prob-\nabilities for individual scores.\nsentences is the same as that of the basic subjectiv-\nity detector. Alternatively, we incorporate the de-\ngree of proximity between pairs of sentences, con-\ntrolled by three parameters. The threshold T spec-\nifies the maximum distance two sentences can be\nseparated by and still be considered proximal. The\nnon-increasing function f(d) specifies how the in-\nfluence of proximal sentences decays with respect to\ndistance d; in our experiments, we tried f(d) = 1,\ne1−d, and 1/d2. The constant c controls the relative\ninfluence of the association scores: a larger c makes\nthe minimum-cut algorithm more loath to put prox-\nimal sentences in different classes. With these in\nhand8, we set (for j > i)\nassoc(si, sj)\ndef\n=\n{\nf(j − i) · c if (j − i) ≤ T ;\n0 otherwise.\n4 Experimental Results\nBelow, we report average accuracies computed by\nten-fold cross-validation over the polarity dataset.\nSection 4.1 examines our basic subjectivity extrac-\ntion algorithms, which are based on individual-\nsentence predictions alone. Section 4.2 evaluates\nthe more sophisticated form of subjectivity extrac-\ntion that incorporates context information via the\nminimum-cut paradigm.\nAs we will see, the use of subjectivity extracts\ncan in the best case provide satisfying improve-\nment in polarity classification, and otherwise can\nat least yield polarity-classification accuracies indis-\ntinguishable from employing the full review. At the\nsame time, the extracts we create are both smaller\non average than the original document and more\neffective as input to a default polarity classifier\nthan the same-length counterparts produced by stan-\ndard summarization tactics (e.g., first- or last-N sen-\ntences). We therefore conclude that subjectivity ex-\ntraction produces effective summaries of document\nsentiment.\n4.1 Basic subjectivity extraction\nAs noted in Section 3, both Naive Bayes and SVMs\ncan be trained on our subjectivity dataset and then\nused as a basic subjectivity detector. The former has\nsomewhat better average ten-fold cross-validation\nperformance on the subjectivity dataset (92% vs.\n90%), and so for space reasons, our initial discus-\nsions will focus on the results attained via NB sub-\njectivity detection.\n8Parameter training is driven by optimizing the performance\nof the downstream polarity classifier rather than the detector\nitself because the subjectivity dataset’s sentences come from\ndifferent reviews, and so are never proximal.\n.\n.\n.\n.\n.\n.\nsubsub\nNB NB\ns1\ns2\ns3\ns4\ns_n\n\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\u0001\nconstruct\ngraph\ncompute\nmin. cut\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0000\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\u0001\nextract\ncreate\ns1\ns4\nm−sentence extract\n(m<=n)\n\u0000\u0000\u0000\n\u0000\u0000\u0000\n\u0001\u0001\u0001\n\u0001\u0001\u0001\nn−sentence review\nv2\nv3\nv1\ns t\nv n\nv1\nv2\ns\nv3\nedge crossing the cut\nv n\nt\nproximity link\nindividual subjectivity−probability linkPr\n1−Pr   (s1)Pr   (s1)\n\u0000\u0000\u0000\u0000\u0000\u0001\u0001\u0001\u0001\u0001\nFigure 3: Graph-cut-based creation of subjective extracts.\nEmploying Naive Bayes as a subjectivity detec-\ntor (ExtractNB) in conjunction with a Naive Bayes\ndocument-level polarity classifier achieves 86.4%\naccuracy.9 This is a clear improvement over the\n82.8% that results when no extraction is applied\n(Full review); indeed, the difference is highly sta-\ntistically significant (p < 0.01, paired t-test). With\nSVMs as the polarity classifier instead, the Full re-\nview performance rises to 87.15%, but comparison\nvia the paired t-test reveals that this is statistically\nindistinguishable from the 86.4% that is achieved by\nrunning the SVM polarity classifier on ExtractNB\ninput. (More improvements to extraction perfor-\nmance are reported later in this section.)\nThese findings indicate10 that the extracts pre-\nserve (and, in the NB polarity-classifier case, appar-\nently clarify) the sentiment information in the orig-\ninating documents, and thus are good summaries\nfrom the polarity-classification point of view. Fur-\nther support comes from a “flipping” experiment:\nif we give as input to the default polarity classifier\nan extract consisting of the sentences labeled ob-\njective, accuracy drops dramatically to 71% for NB\nand 67% for SVMs. This confirms our hypothesis\nthat sentences discarded by the subjectivity extrac-\ntion process are indeed much less indicative of sen-\ntiment polarity.\nMoreover, the subjectivity extracts are much\nmore compact than the original documents (an im-\nportant feature for a summary to have): they contain\non average only about 60% of the source reviews’\nwords. (This word preservation rate is plotted along\nthe x-axis in the graphs in Figure 5.) This prompts\nus to study how much reduction of the original doc-\numents subjectivity detectors can perform and still\n9This result and others are depicted in Figure 5; for now,\nconsider only the y-axis in those plots.\n10Recall that direct evidence is not available because the po-\nlarity dataset’s sentences lack subjectivity labels.\naccurately represent the texts’ sentiment informa-\ntion.\nWe can create subjectivity extracts of varying\nlengths by taking just the N most subjective sen-\ntences11 from the originating review. As one\nbaseline to compare against, we take the canoni-\ncal summarization standard of extracting the first\nN sentences — in general settings, authors of-\nten begin documents with an overview. We also\nconsider the last N sentences: in many docu-\nments, concluding material may be a good sum-\nmary, and www.rottentomatoes.com tends to se-\nlect “snippets” from the end of movie reviews\n(Beineke et al., 2004). Finally, as a sanity check,\nwe include results from the N least subjective sen-\ntences according to Naive Bayes.\nFigure 4 shows the polarity classifier results as\nN ranges between 1 and 40. Our first observation\nis that the NB detector provides very good “bang\nfor the buck”: with subjectivity extracts containing\nas few as 15 sentences, accuracy is quite close to\nwhat one gets if the entire review is used. In fact,\nfor the NB polarity classifier, just using the 5 most\nsubjective sentences is almost as informative as the\nFull review while containing on average only about\n22% of the source reviews’ words.\nAlso, it so happens that at N = 30, performance\nis actually slightly better than (but statistically in-\ndistinguishable from) Full review even when the\nSVM default polarity classifier is used (87.2% vs.\n87.15%).12 This suggests potentially effective ex-\ntraction alternatives other than using a fixed proba-\n11These are the N sentences assigned the highest probability\nby the basic NB detector, regardless of whether their probabil-\nities exceed 50% and so would actually be classified as subjec-\ntive by Naive Bayes. For reviews with fewer than N sentences,\nthe entire review will be returned.\n12Note that roughly half of the documents in the polarity\ndataset contain more than 30 sentences (average=32.3, standard\ndeviation 15).\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 1  5  10  15  20  25  30  35  40\nAv\ner\nag\ne \nac\ncu\nra\ncy\nN\nAccuracy for N-sentence abstracts (def =  NB)\nN most subjective sentences\nlast N sentences\nfirst N sentences\nN least subjective sentences\nFull review\n 55\n 60\n 65\n 70\n 75\n 80\n 85\n 90\n 1  5  10  15  20  25  30  35  40\nAv\ner\nag\ne \nac\ncu\nra\ncy\nN\nAccuracy for N-sentence abstracts (def = SVM)\nN most subjective sentences\nlast N sentences\nfirst N sentences\nN least subjective sentences\nFull review\nFigure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.\n 83\n 83.5\n 84\n 84.5\n 85\n 85.5\n 86\n 86.5\n 87\n 0.6  0.7  0.8  0.9  1  1.1\nAv\ner\nag\ne \nac\ncu\nra\ncy\n% of words extracted\nAccuracy for subjective abstracts (def = NB)\ndifference in accuracy \nExtractSVM+Prox\nExtractNB+Prox\nExtractNB\nExtractSVM\nnot statistically significant\nFull Review\nindicates statistically significant\nimprovement in accuracy\n 83\n 83.5\n 84\n 84.5\n 85\n 85.5\n 86\n 86.5\n 87\n 0.6  0.7  0.8  0.9  1  1.1\nAv\ner\nag\ne \nac\ncu\nra\ncy\n% of words extracted\nAccuracy for subjective abstracts (def = SVM)\ndifference in accuracy ExtractNB+Prox\nExtractSVM+Prox\nExtractSVM\nExtractNB\nnot statistically significant\nFull Review\nimprovement in accuracy\nindicates statistically significant\nFigure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.\nAlso indicated are results for some statistical significance tests.\nbility threshold (which resulted in the lower accu-\nracy of 86.4% reported above).\nFurthermore, we see in Figure 4 that the N most-\nsubjective-sentences method generally outperforms\nthe other baseline summarization methods (which\nperhaps suggests that sentiment summarization can-\nnot be treated the same as topic-based summariza-\ntion, although this conjecture would need to be veri-\nfied on other domains and data). It’s also interesting\nto observe how much better the last N sentences are\nthan the first N sentences; this may reflect a (hardly\nsurprising) tendency for movie-review authors to\nplace plot descriptions at the beginning rather than\nthe end of the text and conclude with overtly opin-\nionated statements.\n4.2 Incorporating context information\nThe previous section demonstrated the value of\nsubjectivity detection. We now examine whether\ncontext information, particularly regarding sentence\nproximity, can further improve subjectivity extrac-\ntion. As discussed in Section 2.2 and 3, con-\ntextual constraints are easily incorporated via the\nminimum-cut formalism but are not natural inputs\nfor standard Naive Bayes and SVMs.\nFigure 5 shows the effect of adding in\nproximity information. ExtractNB+Prox and\nExtractSVM+Prox are the graph-based subjectivity\ndetectors using Naive Bayes and SVMs, respec-\ntively, for the individual scores; we depict the\nbest performance achieved by a single setting of\nthe three proximity-related edge-weight parameters\nover all ten data folds13 (parameter selection was\nnot a focus of the current work). The two compar-\nisons we are most interested in are ExtractNB+Prox\nversus ExtractNB and ExtractSVM+Prox versus\n13Parameters are chosen from T ∈ {1, 2, 3}, f(d) ∈\n{1, e1−d, 1/d2}, and c ∈ [0, 1] at intervals of 0.1.\nExtractSVM.\nWe see that the context-aware graph-based sub-\njectivity detectors tend to create extracts that are\nmore informative (statistically significant so (paired\nt-test) for SVM subjectivity detectors only), al-\nthough these extracts are longer than their context-\nblind counterparts. We note that the performance\nenhancements cannot be attributed entirely to the\nmere inclusion of more sentences regardless of\nwhether they are subjective or not — one counter-\nargument is that Full review yielded substantially\nworse results for the NB default polarity classifier—\nand at any rate, the graph-derived extracts are still\nsubstantially more concise than the full texts.\nNow, while incorporating a bias for assigning\nnearby sentences to the same category into NB and\nSVM subjectivity detectors seems to require some\nnon-obvious feature engineering, we also wish\nto investigate whether our graph-based paradigm\nmakes better use of contextual constraints that can\nbe (more or less) easily encoded into the input of\nstandard classifiers. For illustrative purposes, we\nconsider paragraph-boundary information, looking\nonly at SVM subjectivity detection for simplicity’s\nsake.\nIt seems intuitively plausible that paragraph\nboundaries (an approximation to discourse bound-\naries) loosen coherence constraints between nearby\nsentences. To capture this notion for minimum-cut-\nbased classification, we can simply reduce the as-\nsociation scores for all pairs of sentences that oc-\ncur in different paragraphs by multiplying them by\na cross-paragraph-boundary weight w ∈ [0, 1]. For\nstandard classifiers, we can employ the trick of hav-\ning the detector treat paragraphs, rather than sen-\ntences, as the basic unit to be labeled. This en-\nables the standard classifier to utilize coherence be-\ntween sentences in the same paragraph; on the other\nhand, it also (probably unavoidably) poses a hard\nconstraint that all of a paragraph’s sentences get the\nsame label, which increases noise sensitivity.14 Our\nexperiments reveal the graph-cut formulation to be\nthe better approach: for both default polarity clas-\nsifiers (NB and SVM), some choice of parameters\n(including w) for ExtractSVM+Prox yields statisti-\ncally significant improvement over its paragraph-\nunit non-graph counterpart (NB: 86.4% vs. 85.2%;\nSVM: 86.15% vs. 85.45%).\n5 Conclusions\nWe examined the relation between subjectivity de-\ntection and polarity classification, showing that sub-\n14For example, in the data we used, boundaries may have\nbeen missed due to malformed html.\njectivity detection can compress reviews into much\nshorter extracts that still retain polarity information\nat a level comparable to that of the full review. In\nfact, for the Naive Bayes polarity classifier, the sub-\njectivity extracts are shown to be more effective in-\nput than the originating document, which suggests\nthat they are not only shorter, but also “cleaner” rep-\nresentations of the intended polarity.\nWe have also shown that employing the\nminimum-cut framework results in the develop-\nment of efficient algorithms for sentiment analy-\nsis. Utilizing contextual information via this frame-\nwork can lead to statistically significant improve-\nment in polarity-classification accuracy. Directions\nfor future research include developing parameter-\nselection techniques, incorporating other sources of\ncontextual cues besides sentence proximity, and in-\nvestigating other means for modeling such informa-\ntion.\nAcknowledgments\nWe thank Eric Breck, Claire Cardie, Rich Caruana,\nYejin Choi, Shimon Edelman, Thorsten Joachims,\nJon Kleinberg, Oren Kurland, Art Munson, Vincent\nNg, Fernando Pereira, Ves Stoyanov, Ramin Zabih,\nand the anonymous reviewers for helpful comments.\nThis paper is based upon work supported in part\nby the National Science Foundation under grants\nITR/IM IIS-0081334 and IIS-0329064, a Cornell\nGraduate Fellowship in Cognitive Studies, and by\nan Alfred P. Sloan Research Fellowship. Any opin-\nions, findings, and conclusions or recommendations\nexpressed above are those of the authors and do not\nnecessarily reflect the views of the National Science\nFoundation or Sloan Foundation.\nReferences\nAgrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-\nnan Srikant, and Yirong Xu. 2003. Mining news-\ngroups using networks arising from social behav-\nior. In WWW, pages 529–535.\nAhuja, Ravindra, Thomas L. Magnanti, and\nJames B. Orlin. 1993. Network Flows: Theory,\nAlgorithms, and Applications. Prentice Hall.\nBeineke, Philip, Trevor Hastie, Christopher Man-\nning, and Shivakumar Vaithyanathan. 2004.\nExploring sentiment summarization. In AAAI\nSpring Symposium on Exploring Attitude and Af-\nfect in Text: Theories and Applications (AAAI\ntech report SS-04-07).\nBlum, Avrim and Shuchi Chawla. 2001. Learning\nfrom labeled and unlabeled data using graph min-\ncuts. In Intl. Conf. on Machine Learning (ICML),\npages 19–26.\nBoykov, Yuri, Olga Veksler, and Ramin Zabih.\n1999. Fast approximate energy minimization via\ngraph cuts. In Intl. Conf. on Computer Vision\n(ICCV), pages 377–384. Journal version in IEEE\nTrans. Pattern Analysis and Machine Intelligence\n(PAMI) 23(11):1222–1239, 2001.\nCardie, Claire, Janyce Wiebe, Theresa Wilson, and\nDiane Litman. 2003. Combining low-level and\nsummary representations of opinions for multi-\nperspective question answering. In AAAI Spring\nSymposium on New Directions in Question An-\nswering, pages 20–27.\nCormen, Thomas H., Charles E. Leiserson, and\nRonald L. Rivest. 1990. Introduction to Algo-\nrithms. MIT Press.\nDas, Sanjiv and Mike Chen. 2001. Yahoo! for\nAmazon: Extracting market sentiment from stock\nmessage boards. In Asia Pacific Finance Associ-\nation Annual Conf. (APFA).\nDave, Kushal, Steve Lawrence, and David M. Pen-\nnock. 2003. Mining the peanut gallery: Opinion\nextraction and semantic classification of product\nreviews. In WWW, pages 519–528.\nDini, Luca and Giampaolo Mazzini. 2002. Opin-\nion classification through information extraction.\nIn Intl. Conf. on Data Mining Methods and\nDatabases for Engineering, Finance and Other\nFields, pages 299–310.\nDurbin, Stephen D., J. Neal Richter, and Doug\nWarner. 2003. A system for affective rating of\ntexts. In KDD Wksp. on Operational Text Classi-\nfication Systems (OTC-3).\nHatzivassiloglou, Vasileios and Kathleen Mc-\nKeown. 1997. Predicting the semantic orienta-\ntion of adjectives. In 35th ACL/8th EACL, pages\n174–181.\nJoachims, Thorsten. 2003. Transductive learning\nvia spectral graph partitioning. In Intl. Conf. on\nMachine Learning (ICML).\nLiu, Hugo, Henry Lieberman, and Ted Selker.\n2003. A model of textual affect sensing using\nreal-world knowledge. In Intelligent User Inter-\nfaces (IUI), pages 125–132.\nMontes-y-Go´mez, Manuel, Aurelio Lo´pez-Lo´pez,\nand Alexander Gelbukh. 1999. Text mining as a\nsocial thermometer. In IJCAI Wksp. on Text Min-\ning, pages 103–107.\nMorinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi,\nand Toshikazu Fukushima. 2002. Mining prod-\nuct reputations on the web. In KDD, pages 341–\n349. Industry track.\nPang, Bo, Lillian Lee, and Shivakumar\nVaithyanathan. 2002. Thumbs up? Senti-\nment classification using machine learning\ntechniques. In EMNLP, pages 79–86.\nQu, Yan, James Shanahan, and Janyce Wiebe, edi-\ntors. 2004. AAAI Spring Symposium on Explor-\ning Attitude and Affect in Text: Theories and Ap-\nplications. AAAI technical report SS-04-07.\nRiloff, Ellen and Janyce Wiebe. 2003. Learning\nextraction patterns for subjective expressions. In\nEMNLP.\nRiloff, Ellen, Janyce Wiebe, and Theresa Wilson.\n2003. Learning subjective nouns using extraction\npattern bootstrapping. In Conf. on Natural Lan-\nguage Learning (CoNLL), pages 25–32.\nSubasic, Pero and Alison Huettner. 2001. Af-\nfect analysis of text using fuzzy semantic typing.\nIEEE Trans. Fuzzy Systems, 9(4):483–496.\nTong, Richard M. 2001. An operational system for\ndetecting and tracking opinions in on-line discus-\nsion. SIGIR Wksp. on Operational Text Classifi-\ncation.\nTurney, Peter. 2002. Thumbs up or thumbs down?\nSemantic orientation applied to unsupervised\nclassification of reviews. In ACL, pages 417–424.\nWiebe, Janyce M. 1994. Tracking point of view in\nnarrative. Computational Linguistics, 20(2):233–\n287.\nYi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu,\nand Wayne Niblack. 2003. Sentiment analyzer:\nExtracting sentiments about a given topic using\nnatural language processing techniques. In IEEE\nIntl. Conf. on Data Mining (ICDM).\nYu, Hong and Vasileios Hatzivassiloglou. 2003.\nTowards answering opinion questions: Separat-\ning facts from opinions and identifying the polar-\nity of opinion sentences. In EMNLP.\n",
      "id": 944740,
      "identifiers": [
        {
          "identifier": "oai:citeseerx.psu:10.1.1.9.9144",
          "type": "OAI_ID"
        },
        {
          "identifier": "2420044",
          "type": "CORE_ID"
        },
        {
          "identifier": "cs/0409058",
          "type": "ARXIV_ID"
        },
        {
          "identifier": "oai:arxiv.org:cs/0409058",
          "type": "OAI_ID"
        },
        {
          "identifier": "24682462",
          "type": "CORE_ID"
        }
      ],
      "title": "A Sentimental Education: Sentiment Analysis Using Subjectivity\n  Summarization Based on Minimum Cuts",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:citeseerx.psu:10.1.1.9.9144",
        "oai:arxiv.org:cs/0409058"
      ],
      "publishedDate": "2004-01-01T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.9144",
        "http://arxiv.org/abs/cs/0409058"
      ],
      "updatedDate": "2020-12-24T15:49:12",
      "yearPublished": 2004,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "http://arxiv.org/abs/cs/0409058"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/944740"
        }
      ]
    },
    {
      "acceptedDate": "",
      "arxivId": null,
      "authors": [
        {
          "name": "Orroquia Barea, Aroa"
        },
        {
          "name": "Salles. Bernal, Soluna"
        }
      ],
      "citationCount": 0,
      "contributors": [],
      "outputs": [
        "https://api.core.ac.uk/v3/outputs/214843773",
        "https://api.core.ac.uk/v3/outputs/323335734"
      ],
      "createdDate": "2019-07-09T14:24:16",
      "dataProviders": [
        {
          "id": 2072,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/2072",
          "logo": "https://api.core.ac.uk/data-providers/2072/logo"
        },
        {
          "id": 11082,
          "name": "",
          "url": "https://api.core.ac.uk/v3/data-providers/11082",
          "logo": "https://api.core.ac.uk/data-providers/11082/logo"
        }
      ],
      "depositedDate": "",
      "abstract": "The use of linguistic resources beyond the scope of language studies, i.e. commercial purposes, has become commonplace since the availability of massive amounts of data and the development of tools to process them. An interesting focus on these materials is provided by Sentiment Analysis (SA) tools and methodologies, which attempt to identify the polarity or semantic orientation of a text, i.e., its positive, negative, or neutral value. Two main approaches have been made in this sense, one based on complex machine-learning algorithms and the other relying principally on lexical knowledge (Taboada et al., 2011). Lingmotif is an example of lexicon-based SA tool offering polarity classification and other related metrics, together with an analysis of the target segments evaluated (Moreno-Ortiz, 2017). Sentiment has been shown to be domain-specific to a large extent (Choi & Cardie, 2008) and it is therefore necessary to study and describe how sentiment is expressed not only in general language, but also in specialized domains. The availability of annotated, domain-specific corpora could greatly enhance the capacity of SA tools.\r\nFurthermore, the demand for a more fine-grained approach requires the identification of specific domain terminology, allowing the recognition of target terms associated with the polarity (Liu, 2012). Most available SA corpora are annotated at the document level, which allows systems to be trained to return the overall orientation of the text. However, more detail is necessary: what aspects exactly are being praised or criticized? This type SA is known as Aspect-Based Sentiment Analysis (ABSA), and attempts to extract more fined-grained knowledge. ABSA has attracted the attention of recent SemEval shared-tasks (Pontiki et al., 2015)",
      "documentType": "research",
      "doi": null,
      "downloadUrl": "https://core.ac.uk/download/214843773.pdf",
      "fieldOfStudy": null,
      "fullText": "SentiTur: Building Linguistic Resources for Aspect-Based Sentiment Analysis in the Tourism Sector  Soluna Salles and Aroa Orrequia-Barea   The use of linguistic resources beyond the scope of language studies, i.e. commercial purposes, has become commonplace since the availability of massive amounts of data and the development of tools to process them.  An interesting focus on these materials is provided by Sentiment Analysis (SA) tools and methodologies, which attempt to identify the polarity or semantic orientation of a text, i.e., its positive, negative, or neutral value. Two main approaches have been made in this sense, one based on complex machine-learning algorithms and the other relying principally on lexical knowledge (Taboada et al., 2011). Lingmotif is an example of lexicon-based SA tool offering polarity classification and other related metrics, together with an analysis of the target segments evaluated (Moreno-Ortiz, 2017). Sentiment has been shown to be domain-specific to a large extent (Choi & Cardie, 2008) and it is therefore necessary to study and describe how sentiment is expressed not only in general language, but also in specialized domains. The availability of annotated, domain-specific corpora could greatly enhance the capacity of SA tools.   Furthermore, the demand for a more fine-grained approach requires the identification of specific domain terminology, allowing the recognition of target terms associated with the polarity (Liu, 2012). Most available SA corpora are annotated at the document level, which allows systems to be trained to return the overall orientation of the text. However, more detail is necessary: what aspects exactly are being praised or criticized? This type SA is known as Aspect-Based Sentiment Analysis (ABSA), and attempts to extract more fined-grained knowledge. ABSA has attracted the attention of recent SemEval shared-tasks  (Pontiki et al., 2015).  We propose the creation of the SentiTur corpus, a bilingual (Spanish-English), aspect-annotated corpus of user reviews covering three sectors of the tourism industry: accommodation, catering, and car rental. Reviews obtained from online platforms (Tripavisor and Booking, among others) are being annotated according to an annotation schema by means of the collaborative annotation tool Brat (Stenetorp et al., 2012). Five annotators work initially in a preliminary dataset, so as to validate the schema and the methodology proposed. Inter annotator agreement (IAA) is computed measuring the pairwise agreement among annotators by the Kappa coefficient (K) (Siegel, 1988). These results are used to revise and adjust the annotation schema before it is employed to annotated the general corpus. Finally, the corpus is offered in standard XML format suitable as input of Sentiment Analysis tools.  Key words: SentiTur, Aspect-based Sentiment Analysis, Sentiment Analysis, tourism.  References  Choi, Y., & Cardie, C. (2008). Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis, (October), 793–801. Liu, B. (2012). Sentiment Aanalysis and Opinion Mining: Synthesis Lectures on Human Language Technologies. Morgan & Claypool Publishers. Moreno-Ortiz, A. (2017). Lingmotif: Sentiment Analysis for the Digital Humanities. In Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017 (pp. 73–76). Pontiki, M., Galanis, D., & Papageorgiou, H. (2015). SemEval-2015 Task 12: Aspect-Based Sentiment Analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado, June 4-5, 2015. Siegel, S. (1988). Nonparametric Statistics for the Behavioral Science. McGraw-Hill. Stenetorp, P., Pyysalo, S., Topi, G., Ohta, T., Ananiadou, S., & Tsujii, J. (2012). BRAT: a Web-based Tool for NLP-Assisted Text Annotation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, Avignon, France, April 23-27 2012. (pp. 102–107). Taboada, M., Brooke, J., & Voll, K. (2011). Lexicon-Based Methods for Sentiment Analysis, (September 2010).   ",
      "id": 7649256,
      "identifiers": [
        {
          "identifier": "oai:riuma.uma.es:10630/15645",
          "type": "OAI_ID"
        },
        {
          "identifier": "214843773",
          "type": "CORE_ID"
        },
        {
          "identifier": "323335734",
          "type": "CORE_ID"
        }
      ],
      "title": "SentiTur: Building Linguistic Resources for Aspect-Based Sentiment Analysis in the Tourism Sector",
      "language": {
        "code": "en",
        "name": "English"
      },
      "magId": null,
      "oaiIds": [
        "oai:riuma.uma.es:10630/15645"
      ],
      "publishedDate": "2018-04-30T00:00:00",
      "publisher": "",
      "pubmedId": null,
      "references": [],
      "sourceFulltextUrls": [
        "https://riuma.uma.es/xmlui/bitstream/10630/15645/1/Salles_Aesla_2018.pdf"
      ],
      "updatedDate": "2022-03-20T00:45:59",
      "yearPublished": 2018,
      "journals": [],
      "links": [
        {
          "type": "download",
          "url": "https://core.ac.uk/download/214843773.pdf"
        },
        {
          "type": "reader",
          "url": "https://core.ac.uk/reader/214843773"
        },
        {
          "type": "thumbnail_m",
          "url": "https://core.ac.uk/image/214843773/large"
        },
        {
          "type": "thumbnail_l",
          "url": "https://core.ac.uk/image/214843773/large"
        },
        {
          "type": "display",
          "url": "https://core.ac.uk/works/7649256"
        }
      ]
    }
  ],
  "searchId": "37face988bd738c2e5b9e5485439a354"
}